{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html>\n",
    "\n",
    "<head>\n",
    "<meta http-equiv=Content-Type content=\"text/html; charset=windows-1252\">\n",
    "<meta name=Generator content=\"Microsoft Word 15 (filtered)\">\n",
    "<style>\n",
    "<!--\n",
    " /* Font Definitions */\n",
    " @font-face\n",
    "\t{font-family:\"Cambria Math\";\n",
    "\tpanose-1:2 4 5 3 5 4 6 3 2 4;}\n",
    " /* Style Definitions */\n",
    " p.MsoNormal, li.MsoNormal, div.MsoNormal\n",
    "\t{margin:0in;\n",
    "\tmargin-bottom:.0001pt;\n",
    "\tline-height:115%;\n",
    "\tfont-size:11.0pt;\n",
    "\tfont-family:\"Arial\",sans-serif;\n",
    "\tcolor:black;}\n",
    "h1\n",
    "\t{margin-top:20.0pt;\n",
    "\tmargin-right:0in;\n",
    "\tmargin-bottom:6.0pt;\n",
    "\tmargin-left:0in;\n",
    "\tline-height:115%;\n",
    "\tpage-break-after:avoid;\n",
    "\tfont-size:20.0pt;\n",
    "\tfont-family:\"Arial\",sans-serif;\n",
    "\tcolor:black;\n",
    "\tfont-weight:normal;}\n",
    "h2\n",
    "\t{margin-top:.25in;\n",
    "\tmargin-right:0in;\n",
    "\tmargin-bottom:6.0pt;\n",
    "\tmargin-left:0in;\n",
    "\tline-height:115%;\n",
    "\tpage-break-after:avoid;\n",
    "\tfont-size:16.0pt;\n",
    "\tfont-family:\"Arial\",sans-serif;\n",
    "\tcolor:black;\n",
    "\tfont-weight:normal;}\n",
    "h3\n",
    "\t{margin-top:16.0pt;\n",
    "\tmargin-right:0in;\n",
    "\tmargin-bottom:4.0pt;\n",
    "\tmargin-left:0in;\n",
    "\tline-height:115%;\n",
    "\tpage-break-after:avoid;\n",
    "\tfont-size:14.0pt;\n",
    "\tfont-family:\"Arial\",sans-serif;\n",
    "\tcolor:#434343;\n",
    "\tfont-weight:normal;}\n",
    "h4\n",
    "\t{margin-top:14.0pt;\n",
    "\tmargin-right:0in;\n",
    "\tmargin-bottom:4.0pt;\n",
    "\tmargin-left:0in;\n",
    "\tline-height:115%;\n",
    "\tpage-break-after:avoid;\n",
    "\tfont-size:12.0pt;\n",
    "\tfont-family:\"Arial\",sans-serif;\n",
    "\tcolor:#666666;\n",
    "\tfont-weight:normal;}\n",
    "h5\n",
    "\t{margin-top:12.0pt;\n",
    "\tmargin-right:0in;\n",
    "\tmargin-bottom:4.0pt;\n",
    "\tmargin-left:0in;\n",
    "\tline-height:115%;\n",
    "\tpage-break-after:avoid;\n",
    "\tfont-size:11.0pt;\n",
    "\tfont-family:\"Arial\",sans-serif;\n",
    "\tcolor:#666666;\n",
    "\tfont-weight:normal;}\n",
    "h6\n",
    "\t{margin-top:12.0pt;\n",
    "\tmargin-right:0in;\n",
    "\tmargin-bottom:4.0pt;\n",
    "\tmargin-left:0in;\n",
    "\tline-height:115%;\n",
    "\tpage-break-after:avoid;\n",
    "\tfont-size:11.0pt;\n",
    "\tfont-family:\"Arial\",sans-serif;\n",
    "\tcolor:#666666;\n",
    "\tfont-weight:normal;\n",
    "\tfont-style:italic;}\n",
    "p.MsoHeader, li.MsoHeader, div.MsoHeader\n",
    "\t{mso-style-link:\"Header Char\";\n",
    "\tmargin:0in;\n",
    "\tmargin-bottom:.0001pt;\n",
    "\tfont-size:11.0pt;\n",
    "\tfont-family:\"Arial\",sans-serif;\n",
    "\tcolor:black;}\n",
    "p.MsoFooter, li.MsoFooter, div.MsoFooter\n",
    "\t{mso-style-link:\"Footer Char\";\n",
    "\tmargin:0in;\n",
    "\tmargin-bottom:.0001pt;\n",
    "\tfont-size:11.0pt;\n",
    "\tfont-family:\"Arial\",sans-serif;\n",
    "\tcolor:black;}\n",
    "p.MsoTitle, li.MsoTitle, div.MsoTitle\n",
    "\t{margin-top:0in;\n",
    "\tmargin-right:0in;\n",
    "\tmargin-bottom:3.0pt;\n",
    "\tmargin-left:0in;\n",
    "\tline-height:115%;\n",
    "\tpage-break-after:avoid;\n",
    "\tfont-size:26.0pt;\n",
    "\tfont-family:\"Arial\",sans-serif;\n",
    "\tcolor:black;}\n",
    "p.MsoSubtitle, li.MsoSubtitle, div.MsoSubtitle\n",
    "\t{margin-top:0in;\n",
    "\tmargin-right:0in;\n",
    "\tmargin-bottom:16.0pt;\n",
    "\tmargin-left:0in;\n",
    "\tline-height:115%;\n",
    "\tpage-break-after:avoid;\n",
    "\tfont-size:15.0pt;\n",
    "\tfont-family:\"Arial\",sans-serif;\n",
    "\tcolor:#666666;}\n",
    "a:link, span.MsoHyperlink\n",
    "\t{color:blue;\n",
    "\ttext-decoration:underline;}\n",
    "a:visited, span.MsoHyperlinkFollowed\n",
    "\t{color:purple;\n",
    "\ttext-decoration:underline;}\n",
    "p.MsoListParagraph, li.MsoListParagraph, div.MsoListParagraph\n",
    "\t{margin-top:0in;\n",
    "\tmargin-right:0in;\n",
    "\tmargin-bottom:0in;\n",
    "\tmargin-left:.5in;\n",
    "\tmargin-bottom:.0001pt;\n",
    "\tline-height:115%;\n",
    "\tfont-size:11.0pt;\n",
    "\tfont-family:\"Arial\",sans-serif;\n",
    "\tcolor:black;}\n",
    "p.MsoListParagraphCxSpFirst, li.MsoListParagraphCxSpFirst, div.MsoListParagraphCxSpFirst\n",
    "\t{margin-top:0in;\n",
    "\tmargin-right:0in;\n",
    "\tmargin-bottom:0in;\n",
    "\tmargin-left:.5in;\n",
    "\tmargin-bottom:.0001pt;\n",
    "\tline-height:115%;\n",
    "\tfont-size:11.0pt;\n",
    "\tfont-family:\"Arial\",sans-serif;\n",
    "\tcolor:black;}\n",
    "p.MsoListParagraphCxSpMiddle, li.MsoListParagraphCxSpMiddle, div.MsoListParagraphCxSpMiddle\n",
    "\t{margin-top:0in;\n",
    "\tmargin-right:0in;\n",
    "\tmargin-bottom:0in;\n",
    "\tmargin-left:.5in;\n",
    "\tmargin-bottom:.0001pt;\n",
    "\tline-height:115%;\n",
    "\tfont-size:11.0pt;\n",
    "\tfont-family:\"Arial\",sans-serif;\n",
    "\tcolor:black;}\n",
    "p.MsoListParagraphCxSpLast, li.MsoListParagraphCxSpLast, div.MsoListParagraphCxSpLast\n",
    "\t{margin-top:0in;\n",
    "\tmargin-right:0in;\n",
    "\tmargin-bottom:0in;\n",
    "\tmargin-left:.5in;\n",
    "\tmargin-bottom:.0001pt;\n",
    "\tline-height:115%;\n",
    "\tfont-size:11.0pt;\n",
    "\tfont-family:\"Arial\",sans-serif;\n",
    "\tcolor:black;}\n",
    "span.HeaderChar\n",
    "\t{mso-style-name:\"Header Char\";\n",
    "\tmso-style-link:Header;}\n",
    "span.FooterChar\n",
    "\t{mso-style-name:\"Footer Char\";\n",
    "\tmso-style-link:Footer;}\n",
    ".MsoChpDefault\n",
    "\t{font-family:\"Arial\",sans-serif;\n",
    "\tcolor:black;}\n",
    ".MsoPapDefault\n",
    "\t{line-height:115%;}\n",
    " /* Page Definitions */\n",
    " @page WordSection1\n",
    "\t{size:8.5in 11.0in;\n",
    "\tmargin:1.0in 1.0in 1.0in 1.0in;}\n",
    "div.WordSection1\n",
    "\t{page:WordSection1;}\n",
    " /* List Definitions */\n",
    " ol\n",
    "\t{margin-bottom:0in;}\n",
    "ul\n",
    "\t{margin-bottom:0in;}\n",
    "-->\n",
    "</style>\n",
    "\n",
    "</head>\n",
    "\n",
    "<body lang=EN-US link=blue vlink=purple>\n",
    "\n",
    "<div class=WordSection1>\n",
    "\n",
    "<p class=MsoNormal align=center style='text-align:center'><span lang=EN\n",
    "style='font-size:24.0pt;line-height:115%;font-family:\"Times New Roman\",serif'>Analyzing\n",
    "Parameter Performance on Deep Belief Networks</span></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>&nbsp;</span></p>\n",
    "\n",
    "<p class=MsoNormal><b><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>Andy Thai                                                                                          </span></b><span\n",
    "lang=EN style='font-size:12.0pt;line-height:115%;font-family:\"Times New Roman\",serif'>ANDYTHAI@UCSD.EDU</span></p>\n",
    "\n",
    "<p class=MsoNormal><i><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>Department of Cognitive Science,\n",
    "Department of Mathematics</span></i></p>\n",
    "\n",
    "<p class=MsoNormal><i><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>University of California, San Diego</span></i></p>\n",
    "\n",
    "<p class=MsoNormal><i><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>La Jolla, CA</span></i></p>\n",
    "\n",
    "<p class=MsoNormal><b><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>&nbsp;</span></b></p>\n",
    "\n",
    "<p class=MsoNormal><b><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>&nbsp;</span></b></p>\n",
    "\n",
    "<p class=MsoNormal align=center style='margin-bottom:6.0pt;text-align:center'><b><span\n",
    "lang=EN style='font-size:16.0pt;line-height:115%;font-family:\"Times New Roman\",serif'>Abstract</span></b></p>\n",
    "\n",
    "<p class=MsoNormal style='margin-top:0in;margin-right:.6in;margin-bottom:6.0pt;\n",
    "margin-left:.6in'><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>The rise of deep learning in recent years\n",
    "has led to adaptations of many different supervised learning models, each made\n",
    "to tackle subsets of problems. Deep belief networks are a unique type of model,\n",
    "made up of restricted Boltzmann machines (RBMs) stacked upon each other to\n",
    "enable hierarchical representations of data. For these networks to achieve peak\n",
    "performance on datasets, tuning parameters to be optimal is extremely key. This\n",
    "paper will explore parameter optimization within deep belief networks. Using\n",
    "MNIST, I look to find the best possible parameters for a deep belief network\n",
    "and compare its performance against other standard models used throughout\n",
    "supervised classification. </span></p>\n",
    "\n",
    "<p class=MsoNormal style='margin-top:0in;margin-right:.6in;margin-bottom:0in;\n",
    "margin-left:.6in;margin-bottom:.0001pt'><b><span lang=EN style='font-size:12.0pt;\n",
    "line-height:115%;font-family:\"Times New Roman\",serif'>Keywords:</span></b><span\n",
    "lang=EN style='font-size:12.0pt;line-height:115%;font-family:\"Times New Roman\",serif'>\n",
    "deep belief networks, SVM, MNIST, supervised learning, neural networks, deep\n",
    "learning</span></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>&nbsp;</span></p>\n",
    "\n",
    "<p class=MsoNormal style='margin-bottom:6.0pt'><b><span lang=EN\n",
    "style='font-size:16.0pt;line-height:115%;font-family:\"Times New Roman\",serif'>1.\n",
    "Introduction</span></b></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>Recent upgrades in hardware and computing\n",
    "have led to an explosion of innovation and development within the fields of\n",
    "machine learning. Faster, more efficient hardware components have allowed deep\n",
    "learning models, previously too slow to run on most computers, to be more\n",
    "easily adapted to everyday general use. </span></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>&nbsp;</span></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>Early models using backpropagation and\n",
    "gradient-based learning methods came across the vanishing gradient problem,\n",
    "where gradient updates to network weights could potentially get the weight\n",
    "stuck on a value so small to the point where weights could no longer be\n",
    "updated. This would lead to significant issues in training where affected\n",
    "weights remained constant for an entire training session.</span></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>&nbsp;</span></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>The introduction of deep belief networks\n",
    "(Hinton et al., 2006) offered a potential solution to this vanishing gradient\n",
    "problem. This would be done by pre-training each layer, originally set as a\n",
    "Restricted Boltzmann Machine, via unsupervised training to initialize the\n",
    "weights, then training them again via the supervised learning method. These\n",
    "deep belief networks cemented the foundation for future work on neural\n",
    "networks, and ultimately were a huge factor in the explosion of deep learning\n",
    "for the near future. </span></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>&nbsp;</span></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>However, to properly utilize these deep\n",
    "belief networks to their fullest extent, careful attention must be paid to\n",
    "their parameters, which dictate how the model behaves when training itself with\n",
    "training datasets. I take a formulaic approach to tuning the parameters on the\n",
    "MNIST dataset, iteratively testing each parameter on a model and gauging\n",
    "accuracy to achieve a set of parameters that output the best overall\n",
    "performance. Results can potentially lead to optimal parameters for training on\n",
    "sets similar to the MNIST handwritten digit set, such as the Stanford OCR. </span></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>&nbsp;</span></p>\n",
    "\n",
    "<p class=MsoNormal style='margin-bottom:6.0pt'><b><span lang=EN\n",
    "style='font-size:16.0pt;line-height:115%;font-family:\"Times New Roman\",serif'>2.\n",
    "Method</span></b></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>The deep belief networks library used here\n",
    "were found at albertbup’s deep-belief-network GitHub repository. The dataset\n",
    "these networks were trained on was taken from the MNIST handwritten digits. The\n",
    "dataset was loaded into a Jupyter Python file, where each handwritten digit\n",
    "data point is represented by a NumPy array holding 784 integers from 0-255\n",
    "representing a pixel hue (0 being white, and 255 being the darkest black). The\n",
    "dataset was normalized from 0 to 1 by dividing each pixel value by 256 to\n",
    "obtain floating point numbers. To cut down on training time, the dataset was\n",
    "cut down to 1000 data points made up of 10 classes. The first 100 data points\n",
    "for each class is taken and put into the final set used for this experiment.\n",
    "The set is then split into a training set and test set with an 80/20 split.  </span></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>&nbsp;</span></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>To tune the hyperparameters, a series of\n",
    "tests were run gauging performance on the default deep belief network settings\n",
    "with the one parameter changed. The parameters tested were the number of layers\n",
    "(1, 2, or 3), pre-training learning rates (RBM), learning rate, pre-training\n",
    "lengths, and backpropagation iterations. For each parameter, the best performing\n",
    "setting aws saved and noted for future continued benchmarks.</span></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>&nbsp;</span></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>The first test was run on networks with\n",
    "one hidden layer, with varying units: 100, 200, 300, 400, and 500 units. The next\n",
    "test was run on networks with two hidden layers, with a varying number of units\n",
    "in each layer. The first layer had either 100 or 500 units, while the second\n",
    "layer had either 200 or 500 units. The specific settings tested were: [100,\n",
    "200], [100, 500], [500, 200], [500, 500]. The last test for the number of\n",
    "layers was run with three hidden layers. The first layer had either 100 or 500\n",
    "units, the second layer had either 200 or 500 units, and the third layer had\n",
    "either 300 or 500 units. The specific settings tested were: [100, 200, 300],\n",
    "[500, 200, 300], [100, 500, 300], [100, 200, 500], [500, 500, 500], [500, 200,\n",
    "500], [100, 500, 500], and [500, 500, 300]. </span></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>&nbsp;</span></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>The next parameter tested was the\n",
    "pre-training (RBM) learning rates. This parameter determines the learning rate\n",
    "of the network in the pre-training stage. The learning rates tested were 0.01,\n",
    "0.05, 0.10, 0.50, and 1.00. </span></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>&nbsp;</span></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>Afterwards, the learning rate parameter\n",
    "was tested. This learning rate determines the learning rate of the network in\n",
    "the fine-tuning stage. Like the pre-training learning rate, a similar process\n",
    "will be run for the network’s learning rate. The learning rates tested will be\n",
    "0.01, 0.05, 0.10, 0.50, and 1.00. </span></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>&nbsp;</span></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>Next was the pre-training length, which\n",
    "determines the number of epochs each layer is run in the pre-training stage.\n",
    "The number of epochs tested was 1, 2, 5, 10, 20, 30, 40, 50, 100, and 200. </span></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>&nbsp;</span></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>After that, the fine-tuning / backprop\n",
    "lengths, which determine the number of epochs the network will be run in the\n",
    "fine-tuning stage, was tested. The specific lengths tested were 1, 5, 10, 20,\n",
    "50, 100, and 200. </span></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>&nbsp;</span></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>After running through all these\n",
    "parameters, the best performing settings for each parameter was chosen for the\n",
    "first benchmark tests. These tests consisted of the best performing parameters\n",
    "with either a one hidden layer, two hidden-layer, or three hidden-layer network\n",
    "setup. The best performing model out of all the first initial benchmark tests\n",
    "was taken. Those settings would then be run on either the sigmoidal or ReLu\n",
    "activation function, with varying dropout settings for each model. Eleven\n",
    "dropout settings will be run on the models: 0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6,\n",
    "0.7, 0.8, 0.9, and 1.0, for a total of 22 tests in the second benchmark. The\n",
    "test with the highest accuracy of these 22 tests was chosen as the final model.\n",
    "</span></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>&nbsp;</span></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>After choosing the best model, a series of\n",
    "alternate models was run on the MNIST set. The accuracy of these models was\n",
    "compared with the final model. The models chosen are taken from the SKLearn\n",
    "library: LinearSVC, DecisionTreeClassifier, AdaBoostClassifier,\n",
    "RandomForestClassifier, and four different SVC models. The linearSVC model used\n",
    "four different C-values. The SVC models were made up of the default parameters,\n",
    "using a 3<sup>rd</sup> degree polynomial with an rbf kernel. The C parameters\n",
    "for both the linearSVC and the SVC were 0.1, 1, 10, and 100. </span></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>&nbsp;</span></p>\n",
    "\n",
    "<b><span lang=EN style='font-size:16.0pt;line-height:115%;font-family:\"Times New Roman\",serif'><br\n",
    "clear=all style='page-break-before:always'>\n",
    "</span></b>\n",
    "\n",
    "<p class=MsoNormal><b><span lang=EN style='font-size:16.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>&nbsp;</span></b></p>\n",
    "\n",
    "<p class=MsoNormal><b><span lang=EN style='font-size:16.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>3. Experiment</span></b></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>&nbsp;</span></p>\n",
    "\n",
    "<p class=MsoNormal><b><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>One-layer</span></b></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>&nbsp;</span></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'><img width=366 height=262 id=\"Picture 2\"\n",
    "src=\"FINAL%20PROJECT_files/image023.gif\"></span></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>Running the tests on a one-layer network, it\n",
    "was found that the most accurate number of units are 200 and 300, which were\n",
    "tied for 91% classification accuracy. </span></p>\n",
    "\n",
    "<p class=MsoNormal><b><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>&nbsp;</span></b></p>\n",
    "\n",
    "<p class=MsoNormal><b><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>Two-layer</span></b></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'><img width=365 height=261 id=\"Picture 3\"\n",
    "src=\"FINAL%20PROJECT_files/image024.gif\"></span></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>Running the tests on a two-layer network, it\n",
    "was found that the most accurate setting is 100 units in the first layer, and\n",
    "500 units in the second layer: [100, 500], with a classification accuracy of\n",
    "92% accuracy. </span></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>&nbsp;</span></p>\n",
    "\n",
    "<b><span lang=EN style='font-size:12.0pt;line-height:115%;font-family:\"Times New Roman\",serif'><br\n",
    "clear=all style='page-break-before:always'>\n",
    "</span></b>\n",
    "\n",
    "<p class=MsoNormal><b><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>&nbsp;</span></b></p>\n",
    "\n",
    "<p class=MsoNormal><b><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>Three-layer</span></b></p>\n",
    "\n",
    "<p class=MsoNormal><b><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'><img width=418 height=335 id=\"Picture 4\"\n",
    "src=\"FINAL%20PROJECT_files/image025.gif\"></span></b></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>Running the tests on a three-layer\n",
    "network, it was found that the most accurate setting is 500 units in the first\n",
    "layer, 200 units in the second layer, and 300 units in the third layer: [500,\n",
    "200, 300], with a classification accuracy of 92.5%. </span></p>\n",
    "\n",
    "<p class=MsoNormal><b><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>&nbsp;</span></b></p>\n",
    "\n",
    "<p class=MsoNormal><b><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>Pre-training (RBM) Learning Rates</span></b></p>\n",
    "\n",
    "<p class=MsoNormal><b><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'><img width=413 height=306 id=\"Picture 5\"\n",
    "src=\"FINAL%20PROJECT_files/image026.gif\"></span></b></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>Running the tests on the pre-training\n",
    "learning rates, it was found that the highest accuracy was with\n",
    "learning_rate_rbm=0.05, with a 91% classification accuracy. </span></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>&nbsp;</span></p>\n",
    "\n",
    "<p class=MsoNormal><b><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>&nbsp;</span></b></p>\n",
    "\n",
    "<p class=MsoNormal><b><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>Learning Rate</span></b></p>\n",
    "\n",
    "<p class=MsoNormal><b><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'><img width=372 height=276 id=\"Picture 6\"\n",
    "src=\"FINAL%20PROJECT_files/image027.gif\"></span></b></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>Running the tests on the learning rates,\n",
    "it was found that the highest accuracy was with learning_rate=0.1, with a 91.5%\n",
    "classification accuracy. </span></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>&nbsp;</span></p>\n",
    "\n",
    "<p class=MsoNormal><b><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>Pre-training Lengths</span></b></p>\n",
    "\n",
    "<p class=MsoNormal><b><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'><img width=441 height=324 id=\"Picture 7\"\n",
    "src=\"FINAL%20PROJECT_files/image028.gif\"></span></b></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>Running the tests on the pre-training lengths,\n",
    "it was found that the highest accuracy was with n_epochs_rbm=40, with a 93%\n",
    "classification accuracy. </span></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>&nbsp;</span></p>\n",
    "\n",
    "<b><span lang=EN style='font-size:12.0pt;line-height:115%;font-family:\"Times New Roman\",serif'><br\n",
    "clear=all style='page-break-before:always'>\n",
    "</span></b>\n",
    "\n",
    "<p class=MsoNormal><b><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>&nbsp;</span></b></p>\n",
    "\n",
    "<p class=MsoNormal><b><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>Fine-tuning / Backprop Lengths</span></b></p>\n",
    "\n",
    "<p class=MsoNormal><b><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'><img width=390 height=287 id=\"Picture 8\"\n",
    "src=\"FINAL%20PROJECT_files/image029.gif\"></span></b></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>Running the tests on the backpropagation\n",
    "epochs, it was found that the highest accuracy was with n_iter_backprop=200,\n",
    "with a 91% classification accuracy. </span></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>&nbsp;</span></p>\n",
    "\n",
    "<p class=MsoNormal><b><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>First Benchmark</span></b></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>Given the results given in previous tests,\n",
    "these following models were benchmarked:</span></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>&nbsp;</span></p>\n",
    "\n",
    "<p class=MsoNormal><i><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>Model 1 (1-layer):</span></i></p>\n",
    "\n",
    "<p class=MsoNormal><i><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>deep_belief_net(hidden_layers_structure=[200],\n",
    "</span></i></p>\n",
    "\n",
    "<p class=MsoNormal><i><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>                      \n",
    "learning_rate_rbm=0.05,</span></i></p>\n",
    "\n",
    "<p class=MsoNormal><i><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>                       learning_rate=0.1,</span></i></p>\n",
    "\n",
    "<p class=MsoNormal><i><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>                       n_epochs_rbm=40,</span></i></p>\n",
    "\n",
    "<p class=MsoNormal><i><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>                      \n",
    "n_iter_backprop=200,</span></i></p>\n",
    "\n",
    "<p class=MsoNormal><i><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>                       batch_size=32,</span></i></p>\n",
    "\n",
    "<p class=MsoNormal><i><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>                      \n",
    "activation_function='relu',</span></i></p>\n",
    "\n",
    "<p class=MsoNormal><i><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>                       dropout_p=0.2)</span></i></p>\n",
    "\n",
    "<p class=MsoNormal><i><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>&nbsp;</span></i></p>\n",
    "\n",
    "<p class=MsoNormal><i><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>Model 2 (1-layer):</span></i></p>\n",
    "\n",
    "<p class=MsoNormal><i><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>deep_belief_net(hidden_layers_structure=[300],\n",
    "</span></i></p>\n",
    "\n",
    "<p class=MsoNormal><i><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>                      \n",
    "learning_rate_rbm=0.05,</span></i></p>\n",
    "\n",
    "<p class=MsoNormal><i><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>                       learning_rate=0.1,</span></i></p>\n",
    "\n",
    "<p class=MsoNormal><i><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>                       n_epochs_rbm=40,</span></i></p>\n",
    "\n",
    "<p class=MsoNormal><i><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>                      \n",
    "n_iter_backprop=200,</span></i></p>\n",
    "\n",
    "<p class=MsoNormal><i><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>                       batch_size=32,</span></i></p>\n",
    "\n",
    "<p class=MsoNormal><i><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>                      \n",
    "activation_function='relu',</span></i></p>\n",
    "\n",
    "<p class=MsoNormal><i><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>                       dropout_p=0.2)</span></i></p>\n",
    "\n",
    "<p class=MsoNormal><i><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>&nbsp;</span></i></p>\n",
    "\n",
    "<p class=MsoNormal><i><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>Model 3 (2-layer):</span></i></p>\n",
    "\n",
    "<p class=MsoNormal><i><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>deep_belief_net(hidden_layers_structure=[100,\n",
    "500], </span></i></p>\n",
    "\n",
    "<p class=MsoNormal><i><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>                      \n",
    "learning_rate_rbm=0.05,</span></i></p>\n",
    "\n",
    "<p class=MsoNormal><i><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>                       learning_rate=0.1,</span></i></p>\n",
    "\n",
    "<p class=MsoNormal><i><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>                       n_epochs_rbm=40,</span></i></p>\n",
    "\n",
    "<p class=MsoNormal><i><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>                       n_iter_backprop=200,</span></i></p>\n",
    "\n",
    "<p class=MsoNormal><i><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>                       batch_size=32,</span></i></p>\n",
    "\n",
    "<p class=MsoNormal><i><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>                      \n",
    "activation_function='relu',</span></i></p>\n",
    "\n",
    "<p class=MsoNormal><i><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>                       dropout_p=0.2)</span></i></p>\n",
    "\n",
    "<p class=MsoNormal><i><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>&nbsp;</span></i></p>\n",
    "\n",
    "<p class=MsoNormal><i><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>Model 4 (3-layer):</span></i></p>\n",
    "\n",
    "<p class=MsoNormal><i><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>deep_belief_net(hidden_layers_structure=[500,\n",
    "200, 300], </span></i></p>\n",
    "\n",
    "<p class=MsoNormal><i><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>                      \n",
    "learning_rate_rbm=0.05,</span></i></p>\n",
    "\n",
    "<p class=MsoNormal><i><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>                       learning_rate=0.1,</span></i></p>\n",
    "\n",
    "<p class=MsoNormal><i><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>                       n_epochs_rbm=40,</span></i></p>\n",
    "\n",
    "<p class=MsoNormal><i><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>                      \n",
    "n_iter_backprop=200,</span></i></p>\n",
    "\n",
    "<p class=MsoNormal><i><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>                       batch_size=32,</span></i></p>\n",
    "\n",
    "<p class=MsoNormal><i><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>                      \n",
    "activation_function='relu',</span></i></p>\n",
    "\n",
    "<p class=MsoNormal><i><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>                       dropout_p=0.2)</span></i></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>&nbsp;</span></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>&nbsp;</span></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'><img width=340 height=275 id=\"Picture 9\"\n",
    "src=\"FINAL%20PROJECT_files/image030.gif\"></span></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>Results from the benchmark indicate that: </span></p>\n",
    "\n",
    "<p class=MsoNormal style='text-indent:.5in'><span lang=EN style='font-size:\n",
    "12.0pt;line-height:115%;font-family:\"Times New Roman\",serif'>Model 1 gives a\n",
    "90% classification accuracy, </span></p>\n",
    "\n",
    "<p class=MsoNormal style='text-indent:.5in'><span lang=EN style='font-size:\n",
    "12.0pt;line-height:115%;font-family:\"Times New Roman\",serif'>Model 2 gives a\n",
    "92% classification accuracy, </span></p>\n",
    "\n",
    "<p class=MsoNormal style='text-indent:.5in'><span lang=EN style='font-size:\n",
    "12.0pt;line-height:115%;font-family:\"Times New Roman\",serif'>Model 3 gives a\n",
    "90.5% classification accuracy.</span></p>\n",
    "\n",
    "<p class=MsoNormal style='text-indent:.5in'><span lang=EN style='font-size:\n",
    "12.0pt;line-height:115%;font-family:\"Times New Roman\",serif'>Model 4 gives a\n",
    "90.5% classification accuracy.</span></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>&nbsp;</span></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>Results from above indicate that the best\n",
    "performing model is model 2, containing one hidden layer with 300 units. </span></p>\n",
    "\n",
    "<p class=MsoNormal><b><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>&nbsp;</span></b></p>\n",
    "\n",
    "<p class=MsoNormal><b><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>Second Benchmark</span></b></p>\n",
    "\n",
    "<p class=MsoNormal><b><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'><img width=375 height=276 id=\"Picture 10\"\n",
    "src=\"FINAL%20PROJECT_files/image031.gif\"></span></b></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>Running through the second benchmark (ReLu\n",
    "activation) indicated the following results: </span></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>            dropout = 0.0  :           88.0%</span></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>            dropout = 0.1  :           89.0%</span></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>            dropout = 0.2  :           92.0%</span></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>            dropout = 0.3  :           90.0%</span></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>            dropout = 0.4  :           89.0%</span></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>            dropout = 0.5  :           89.0%</span></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>            dropout = 0.6  :           87.0%</span></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>            dropout = 0.7  :           90.0%</span></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>            dropout = 0.8  :           80.0%</span></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>            dropout = 0.9  :           15.5%</span></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>            dropout = 1.0  :           10.0%</span></p>\n",
    "\n",
    "<p class=MsoNormal><b><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'><img width=374 height=275 id=\"Picture 11\"\n",
    "src=\"FINAL%20PROJECT_files/image032.gif\"></span></b></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>Running with sigmoidal activation indicated\n",
    "the following results:</span></p>\n",
    "\n",
    "<p class=MsoNormal style='text-indent:.5in'><span lang=EN style='font-size:\n",
    "12.0pt;line-height:115%;font-family:\"Times New Roman\",serif'>dropout = 0.0  :           88.5%</span></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>            dropout = 0.1  :           92.0%</span></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>            dropout = 0.2  :           89.5%</span></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>            dropout = 0.3  :           88.5%</span></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>            dropout = 0.4  :           88.5%</span></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>            dropout = 0.5  :           87.5%</span></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>            dropout = 0.6  :           88.0%</span></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>            dropout = 0.7  :           85.0%</span></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>            dropout = 0.8  :           84.5%</span></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>            dropout = 0.9  :           70.0%</span></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>            dropout = 1.0  :           10.0%</span></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>            </span></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>&nbsp;</span></p>\n",
    "\n",
    "<b><span lang=EN style='font-size:12.0pt;line-height:115%;font-family:\"Times New Roman\",serif'><br\n",
    "clear=all style='page-break-before:always'>\n",
    "</span></b>\n",
    "\n",
    "<p class=MsoNormal><b><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>&nbsp;</span></b></p>\n",
    "\n",
    "<p class=MsoNormal><b><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>Alternate models</span></b></p>\n",
    "\n",
    "<p class=MsoNormal><b><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'><img width=519 height=466 id=\"Picture 12\"\n",
    "src=\"FINAL%20PROJECT_files/image033.gif\"></span></b></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>Running on the alternative models, they\n",
    "outputted:</span></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>&nbsp;</span></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>            LinearSVC                  ( C\n",
    "= 0.1  )      :            86.0%</span></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>            LinearSVC                  ( C\n",
    "= 1     )      :            84.5%</span></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>            LinearSVC                  ( C\n",
    "= 10   )      :            84.0%</span></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>            LinearSVC                  ( C\n",
    "= 100 )      :            84.0%</span></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>            Decision Tree                                     :            66.5%</span></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>            Ada Boost                                           :            39.5%</span></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>            Random Forest                                   :            79.5%</span></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>            SVC                            (\n",
    "C = 0.1  )      :            11.5%</span></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>            SVC                            (\n",
    "C = 1     )      :            80.5%</span></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>            SVC                            (\n",
    "C = 10   )      :            88.5%</span></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>            SVC                            (\n",
    "C = 100 )      :            89.5%</span></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>            </span></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>&nbsp;</span></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>&nbsp;</span></p>\n",
    "\n",
    "<p class=MsoNormal style='margin-bottom:6.0pt'><b><span lang=EN\n",
    "style='font-size:16.0pt;line-height:115%;font-family:\"Times New Roman\",serif'>&nbsp;</span></b></p>\n",
    "\n",
    "<p class=MsoNormal style='margin-bottom:6.0pt'><b><span lang=EN\n",
    "style='font-size:16.0pt;line-height:115%;font-family:\"Times New Roman\",serif'>4.\n",
    "Conclusion</span></b></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>From the given results, I find that the\n",
    "parameters chosen using the process noted outputs two deep belief network\n",
    "settings tied for highest performance at 92% classification accuracy. Using\n",
    "this process results in deep belief networks that significantly outperform\n",
    "other methods of supervised learning as seen with boosting, random forests,\n",
    "decision trees, and the above support vector machines. From the parameter\n",
    "tuning process, we also find that the parameter settings taken from this\n",
    "process have resulted in deep belief network accuracies generally higher than\n",
    "other deep belief network settings, including the default parameters. </span></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>&nbsp;</span></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>On another note, one of the parameters\n",
    "done in the testing phase, specifically the parameter n_epochs_rbm=40, with\n",
    "otherwise default parameters, had a classification accuracy of 93%. This is the\n",
    "highest accuracy performance seen in any of the models tested, including the\n",
    "two final models.  However, this is not as big of an improvement over the final\n",
    "models as it seems, given the rather small training and test set size. </span></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>&nbsp;</span></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>Hopefully, the results found in these\n",
    "experiments will prompt further investigation into parameter tuning for future\n",
    "applications of deep belief networks, especially on similar datasets to MNIST. </span></p>\n",
    "\n",
    "<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;\n",
    "font-family:\"Times New Roman\",serif'>&nbsp;</span></p>\n",
    "\n",
    "<p class=MsoNormal style='margin-bottom:6.0pt'><b><span lang=EN\n",
    "style='font-size:16.0pt;line-height:115%;font-family:\"Times New Roman\",serif'>5.\n",
    "References</span></b></p>\n",
    "\n",
    "<p class=MsoNormal style='margin-bottom:6.0pt'><span lang=EN style='font-size:\n",
    "12.0pt;line-height:115%;font-family:\"Times New Roman\",serif'>H Geoffrey E., O\n",
    "Simon, and T Yee-Whye. A fast learning algorithm for deep belief nets. <i>Neural\n",
    "Comput</i>, 2006.</span></p>\n",
    "\n",
    "</div>\n",
    "\n",
    "</body>\n",
    "\n",
    "</html>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code\n",
    "\n",
    "---------------------------------------\n",
    "\n",
    "Deep belief networks from G. Hinton  \n",
    "(http://www.cs.toronto.edu/~hinton/csc2515/projects/deep1.html) \n",
    "\n",
    "DBN code is available at https://github.com/albertbup/deep-belief-network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set-up**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import NumPy\n",
    "import numpy as np\n",
    "np.random.seed(42)  # Set random seed\n",
    "\n",
    "# Import scikit-learn DBN dependency modules\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.classification import accuracy_score\n",
    "\n",
    "# Import deep-belief network\n",
    "# Found at https://github.com/albertbup/deep-belief-network\n",
    "from dbn.tensorflow import SupervisedDBNClassification\n",
    "\n",
    "# Graphs\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load and format dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "from sklearn.datasets import fetch_mldata\n",
    "mnist = fetch_mldata('MNIST original')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create set of 1000 MNIST datapoints\n",
    "class_count = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "# Get 100 of each class\n",
    "for i in range(0, len(mnist.data)):\n",
    "    target_value = int(mnist.target[i])\n",
    "    if class_count[target_value] < 100:\n",
    "        X.append(mnist.data[i])\n",
    "        Y.append(target_value)\n",
    "        class_count[target_value] += 1\n",
    "    if sum(class_count) == 1000:\n",
    "        break\n",
    "\n",
    "# Convert to NumPy\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "\n",
    "# Scale data for MNIST\n",
    "X = (X / 256).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DBN function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def deep_belief_net(hidden_layers_structure=[256, 256], \n",
    "                    learning_rate_rbm=0.05,\n",
    "                    learning_rate=0.1,\n",
    "                    n_epochs_rbm=10,\n",
    "                    n_iter_backprop=100,\n",
    "                    batch_size=32,\n",
    "                    activation_function='relu',\n",
    "                    dropout_p=0.2):\n",
    "\n",
    "    # Splitting data\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
    "\n",
    "    # Training\n",
    "    classifier = SupervisedDBNClassification(hidden_layers_structure=hidden_layers_structure,\n",
    "                                             learning_rate_rbm=learning_rate_rbm,\n",
    "                                             learning_rate=learning_rate,\n",
    "                                             n_epochs_rbm=n_epochs_rbm,\n",
    "                                             n_iter_backprop=n_iter_backprop,\n",
    "                                             batch_size=batch_size,\n",
    "                                             activation_function=activation_function,\n",
    "                                             dropout_p=dropout_p)\n",
    "    classifier.fit(X_train, Y_train)\n",
    "\n",
    "    # Save the model\n",
    "    classifier.save('model.pkl')\n",
    "\n",
    "    # Restore\n",
    "    classifier = SupervisedDBNClassification.load('model.pkl')\n",
    "\n",
    "    # Test\n",
    "    Y_pred = classifier.predict(X_test)\n",
    "    acc = accuracy_score(Y_test, Y_pred)\n",
    "    print('Done.\\nAccuracy: %f' % acc)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run DBN tests below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Default setting (base performance)**\n",
    "\n",
    "    hidden_layers_structure=[256, 256]\n",
    "    learning_rate_rbm=0.05\n",
    "    learning_rate=0.1\n",
    "    n_epochs_rbm=10\n",
    "    n_iter_backprop=100\n",
    "    batch_size=32\n",
    "    activation_function='relu'\n",
    "    dropout_p=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 74.742325\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 41.013561\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 29.765366\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 39.022728\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 38.707474\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 45.363823\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 35.756557\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 37.316841\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 34.732811\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 26.490023\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 112.534027\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 114.069450\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 75.408974\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 76.183395\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 86.979401\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 107.212440\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 92.068924\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 101.160736\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 83.630653\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 109.224358\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.661431\n",
      ">> Epoch 1 finished \tANN training loss 0.473681\n",
      ">> Epoch 2 finished \tANN training loss 0.381107\n",
      ">> Epoch 3 finished \tANN training loss 0.329544\n",
      ">> Epoch 4 finished \tANN training loss 0.277102\n",
      ">> Epoch 5 finished \tANN training loss 0.237960\n",
      ">> Epoch 6 finished \tANN training loss 0.201121\n",
      ">> Epoch 7 finished \tANN training loss 0.176852\n",
      ">> Epoch 8 finished \tANN training loss 0.175910\n",
      ">> Epoch 9 finished \tANN training loss 0.150462\n",
      ">> Epoch 10 finished \tANN training loss 0.134221\n",
      ">> Epoch 11 finished \tANN training loss 0.106897\n",
      ">> Epoch 12 finished \tANN training loss 0.101303\n",
      ">> Epoch 13 finished \tANN training loss 0.097185\n",
      ">> Epoch 14 finished \tANN training loss 0.090578\n",
      ">> Epoch 15 finished \tANN training loss 0.081134\n",
      ">> Epoch 16 finished \tANN training loss 0.062650\n",
      ">> Epoch 17 finished \tANN training loss 0.054735\n",
      ">> Epoch 18 finished \tANN training loss 0.052028\n",
      ">> Epoch 19 finished \tANN training loss 0.051379\n",
      ">> Epoch 20 finished \tANN training loss 0.041975\n",
      ">> Epoch 21 finished \tANN training loss 0.036709\n",
      ">> Epoch 22 finished \tANN training loss 0.038421\n",
      ">> Epoch 23 finished \tANN training loss 0.034796\n",
      ">> Epoch 24 finished \tANN training loss 0.030439\n",
      ">> Epoch 25 finished \tANN training loss 0.028146\n",
      ">> Epoch 26 finished \tANN training loss 0.024311\n",
      ">> Epoch 27 finished \tANN training loss 0.027123\n",
      ">> Epoch 28 finished \tANN training loss 0.021096\n",
      ">> Epoch 29 finished \tANN training loss 0.022384\n",
      ">> Epoch 30 finished \tANN training loss 0.018908\n",
      ">> Epoch 31 finished \tANN training loss 0.016520\n",
      ">> Epoch 32 finished \tANN training loss 0.016011\n",
      ">> Epoch 33 finished \tANN training loss 0.014118\n",
      ">> Epoch 34 finished \tANN training loss 0.012290\n",
      ">> Epoch 35 finished \tANN training loss 0.011765\n",
      ">> Epoch 36 finished \tANN training loss 0.010858\n",
      ">> Epoch 37 finished \tANN training loss 0.010637\n",
      ">> Epoch 38 finished \tANN training loss 0.011058\n",
      ">> Epoch 39 finished \tANN training loss 0.010356\n",
      ">> Epoch 40 finished \tANN training loss 0.008754\n",
      ">> Epoch 41 finished \tANN training loss 0.008657\n",
      ">> Epoch 42 finished \tANN training loss 0.008185\n",
      ">> Epoch 43 finished \tANN training loss 0.007592\n",
      ">> Epoch 44 finished \tANN training loss 0.007276\n",
      ">> Epoch 45 finished \tANN training loss 0.007210\n",
      ">> Epoch 46 finished \tANN training loss 0.006566\n",
      ">> Epoch 47 finished \tANN training loss 0.005973\n",
      ">> Epoch 48 finished \tANN training loss 0.006141\n",
      ">> Epoch 49 finished \tANN training loss 0.005940\n",
      ">> Epoch 50 finished \tANN training loss 0.005364\n",
      ">> Epoch 51 finished \tANN training loss 0.005246\n",
      ">> Epoch 52 finished \tANN training loss 0.004899\n",
      ">> Epoch 53 finished \tANN training loss 0.004868\n",
      ">> Epoch 54 finished \tANN training loss 0.004056\n",
      ">> Epoch 55 finished \tANN training loss 0.004099\n",
      ">> Epoch 56 finished \tANN training loss 0.003540\n",
      ">> Epoch 57 finished \tANN training loss 0.003511\n",
      ">> Epoch 58 finished \tANN training loss 0.003346\n",
      ">> Epoch 59 finished \tANN training loss 0.003240\n",
      ">> Epoch 60 finished \tANN training loss 0.003048\n",
      ">> Epoch 61 finished \tANN training loss 0.003064\n",
      ">> Epoch 62 finished \tANN training loss 0.003173\n",
      ">> Epoch 63 finished \tANN training loss 0.003033\n",
      ">> Epoch 64 finished \tANN training loss 0.003287\n",
      ">> Epoch 65 finished \tANN training loss 0.003656\n",
      ">> Epoch 66 finished \tANN training loss 0.002517\n",
      ">> Epoch 67 finished \tANN training loss 0.002400\n",
      ">> Epoch 68 finished \tANN training loss 0.002507\n",
      ">> Epoch 69 finished \tANN training loss 0.002370\n",
      ">> Epoch 70 finished \tANN training loss 0.002416\n",
      ">> Epoch 71 finished \tANN training loss 0.002390\n",
      ">> Epoch 72 finished \tANN training loss 0.002906\n",
      ">> Epoch 73 finished \tANN training loss 0.002098\n",
      ">> Epoch 74 finished \tANN training loss 0.002158\n",
      ">> Epoch 75 finished \tANN training loss 0.001844\n",
      ">> Epoch 76 finished \tANN training loss 0.002086\n",
      ">> Epoch 77 finished \tANN training loss 0.002633\n",
      ">> Epoch 78 finished \tANN training loss 0.002115\n",
      ">> Epoch 79 finished \tANN training loss 0.001528\n",
      ">> Epoch 80 finished \tANN training loss 0.001490\n",
      ">> Epoch 81 finished \tANN training loss 0.001441\n",
      ">> Epoch 82 finished \tANN training loss 0.001383\n",
      ">> Epoch 83 finished \tANN training loss 0.001412\n",
      ">> Epoch 84 finished \tANN training loss 0.001336\n",
      ">> Epoch 85 finished \tANN training loss 0.001183\n",
      ">> Epoch 86 finished \tANN training loss 0.001278\n",
      ">> Epoch 87 finished \tANN training loss 0.001399\n",
      ">> Epoch 88 finished \tANN training loss 0.001311\n",
      ">> Epoch 89 finished \tANN training loss 0.001214\n",
      ">> Epoch 90 finished \tANN training loss 0.001467\n",
      ">> Epoch 91 finished \tANN training loss 0.001324\n",
      ">> Epoch 92 finished \tANN training loss 0.001243\n",
      ">> Epoch 93 finished \tANN training loss 0.001247\n",
      ">> Epoch 94 finished \tANN training loss 0.001068\n",
      ">> Epoch 95 finished \tANN training loss 0.001037\n",
      ">> Epoch 96 finished \tANN training loss 0.001000\n",
      ">> Epoch 97 finished \tANN training loss 0.001057\n",
      ">> Epoch 98 finished \tANN training loss 0.000968\n",
      ">> Epoch 99 finished \tANN training loss 0.000883\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.910000\n"
     ]
    }
   ],
   "source": [
    "# Run DBN\n",
    "default_acc = deep_belief_net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.91\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(default_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different 1-layer structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store accuracies\n",
    "onelayer_acc = [0, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setting 1**\n",
    "\n",
    "    hidden_layers_structure=[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 86.976990\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 63.835304\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 47.721466\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 48.712292\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 55.124664\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 55.460400\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 53.136978\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 58.151356\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 63.297531\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 60.489033\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.787348\n",
      ">> Epoch 1 finished \tANN training loss 0.577817\n",
      ">> Epoch 2 finished \tANN training loss 0.461953\n",
      ">> Epoch 3 finished \tANN training loss 0.410259\n",
      ">> Epoch 4 finished \tANN training loss 0.370284\n",
      ">> Epoch 5 finished \tANN training loss 0.330365\n",
      ">> Epoch 6 finished \tANN training loss 0.296728\n",
      ">> Epoch 7 finished \tANN training loss 0.282827\n",
      ">> Epoch 8 finished \tANN training loss 0.254772\n",
      ">> Epoch 9 finished \tANN training loss 0.230176\n",
      ">> Epoch 10 finished \tANN training loss 0.229763\n",
      ">> Epoch 11 finished \tANN training loss 0.192113\n",
      ">> Epoch 12 finished \tANN training loss 0.183068\n",
      ">> Epoch 13 finished \tANN training loss 0.172171\n",
      ">> Epoch 14 finished \tANN training loss 0.153478\n",
      ">> Epoch 15 finished \tANN training loss 0.153599\n",
      ">> Epoch 16 finished \tANN training loss 0.138848\n",
      ">> Epoch 17 finished \tANN training loss 0.123464\n",
      ">> Epoch 18 finished \tANN training loss 0.117612\n",
      ">> Epoch 19 finished \tANN training loss 0.106769\n",
      ">> Epoch 20 finished \tANN training loss 0.099512\n",
      ">> Epoch 21 finished \tANN training loss 0.090460\n",
      ">> Epoch 22 finished \tANN training loss 0.088617\n",
      ">> Epoch 23 finished \tANN training loss 0.078987\n",
      ">> Epoch 24 finished \tANN training loss 0.079952\n",
      ">> Epoch 25 finished \tANN training loss 0.071017\n",
      ">> Epoch 26 finished \tANN training loss 0.068963\n",
      ">> Epoch 27 finished \tANN training loss 0.063790\n",
      ">> Epoch 28 finished \tANN training loss 0.059158\n",
      ">> Epoch 29 finished \tANN training loss 0.054030\n",
      ">> Epoch 30 finished \tANN training loss 0.051049\n",
      ">> Epoch 31 finished \tANN training loss 0.047558\n",
      ">> Epoch 32 finished \tANN training loss 0.045558\n",
      ">> Epoch 33 finished \tANN training loss 0.042773\n",
      ">> Epoch 34 finished \tANN training loss 0.040962\n",
      ">> Epoch 35 finished \tANN training loss 0.037335\n",
      ">> Epoch 36 finished \tANN training loss 0.035929\n",
      ">> Epoch 37 finished \tANN training loss 0.036479\n",
      ">> Epoch 38 finished \tANN training loss 0.031472\n",
      ">> Epoch 39 finished \tANN training loss 0.032730\n",
      ">> Epoch 40 finished \tANN training loss 0.028346\n",
      ">> Epoch 41 finished \tANN training loss 0.027368\n",
      ">> Epoch 42 finished \tANN training loss 0.027131\n",
      ">> Epoch 43 finished \tANN training loss 0.025665\n",
      ">> Epoch 44 finished \tANN training loss 0.024435\n",
      ">> Epoch 45 finished \tANN training loss 0.022174\n",
      ">> Epoch 46 finished \tANN training loss 0.021516\n",
      ">> Epoch 47 finished \tANN training loss 0.020932\n",
      ">> Epoch 48 finished \tANN training loss 0.020583\n",
      ">> Epoch 49 finished \tANN training loss 0.019465\n",
      ">> Epoch 50 finished \tANN training loss 0.019192\n",
      ">> Epoch 51 finished \tANN training loss 0.017391\n",
      ">> Epoch 52 finished \tANN training loss 0.016926\n",
      ">> Epoch 53 finished \tANN training loss 0.016037\n",
      ">> Epoch 54 finished \tANN training loss 0.014989\n",
      ">> Epoch 55 finished \tANN training loss 0.016574\n",
      ">> Epoch 56 finished \tANN training loss 0.015238\n",
      ">> Epoch 57 finished \tANN training loss 0.013714\n",
      ">> Epoch 58 finished \tANN training loss 0.013706\n",
      ">> Epoch 59 finished \tANN training loss 0.013606\n",
      ">> Epoch 60 finished \tANN training loss 0.013038\n",
      ">> Epoch 61 finished \tANN training loss 0.012507\n",
      ">> Epoch 62 finished \tANN training loss 0.012005\n",
      ">> Epoch 63 finished \tANN training loss 0.011266\n",
      ">> Epoch 64 finished \tANN training loss 0.010778\n",
      ">> Epoch 65 finished \tANN training loss 0.010342\n",
      ">> Epoch 66 finished \tANN training loss 0.010704\n",
      ">> Epoch 67 finished \tANN training loss 0.009574\n",
      ">> Epoch 68 finished \tANN training loss 0.009193\n",
      ">> Epoch 69 finished \tANN training loss 0.009129\n",
      ">> Epoch 70 finished \tANN training loss 0.009406\n",
      ">> Epoch 71 finished \tANN training loss 0.009048\n",
      ">> Epoch 72 finished \tANN training loss 0.008465\n",
      ">> Epoch 73 finished \tANN training loss 0.009038\n",
      ">> Epoch 74 finished \tANN training loss 0.007922\n",
      ">> Epoch 75 finished \tANN training loss 0.007606\n",
      ">> Epoch 76 finished \tANN training loss 0.007918\n",
      ">> Epoch 77 finished \tANN training loss 0.007046\n",
      ">> Epoch 78 finished \tANN training loss 0.008020\n",
      ">> Epoch 79 finished \tANN training loss 0.006778\n",
      ">> Epoch 80 finished \tANN training loss 0.007059\n",
      ">> Epoch 81 finished \tANN training loss 0.006585\n",
      ">> Epoch 82 finished \tANN training loss 0.006527\n",
      ">> Epoch 83 finished \tANN training loss 0.006702\n",
      ">> Epoch 84 finished \tANN training loss 0.006153\n",
      ">> Epoch 85 finished \tANN training loss 0.006052\n",
      ">> Epoch 86 finished \tANN training loss 0.005641\n",
      ">> Epoch 87 finished \tANN training loss 0.005605\n",
      ">> Epoch 88 finished \tANN training loss 0.005523\n",
      ">> Epoch 89 finished \tANN training loss 0.005322\n",
      ">> Epoch 90 finished \tANN training loss 0.005478\n",
      ">> Epoch 91 finished \tANN training loss 0.005106\n",
      ">> Epoch 92 finished \tANN training loss 0.005030\n",
      ">> Epoch 93 finished \tANN training loss 0.005018\n",
      ">> Epoch 94 finished \tANN training loss 0.005037\n",
      ">> Epoch 95 finished \tANN training loss 0.004526\n",
      ">> Epoch 96 finished \tANN training loss 0.004715\n",
      ">> Epoch 97 finished \tANN training loss 0.004673\n",
      ">> Epoch 98 finished \tANN training loss 0.004583\n",
      ">> Epoch 99 finished \tANN training loss 0.004510\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.900000\n"
     ]
    }
   ],
   "source": [
    "onelayer_acc[0] = deep_belief_net(hidden_layers_structure=[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.9\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(onelayer_acc[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setting 2**\n",
    "\n",
    "    hidden_layers_structure=[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 52.349266\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 72.684662\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 46.626049\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 43.839184\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 40.715866\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 38.150314\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 40.179611\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 44.788387\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 38.358856\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 36.342644\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.856542\n",
      ">> Epoch 1 finished \tANN training loss 0.590754\n",
      ">> Epoch 2 finished \tANN training loss 0.478319\n",
      ">> Epoch 3 finished \tANN training loss 0.415798\n",
      ">> Epoch 4 finished \tANN training loss 0.363241\n",
      ">> Epoch 5 finished \tANN training loss 0.342769\n",
      ">> Epoch 6 finished \tANN training loss 0.290627\n",
      ">> Epoch 7 finished \tANN training loss 0.278313\n",
      ">> Epoch 8 finished \tANN training loss 0.241844\n",
      ">> Epoch 9 finished \tANN training loss 0.221732\n",
      ">> Epoch 10 finished \tANN training loss 0.208090\n",
      ">> Epoch 11 finished \tANN training loss 0.195600\n",
      ">> Epoch 12 finished \tANN training loss 0.175948\n",
      ">> Epoch 13 finished \tANN training loss 0.176113\n",
      ">> Epoch 14 finished \tANN training loss 0.144608\n",
      ">> Epoch 15 finished \tANN training loss 0.132851\n",
      ">> Epoch 16 finished \tANN training loss 0.123206\n",
      ">> Epoch 17 finished \tANN training loss 0.118496\n",
      ">> Epoch 18 finished \tANN training loss 0.115063\n",
      ">> Epoch 19 finished \tANN training loss 0.101604\n",
      ">> Epoch 20 finished \tANN training loss 0.094504\n",
      ">> Epoch 21 finished \tANN training loss 0.086143\n",
      ">> Epoch 22 finished \tANN training loss 0.082999\n",
      ">> Epoch 23 finished \tANN training loss 0.074577\n",
      ">> Epoch 24 finished \tANN training loss 0.068166\n",
      ">> Epoch 25 finished \tANN training loss 0.064587\n",
      ">> Epoch 26 finished \tANN training loss 0.063026\n",
      ">> Epoch 27 finished \tANN training loss 0.056789\n",
      ">> Epoch 28 finished \tANN training loss 0.051912\n",
      ">> Epoch 29 finished \tANN training loss 0.048790\n",
      ">> Epoch 30 finished \tANN training loss 0.046831\n",
      ">> Epoch 31 finished \tANN training loss 0.043765\n",
      ">> Epoch 32 finished \tANN training loss 0.041510\n",
      ">> Epoch 33 finished \tANN training loss 0.037993\n",
      ">> Epoch 34 finished \tANN training loss 0.036890\n",
      ">> Epoch 35 finished \tANN training loss 0.034095\n",
      ">> Epoch 36 finished \tANN training loss 0.033277\n",
      ">> Epoch 37 finished \tANN training loss 0.030609\n",
      ">> Epoch 38 finished \tANN training loss 0.028846\n",
      ">> Epoch 39 finished \tANN training loss 0.027414\n",
      ">> Epoch 40 finished \tANN training loss 0.025866\n",
      ">> Epoch 41 finished \tANN training loss 0.025596\n",
      ">> Epoch 42 finished \tANN training loss 0.022377\n",
      ">> Epoch 43 finished \tANN training loss 0.021889\n",
      ">> Epoch 44 finished \tANN training loss 0.021763\n",
      ">> Epoch 45 finished \tANN training loss 0.020090\n",
      ">> Epoch 46 finished \tANN training loss 0.019128\n",
      ">> Epoch 47 finished \tANN training loss 0.019097\n",
      ">> Epoch 48 finished \tANN training loss 0.018722\n",
      ">> Epoch 49 finished \tANN training loss 0.016980\n",
      ">> Epoch 50 finished \tANN training loss 0.015873\n",
      ">> Epoch 51 finished \tANN training loss 0.015356\n",
      ">> Epoch 52 finished \tANN training loss 0.015273\n",
      ">> Epoch 53 finished \tANN training loss 0.014214\n",
      ">> Epoch 54 finished \tANN training loss 0.014201\n",
      ">> Epoch 55 finished \tANN training loss 0.013499\n",
      ">> Epoch 56 finished \tANN training loss 0.012644\n",
      ">> Epoch 57 finished \tANN training loss 0.012240\n",
      ">> Epoch 58 finished \tANN training loss 0.011818\n",
      ">> Epoch 59 finished \tANN training loss 0.011327\n",
      ">> Epoch 60 finished \tANN training loss 0.011356\n",
      ">> Epoch 61 finished \tANN training loss 0.010827\n",
      ">> Epoch 62 finished \tANN training loss 0.010376\n",
      ">> Epoch 63 finished \tANN training loss 0.010062\n",
      ">> Epoch 64 finished \tANN training loss 0.009569\n",
      ">> Epoch 65 finished \tANN training loss 0.009572\n",
      ">> Epoch 66 finished \tANN training loss 0.009667\n",
      ">> Epoch 67 finished \tANN training loss 0.009007\n",
      ">> Epoch 68 finished \tANN training loss 0.008775\n",
      ">> Epoch 69 finished \tANN training loss 0.008365\n",
      ">> Epoch 70 finished \tANN training loss 0.008614\n",
      ">> Epoch 71 finished \tANN training loss 0.007897\n",
      ">> Epoch 72 finished \tANN training loss 0.007492\n",
      ">> Epoch 73 finished \tANN training loss 0.007543\n",
      ">> Epoch 74 finished \tANN training loss 0.007750\n",
      ">> Epoch 75 finished \tANN training loss 0.007785\n",
      ">> Epoch 76 finished \tANN training loss 0.007707\n",
      ">> Epoch 77 finished \tANN training loss 0.007456\n",
      ">> Epoch 78 finished \tANN training loss 0.007066\n",
      ">> Epoch 79 finished \tANN training loss 0.006705\n",
      ">> Epoch 80 finished \tANN training loss 0.006266\n",
      ">> Epoch 81 finished \tANN training loss 0.006087\n",
      ">> Epoch 82 finished \tANN training loss 0.006256\n",
      ">> Epoch 83 finished \tANN training loss 0.005950\n",
      ">> Epoch 84 finished \tANN training loss 0.005855\n",
      ">> Epoch 85 finished \tANN training loss 0.005483\n",
      ">> Epoch 86 finished \tANN training loss 0.005128\n",
      ">> Epoch 87 finished \tANN training loss 0.005136\n",
      ">> Epoch 88 finished \tANN training loss 0.004829\n",
      ">> Epoch 89 finished \tANN training loss 0.004799\n",
      ">> Epoch 90 finished \tANN training loss 0.004780\n",
      ">> Epoch 91 finished \tANN training loss 0.004908\n",
      ">> Epoch 92 finished \tANN training loss 0.004728\n",
      ">> Epoch 93 finished \tANN training loss 0.004494\n",
      ">> Epoch 94 finished \tANN training loss 0.004456\n",
      ">> Epoch 95 finished \tANN training loss 0.004336\n",
      ">> Epoch 96 finished \tANN training loss 0.004266\n",
      ">> Epoch 97 finished \tANN training loss 0.004331\n",
      ">> Epoch 98 finished \tANN training loss 0.004199\n",
      ">> Epoch 99 finished \tANN training loss 0.004174\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.910000\n"
     ]
    }
   ],
   "source": [
    "onelayer_acc[1] = deep_belief_net(hidden_layers_structure=[200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.91\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(onelayer_acc[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setting 3**\n",
    "\n",
    "    hidden_layers_structure=[300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 87.902206\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 55.003399\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 40.669594\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 37.418659\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 39.941753\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 47.157059\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 31.197338\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 39.757397\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 33.218170\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 46.521252\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.809349\n",
      ">> Epoch 1 finished \tANN training loss 0.550912\n",
      ">> Epoch 2 finished \tANN training loss 0.458730\n",
      ">> Epoch 3 finished \tANN training loss 0.408621\n",
      ">> Epoch 4 finished \tANN training loss 0.343763\n",
      ">> Epoch 5 finished \tANN training loss 0.327247\n",
      ">> Epoch 6 finished \tANN training loss 0.292876\n",
      ">> Epoch 7 finished \tANN training loss 0.264524\n",
      ">> Epoch 8 finished \tANN training loss 0.235765\n",
      ">> Epoch 9 finished \tANN training loss 0.215654\n",
      ">> Epoch 10 finished \tANN training loss 0.202415\n",
      ">> Epoch 11 finished \tANN training loss 0.187902\n",
      ">> Epoch 12 finished \tANN training loss 0.166092\n",
      ">> Epoch 13 finished \tANN training loss 0.160441\n",
      ">> Epoch 14 finished \tANN training loss 0.142402\n",
      ">> Epoch 15 finished \tANN training loss 0.130498\n",
      ">> Epoch 16 finished \tANN training loss 0.122930\n",
      ">> Epoch 17 finished \tANN training loss 0.111313\n",
      ">> Epoch 18 finished \tANN training loss 0.106092\n",
      ">> Epoch 19 finished \tANN training loss 0.092249\n",
      ">> Epoch 20 finished \tANN training loss 0.089241\n",
      ">> Epoch 21 finished \tANN training loss 0.083307\n",
      ">> Epoch 22 finished \tANN training loss 0.077107\n",
      ">> Epoch 23 finished \tANN training loss 0.070437\n",
      ">> Epoch 24 finished \tANN training loss 0.064246\n",
      ">> Epoch 25 finished \tANN training loss 0.059722\n",
      ">> Epoch 26 finished \tANN training loss 0.056275\n",
      ">> Epoch 27 finished \tANN training loss 0.054445\n",
      ">> Epoch 28 finished \tANN training loss 0.049925\n",
      ">> Epoch 29 finished \tANN training loss 0.048585\n",
      ">> Epoch 30 finished \tANN training loss 0.043062\n",
      ">> Epoch 31 finished \tANN training loss 0.040427\n",
      ">> Epoch 32 finished \tANN training loss 0.037998\n",
      ">> Epoch 33 finished \tANN training loss 0.036308\n",
      ">> Epoch 34 finished \tANN training loss 0.034671\n",
      ">> Epoch 35 finished \tANN training loss 0.031982\n",
      ">> Epoch 36 finished \tANN training loss 0.031280\n",
      ">> Epoch 37 finished \tANN training loss 0.027919\n",
      ">> Epoch 38 finished \tANN training loss 0.027858\n",
      ">> Epoch 39 finished \tANN training loss 0.028015\n",
      ">> Epoch 40 finished \tANN training loss 0.023874\n",
      ">> Epoch 41 finished \tANN training loss 0.024393\n",
      ">> Epoch 42 finished \tANN training loss 0.022408\n",
      ">> Epoch 43 finished \tANN training loss 0.023429\n",
      ">> Epoch 44 finished \tANN training loss 0.019635\n",
      ">> Epoch 45 finished \tANN training loss 0.019911\n",
      ">> Epoch 46 finished \tANN training loss 0.018749\n",
      ">> Epoch 47 finished \tANN training loss 0.018279\n",
      ">> Epoch 48 finished \tANN training loss 0.016492\n",
      ">> Epoch 49 finished \tANN training loss 0.016776\n",
      ">> Epoch 50 finished \tANN training loss 0.016031\n",
      ">> Epoch 51 finished \tANN training loss 0.014930\n",
      ">> Epoch 52 finished \tANN training loss 0.014278\n",
      ">> Epoch 53 finished \tANN training loss 0.014475\n",
      ">> Epoch 54 finished \tANN training loss 0.013630\n",
      ">> Epoch 55 finished \tANN training loss 0.013575\n",
      ">> Epoch 56 finished \tANN training loss 0.013000\n",
      ">> Epoch 57 finished \tANN training loss 0.011852\n",
      ">> Epoch 58 finished \tANN training loss 0.011353\n",
      ">> Epoch 59 finished \tANN training loss 0.010982\n",
      ">> Epoch 60 finished \tANN training loss 0.010344\n",
      ">> Epoch 61 finished \tANN training loss 0.010204\n",
      ">> Epoch 62 finished \tANN training loss 0.009919\n",
      ">> Epoch 63 finished \tANN training loss 0.009515\n",
      ">> Epoch 64 finished \tANN training loss 0.009078\n",
      ">> Epoch 65 finished \tANN training loss 0.008570\n",
      ">> Epoch 66 finished \tANN training loss 0.008352\n",
      ">> Epoch 67 finished \tANN training loss 0.008462\n",
      ">> Epoch 68 finished \tANN training loss 0.007986\n",
      ">> Epoch 69 finished \tANN training loss 0.007929\n",
      ">> Epoch 70 finished \tANN training loss 0.007431\n",
      ">> Epoch 71 finished \tANN training loss 0.007339\n",
      ">> Epoch 72 finished \tANN training loss 0.007707\n",
      ">> Epoch 73 finished \tANN training loss 0.006715\n",
      ">> Epoch 74 finished \tANN training loss 0.006569\n",
      ">> Epoch 75 finished \tANN training loss 0.006192\n",
      ">> Epoch 76 finished \tANN training loss 0.006381\n",
      ">> Epoch 77 finished \tANN training loss 0.006126\n",
      ">> Epoch 78 finished \tANN training loss 0.006186\n",
      ">> Epoch 79 finished \tANN training loss 0.005830\n",
      ">> Epoch 80 finished \tANN training loss 0.005903\n",
      ">> Epoch 81 finished \tANN training loss 0.005633\n",
      ">> Epoch 82 finished \tANN training loss 0.005361\n",
      ">> Epoch 83 finished \tANN training loss 0.005482\n",
      ">> Epoch 84 finished \tANN training loss 0.005286\n",
      ">> Epoch 85 finished \tANN training loss 0.004962\n",
      ">> Epoch 86 finished \tANN training loss 0.004915\n",
      ">> Epoch 87 finished \tANN training loss 0.004904\n",
      ">> Epoch 88 finished \tANN training loss 0.004724\n",
      ">> Epoch 89 finished \tANN training loss 0.004845\n",
      ">> Epoch 90 finished \tANN training loss 0.004529\n",
      ">> Epoch 91 finished \tANN training loss 0.004636\n",
      ">> Epoch 92 finished \tANN training loss 0.004444\n",
      ">> Epoch 93 finished \tANN training loss 0.004189\n",
      ">> Epoch 94 finished \tANN training loss 0.004262\n",
      ">> Epoch 95 finished \tANN training loss 0.004398\n",
      ">> Epoch 96 finished \tANN training loss 0.003976\n",
      ">> Epoch 97 finished \tANN training loss 0.004088\n",
      ">> Epoch 98 finished \tANN training loss 0.003930\n",
      ">> Epoch 99 finished \tANN training loss 0.003776\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.910000\n"
     ]
    }
   ],
   "source": [
    "onelayer_acc[2] = deep_belief_net(hidden_layers_structure=[300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.91\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(onelayer_acc[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setting 4**\n",
    "\n",
    "    hidden_layers_structure=[400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 49.054512\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 90.852394\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 54.142891\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 68.925636\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 59.368515\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 53.105038\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 49.493587\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 54.296677\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 41.562691\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 35.099659\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.792912\n",
      ">> Epoch 1 finished \tANN training loss 0.560390\n",
      ">> Epoch 2 finished \tANN training loss 0.456119\n",
      ">> Epoch 3 finished \tANN training loss 0.393785\n",
      ">> Epoch 4 finished \tANN training loss 0.362233\n",
      ">> Epoch 5 finished \tANN training loss 0.321579\n",
      ">> Epoch 6 finished \tANN training loss 0.302900\n",
      ">> Epoch 7 finished \tANN training loss 0.277325\n",
      ">> Epoch 8 finished \tANN training loss 0.240903\n",
      ">> Epoch 9 finished \tANN training loss 0.226532\n",
      ">> Epoch 10 finished \tANN training loss 0.202836\n",
      ">> Epoch 11 finished \tANN training loss 0.185821\n",
      ">> Epoch 12 finished \tANN training loss 0.176060\n",
      ">> Epoch 13 finished \tANN training loss 0.158493\n",
      ">> Epoch 14 finished \tANN training loss 0.147478\n",
      ">> Epoch 15 finished \tANN training loss 0.136997\n",
      ">> Epoch 16 finished \tANN training loss 0.131882\n",
      ">> Epoch 17 finished \tANN training loss 0.120900\n",
      ">> Epoch 18 finished \tANN training loss 0.107460\n",
      ">> Epoch 19 finished \tANN training loss 0.101513\n",
      ">> Epoch 20 finished \tANN training loss 0.091230\n",
      ">> Epoch 21 finished \tANN training loss 0.082328\n",
      ">> Epoch 22 finished \tANN training loss 0.083925\n",
      ">> Epoch 23 finished \tANN training loss 0.075263\n",
      ">> Epoch 24 finished \tANN training loss 0.069971\n",
      ">> Epoch 25 finished \tANN training loss 0.066040\n",
      ">> Epoch 26 finished \tANN training loss 0.059458\n",
      ">> Epoch 27 finished \tANN training loss 0.054597\n",
      ">> Epoch 28 finished \tANN training loss 0.051358\n",
      ">> Epoch 29 finished \tANN training loss 0.049066\n",
      ">> Epoch 30 finished \tANN training loss 0.045689\n",
      ">> Epoch 31 finished \tANN training loss 0.043978\n",
      ">> Epoch 32 finished \tANN training loss 0.040109\n",
      ">> Epoch 33 finished \tANN training loss 0.038521\n",
      ">> Epoch 34 finished \tANN training loss 0.035331\n",
      ">> Epoch 35 finished \tANN training loss 0.033475\n",
      ">> Epoch 36 finished \tANN training loss 0.031607\n",
      ">> Epoch 37 finished \tANN training loss 0.029440\n",
      ">> Epoch 38 finished \tANN training loss 0.028656\n",
      ">> Epoch 39 finished \tANN training loss 0.030513\n",
      ">> Epoch 40 finished \tANN training loss 0.026177\n",
      ">> Epoch 41 finished \tANN training loss 0.025393\n",
      ">> Epoch 42 finished \tANN training loss 0.026700\n",
      ">> Epoch 43 finished \tANN training loss 0.021883\n",
      ">> Epoch 44 finished \tANN training loss 0.021498\n",
      ">> Epoch 45 finished \tANN training loss 0.021088\n",
      ">> Epoch 46 finished \tANN training loss 0.019419\n",
      ">> Epoch 47 finished \tANN training loss 0.018494\n",
      ">> Epoch 48 finished \tANN training loss 0.017926\n",
      ">> Epoch 49 finished \tANN training loss 0.017273\n",
      ">> Epoch 50 finished \tANN training loss 0.017615\n",
      ">> Epoch 51 finished \tANN training loss 0.015977\n",
      ">> Epoch 52 finished \tANN training loss 0.014584\n",
      ">> Epoch 53 finished \tANN training loss 0.013821\n",
      ">> Epoch 54 finished \tANN training loss 0.013653\n",
      ">> Epoch 55 finished \tANN training loss 0.012896\n",
      ">> Epoch 56 finished \tANN training loss 0.012485\n",
      ">> Epoch 57 finished \tANN training loss 0.012098\n",
      ">> Epoch 58 finished \tANN training loss 0.011524\n",
      ">> Epoch 59 finished \tANN training loss 0.011108\n",
      ">> Epoch 60 finished \tANN training loss 0.010978\n",
      ">> Epoch 61 finished \tANN training loss 0.010789\n",
      ">> Epoch 62 finished \tANN training loss 0.010164\n",
      ">> Epoch 63 finished \tANN training loss 0.009842\n",
      ">> Epoch 64 finished \tANN training loss 0.009203\n",
      ">> Epoch 65 finished \tANN training loss 0.008949\n",
      ">> Epoch 66 finished \tANN training loss 0.008519\n",
      ">> Epoch 67 finished \tANN training loss 0.008319\n",
      ">> Epoch 68 finished \tANN training loss 0.008626\n",
      ">> Epoch 69 finished \tANN training loss 0.008294\n",
      ">> Epoch 70 finished \tANN training loss 0.007995\n",
      ">> Epoch 71 finished \tANN training loss 0.007694\n",
      ">> Epoch 72 finished \tANN training loss 0.007618\n",
      ">> Epoch 73 finished \tANN training loss 0.007048\n",
      ">> Epoch 74 finished \tANN training loss 0.006974\n",
      ">> Epoch 75 finished \tANN training loss 0.007001\n",
      ">> Epoch 76 finished \tANN training loss 0.006803\n",
      ">> Epoch 77 finished \tANN training loss 0.006330\n",
      ">> Epoch 78 finished \tANN training loss 0.006589\n",
      ">> Epoch 79 finished \tANN training loss 0.006382\n",
      ">> Epoch 80 finished \tANN training loss 0.006140\n",
      ">> Epoch 81 finished \tANN training loss 0.005898\n",
      ">> Epoch 82 finished \tANN training loss 0.005944\n",
      ">> Epoch 83 finished \tANN training loss 0.005474\n",
      ">> Epoch 84 finished \tANN training loss 0.005404\n",
      ">> Epoch 85 finished \tANN training loss 0.005225\n",
      ">> Epoch 86 finished \tANN training loss 0.005078\n",
      ">> Epoch 87 finished \tANN training loss 0.005013\n",
      ">> Epoch 88 finished \tANN training loss 0.004891\n",
      ">> Epoch 89 finished \tANN training loss 0.005059\n",
      ">> Epoch 90 finished \tANN training loss 0.005261\n",
      ">> Epoch 91 finished \tANN training loss 0.004610\n",
      ">> Epoch 92 finished \tANN training loss 0.004801\n",
      ">> Epoch 93 finished \tANN training loss 0.004589\n",
      ">> Epoch 94 finished \tANN training loss 0.004381\n",
      ">> Epoch 95 finished \tANN training loss 0.004123\n",
      ">> Epoch 96 finished \tANN training loss 0.003979\n",
      ">> Epoch 97 finished \tANN training loss 0.003998\n",
      ">> Epoch 98 finished \tANN training loss 0.003940\n",
      ">> Epoch 99 finished \tANN training loss 0.004037\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.905000\n"
     ]
    }
   ],
   "source": [
    "onelayer_acc[3] = deep_belief_net(hidden_layers_structure=[400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.905\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(onelayer_acc[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setting 5**\n",
    "\n",
    "    hidden_layers_structure=[500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 42.600475\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 38.644325\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 50.377831\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 37.820583\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 95.727844\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 49.393799\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 49.923161\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 27.079639\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 30.330034\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 31.138687\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.798674\n",
      ">> Epoch 1 finished \tANN training loss 0.554613\n",
      ">> Epoch 2 finished \tANN training loss 0.452316\n",
      ">> Epoch 3 finished \tANN training loss 0.381789\n",
      ">> Epoch 4 finished \tANN training loss 0.329494\n",
      ">> Epoch 5 finished \tANN training loss 0.295961\n",
      ">> Epoch 6 finished \tANN training loss 0.267486\n",
      ">> Epoch 7 finished \tANN training loss 0.241804\n",
      ">> Epoch 8 finished \tANN training loss 0.235162\n",
      ">> Epoch 9 finished \tANN training loss 0.204400\n",
      ">> Epoch 10 finished \tANN training loss 0.190618\n",
      ">> Epoch 11 finished \tANN training loss 0.173614\n",
      ">> Epoch 12 finished \tANN training loss 0.164421\n",
      ">> Epoch 13 finished \tANN training loss 0.147224\n",
      ">> Epoch 14 finished \tANN training loss 0.130776\n",
      ">> Epoch 15 finished \tANN training loss 0.121806\n",
      ">> Epoch 16 finished \tANN training loss 0.111645\n",
      ">> Epoch 17 finished \tANN training loss 0.114588\n",
      ">> Epoch 18 finished \tANN training loss 0.103425\n",
      ">> Epoch 19 finished \tANN training loss 0.091205\n",
      ">> Epoch 20 finished \tANN training loss 0.081668\n",
      ">> Epoch 21 finished \tANN training loss 0.074841\n",
      ">> Epoch 22 finished \tANN training loss 0.070446\n",
      ">> Epoch 23 finished \tANN training loss 0.064421\n",
      ">> Epoch 24 finished \tANN training loss 0.059264\n",
      ">> Epoch 25 finished \tANN training loss 0.054818\n",
      ">> Epoch 26 finished \tANN training loss 0.052259\n",
      ">> Epoch 27 finished \tANN training loss 0.049628\n",
      ">> Epoch 28 finished \tANN training loss 0.045563\n",
      ">> Epoch 29 finished \tANN training loss 0.043829\n",
      ">> Epoch 30 finished \tANN training loss 0.041754\n",
      ">> Epoch 31 finished \tANN training loss 0.038039\n",
      ">> Epoch 32 finished \tANN training loss 0.035826\n",
      ">> Epoch 33 finished \tANN training loss 0.034101\n",
      ">> Epoch 34 finished \tANN training loss 0.032588\n",
      ">> Epoch 35 finished \tANN training loss 0.037622\n",
      ">> Epoch 36 finished \tANN training loss 0.027654\n",
      ">> Epoch 37 finished \tANN training loss 0.026445\n",
      ">> Epoch 38 finished \tANN training loss 0.025119\n",
      ">> Epoch 39 finished \tANN training loss 0.023703\n",
      ">> Epoch 40 finished \tANN training loss 0.022303\n",
      ">> Epoch 41 finished \tANN training loss 0.021529\n",
      ">> Epoch 42 finished \tANN training loss 0.021216\n",
      ">> Epoch 43 finished \tANN training loss 0.019514\n",
      ">> Epoch 44 finished \tANN training loss 0.018915\n",
      ">> Epoch 45 finished \tANN training loss 0.017770\n",
      ">> Epoch 46 finished \tANN training loss 0.018500\n",
      ">> Epoch 47 finished \tANN training loss 0.016693\n",
      ">> Epoch 48 finished \tANN training loss 0.015336\n",
      ">> Epoch 49 finished \tANN training loss 0.015308\n",
      ">> Epoch 50 finished \tANN training loss 0.014903\n",
      ">> Epoch 51 finished \tANN training loss 0.013932\n",
      ">> Epoch 52 finished \tANN training loss 0.014617\n",
      ">> Epoch 53 finished \tANN training loss 0.013093\n",
      ">> Epoch 54 finished \tANN training loss 0.012403\n",
      ">> Epoch 55 finished \tANN training loss 0.011971\n",
      ">> Epoch 56 finished \tANN training loss 0.011769\n",
      ">> Epoch 57 finished \tANN training loss 0.010903\n",
      ">> Epoch 58 finished \tANN training loss 0.011059\n",
      ">> Epoch 59 finished \tANN training loss 0.010350\n",
      ">> Epoch 60 finished \tANN training loss 0.010125\n",
      ">> Epoch 61 finished \tANN training loss 0.009666\n",
      ">> Epoch 62 finished \tANN training loss 0.010602\n",
      ">> Epoch 63 finished \tANN training loss 0.009305\n",
      ">> Epoch 64 finished \tANN training loss 0.009050\n",
      ">> Epoch 65 finished \tANN training loss 0.008513\n",
      ">> Epoch 66 finished \tANN training loss 0.008434\n",
      ">> Epoch 67 finished \tANN training loss 0.007942\n",
      ">> Epoch 68 finished \tANN training loss 0.008066\n",
      ">> Epoch 69 finished \tANN training loss 0.008938\n",
      ">> Epoch 70 finished \tANN training loss 0.007838\n",
      ">> Epoch 71 finished \tANN training loss 0.007490\n",
      ">> Epoch 72 finished \tANN training loss 0.007038\n",
      ">> Epoch 73 finished \tANN training loss 0.007209\n",
      ">> Epoch 74 finished \tANN training loss 0.006680\n",
      ">> Epoch 75 finished \tANN training loss 0.006607\n",
      ">> Epoch 76 finished \tANN training loss 0.006495\n",
      ">> Epoch 77 finished \tANN training loss 0.005989\n",
      ">> Epoch 78 finished \tANN training loss 0.006087\n",
      ">> Epoch 79 finished \tANN training loss 0.005715\n",
      ">> Epoch 80 finished \tANN training loss 0.006298\n",
      ">> Epoch 81 finished \tANN training loss 0.005512\n",
      ">> Epoch 82 finished \tANN training loss 0.005520\n",
      ">> Epoch 83 finished \tANN training loss 0.005228\n",
      ">> Epoch 84 finished \tANN training loss 0.005405\n",
      ">> Epoch 85 finished \tANN training loss 0.004982\n",
      ">> Epoch 86 finished \tANN training loss 0.005140\n",
      ">> Epoch 87 finished \tANN training loss 0.005102\n",
      ">> Epoch 88 finished \tANN training loss 0.004754\n",
      ">> Epoch 89 finished \tANN training loss 0.004679\n",
      ">> Epoch 90 finished \tANN training loss 0.004527\n",
      ">> Epoch 91 finished \tANN training loss 0.004462\n",
      ">> Epoch 92 finished \tANN training loss 0.004584\n",
      ">> Epoch 93 finished \tANN training loss 0.004337\n",
      ">> Epoch 94 finished \tANN training loss 0.004321\n",
      ">> Epoch 95 finished \tANN training loss 0.003943\n",
      ">> Epoch 96 finished \tANN training loss 0.004028\n",
      ">> Epoch 97 finished \tANN training loss 0.003730\n",
      ">> Epoch 98 finished \tANN training loss 0.003666\n",
      ">> Epoch 99 finished \tANN training loss 0.003641\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.885000\n"
     ]
    }
   ],
   "source": [
    "onelayer_acc[4] = deep_belief_net(hidden_layers_structure=[500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.885\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(onelayer_acc[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.90000000000000002, 0.91000000000000003, 0.91000000000000003, 0.90500000000000003, 0.88500000000000001]\n",
      "Most accurate 1-layer setting is Setting 2\n"
     ]
    }
   ],
   "source": [
    "# Collated results\n",
    "print(onelayer_acc)\n",
    "print('Most accurate 1-layer setting is Setting ' + str(onelayer_acc.index(max(onelayer_acc)) + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1cd026628d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create bar graph\n",
    "plt.clf()\n",
    "objects = ('[100]', '[200]', '[300]', '[400]', '[500]')\n",
    "y_pos = np.arange(len(objects))\n",
    "performance = [0.9,0.91,0.91,0.905,0.885]\n",
    " \n",
    "plt.bar(y_pos, performance, align='center', alpha=0.5)\n",
    "plt.xticks(y_pos, objects)\n",
    "plt.xlabel('Layer Settings')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('One-layer Settings')\n",
    " \n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different 2-layer structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store accuracies\n",
    "twolayer_acc = [0, 0, 0, 0, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing layer_1=100 performance:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setting 1 – (100) on 200**\n",
    "\n",
    "    hidden_layers_structure=[100, 200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 43.735626\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 69.860291\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 52.413918\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 61.957706\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 58.830509\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 55.932449\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 53.635048\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 79.695244\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 65.988762\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 81.554207\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 360.266235\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 431.059296\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 448.964172\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 555.941589\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 502.545776\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 604.778809\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 668.629089\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 631.405762\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 762.210938\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 780.738342\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.710166\n",
      ">> Epoch 1 finished \tANN training loss 0.513591\n",
      ">> Epoch 2 finished \tANN training loss 0.398980\n",
      ">> Epoch 3 finished \tANN training loss 0.339667\n",
      ">> Epoch 4 finished \tANN training loss 0.307840\n",
      ">> Epoch 5 finished \tANN training loss 0.261378\n",
      ">> Epoch 6 finished \tANN training loss 0.264133\n",
      ">> Epoch 7 finished \tANN training loss 0.238512\n",
      ">> Epoch 8 finished \tANN training loss 0.195845\n",
      ">> Epoch 9 finished \tANN training loss 0.176364\n",
      ">> Epoch 10 finished \tANN training loss 0.152304\n",
      ">> Epoch 11 finished \tANN training loss 0.137684\n",
      ">> Epoch 12 finished \tANN training loss 0.128414\n",
      ">> Epoch 13 finished \tANN training loss 0.117642\n",
      ">> Epoch 14 finished \tANN training loss 0.105372\n",
      ">> Epoch 15 finished \tANN training loss 0.103730\n",
      ">> Epoch 16 finished \tANN training loss 0.087785\n",
      ">> Epoch 17 finished \tANN training loss 0.073750\n",
      ">> Epoch 18 finished \tANN training loss 0.066918\n",
      ">> Epoch 19 finished \tANN training loss 0.071870\n",
      ">> Epoch 20 finished \tANN training loss 0.057518\n",
      ">> Epoch 21 finished \tANN training loss 0.051202\n",
      ">> Epoch 22 finished \tANN training loss 0.055200\n",
      ">> Epoch 23 finished \tANN training loss 0.048699\n",
      ">> Epoch 24 finished \tANN training loss 0.040021\n",
      ">> Epoch 25 finished \tANN training loss 0.037071\n",
      ">> Epoch 26 finished \tANN training loss 0.034395\n",
      ">> Epoch 27 finished \tANN training loss 0.030486\n",
      ">> Epoch 28 finished \tANN training loss 0.037661\n",
      ">> Epoch 29 finished \tANN training loss 0.028354\n",
      ">> Epoch 30 finished \tANN training loss 0.028711\n",
      ">> Epoch 31 finished \tANN training loss 0.022794\n",
      ">> Epoch 32 finished \tANN training loss 0.021876\n",
      ">> Epoch 33 finished \tANN training loss 0.022532\n",
      ">> Epoch 34 finished \tANN training loss 0.018598\n",
      ">> Epoch 35 finished \tANN training loss 0.021609\n",
      ">> Epoch 36 finished \tANN training loss 0.021850\n",
      ">> Epoch 37 finished \tANN training loss 0.017889\n",
      ">> Epoch 38 finished \tANN training loss 0.017366\n",
      ">> Epoch 39 finished \tANN training loss 0.018041\n",
      ">> Epoch 40 finished \tANN training loss 0.016547\n",
      ">> Epoch 41 finished \tANN training loss 0.015665\n",
      ">> Epoch 42 finished \tANN training loss 0.014554\n",
      ">> Epoch 43 finished \tANN training loss 0.012336\n",
      ">> Epoch 44 finished \tANN training loss 0.010441\n",
      ">> Epoch 45 finished \tANN training loss 0.010772\n",
      ">> Epoch 46 finished \tANN training loss 0.011054\n",
      ">> Epoch 47 finished \tANN training loss 0.010722\n",
      ">> Epoch 48 finished \tANN training loss 0.010356\n",
      ">> Epoch 49 finished \tANN training loss 0.010196\n",
      ">> Epoch 50 finished \tANN training loss 0.009778\n",
      ">> Epoch 51 finished \tANN training loss 0.011873\n",
      ">> Epoch 52 finished \tANN training loss 0.008751\n",
      ">> Epoch 53 finished \tANN training loss 0.008284\n",
      ">> Epoch 54 finished \tANN training loss 0.007183\n",
      ">> Epoch 55 finished \tANN training loss 0.007568\n",
      ">> Epoch 56 finished \tANN training loss 0.006002\n",
      ">> Epoch 57 finished \tANN training loss 0.005474\n",
      ">> Epoch 58 finished \tANN training loss 0.007555\n",
      ">> Epoch 59 finished \tANN training loss 0.006181\n",
      ">> Epoch 60 finished \tANN training loss 0.004627\n",
      ">> Epoch 61 finished \tANN training loss 0.005255\n",
      ">> Epoch 62 finished \tANN training loss 0.005082\n",
      ">> Epoch 63 finished \tANN training loss 0.004754\n",
      ">> Epoch 64 finished \tANN training loss 0.004437\n",
      ">> Epoch 65 finished \tANN training loss 0.004095\n",
      ">> Epoch 66 finished \tANN training loss 0.003809\n",
      ">> Epoch 67 finished \tANN training loss 0.003604\n",
      ">> Epoch 68 finished \tANN training loss 0.003691\n",
      ">> Epoch 69 finished \tANN training loss 0.003116\n",
      ">> Epoch 70 finished \tANN training loss 0.003221\n",
      ">> Epoch 71 finished \tANN training loss 0.003310\n",
      ">> Epoch 72 finished \tANN training loss 0.003188\n",
      ">> Epoch 73 finished \tANN training loss 0.003459\n",
      ">> Epoch 74 finished \tANN training loss 0.003284\n",
      ">> Epoch 75 finished \tANN training loss 0.002931\n",
      ">> Epoch 76 finished \tANN training loss 0.003030\n",
      ">> Epoch 77 finished \tANN training loss 0.003313\n",
      ">> Epoch 78 finished \tANN training loss 0.002980\n",
      ">> Epoch 79 finished \tANN training loss 0.002975\n",
      ">> Epoch 80 finished \tANN training loss 0.002494\n",
      ">> Epoch 81 finished \tANN training loss 0.002394\n",
      ">> Epoch 82 finished \tANN training loss 0.002521\n",
      ">> Epoch 83 finished \tANN training loss 0.003384\n",
      ">> Epoch 84 finished \tANN training loss 0.002047\n",
      ">> Epoch 85 finished \tANN training loss 0.002407\n",
      ">> Epoch 86 finished \tANN training loss 0.002197\n",
      ">> Epoch 87 finished \tANN training loss 0.001881\n",
      ">> Epoch 88 finished \tANN training loss 0.001671\n",
      ">> Epoch 89 finished \tANN training loss 0.002061\n",
      ">> Epoch 90 finished \tANN training loss 0.001710\n",
      ">> Epoch 91 finished \tANN training loss 0.002201\n",
      ">> Epoch 92 finished \tANN training loss 0.001971\n",
      ">> Epoch 93 finished \tANN training loss 0.002462\n",
      ">> Epoch 94 finished \tANN training loss 0.001763\n",
      ">> Epoch 95 finished \tANN training loss 0.001852\n",
      ">> Epoch 96 finished \tANN training loss 0.001465\n",
      ">> Epoch 97 finished \tANN training loss 0.001718\n",
      ">> Epoch 98 finished \tANN training loss 0.001340\n",
      ">> Epoch 99 finished \tANN training loss 0.001242\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.905000\n"
     ]
    }
   ],
   "source": [
    "twolayer_acc[0] = deep_belief_net(hidden_layers_structure=[100, 200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.905\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(twolayer_acc[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setting 2 – (100) on 500**\n",
    "\n",
    "    hidden_layers_structure=[100, 500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 77.752686\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 52.723736\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 53.163139\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 52.002865\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 67.212433\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 57.364613\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 70.963165\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 53.970383\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 76.276886\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 66.696815\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 142.798584\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 265.637207\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 189.706680\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 182.475662\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 246.715027\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 322.739990\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 283.961273\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 416.864136\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 503.759216\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 375.786560\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.725721\n",
      ">> Epoch 1 finished \tANN training loss 0.555961\n",
      ">> Epoch 2 finished \tANN training loss 0.455945\n",
      ">> Epoch 3 finished \tANN training loss 0.362099\n",
      ">> Epoch 4 finished \tANN training loss 0.304426\n",
      ">> Epoch 5 finished \tANN training loss 0.287528\n",
      ">> Epoch 6 finished \tANN training loss 0.266063\n",
      ">> Epoch 7 finished \tANN training loss 0.209929\n",
      ">> Epoch 8 finished \tANN training loss 0.197395\n",
      ">> Epoch 9 finished \tANN training loss 0.192417\n",
      ">> Epoch 10 finished \tANN training loss 0.157767\n",
      ">> Epoch 11 finished \tANN training loss 0.152469\n",
      ">> Epoch 12 finished \tANN training loss 0.123194\n",
      ">> Epoch 13 finished \tANN training loss 0.117787\n",
      ">> Epoch 14 finished \tANN training loss 0.103535\n",
      ">> Epoch 15 finished \tANN training loss 0.099334\n",
      ">> Epoch 16 finished \tANN training loss 0.088828\n",
      ">> Epoch 17 finished \tANN training loss 0.084695\n",
      ">> Epoch 18 finished \tANN training loss 0.070887\n",
      ">> Epoch 19 finished \tANN training loss 0.071093\n",
      ">> Epoch 20 finished \tANN training loss 0.066874\n",
      ">> Epoch 21 finished \tANN training loss 0.058676\n",
      ">> Epoch 22 finished \tANN training loss 0.047950\n",
      ">> Epoch 23 finished \tANN training loss 0.040768\n",
      ">> Epoch 24 finished \tANN training loss 0.041859\n",
      ">> Epoch 25 finished \tANN training loss 0.038428\n",
      ">> Epoch 26 finished \tANN training loss 0.034559\n",
      ">> Epoch 27 finished \tANN training loss 0.034939\n",
      ">> Epoch 28 finished \tANN training loss 0.034862\n",
      ">> Epoch 29 finished \tANN training loss 0.031442\n",
      ">> Epoch 30 finished \tANN training loss 0.026234\n",
      ">> Epoch 31 finished \tANN training loss 0.024811\n",
      ">> Epoch 32 finished \tANN training loss 0.024139\n",
      ">> Epoch 33 finished \tANN training loss 0.025639\n",
      ">> Epoch 34 finished \tANN training loss 0.018450\n",
      ">> Epoch 35 finished \tANN training loss 0.019408\n",
      ">> Epoch 36 finished \tANN training loss 0.015877\n",
      ">> Epoch 37 finished \tANN training loss 0.014950\n",
      ">> Epoch 38 finished \tANN training loss 0.015253\n",
      ">> Epoch 39 finished \tANN training loss 0.014468\n",
      ">> Epoch 40 finished \tANN training loss 0.028219\n",
      ">> Epoch 41 finished \tANN training loss 0.012077\n",
      ">> Epoch 42 finished \tANN training loss 0.012430\n",
      ">> Epoch 43 finished \tANN training loss 0.011440\n",
      ">> Epoch 44 finished \tANN training loss 0.009415\n",
      ">> Epoch 45 finished \tANN training loss 0.010727\n",
      ">> Epoch 46 finished \tANN training loss 0.009735\n",
      ">> Epoch 47 finished \tANN training loss 0.009666\n",
      ">> Epoch 48 finished \tANN training loss 0.008429\n",
      ">> Epoch 49 finished \tANN training loss 0.011000\n",
      ">> Epoch 50 finished \tANN training loss 0.007966\n",
      ">> Epoch 51 finished \tANN training loss 0.009881\n",
      ">> Epoch 52 finished \tANN training loss 0.008241\n",
      ">> Epoch 53 finished \tANN training loss 0.009118\n",
      ">> Epoch 54 finished \tANN training loss 0.007182\n",
      ">> Epoch 55 finished \tANN training loss 0.005784\n",
      ">> Epoch 56 finished \tANN training loss 0.005853\n",
      ">> Epoch 57 finished \tANN training loss 0.005524\n",
      ">> Epoch 58 finished \tANN training loss 0.005711\n",
      ">> Epoch 59 finished \tANN training loss 0.005407\n",
      ">> Epoch 60 finished \tANN training loss 0.005191\n",
      ">> Epoch 61 finished \tANN training loss 0.004779\n",
      ">> Epoch 62 finished \tANN training loss 0.004540\n",
      ">> Epoch 63 finished \tANN training loss 0.004728\n",
      ">> Epoch 64 finished \tANN training loss 0.003845\n",
      ">> Epoch 65 finished \tANN training loss 0.004285\n",
      ">> Epoch 66 finished \tANN training loss 0.004571\n",
      ">> Epoch 67 finished \tANN training loss 0.004412\n",
      ">> Epoch 68 finished \tANN training loss 0.003766\n",
      ">> Epoch 69 finished \tANN training loss 0.003805\n",
      ">> Epoch 70 finished \tANN training loss 0.003945\n",
      ">> Epoch 71 finished \tANN training loss 0.003950\n",
      ">> Epoch 72 finished \tANN training loss 0.003225\n",
      ">> Epoch 73 finished \tANN training loss 0.003514\n",
      ">> Epoch 74 finished \tANN training loss 0.003312\n",
      ">> Epoch 75 finished \tANN training loss 0.003424\n",
      ">> Epoch 76 finished \tANN training loss 0.002760\n",
      ">> Epoch 77 finished \tANN training loss 0.002403\n",
      ">> Epoch 78 finished \tANN training loss 0.003159\n",
      ">> Epoch 79 finished \tANN training loss 0.002689\n",
      ">> Epoch 80 finished \tANN training loss 0.001958\n",
      ">> Epoch 81 finished \tANN training loss 0.002782\n",
      ">> Epoch 82 finished \tANN training loss 0.002177\n",
      ">> Epoch 83 finished \tANN training loss 0.002044\n",
      ">> Epoch 84 finished \tANN training loss 0.002169\n",
      ">> Epoch 85 finished \tANN training loss 0.001879\n",
      ">> Epoch 86 finished \tANN training loss 0.002688\n",
      ">> Epoch 87 finished \tANN training loss 0.003280\n",
      ">> Epoch 88 finished \tANN training loss 0.002178\n",
      ">> Epoch 89 finished \tANN training loss 0.002332\n",
      ">> Epoch 90 finished \tANN training loss 0.002143\n",
      ">> Epoch 91 finished \tANN training loss 0.001624\n",
      ">> Epoch 92 finished \tANN training loss 0.001732\n",
      ">> Epoch 93 finished \tANN training loss 0.001514\n",
      ">> Epoch 94 finished \tANN training loss 0.001734\n",
      ">> Epoch 95 finished \tANN training loss 0.001677\n",
      ">> Epoch 96 finished \tANN training loss 0.001513\n",
      ">> Epoch 97 finished \tANN training loss 0.001382\n",
      ">> Epoch 98 finished \tANN training loss 0.001374\n",
      ">> Epoch 99 finished \tANN training loss 0.001510\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.920000\n"
     ]
    }
   ],
   "source": [
    "twolayer_acc[1] = deep_belief_net(hidden_layers_structure=[100, 500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.92\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(twolayer_acc[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing layer_1=500 performance:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setting 3 – (500) on 200**\n",
    "\n",
    "    hidden_layers_structure=[500, 200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 89.590607\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 37.799076\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 57.134327\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 54.106468\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 41.545689\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 28.666601\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 27.435949\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 40.246414\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 21.926275\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 33.239376\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 98.059181\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 174.640564\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 166.056488\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 173.509491\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 190.788391\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 228.330200\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 243.219604\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 243.815216\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 288.703979\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 325.087891\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.739812\n",
      ">> Epoch 1 finished \tANN training loss 0.454461\n",
      ">> Epoch 2 finished \tANN training loss 0.383898\n",
      ">> Epoch 3 finished \tANN training loss 0.338644\n",
      ">> Epoch 4 finished \tANN training loss 0.304319\n",
      ">> Epoch 5 finished \tANN training loss 0.244198\n",
      ">> Epoch 6 finished \tANN training loss 0.224944\n",
      ">> Epoch 7 finished \tANN training loss 0.199082\n",
      ">> Epoch 8 finished \tANN training loss 0.182059\n",
      ">> Epoch 9 finished \tANN training loss 0.151635\n",
      ">> Epoch 10 finished \tANN training loss 0.144171\n",
      ">> Epoch 11 finished \tANN training loss 0.115367\n",
      ">> Epoch 12 finished \tANN training loss 0.117670\n",
      ">> Epoch 13 finished \tANN training loss 0.121261\n",
      ">> Epoch 14 finished \tANN training loss 0.085923\n",
      ">> Epoch 15 finished \tANN training loss 0.075368\n",
      ">> Epoch 16 finished \tANN training loss 0.071203\n",
      ">> Epoch 17 finished \tANN training loss 0.061213\n",
      ">> Epoch 18 finished \tANN training loss 0.053001\n",
      ">> Epoch 19 finished \tANN training loss 0.048034\n",
      ">> Epoch 20 finished \tANN training loss 0.043710\n",
      ">> Epoch 21 finished \tANN training loss 0.039093\n",
      ">> Epoch 22 finished \tANN training loss 0.041623\n",
      ">> Epoch 23 finished \tANN training loss 0.031440\n",
      ">> Epoch 24 finished \tANN training loss 0.029539\n",
      ">> Epoch 25 finished \tANN training loss 0.027321\n",
      ">> Epoch 26 finished \tANN training loss 0.024297\n",
      ">> Epoch 27 finished \tANN training loss 0.021936\n",
      ">> Epoch 28 finished \tANN training loss 0.021124\n",
      ">> Epoch 29 finished \tANN training loss 0.024465\n",
      ">> Epoch 30 finished \tANN training loss 0.018379\n",
      ">> Epoch 31 finished \tANN training loss 0.016287\n",
      ">> Epoch 32 finished \tANN training loss 0.014820\n",
      ">> Epoch 33 finished \tANN training loss 0.015014\n",
      ">> Epoch 34 finished \tANN training loss 0.012345\n",
      ">> Epoch 35 finished \tANN training loss 0.012518\n",
      ">> Epoch 36 finished \tANN training loss 0.012737\n",
      ">> Epoch 37 finished \tANN training loss 0.011847\n",
      ">> Epoch 38 finished \tANN training loss 0.010565\n",
      ">> Epoch 39 finished \tANN training loss 0.009242\n",
      ">> Epoch 40 finished \tANN training loss 0.009036\n",
      ">> Epoch 41 finished \tANN training loss 0.009443\n",
      ">> Epoch 42 finished \tANN training loss 0.008202\n",
      ">> Epoch 43 finished \tANN training loss 0.011443\n",
      ">> Epoch 44 finished \tANN training loss 0.006845\n",
      ">> Epoch 45 finished \tANN training loss 0.006734\n",
      ">> Epoch 46 finished \tANN training loss 0.005756\n",
      ">> Epoch 47 finished \tANN training loss 0.005248\n",
      ">> Epoch 48 finished \tANN training loss 0.006864\n",
      ">> Epoch 49 finished \tANN training loss 0.005188\n",
      ">> Epoch 50 finished \tANN training loss 0.005877\n",
      ">> Epoch 51 finished \tANN training loss 0.004740\n",
      ">> Epoch 52 finished \tANN training loss 0.004359\n",
      ">> Epoch 53 finished \tANN training loss 0.004120\n",
      ">> Epoch 54 finished \tANN training loss 0.004402\n",
      ">> Epoch 55 finished \tANN training loss 0.004480\n",
      ">> Epoch 56 finished \tANN training loss 0.003810\n",
      ">> Epoch 57 finished \tANN training loss 0.003917\n",
      ">> Epoch 58 finished \tANN training loss 0.003295\n",
      ">> Epoch 59 finished \tANN training loss 0.002996\n",
      ">> Epoch 60 finished \tANN training loss 0.002722\n",
      ">> Epoch 61 finished \tANN training loss 0.002538\n",
      ">> Epoch 62 finished \tANN training loss 0.002431\n",
      ">> Epoch 63 finished \tANN training loss 0.002842\n",
      ">> Epoch 64 finished \tANN training loss 0.002769\n",
      ">> Epoch 65 finished \tANN training loss 0.002154\n",
      ">> Epoch 66 finished \tANN training loss 0.002520\n",
      ">> Epoch 67 finished \tANN training loss 0.001927\n",
      ">> Epoch 68 finished \tANN training loss 0.001995\n",
      ">> Epoch 69 finished \tANN training loss 0.002096\n",
      ">> Epoch 70 finished \tANN training loss 0.001781\n",
      ">> Epoch 71 finished \tANN training loss 0.001815\n",
      ">> Epoch 72 finished \tANN training loss 0.001487\n",
      ">> Epoch 73 finished \tANN training loss 0.001612\n",
      ">> Epoch 74 finished \tANN training loss 0.001554\n",
      ">> Epoch 75 finished \tANN training loss 0.001294\n",
      ">> Epoch 76 finished \tANN training loss 0.001397\n",
      ">> Epoch 77 finished \tANN training loss 0.002263\n",
      ">> Epoch 78 finished \tANN training loss 0.001267\n",
      ">> Epoch 79 finished \tANN training loss 0.001496\n",
      ">> Epoch 80 finished \tANN training loss 0.001183\n",
      ">> Epoch 81 finished \tANN training loss 0.001197\n",
      ">> Epoch 82 finished \tANN training loss 0.001516\n",
      ">> Epoch 83 finished \tANN training loss 0.001149\n",
      ">> Epoch 84 finished \tANN training loss 0.001015\n",
      ">> Epoch 85 finished \tANN training loss 0.001055\n",
      ">> Epoch 86 finished \tANN training loss 0.000982\n",
      ">> Epoch 87 finished \tANN training loss 0.000952\n",
      ">> Epoch 88 finished \tANN training loss 0.001089\n",
      ">> Epoch 89 finished \tANN training loss 0.001359\n",
      ">> Epoch 90 finished \tANN training loss 0.000992\n",
      ">> Epoch 91 finished \tANN training loss 0.001130\n",
      ">> Epoch 92 finished \tANN training loss 0.000978\n",
      ">> Epoch 93 finished \tANN training loss 0.000927\n",
      ">> Epoch 94 finished \tANN training loss 0.001100\n",
      ">> Epoch 95 finished \tANN training loss 0.000900\n",
      ">> Epoch 96 finished \tANN training loss 0.001113\n",
      ">> Epoch 97 finished \tANN training loss 0.000988\n",
      ">> Epoch 98 finished \tANN training loss 0.001640\n",
      ">> Epoch 99 finished \tANN training loss 0.001042\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.890000\n"
     ]
    }
   ],
   "source": [
    "twolayer_acc[2] = deep_belief_net(hidden_layers_structure=[500, 200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.89\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(twolayer_acc[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setting 4 – (500) on 500**\n",
    "\n",
    "    hidden_layers_structure=[500, 500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 46.529064\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 66.733582\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 51.008633\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 34.017078\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 30.529028\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 27.276186\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 28.727407\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 43.306202\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 33.214878\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 35.673023\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 113.897949\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 122.613113\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 76.815407\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 100.304230\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 176.164490\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 117.903763\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 118.125267\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 63.958897\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 135.292709\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 98.156700\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.678442\n",
      ">> Epoch 1 finished \tANN training loss 0.424808\n",
      ">> Epoch 2 finished \tANN training loss 0.375037\n",
      ">> Epoch 3 finished \tANN training loss 0.296220\n",
      ">> Epoch 4 finished \tANN training loss 0.246093\n",
      ">> Epoch 5 finished \tANN training loss 0.224821\n",
      ">> Epoch 6 finished \tANN training loss 0.198668\n",
      ">> Epoch 7 finished \tANN training loss 0.177970\n",
      ">> Epoch 8 finished \tANN training loss 0.159623\n",
      ">> Epoch 9 finished \tANN training loss 0.133888\n",
      ">> Epoch 10 finished \tANN training loss 0.128539\n",
      ">> Epoch 11 finished \tANN training loss 0.107228\n",
      ">> Epoch 12 finished \tANN training loss 0.100005\n",
      ">> Epoch 13 finished \tANN training loss 0.088832\n",
      ">> Epoch 14 finished \tANN training loss 0.082100\n",
      ">> Epoch 15 finished \tANN training loss 0.067345\n",
      ">> Epoch 16 finished \tANN training loss 0.059807\n",
      ">> Epoch 17 finished \tANN training loss 0.052954\n",
      ">> Epoch 18 finished \tANN training loss 0.047900\n",
      ">> Epoch 19 finished \tANN training loss 0.045992\n",
      ">> Epoch 20 finished \tANN training loss 0.039242\n",
      ">> Epoch 21 finished \tANN training loss 0.038365\n",
      ">> Epoch 22 finished \tANN training loss 0.032795\n",
      ">> Epoch 23 finished \tANN training loss 0.029510\n",
      ">> Epoch 24 finished \tANN training loss 0.026360\n",
      ">> Epoch 25 finished \tANN training loss 0.026042\n",
      ">> Epoch 26 finished \tANN training loss 0.027286\n",
      ">> Epoch 27 finished \tANN training loss 0.020565\n",
      ">> Epoch 28 finished \tANN training loss 0.019415\n",
      ">> Epoch 29 finished \tANN training loss 0.016214\n",
      ">> Epoch 30 finished \tANN training loss 0.016160\n",
      ">> Epoch 31 finished \tANN training loss 0.017187\n",
      ">> Epoch 32 finished \tANN training loss 0.014296\n",
      ">> Epoch 33 finished \tANN training loss 0.014186\n",
      ">> Epoch 34 finished \tANN training loss 0.011290\n",
      ">> Epoch 35 finished \tANN training loss 0.010938\n",
      ">> Epoch 36 finished \tANN training loss 0.011059\n",
      ">> Epoch 37 finished \tANN training loss 0.009930\n",
      ">> Epoch 38 finished \tANN training loss 0.010398\n",
      ">> Epoch 39 finished \tANN training loss 0.008298\n",
      ">> Epoch 40 finished \tANN training loss 0.007541\n",
      ">> Epoch 41 finished \tANN training loss 0.007224\n",
      ">> Epoch 42 finished \tANN training loss 0.006695\n",
      ">> Epoch 43 finished \tANN training loss 0.008653\n",
      ">> Epoch 44 finished \tANN training loss 0.006183\n",
      ">> Epoch 45 finished \tANN training loss 0.006785\n",
      ">> Epoch 46 finished \tANN training loss 0.005258\n",
      ">> Epoch 47 finished \tANN training loss 0.005046\n",
      ">> Epoch 48 finished \tANN training loss 0.005256\n",
      ">> Epoch 49 finished \tANN training loss 0.005673\n",
      ">> Epoch 50 finished \tANN training loss 0.004570\n",
      ">> Epoch 51 finished \tANN training loss 0.004244\n",
      ">> Epoch 52 finished \tANN training loss 0.004006\n",
      ">> Epoch 53 finished \tANN training loss 0.003882\n",
      ">> Epoch 54 finished \tANN training loss 0.003236\n",
      ">> Epoch 55 finished \tANN training loss 0.003414\n",
      ">> Epoch 56 finished \tANN training loss 0.003563\n",
      ">> Epoch 57 finished \tANN training loss 0.003114\n",
      ">> Epoch 58 finished \tANN training loss 0.003044\n",
      ">> Epoch 59 finished \tANN training loss 0.003371\n",
      ">> Epoch 60 finished \tANN training loss 0.002759\n",
      ">> Epoch 61 finished \tANN training loss 0.003110\n",
      ">> Epoch 62 finished \tANN training loss 0.002729\n",
      ">> Epoch 63 finished \tANN training loss 0.002817\n",
      ">> Epoch 64 finished \tANN training loss 0.002660\n",
      ">> Epoch 65 finished \tANN training loss 0.002363\n",
      ">> Epoch 66 finished \tANN training loss 0.002338\n",
      ">> Epoch 67 finished \tANN training loss 0.002152\n",
      ">> Epoch 68 finished \tANN training loss 0.002214\n",
      ">> Epoch 69 finished \tANN training loss 0.002184\n",
      ">> Epoch 70 finished \tANN training loss 0.002136\n",
      ">> Epoch 71 finished \tANN training loss 0.002225\n",
      ">> Epoch 72 finished \tANN training loss 0.002041\n",
      ">> Epoch 73 finished \tANN training loss 0.001975\n",
      ">> Epoch 74 finished \tANN training loss 0.002181\n",
      ">> Epoch 75 finished \tANN training loss 0.001712\n",
      ">> Epoch 76 finished \tANN training loss 0.001743\n",
      ">> Epoch 77 finished \tANN training loss 0.001605\n",
      ">> Epoch 78 finished \tANN training loss 0.001575\n",
      ">> Epoch 79 finished \tANN training loss 0.001453\n",
      ">> Epoch 80 finished \tANN training loss 0.001423\n",
      ">> Epoch 81 finished \tANN training loss 0.001296\n",
      ">> Epoch 82 finished \tANN training loss 0.001214\n",
      ">> Epoch 83 finished \tANN training loss 0.001397\n",
      ">> Epoch 84 finished \tANN training loss 0.001072\n",
      ">> Epoch 85 finished \tANN training loss 0.001148\n",
      ">> Epoch 86 finished \tANN training loss 0.001103\n",
      ">> Epoch 87 finished \tANN training loss 0.001128\n",
      ">> Epoch 88 finished \tANN training loss 0.000970\n",
      ">> Epoch 89 finished \tANN training loss 0.000958\n",
      ">> Epoch 90 finished \tANN training loss 0.001050\n",
      ">> Epoch 91 finished \tANN training loss 0.000907\n",
      ">> Epoch 92 finished \tANN training loss 0.001002\n",
      ">> Epoch 93 finished \tANN training loss 0.000934\n",
      ">> Epoch 94 finished \tANN training loss 0.000908\n",
      ">> Epoch 95 finished \tANN training loss 0.000919\n",
      ">> Epoch 96 finished \tANN training loss 0.000895\n",
      ">> Epoch 97 finished \tANN training loss 0.001143\n",
      ">> Epoch 98 finished \tANN training loss 0.001023\n",
      ">> Epoch 99 finished \tANN training loss 0.000763\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.890000\n"
     ]
    }
   ],
   "source": [
    "twolayer_acc[3] = deep_belief_net(hidden_layers_structure=[500, 500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.89\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(twolayer_acc[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.90500000000000003, 0.92000000000000004, 0.89000000000000001, 0.89000000000000001, 0.90500000000000003, 0.89000000000000001, 0.92000000000000004, 0.89000000000000001]\n",
      "Most accurate 2-layer setting is Setting 2\n"
     ]
    }
   ],
   "source": [
    "# Collated results\n",
    "print(twolayer_acc)\n",
    "print('Most accurate 2-layer setting is Setting ' + str(twolayer_acc.index(max(twolayer_acc)) + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1cd039f5940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create bar graph\n",
    "plt.clf()\n",
    "objects = ('[100, 200]', '[100, 500]', '[500, 200]', '[500, 500]')\n",
    "y_pos = np.arange(len(objects))\n",
    "performance = [0.905,0.92,0.89,0.89]\n",
    "\n",
    "plt.bar(y_pos, performance, align='center', alpha=0.5)\n",
    "plt.xticks(y_pos, objects)\n",
    "plt.xlabel('Layer Settings')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Two-layer Settings')\n",
    " \n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different 3-layer structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store accuracies\n",
    "threelayer_acc = [0, 0, 0, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setting 1 – Baseline**\n",
    "\n",
    "    hidden_layers_structure=[100, 200, 300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 48.776649\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 77.958260\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 96.530106\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 82.910736\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 116.125664\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 90.105782\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 66.176308\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 96.009384\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 87.633675\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 70.791771\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 247.282806\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 292.829712\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 302.180573\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 377.232025\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 518.424988\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 440.123657\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 448.164154\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 539.442139\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 556.956604\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 612.908630\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 18618.408203\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 17771.822266\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 16998.207031\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 21979.873047\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 27382.714844\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 39169.394531\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 44458.195312\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 41281.765625\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 48748.398438\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 37989.843750\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 1.048735\n",
      ">> Epoch 1 finished \tANN training loss 0.718359\n",
      ">> Epoch 2 finished \tANN training loss 0.557308\n",
      ">> Epoch 3 finished \tANN training loss 0.523559\n",
      ">> Epoch 4 finished \tANN training loss 0.390225\n",
      ">> Epoch 5 finished \tANN training loss 0.318191\n",
      ">> Epoch 6 finished \tANN training loss 0.276111\n",
      ">> Epoch 7 finished \tANN training loss 0.239038\n",
      ">> Epoch 8 finished \tANN training loss 0.222058\n",
      ">> Epoch 9 finished \tANN training loss 0.190678\n",
      ">> Epoch 10 finished \tANN training loss 0.147539\n",
      ">> Epoch 11 finished \tANN training loss 0.140231\n",
      ">> Epoch 12 finished \tANN training loss 0.120642\n",
      ">> Epoch 13 finished \tANN training loss 0.108311\n",
      ">> Epoch 14 finished \tANN training loss 0.081935\n",
      ">> Epoch 15 finished \tANN training loss 0.076665\n",
      ">> Epoch 16 finished \tANN training loss 0.084998\n",
      ">> Epoch 17 finished \tANN training loss 0.067256\n",
      ">> Epoch 18 finished \tANN training loss 0.057310\n",
      ">> Epoch 19 finished \tANN training loss 0.047843\n",
      ">> Epoch 20 finished \tANN training loss 0.053459\n",
      ">> Epoch 21 finished \tANN training loss 0.044084\n",
      ">> Epoch 22 finished \tANN training loss 0.053099\n",
      ">> Epoch 23 finished \tANN training loss 0.035367\n",
      ">> Epoch 24 finished \tANN training loss 0.033165\n",
      ">> Epoch 25 finished \tANN training loss 0.037883\n",
      ">> Epoch 26 finished \tANN training loss 0.029208\n",
      ">> Epoch 27 finished \tANN training loss 0.047251\n",
      ">> Epoch 28 finished \tANN training loss 0.024188\n",
      ">> Epoch 29 finished \tANN training loss 0.026445\n",
      ">> Epoch 30 finished \tANN training loss 0.019255\n",
      ">> Epoch 31 finished \tANN training loss 0.024286\n",
      ">> Epoch 32 finished \tANN training loss 0.019467\n",
      ">> Epoch 33 finished \tANN training loss 0.017768\n",
      ">> Epoch 34 finished \tANN training loss 0.022086\n",
      ">> Epoch 35 finished \tANN training loss 0.016425\n",
      ">> Epoch 36 finished \tANN training loss 0.017146\n",
      ">> Epoch 37 finished \tANN training loss 0.020152\n",
      ">> Epoch 38 finished \tANN training loss 0.011766\n",
      ">> Epoch 39 finished \tANN training loss 0.009566\n",
      ">> Epoch 40 finished \tANN training loss 0.015626\n",
      ">> Epoch 41 finished \tANN training loss 0.010250\n",
      ">> Epoch 42 finished \tANN training loss 0.011436\n",
      ">> Epoch 43 finished \tANN training loss 0.012583\n",
      ">> Epoch 44 finished \tANN training loss 0.026032\n",
      ">> Epoch 45 finished \tANN training loss 0.012718\n",
      ">> Epoch 46 finished \tANN training loss 0.008813\n",
      ">> Epoch 47 finished \tANN training loss 0.012629\n",
      ">> Epoch 48 finished \tANN training loss 0.007918\n",
      ">> Epoch 49 finished \tANN training loss 0.008212\n",
      ">> Epoch 50 finished \tANN training loss 0.011280\n",
      ">> Epoch 51 finished \tANN training loss 0.010441\n",
      ">> Epoch 52 finished \tANN training loss 0.005996\n",
      ">> Epoch 53 finished \tANN training loss 0.005710\n",
      ">> Epoch 54 finished \tANN training loss 0.006742\n",
      ">> Epoch 55 finished \tANN training loss 0.006059\n",
      ">> Epoch 56 finished \tANN training loss 0.006146\n",
      ">> Epoch 57 finished \tANN training loss 0.004724\n",
      ">> Epoch 58 finished \tANN training loss 0.006122\n",
      ">> Epoch 59 finished \tANN training loss 0.004437\n",
      ">> Epoch 60 finished \tANN training loss 0.003719\n",
      ">> Epoch 61 finished \tANN training loss 0.003949\n",
      ">> Epoch 62 finished \tANN training loss 0.003071\n",
      ">> Epoch 63 finished \tANN training loss 0.003466\n",
      ">> Epoch 64 finished \tANN training loss 0.003300\n",
      ">> Epoch 65 finished \tANN training loss 0.004619\n",
      ">> Epoch 66 finished \tANN training loss 0.005675\n",
      ">> Epoch 67 finished \tANN training loss 0.003084\n",
      ">> Epoch 68 finished \tANN training loss 0.002994\n",
      ">> Epoch 69 finished \tANN training loss 0.003212\n",
      ">> Epoch 70 finished \tANN training loss 0.007424\n",
      ">> Epoch 71 finished \tANN training loss 0.011309\n",
      ">> Epoch 72 finished \tANN training loss 0.003106\n",
      ">> Epoch 73 finished \tANN training loss 0.003080\n",
      ">> Epoch 74 finished \tANN training loss 0.002585\n",
      ">> Epoch 75 finished \tANN training loss 0.002552\n",
      ">> Epoch 76 finished \tANN training loss 0.002590\n",
      ">> Epoch 77 finished \tANN training loss 0.004292\n",
      ">> Epoch 78 finished \tANN training loss 0.002162\n",
      ">> Epoch 79 finished \tANN training loss 0.003070\n",
      ">> Epoch 80 finished \tANN training loss 0.001826\n",
      ">> Epoch 81 finished \tANN training loss 0.001511\n",
      ">> Epoch 82 finished \tANN training loss 0.002201\n",
      ">> Epoch 83 finished \tANN training loss 0.002240\n",
      ">> Epoch 84 finished \tANN training loss 0.002084\n",
      ">> Epoch 85 finished \tANN training loss 0.001534\n",
      ">> Epoch 86 finished \tANN training loss 0.001456\n",
      ">> Epoch 87 finished \tANN training loss 0.001200\n",
      ">> Epoch 88 finished \tANN training loss 0.000910\n",
      ">> Epoch 89 finished \tANN training loss 0.001506\n",
      ">> Epoch 90 finished \tANN training loss 0.001334\n",
      ">> Epoch 91 finished \tANN training loss 0.001055\n",
      ">> Epoch 92 finished \tANN training loss 0.000998\n",
      ">> Epoch 93 finished \tANN training loss 0.001161\n",
      ">> Epoch 94 finished \tANN training loss 0.001203\n",
      ">> Epoch 95 finished \tANN training loss 0.001659\n",
      ">> Epoch 96 finished \tANN training loss 0.000997\n",
      ">> Epoch 97 finished \tANN training loss 0.001004\n",
      ">> Epoch 98 finished \tANN training loss 0.001538\n",
      ">> Epoch 99 finished \tANN training loss 0.000857\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.900000\n"
     ]
    }
   ],
   "source": [
    "threelayer_acc[0] = deep_belief_net(hidden_layers_structure=[100, 200, 300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.9\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(threelayer_acc[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setting 2 – 1st Layer = 500**\n",
    "\n",
    "    hidden_layers_structure=[500, 200, 300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 57.344833\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 65.222092\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 36.743771\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 38.844582\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 44.889423\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 23.421108\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 30.931402\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 40.723019\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 30.118111\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 40.697941\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 281.745117\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 238.984146\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 232.955353\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 247.143829\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 279.849365\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 174.890121\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 195.232620\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 218.132187\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 241.617111\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 261.642883\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 2737.657471\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 3818.996338\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 5488.172363\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 5364.857422\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 7427.385742\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 8336.666992\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 10444.168945\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 11059.750977\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 9636.936523\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 11250.191406\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.703324\n",
      ">> Epoch 1 finished \tANN training loss 0.478931\n",
      ">> Epoch 2 finished \tANN training loss 0.396747\n",
      ">> Epoch 3 finished \tANN training loss 0.310771\n",
      ">> Epoch 4 finished \tANN training loss 0.271994\n",
      ">> Epoch 5 finished \tANN training loss 0.212138\n",
      ">> Epoch 6 finished \tANN training loss 0.169806\n",
      ">> Epoch 7 finished \tANN training loss 0.140385\n",
      ">> Epoch 8 finished \tANN training loss 0.138881\n",
      ">> Epoch 9 finished \tANN training loss 0.128901\n",
      ">> Epoch 10 finished \tANN training loss 0.088348\n",
      ">> Epoch 11 finished \tANN training loss 0.080008\n",
      ">> Epoch 12 finished \tANN training loss 0.069058\n",
      ">> Epoch 13 finished \tANN training loss 0.053698\n",
      ">> Epoch 14 finished \tANN training loss 0.059593\n",
      ">> Epoch 15 finished \tANN training loss 0.044812\n",
      ">> Epoch 16 finished \tANN training loss 0.033827\n",
      ">> Epoch 17 finished \tANN training loss 0.029094\n",
      ">> Epoch 18 finished \tANN training loss 0.033973\n",
      ">> Epoch 19 finished \tANN training loss 0.026899\n",
      ">> Epoch 20 finished \tANN training loss 0.023098\n",
      ">> Epoch 21 finished \tANN training loss 0.045296\n",
      ">> Epoch 22 finished \tANN training loss 0.019485\n",
      ">> Epoch 23 finished \tANN training loss 0.014655\n",
      ">> Epoch 24 finished \tANN training loss 0.014383\n",
      ">> Epoch 25 finished \tANN training loss 0.014686\n",
      ">> Epoch 26 finished \tANN training loss 0.014275\n",
      ">> Epoch 27 finished \tANN training loss 0.010776\n",
      ">> Epoch 28 finished \tANN training loss 0.008901\n",
      ">> Epoch 29 finished \tANN training loss 0.008485\n",
      ">> Epoch 30 finished \tANN training loss 0.007667\n",
      ">> Epoch 31 finished \tANN training loss 0.006525\n",
      ">> Epoch 32 finished \tANN training loss 0.006767\n",
      ">> Epoch 33 finished \tANN training loss 0.006382\n",
      ">> Epoch 34 finished \tANN training loss 0.006764\n",
      ">> Epoch 35 finished \tANN training loss 0.004568\n",
      ">> Epoch 36 finished \tANN training loss 0.005110\n",
      ">> Epoch 37 finished \tANN training loss 0.005352\n",
      ">> Epoch 38 finished \tANN training loss 0.004307\n",
      ">> Epoch 39 finished \tANN training loss 0.003712\n",
      ">> Epoch 40 finished \tANN training loss 0.003149\n",
      ">> Epoch 41 finished \tANN training loss 0.004384\n",
      ">> Epoch 42 finished \tANN training loss 0.002963\n",
      ">> Epoch 43 finished \tANN training loss 0.003058\n",
      ">> Epoch 44 finished \tANN training loss 0.004003\n",
      ">> Epoch 45 finished \tANN training loss 0.002533\n",
      ">> Epoch 46 finished \tANN training loss 0.002238\n",
      ">> Epoch 47 finished \tANN training loss 0.001924\n",
      ">> Epoch 48 finished \tANN training loss 0.002303\n",
      ">> Epoch 49 finished \tANN training loss 0.001635\n",
      ">> Epoch 50 finished \tANN training loss 0.001745\n",
      ">> Epoch 51 finished \tANN training loss 0.001279\n",
      ">> Epoch 52 finished \tANN training loss 0.001394\n",
      ">> Epoch 53 finished \tANN training loss 0.005309\n",
      ">> Epoch 54 finished \tANN training loss 0.002727\n",
      ">> Epoch 55 finished \tANN training loss 0.001293\n",
      ">> Epoch 56 finished \tANN training loss 0.001096\n",
      ">> Epoch 57 finished \tANN training loss 0.001456\n",
      ">> Epoch 58 finished \tANN training loss 0.000962\n",
      ">> Epoch 59 finished \tANN training loss 0.001269\n",
      ">> Epoch 60 finished \tANN training loss 0.003433\n",
      ">> Epoch 61 finished \tANN training loss 0.001575\n",
      ">> Epoch 62 finished \tANN training loss 0.000889\n",
      ">> Epoch 63 finished \tANN training loss 0.001236\n",
      ">> Epoch 64 finished \tANN training loss 0.001697\n",
      ">> Epoch 65 finished \tANN training loss 0.001059\n",
      ">> Epoch 66 finished \tANN training loss 0.001085\n",
      ">> Epoch 67 finished \tANN training loss 0.001179\n",
      ">> Epoch 68 finished \tANN training loss 0.001136\n",
      ">> Epoch 69 finished \tANN training loss 0.001136\n",
      ">> Epoch 70 finished \tANN training loss 0.000898\n",
      ">> Epoch 71 finished \tANN training loss 0.000630\n",
      ">> Epoch 72 finished \tANN training loss 0.000571\n",
      ">> Epoch 73 finished \tANN training loss 0.000813\n",
      ">> Epoch 74 finished \tANN training loss 0.000590\n",
      ">> Epoch 75 finished \tANN training loss 0.000545\n",
      ">> Epoch 76 finished \tANN training loss 0.000517\n",
      ">> Epoch 77 finished \tANN training loss 0.000598\n",
      ">> Epoch 78 finished \tANN training loss 0.000684\n",
      ">> Epoch 79 finished \tANN training loss 0.000607\n",
      ">> Epoch 80 finished \tANN training loss 0.000638\n",
      ">> Epoch 81 finished \tANN training loss 0.000651\n",
      ">> Epoch 82 finished \tANN training loss 0.000378\n",
      ">> Epoch 83 finished \tANN training loss 0.000516\n",
      ">> Epoch 84 finished \tANN training loss 0.003198\n",
      ">> Epoch 85 finished \tANN training loss 0.000389\n",
      ">> Epoch 86 finished \tANN training loss 0.000882\n",
      ">> Epoch 87 finished \tANN training loss 0.000349\n",
      ">> Epoch 88 finished \tANN training loss 0.000710\n",
      ">> Epoch 89 finished \tANN training loss 0.000895\n",
      ">> Epoch 90 finished \tANN training loss 0.000631\n",
      ">> Epoch 91 finished \tANN training loss 0.000354\n",
      ">> Epoch 92 finished \tANN training loss 0.000271\n",
      ">> Epoch 93 finished \tANN training loss 0.000350\n",
      ">> Epoch 94 finished \tANN training loss 0.000304\n",
      ">> Epoch 95 finished \tANN training loss 0.000320\n",
      ">> Epoch 96 finished \tANN training loss 0.000403\n",
      ">> Epoch 97 finished \tANN training loss 0.000309\n",
      ">> Epoch 98 finished \tANN training loss 0.000244\n",
      ">> Epoch 99 finished \tANN training loss 0.000229\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.925000\n"
     ]
    }
   ],
   "source": [
    "threelayer_acc[1] = deep_belief_net(hidden_layers_structure=[500, 200, 300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.925\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(threelayer_acc[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setting 3 – 2nd Layer = 500**\n",
    "\n",
    "    hidden_layers_structure=[100, 500, 300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 59.564102\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 59.761078\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 57.921844\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 46.745922\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 65.336388\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 107.946800\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 93.640976\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 101.977051\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 98.326408\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 103.741455\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 329.913818\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 587.393494\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 238.993225\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 555.764648\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 475.240692\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 475.708588\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 442.185699\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 499.192139\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 555.016052\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 750.540283\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 45197.039062\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 42511.648438\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 57938.121094\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 45343.339844\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 52561.078125\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 95396.632812\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 74806.039062\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 77988.851562\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 120865.539062\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 94011.882812\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 1.533851\n",
      ">> Epoch 1 finished \tANN training loss 1.276689\n",
      ">> Epoch 2 finished \tANN training loss 0.949397\n",
      ">> Epoch 3 finished \tANN training loss 0.714508\n",
      ">> Epoch 4 finished \tANN training loss 0.554281\n",
      ">> Epoch 5 finished \tANN training loss 0.477935\n",
      ">> Epoch 6 finished \tANN training loss 0.420389\n",
      ">> Epoch 7 finished \tANN training loss 0.354417\n",
      ">> Epoch 8 finished \tANN training loss 0.330924\n",
      ">> Epoch 9 finished \tANN training loss 0.291550\n",
      ">> Epoch 10 finished \tANN training loss 0.225266\n",
      ">> Epoch 11 finished \tANN training loss 0.193930\n",
      ">> Epoch 12 finished \tANN training loss 0.200787\n",
      ">> Epoch 13 finished \tANN training loss 0.158300\n",
      ">> Epoch 14 finished \tANN training loss 0.148867\n",
      ">> Epoch 15 finished \tANN training loss 0.137767\n",
      ">> Epoch 16 finished \tANN training loss 0.122214\n",
      ">> Epoch 17 finished \tANN training loss 0.120279\n",
      ">> Epoch 18 finished \tANN training loss 0.109280\n",
      ">> Epoch 19 finished \tANN training loss 0.103567\n",
      ">> Epoch 20 finished \tANN training loss 0.091581\n",
      ">> Epoch 21 finished \tANN training loss 0.066786\n",
      ">> Epoch 22 finished \tANN training loss 0.073343\n",
      ">> Epoch 23 finished \tANN training loss 0.066734\n",
      ">> Epoch 24 finished \tANN training loss 0.076449\n",
      ">> Epoch 25 finished \tANN training loss 0.048777\n",
      ">> Epoch 26 finished \tANN training loss 0.044279\n",
      ">> Epoch 27 finished \tANN training loss 0.051107\n",
      ">> Epoch 28 finished \tANN training loss 0.043139\n",
      ">> Epoch 29 finished \tANN training loss 0.040017\n",
      ">> Epoch 30 finished \tANN training loss 0.038079\n",
      ">> Epoch 31 finished \tANN training loss 0.029705\n",
      ">> Epoch 32 finished \tANN training loss 0.029876\n",
      ">> Epoch 33 finished \tANN training loss 0.031328\n",
      ">> Epoch 34 finished \tANN training loss 0.028740\n",
      ">> Epoch 35 finished \tANN training loss 0.025744\n",
      ">> Epoch 36 finished \tANN training loss 0.028364\n",
      ">> Epoch 37 finished \tANN training loss 0.025740\n",
      ">> Epoch 38 finished \tANN training loss 0.027433\n",
      ">> Epoch 39 finished \tANN training loss 0.016523\n",
      ">> Epoch 40 finished \tANN training loss 0.017461\n",
      ">> Epoch 41 finished \tANN training loss 0.021345\n",
      ">> Epoch 42 finished \tANN training loss 0.024730\n",
      ">> Epoch 43 finished \tANN training loss 0.021354\n",
      ">> Epoch 44 finished \tANN training loss 0.014698\n",
      ">> Epoch 45 finished \tANN training loss 0.013888\n",
      ">> Epoch 46 finished \tANN training loss 0.014201\n",
      ">> Epoch 47 finished \tANN training loss 0.036601\n",
      ">> Epoch 48 finished \tANN training loss 0.013088\n",
      ">> Epoch 49 finished \tANN training loss 0.010723\n",
      ">> Epoch 50 finished \tANN training loss 0.016645\n",
      ">> Epoch 51 finished \tANN training loss 0.012544\n",
      ">> Epoch 52 finished \tANN training loss 0.012440\n",
      ">> Epoch 53 finished \tANN training loss 0.011306\n",
      ">> Epoch 54 finished \tANN training loss 0.008115\n",
      ">> Epoch 55 finished \tANN training loss 0.009665\n",
      ">> Epoch 56 finished \tANN training loss 0.008852\n",
      ">> Epoch 57 finished \tANN training loss 0.005930\n",
      ">> Epoch 58 finished \tANN training loss 0.006430\n",
      ">> Epoch 59 finished \tANN training loss 0.007473\n",
      ">> Epoch 60 finished \tANN training loss 0.008734\n",
      ">> Epoch 61 finished \tANN training loss 0.005969\n",
      ">> Epoch 62 finished \tANN training loss 0.006821\n",
      ">> Epoch 63 finished \tANN training loss 0.012818\n",
      ">> Epoch 64 finished \tANN training loss 0.006060\n",
      ">> Epoch 65 finished \tANN training loss 0.009088\n",
      ">> Epoch 66 finished \tANN training loss 0.005121\n",
      ">> Epoch 67 finished \tANN training loss 0.017593\n",
      ">> Epoch 68 finished \tANN training loss 0.005724\n",
      ">> Epoch 69 finished \tANN training loss 0.004105\n",
      ">> Epoch 70 finished \tANN training loss 0.004846\n",
      ">> Epoch 71 finished \tANN training loss 0.004525\n",
      ">> Epoch 72 finished \tANN training loss 0.005610\n",
      ">> Epoch 73 finished \tANN training loss 0.016978\n",
      ">> Epoch 74 finished \tANN training loss 0.004154\n",
      ">> Epoch 75 finished \tANN training loss 0.005548\n",
      ">> Epoch 76 finished \tANN training loss 0.004250\n",
      ">> Epoch 77 finished \tANN training loss 0.005915\n",
      ">> Epoch 78 finished \tANN training loss 0.005880\n",
      ">> Epoch 79 finished \tANN training loss 0.005053\n",
      ">> Epoch 80 finished \tANN training loss 0.003347\n",
      ">> Epoch 81 finished \tANN training loss 0.003937\n",
      ">> Epoch 82 finished \tANN training loss 0.004959\n",
      ">> Epoch 83 finished \tANN training loss 0.003024\n",
      ">> Epoch 84 finished \tANN training loss 0.002971\n",
      ">> Epoch 85 finished \tANN training loss 0.002826\n",
      ">> Epoch 86 finished \tANN training loss 0.002263\n",
      ">> Epoch 87 finished \tANN training loss 0.002154\n",
      ">> Epoch 88 finished \tANN training loss 0.002168\n",
      ">> Epoch 89 finished \tANN training loss 0.002966\n",
      ">> Epoch 90 finished \tANN training loss 0.004537\n",
      ">> Epoch 91 finished \tANN training loss 0.003439\n",
      ">> Epoch 92 finished \tANN training loss 0.002118\n",
      ">> Epoch 93 finished \tANN training loss 0.002449\n",
      ">> Epoch 94 finished \tANN training loss 0.001787\n",
      ">> Epoch 95 finished \tANN training loss 0.001540\n",
      ">> Epoch 96 finished \tANN training loss 0.002248\n",
      ">> Epoch 97 finished \tANN training loss 0.001464\n",
      ">> Epoch 98 finished \tANN training loss 0.008587\n",
      ">> Epoch 99 finished \tANN training loss 0.004158\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.895000\n"
     ]
    }
   ],
   "source": [
    "threelayer_acc[2] = deep_belief_net(hidden_layers_structure=[100, 500, 300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.895\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(threelayer_acc[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setting 4 – 3rd Layer = 500**\n",
    "\n",
    "    hidden_layers_structure=[100, 200, 500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 75.150208\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 61.374603\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 64.751137\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 63.384277\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 54.535011\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 56.678871\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 75.155716\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 88.478317\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 77.702446\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 84.700813\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 577.839172\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 415.941925\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 600.284607\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 793.621643\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 637.713135\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 843.325439\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 760.770752\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 810.339233\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 955.818298\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 911.069092\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 11445.860352\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 25202.421875\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 37259.140625\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 39311.695312\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 40274.070312\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 59256.753906\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 59931.890625\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 61257.945312\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 54933.531250\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 63609.621094\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 1.258452\n",
      ">> Epoch 1 finished \tANN training loss 1.083188\n",
      ">> Epoch 2 finished \tANN training loss 0.613726\n",
      ">> Epoch 3 finished \tANN training loss 0.504612\n",
      ">> Epoch 4 finished \tANN training loss 0.415223\n",
      ">> Epoch 5 finished \tANN training loss 0.370487\n",
      ">> Epoch 6 finished \tANN training loss 0.348263\n",
      ">> Epoch 7 finished \tANN training loss 0.269174\n",
      ">> Epoch 8 finished \tANN training loss 0.226006\n",
      ">> Epoch 9 finished \tANN training loss 0.210821\n",
      ">> Epoch 10 finished \tANN training loss 0.169759\n",
      ">> Epoch 11 finished \tANN training loss 0.165086\n",
      ">> Epoch 12 finished \tANN training loss 0.147889\n",
      ">> Epoch 13 finished \tANN training loss 0.136483\n",
      ">> Epoch 14 finished \tANN training loss 0.116200\n",
      ">> Epoch 15 finished \tANN training loss 0.107202\n",
      ">> Epoch 16 finished \tANN training loss 0.117964\n",
      ">> Epoch 17 finished \tANN training loss 0.088797\n",
      ">> Epoch 18 finished \tANN training loss 0.070382\n",
      ">> Epoch 19 finished \tANN training loss 0.077285\n",
      ">> Epoch 20 finished \tANN training loss 0.070289\n",
      ">> Epoch 21 finished \tANN training loss 0.069854\n",
      ">> Epoch 22 finished \tANN training loss 0.058113\n",
      ">> Epoch 23 finished \tANN training loss 0.048474\n",
      ">> Epoch 24 finished \tANN training loss 0.049994\n",
      ">> Epoch 25 finished \tANN training loss 0.043107\n",
      ">> Epoch 26 finished \tANN training loss 0.039130\n",
      ">> Epoch 27 finished \tANN training loss 0.034399\n",
      ">> Epoch 28 finished \tANN training loss 0.047503\n",
      ">> Epoch 29 finished \tANN training loss 0.029602\n",
      ">> Epoch 30 finished \tANN training loss 0.027452\n",
      ">> Epoch 31 finished \tANN training loss 0.025084\n",
      ">> Epoch 32 finished \tANN training loss 0.026054\n",
      ">> Epoch 33 finished \tANN training loss 0.023706\n",
      ">> Epoch 34 finished \tANN training loss 0.031561\n",
      ">> Epoch 35 finished \tANN training loss 0.018627\n",
      ">> Epoch 36 finished \tANN training loss 0.018784\n",
      ">> Epoch 37 finished \tANN training loss 0.015933\n",
      ">> Epoch 38 finished \tANN training loss 0.151235\n",
      ">> Epoch 39 finished \tANN training loss 0.027518\n",
      ">> Epoch 40 finished \tANN training loss 0.022655\n",
      ">> Epoch 41 finished \tANN training loss 0.021426\n",
      ">> Epoch 42 finished \tANN training loss 0.018086\n",
      ">> Epoch 43 finished \tANN training loss 0.014521\n",
      ">> Epoch 44 finished \tANN training loss 0.014864\n",
      ">> Epoch 45 finished \tANN training loss 0.017260\n",
      ">> Epoch 46 finished \tANN training loss 0.026693\n",
      ">> Epoch 47 finished \tANN training loss 0.013925\n",
      ">> Epoch 48 finished \tANN training loss 0.012750\n",
      ">> Epoch 49 finished \tANN training loss 0.008987\n",
      ">> Epoch 50 finished \tANN training loss 0.009832\n",
      ">> Epoch 51 finished \tANN training loss 0.010033\n",
      ">> Epoch 52 finished \tANN training loss 0.012602\n",
      ">> Epoch 53 finished \tANN training loss 0.008046\n",
      ">> Epoch 54 finished \tANN training loss 0.010736\n",
      ">> Epoch 55 finished \tANN training loss 0.006261\n",
      ">> Epoch 56 finished \tANN training loss 0.021140\n",
      ">> Epoch 57 finished \tANN training loss 0.006422\n",
      ">> Epoch 58 finished \tANN training loss 0.006872\n",
      ">> Epoch 59 finished \tANN training loss 0.007119\n",
      ">> Epoch 60 finished \tANN training loss 0.007198\n",
      ">> Epoch 61 finished \tANN training loss 0.005168\n",
      ">> Epoch 62 finished \tANN training loss 0.007424\n",
      ">> Epoch 63 finished \tANN training loss 0.006021\n",
      ">> Epoch 64 finished \tANN training loss 0.005989\n",
      ">> Epoch 65 finished \tANN training loss 0.005185\n",
      ">> Epoch 66 finished \tANN training loss 0.004433\n",
      ">> Epoch 67 finished \tANN training loss 0.003414\n",
      ">> Epoch 68 finished \tANN training loss 0.004208\n",
      ">> Epoch 69 finished \tANN training loss 0.007795\n",
      ">> Epoch 70 finished \tANN training loss 0.005307\n",
      ">> Epoch 71 finished \tANN training loss 0.004187\n",
      ">> Epoch 72 finished \tANN training loss 0.004377\n",
      ">> Epoch 73 finished \tANN training loss 0.003942\n",
      ">> Epoch 74 finished \tANN training loss 0.003532\n",
      ">> Epoch 75 finished \tANN training loss 0.003018\n",
      ">> Epoch 76 finished \tANN training loss 0.003464\n",
      ">> Epoch 77 finished \tANN training loss 0.003108\n",
      ">> Epoch 78 finished \tANN training loss 0.002644\n",
      ">> Epoch 79 finished \tANN training loss 0.002728\n",
      ">> Epoch 80 finished \tANN training loss 0.002868\n",
      ">> Epoch 81 finished \tANN training loss 0.005210\n",
      ">> Epoch 82 finished \tANN training loss 0.003154\n",
      ">> Epoch 83 finished \tANN training loss 0.002397\n",
      ">> Epoch 84 finished \tANN training loss 0.003985\n",
      ">> Epoch 85 finished \tANN training loss 0.002626\n",
      ">> Epoch 86 finished \tANN training loss 0.001937\n",
      ">> Epoch 87 finished \tANN training loss 0.001281\n",
      ">> Epoch 88 finished \tANN training loss 0.001930\n",
      ">> Epoch 89 finished \tANN training loss 0.001422\n",
      ">> Epoch 90 finished \tANN training loss 0.001669\n",
      ">> Epoch 91 finished \tANN training loss 0.001492\n",
      ">> Epoch 92 finished \tANN training loss 0.001529\n",
      ">> Epoch 93 finished \tANN training loss 0.002800\n",
      ">> Epoch 94 finished \tANN training loss 0.002059\n",
      ">> Epoch 95 finished \tANN training loss 0.002790\n",
      ">> Epoch 96 finished \tANN training loss 0.001995\n",
      ">> Epoch 97 finished \tANN training loss 0.001121\n",
      ">> Epoch 98 finished \tANN training loss 0.000980\n",
      ">> Epoch 99 finished \tANN training loss 0.001216\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.870000\n"
     ]
    }
   ],
   "source": [
    "threelayer_acc[3] = deep_belief_net(hidden_layers_structure=[100, 200, 500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.87\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(threelayer_acc[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setting 5 – 1st Layer = 500, 2nd Layer = 500**\n",
    "\n",
    "    hidden_layers_structure=[500, 500, 300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 68.491608\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 56.644928\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 53.951054\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 44.317753\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 37.582027\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 26.515465\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 31.068565\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 28.296555\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 26.028257\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 25.418171\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 152.616257\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 63.525909\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 43.410030\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 52.196709\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 96.214417\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 58.725548\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 39.154755\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 90.772125\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 41.436943\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 65.333107\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 315.171967\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 704.780334\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 604.968628\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 875.380310\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 853.201416\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 839.794739\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 960.186707\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 959.082336\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 1235.092163\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 1277.223877\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.642280\n",
      ">> Epoch 1 finished \tANN training loss 0.512841\n",
      ">> Epoch 2 finished \tANN training loss 0.311047\n",
      ">> Epoch 3 finished \tANN training loss 0.261270\n",
      ">> Epoch 4 finished \tANN training loss 0.237309\n",
      ">> Epoch 5 finished \tANN training loss 0.194324\n",
      ">> Epoch 6 finished \tANN training loss 0.203730\n",
      ">> Epoch 7 finished \tANN training loss 0.144550\n",
      ">> Epoch 8 finished \tANN training loss 0.121052\n",
      ">> Epoch 9 finished \tANN training loss 0.117892\n",
      ">> Epoch 10 finished \tANN training loss 0.083755\n",
      ">> Epoch 11 finished \tANN training loss 0.073921\n",
      ">> Epoch 12 finished \tANN training loss 0.068480\n",
      ">> Epoch 13 finished \tANN training loss 0.050775\n",
      ">> Epoch 14 finished \tANN training loss 0.044401\n",
      ">> Epoch 15 finished \tANN training loss 0.039711\n",
      ">> Epoch 16 finished \tANN training loss 0.031740\n",
      ">> Epoch 17 finished \tANN training loss 0.028511\n",
      ">> Epoch 18 finished \tANN training loss 0.025597\n",
      ">> Epoch 19 finished \tANN training loss 0.021941\n",
      ">> Epoch 20 finished \tANN training loss 0.016981\n",
      ">> Epoch 21 finished \tANN training loss 0.019400\n",
      ">> Epoch 22 finished \tANN training loss 0.019374\n",
      ">> Epoch 23 finished \tANN training loss 0.031542\n",
      ">> Epoch 24 finished \tANN training loss 0.016698\n",
      ">> Epoch 25 finished \tANN training loss 0.011849\n",
      ">> Epoch 26 finished \tANN training loss 0.011747\n",
      ">> Epoch 27 finished \tANN training loss 0.009666\n",
      ">> Epoch 28 finished \tANN training loss 0.007762\n",
      ">> Epoch 29 finished \tANN training loss 0.009268\n",
      ">> Epoch 30 finished \tANN training loss 0.009688\n",
      ">> Epoch 31 finished \tANN training loss 0.009016\n",
      ">> Epoch 32 finished \tANN training loss 0.006719\n",
      ">> Epoch 33 finished \tANN training loss 0.007408\n",
      ">> Epoch 34 finished \tANN training loss 0.006457\n",
      ">> Epoch 35 finished \tANN training loss 0.005747\n",
      ">> Epoch 36 finished \tANN training loss 0.004501\n",
      ">> Epoch 37 finished \tANN training loss 0.004402\n",
      ">> Epoch 38 finished \tANN training loss 0.003900\n",
      ">> Epoch 39 finished \tANN training loss 0.003608\n",
      ">> Epoch 40 finished \tANN training loss 0.002960\n",
      ">> Epoch 41 finished \tANN training loss 0.002813\n",
      ">> Epoch 42 finished \tANN training loss 0.002161\n",
      ">> Epoch 43 finished \tANN training loss 0.002619\n",
      ">> Epoch 44 finished \tANN training loss 0.003579\n",
      ">> Epoch 45 finished \tANN training loss 0.002943\n",
      ">> Epoch 46 finished \tANN training loss 0.002186\n",
      ">> Epoch 47 finished \tANN training loss 0.001951\n",
      ">> Epoch 48 finished \tANN training loss 0.004580\n",
      ">> Epoch 49 finished \tANN training loss 0.003021\n",
      ">> Epoch 50 finished \tANN training loss 0.003405\n",
      ">> Epoch 51 finished \tANN training loss 0.001844\n",
      ">> Epoch 52 finished \tANN training loss 0.001807\n",
      ">> Epoch 53 finished \tANN training loss 0.001755\n",
      ">> Epoch 54 finished \tANN training loss 0.001252\n",
      ">> Epoch 55 finished \tANN training loss 0.001320\n",
      ">> Epoch 56 finished \tANN training loss 0.001371\n",
      ">> Epoch 57 finished \tANN training loss 0.001298\n",
      ">> Epoch 58 finished \tANN training loss 0.000955\n",
      ">> Epoch 59 finished \tANN training loss 0.001305\n",
      ">> Epoch 60 finished \tANN training loss 0.001030\n",
      ">> Epoch 61 finished \tANN training loss 0.001128\n",
      ">> Epoch 62 finished \tANN training loss 0.000973\n",
      ">> Epoch 63 finished \tANN training loss 0.000982\n",
      ">> Epoch 64 finished \tANN training loss 0.001252\n",
      ">> Epoch 65 finished \tANN training loss 0.001098\n",
      ">> Epoch 66 finished \tANN training loss 0.000788\n",
      ">> Epoch 67 finished \tANN training loss 0.000999\n",
      ">> Epoch 68 finished \tANN training loss 0.001126\n",
      ">> Epoch 69 finished \tANN training loss 0.000856\n",
      ">> Epoch 70 finished \tANN training loss 0.000873\n",
      ">> Epoch 71 finished \tANN training loss 0.000717\n",
      ">> Epoch 72 finished \tANN training loss 0.000775\n",
      ">> Epoch 73 finished \tANN training loss 0.000858\n",
      ">> Epoch 74 finished \tANN training loss 0.000935\n",
      ">> Epoch 75 finished \tANN training loss 0.000911\n",
      ">> Epoch 76 finished \tANN training loss 0.000481\n",
      ">> Epoch 77 finished \tANN training loss 0.000534\n",
      ">> Epoch 78 finished \tANN training loss 0.000715\n",
      ">> Epoch 79 finished \tANN training loss 0.000511\n",
      ">> Epoch 80 finished \tANN training loss 0.000445\n",
      ">> Epoch 81 finished \tANN training loss 0.000527\n",
      ">> Epoch 82 finished \tANN training loss 0.000599\n",
      ">> Epoch 83 finished \tANN training loss 0.000532\n",
      ">> Epoch 84 finished \tANN training loss 0.000725\n",
      ">> Epoch 85 finished \tANN training loss 0.000393\n",
      ">> Epoch 86 finished \tANN training loss 0.000442\n",
      ">> Epoch 87 finished \tANN training loss 0.000435\n",
      ">> Epoch 88 finished \tANN training loss 0.000510\n",
      ">> Epoch 89 finished \tANN training loss 0.000390\n",
      ">> Epoch 90 finished \tANN training loss 0.000425\n",
      ">> Epoch 91 finished \tANN training loss 0.000335\n",
      ">> Epoch 92 finished \tANN training loss 0.000317\n",
      ">> Epoch 93 finished \tANN training loss 0.000315\n",
      ">> Epoch 94 finished \tANN training loss 0.000415\n",
      ">> Epoch 95 finished \tANN training loss 0.000503\n",
      ">> Epoch 96 finished \tANN training loss 0.000396\n",
      ">> Epoch 97 finished \tANN training loss 0.000376\n",
      ">> Epoch 98 finished \tANN training loss 0.000273\n",
      ">> Epoch 99 finished \tANN training loss 0.000261\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.910000\n"
     ]
    }
   ],
   "source": [
    "threelayer_acc[4] = deep_belief_net(hidden_layers_structure=[500, 500, 300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.91\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(threelayer_acc[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setting 6 – 1st Layer = 500, 3rd Layer = 500**\n",
    "\n",
    "    hidden_layers_structure=[500, 200, 500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 78.036766\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 52.033066\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 32.189144\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 37.730347\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 38.358086\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 45.428146\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 39.048912\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 56.111622\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 23.523413\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 37.212223\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 123.392220\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 132.450577\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 221.457428\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 173.573120\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 181.331360\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 233.512924\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 192.164215\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 229.156311\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 271.157501\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 276.509216\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 1931.465210\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 3652.421631\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 4494.917969\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 5545.651855\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 4545.560547\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 4857.675781\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 5941.788574\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 6031.382324\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 5178.402344\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 6309.609863\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.713952\n",
      ">> Epoch 1 finished \tANN training loss 0.632705\n",
      ">> Epoch 2 finished \tANN training loss 0.366221\n",
      ">> Epoch 3 finished \tANN training loss 0.287641\n",
      ">> Epoch 4 finished \tANN training loss 0.274968\n",
      ">> Epoch 5 finished \tANN training loss 0.207943\n",
      ">> Epoch 6 finished \tANN training loss 0.183766\n",
      ">> Epoch 7 finished \tANN training loss 0.139385\n",
      ">> Epoch 8 finished \tANN training loss 0.133136\n",
      ">> Epoch 9 finished \tANN training loss 0.128391\n",
      ">> Epoch 10 finished \tANN training loss 0.115430\n",
      ">> Epoch 11 finished \tANN training loss 0.134924\n",
      ">> Epoch 12 finished \tANN training loss 0.080716\n",
      ">> Epoch 13 finished \tANN training loss 0.070322\n",
      ">> Epoch 14 finished \tANN training loss 0.081943\n",
      ">> Epoch 15 finished \tANN training loss 0.052324\n",
      ">> Epoch 16 finished \tANN training loss 0.054598\n",
      ">> Epoch 17 finished \tANN training loss 0.047250\n",
      ">> Epoch 18 finished \tANN training loss 0.038195\n",
      ">> Epoch 19 finished \tANN training loss 0.039413\n",
      ">> Epoch 20 finished \tANN training loss 0.026421\n",
      ">> Epoch 21 finished \tANN training loss 0.022356\n",
      ">> Epoch 22 finished \tANN training loss 0.019117\n",
      ">> Epoch 23 finished \tANN training loss 0.015601\n",
      ">> Epoch 24 finished \tANN training loss 0.017516\n",
      ">> Epoch 25 finished \tANN training loss 0.017139\n",
      ">> Epoch 26 finished \tANN training loss 0.017296\n",
      ">> Epoch 27 finished \tANN training loss 0.012669\n",
      ">> Epoch 28 finished \tANN training loss 0.012581\n",
      ">> Epoch 29 finished \tANN training loss 0.012118\n",
      ">> Epoch 30 finished \tANN training loss 0.008638\n",
      ">> Epoch 31 finished \tANN training loss 0.007331\n",
      ">> Epoch 32 finished \tANN training loss 0.008692\n",
      ">> Epoch 33 finished \tANN training loss 0.008452\n",
      ">> Epoch 34 finished \tANN training loss 0.005836\n",
      ">> Epoch 35 finished \tANN training loss 0.007319\n",
      ">> Epoch 36 finished \tANN training loss 0.005158\n",
      ">> Epoch 37 finished \tANN training loss 0.005184\n",
      ">> Epoch 38 finished \tANN training loss 0.003938\n",
      ">> Epoch 39 finished \tANN training loss 0.008897\n",
      ">> Epoch 40 finished \tANN training loss 0.004148\n",
      ">> Epoch 41 finished \tANN training loss 0.003720\n",
      ">> Epoch 42 finished \tANN training loss 0.002842\n",
      ">> Epoch 43 finished \tANN training loss 0.002704\n",
      ">> Epoch 44 finished \tANN training loss 0.004034\n",
      ">> Epoch 45 finished \tANN training loss 0.003140\n",
      ">> Epoch 46 finished \tANN training loss 0.003658\n",
      ">> Epoch 47 finished \tANN training loss 0.002641\n",
      ">> Epoch 48 finished \tANN training loss 0.002161\n",
      ">> Epoch 49 finished \tANN training loss 0.002023\n",
      ">> Epoch 50 finished \tANN training loss 0.002653\n",
      ">> Epoch 51 finished \tANN training loss 0.001711\n",
      ">> Epoch 52 finished \tANN training loss 0.001963\n",
      ">> Epoch 53 finished \tANN training loss 0.002100\n",
      ">> Epoch 54 finished \tANN training loss 0.001490\n",
      ">> Epoch 55 finished \tANN training loss 0.001593\n",
      ">> Epoch 56 finished \tANN training loss 0.001803\n",
      ">> Epoch 57 finished \tANN training loss 0.001548\n",
      ">> Epoch 58 finished \tANN training loss 0.002671\n",
      ">> Epoch 59 finished \tANN training loss 0.002320\n",
      ">> Epoch 60 finished \tANN training loss 0.001416\n",
      ">> Epoch 61 finished \tANN training loss 0.001142\n",
      ">> Epoch 62 finished \tANN training loss 0.001347\n",
      ">> Epoch 63 finished \tANN training loss 0.001051\n",
      ">> Epoch 64 finished \tANN training loss 0.001429\n",
      ">> Epoch 65 finished \tANN training loss 0.001294\n",
      ">> Epoch 66 finished \tANN training loss 0.000963\n",
      ">> Epoch 67 finished \tANN training loss 0.000795\n",
      ">> Epoch 68 finished \tANN training loss 0.000796\n",
      ">> Epoch 69 finished \tANN training loss 0.000556\n",
      ">> Epoch 70 finished \tANN training loss 0.000769\n",
      ">> Epoch 71 finished \tANN training loss 0.000558\n",
      ">> Epoch 72 finished \tANN training loss 0.000450\n",
      ">> Epoch 73 finished \tANN training loss 0.000430\n",
      ">> Epoch 74 finished \tANN training loss 0.000453\n",
      ">> Epoch 75 finished \tANN training loss 0.000546\n",
      ">> Epoch 76 finished \tANN training loss 0.000422\n",
      ">> Epoch 77 finished \tANN training loss 0.000460\n",
      ">> Epoch 78 finished \tANN training loss 0.000437\n",
      ">> Epoch 79 finished \tANN training loss 0.000516\n",
      ">> Epoch 80 finished \tANN training loss 0.000723\n",
      ">> Epoch 81 finished \tANN training loss 0.000454\n",
      ">> Epoch 82 finished \tANN training loss 0.000410\n",
      ">> Epoch 83 finished \tANN training loss 0.000487\n",
      ">> Epoch 84 finished \tANN training loss 0.000508\n",
      ">> Epoch 85 finished \tANN training loss 0.001063\n",
      ">> Epoch 86 finished \tANN training loss 0.000693\n",
      ">> Epoch 87 finished \tANN training loss 0.000534\n",
      ">> Epoch 88 finished \tANN training loss 0.000694\n",
      ">> Epoch 89 finished \tANN training loss 0.000495\n",
      ">> Epoch 90 finished \tANN training loss 0.000531\n",
      ">> Epoch 91 finished \tANN training loss 0.000385\n",
      ">> Epoch 92 finished \tANN training loss 0.000477\n",
      ">> Epoch 93 finished \tANN training loss 0.000299\n",
      ">> Epoch 94 finished \tANN training loss 0.001446\n",
      ">> Epoch 95 finished \tANN training loss 0.000973\n",
      ">> Epoch 96 finished \tANN training loss 0.000481\n",
      ">> Epoch 97 finished \tANN training loss 0.000551\n",
      ">> Epoch 98 finished \tANN training loss 0.000467\n",
      ">> Epoch 99 finished \tANN training loss 0.000395\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.875000\n"
     ]
    }
   ],
   "source": [
    "threelayer_acc[5] = deep_belief_net(hidden_layers_structure=[500, 200, 500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.875\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(threelayer_acc[5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setting 7 – 2nd Layer = 500, 3rd Layer = 500**\n",
    "\n",
    "    hidden_layers_structure=[100, 500, 500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 82.229980\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 46.168163\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 51.044132\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 76.949280\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 87.207382\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 85.926399\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 81.613663\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 87.550720\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 112.198029\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 93.661072\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 303.795593\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 469.277771\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 314.409546\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 261.004852\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 333.697815\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 439.731628\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 470.187653\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 351.625122\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 363.454834\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 631.564026\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 28168.574219\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 18858.269531\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 21156.072266\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 24177.750000\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 40059.156250\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 48131.300781\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 62412.929688\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 79066.500000\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 71365.078125\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 86572.859375\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 1.422574\n",
      ">> Epoch 1 finished \tANN training loss 1.026839\n",
      ">> Epoch 2 finished \tANN training loss 0.847494\n",
      ">> Epoch 3 finished \tANN training loss 0.729024\n",
      ">> Epoch 4 finished \tANN training loss 0.589623\n",
      ">> Epoch 5 finished \tANN training loss 0.555054\n",
      ">> Epoch 6 finished \tANN training loss 0.436554\n",
      ">> Epoch 7 finished \tANN training loss 0.393142\n",
      ">> Epoch 8 finished \tANN training loss 0.325833\n",
      ">> Epoch 9 finished \tANN training loss 0.298387\n",
      ">> Epoch 10 finished \tANN training loss 0.253825\n",
      ">> Epoch 11 finished \tANN training loss 0.244105\n",
      ">> Epoch 12 finished \tANN training loss 0.203849\n",
      ">> Epoch 13 finished \tANN training loss 0.195522\n",
      ">> Epoch 14 finished \tANN training loss 0.173128\n",
      ">> Epoch 15 finished \tANN training loss 0.150871\n",
      ">> Epoch 16 finished \tANN training loss 0.145122\n",
      ">> Epoch 17 finished \tANN training loss 0.114257\n",
      ">> Epoch 18 finished \tANN training loss 0.119007\n",
      ">> Epoch 19 finished \tANN training loss 0.105456\n",
      ">> Epoch 20 finished \tANN training loss 0.104219\n",
      ">> Epoch 21 finished \tANN training loss 0.080118\n",
      ">> Epoch 22 finished \tANN training loss 0.063676\n",
      ">> Epoch 23 finished \tANN training loss 0.077698\n",
      ">> Epoch 24 finished \tANN training loss 0.065270\n",
      ">> Epoch 25 finished \tANN training loss 0.063665\n",
      ">> Epoch 26 finished \tANN training loss 0.062546\n",
      ">> Epoch 27 finished \tANN training loss 0.049267\n",
      ">> Epoch 28 finished \tANN training loss 0.053107\n",
      ">> Epoch 29 finished \tANN training loss 0.039250\n",
      ">> Epoch 30 finished \tANN training loss 0.035018\n",
      ">> Epoch 31 finished \tANN training loss 0.036830\n",
      ">> Epoch 32 finished \tANN training loss 0.030265\n",
      ">> Epoch 33 finished \tANN training loss 0.034772\n",
      ">> Epoch 34 finished \tANN training loss 0.030809\n",
      ">> Epoch 35 finished \tANN training loss 0.038304\n",
      ">> Epoch 36 finished \tANN training loss 0.030574\n",
      ">> Epoch 37 finished \tANN training loss 0.022830\n",
      ">> Epoch 38 finished \tANN training loss 0.024702\n",
      ">> Epoch 39 finished \tANN training loss 0.021393\n",
      ">> Epoch 40 finished \tANN training loss 0.017328\n",
      ">> Epoch 41 finished \tANN training loss 0.019605\n",
      ">> Epoch 42 finished \tANN training loss 0.020833\n",
      ">> Epoch 43 finished \tANN training loss 0.023375\n",
      ">> Epoch 44 finished \tANN training loss 0.018717\n",
      ">> Epoch 45 finished \tANN training loss 0.020408\n",
      ">> Epoch 46 finished \tANN training loss 0.015725\n",
      ">> Epoch 47 finished \tANN training loss 0.016227\n",
      ">> Epoch 48 finished \tANN training loss 0.027124\n",
      ">> Epoch 49 finished \tANN training loss 0.018069\n",
      ">> Epoch 50 finished \tANN training loss 0.011587\n",
      ">> Epoch 51 finished \tANN training loss 0.014348\n",
      ">> Epoch 52 finished \tANN training loss 0.014379\n",
      ">> Epoch 53 finished \tANN training loss 0.017997\n",
      ">> Epoch 54 finished \tANN training loss 0.014161\n",
      ">> Epoch 55 finished \tANN training loss 0.009151\n",
      ">> Epoch 56 finished \tANN training loss 0.010320\n",
      ">> Epoch 57 finished \tANN training loss 0.008837\n",
      ">> Epoch 58 finished \tANN training loss 0.006893\n",
      ">> Epoch 59 finished \tANN training loss 0.010685\n",
      ">> Epoch 60 finished \tANN training loss 0.007026\n",
      ">> Epoch 61 finished \tANN training loss 0.006646\n",
      ">> Epoch 62 finished \tANN training loss 0.007347\n",
      ">> Epoch 63 finished \tANN training loss 0.006674\n",
      ">> Epoch 64 finished \tANN training loss 0.010059\n",
      ">> Epoch 65 finished \tANN training loss 0.008058\n",
      ">> Epoch 66 finished \tANN training loss 0.006482\n",
      ">> Epoch 67 finished \tANN training loss 0.005870\n",
      ">> Epoch 68 finished \tANN training loss 0.006228\n",
      ">> Epoch 69 finished \tANN training loss 0.006360\n",
      ">> Epoch 70 finished \tANN training loss 0.008768\n",
      ">> Epoch 71 finished \tANN training loss 0.005981\n",
      ">> Epoch 72 finished \tANN training loss 0.007048\n",
      ">> Epoch 73 finished \tANN training loss 0.006741\n",
      ">> Epoch 74 finished \tANN training loss 0.007541\n",
      ">> Epoch 75 finished \tANN training loss 0.010282\n",
      ">> Epoch 76 finished \tANN training loss 0.008161\n",
      ">> Epoch 77 finished \tANN training loss 0.006051\n",
      ">> Epoch 78 finished \tANN training loss 0.005904\n",
      ">> Epoch 79 finished \tANN training loss 0.006657\n",
      ">> Epoch 80 finished \tANN training loss 0.005100\n",
      ">> Epoch 81 finished \tANN training loss 0.005631\n",
      ">> Epoch 82 finished \tANN training loss 0.007407\n",
      ">> Epoch 83 finished \tANN training loss 0.005248\n",
      ">> Epoch 84 finished \tANN training loss 0.004808\n",
      ">> Epoch 85 finished \tANN training loss 0.004339\n",
      ">> Epoch 86 finished \tANN training loss 0.006076\n",
      ">> Epoch 87 finished \tANN training loss 0.004384\n",
      ">> Epoch 88 finished \tANN training loss 0.004184\n",
      ">> Epoch 89 finished \tANN training loss 0.004218\n",
      ">> Epoch 90 finished \tANN training loss 0.003687\n",
      ">> Epoch 91 finished \tANN training loss 0.004417\n",
      ">> Epoch 92 finished \tANN training loss 0.004836\n",
      ">> Epoch 93 finished \tANN training loss 0.004638\n",
      ">> Epoch 94 finished \tANN training loss 0.004646\n",
      ">> Epoch 95 finished \tANN training loss 0.003542\n",
      ">> Epoch 96 finished \tANN training loss 0.005069\n",
      ">> Epoch 97 finished \tANN training loss 0.003526\n",
      ">> Epoch 98 finished \tANN training loss 0.003974\n",
      ">> Epoch 99 finished \tANN training loss 0.004375\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.870000\n"
     ]
    }
   ],
   "source": [
    "threelayer_acc[6] = deep_belief_net(hidden_layers_structure=[100, 500, 500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.87\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(threelayer_acc[6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.90000000000000002, 0.92500000000000004, 0.89500000000000002, 0.87, 0.91000000000000003, 0.875, 0.87]\n",
      "Most accurate 3-layer setting is Setting 2\n"
     ]
    }
   ],
   "source": [
    "# Collated results\n",
    "print(threelayer_acc)\n",
    "print('Most accurate 3-layer setting is Setting ' + str(threelayer_acc.index(max(threelayer_acc)) + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAE8CAYAAADe7fZ4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1cd03ccbd68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create bar graph\n",
    "plt.clf()\n",
    "objects = ('[100, 200, 300]', '[500, 200, 300]', '[100, 500, 300]', '[100, 200, 500]', '[500, 500, 300]', '[500, 200, 500]', '[100, 500, 500]')\n",
    "y_pos = np.arange(len(objects))\n",
    "performance = [0.9,0.925,0.895,0.87, 0.91, 0.875, 0.87]\n",
    "\n",
    "plt.bar(y_pos, performance, align='center', alpha=0.5, width=0.5)\n",
    "plt.xticks(y_pos, objects, rotation=30)\n",
    "plt.xlabel('Layer Settings')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Three-layer Settings')\n",
    "\n",
    " \n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different Pre-training (RBM) Learning Rates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store accuracies\n",
    "rbm_acc = [0, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setting 1**\n",
    "\n",
    "    learning_rate_rbm=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 48.429943\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 44.728573\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 37.250076\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 31.907465\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 28.755913\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 26.740873\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 25.324945\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 23.896397\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 22.649555\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 21.804190\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 25.185154\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 18.686335\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 10.709770\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 7.455266\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 5.747530\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 4.696509\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 3.895706\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 3.339420\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 3.075150\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 2.861675\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 1.026684\n",
      ">> Epoch 1 finished \tANN training loss 0.738023\n",
      ">> Epoch 2 finished \tANN training loss 0.599452\n",
      ">> Epoch 3 finished \tANN training loss 0.489238\n",
      ">> Epoch 4 finished \tANN training loss 0.437557\n",
      ">> Epoch 5 finished \tANN training loss 0.374682\n",
      ">> Epoch 6 finished \tANN training loss 0.335208\n",
      ">> Epoch 7 finished \tANN training loss 0.306487\n",
      ">> Epoch 8 finished \tANN training loss 0.257842\n",
      ">> Epoch 9 finished \tANN training loss 0.245150\n",
      ">> Epoch 10 finished \tANN training loss 0.201488\n",
      ">> Epoch 11 finished \tANN training loss 0.204628\n",
      ">> Epoch 12 finished \tANN training loss 0.157771\n",
      ">> Epoch 13 finished \tANN training loss 0.138897\n",
      ">> Epoch 14 finished \tANN training loss 0.129000\n",
      ">> Epoch 15 finished \tANN training loss 0.123893\n",
      ">> Epoch 16 finished \tANN training loss 0.107592\n",
      ">> Epoch 17 finished \tANN training loss 0.087158\n",
      ">> Epoch 18 finished \tANN training loss 0.083540\n",
      ">> Epoch 19 finished \tANN training loss 0.117613\n",
      ">> Epoch 20 finished \tANN training loss 0.075871\n",
      ">> Epoch 21 finished \tANN training loss 0.071079\n",
      ">> Epoch 22 finished \tANN training loss 0.073331\n",
      ">> Epoch 23 finished \tANN training loss 0.054254\n",
      ">> Epoch 24 finished \tANN training loss 0.047814\n",
      ">> Epoch 25 finished \tANN training loss 0.040908\n",
      ">> Epoch 26 finished \tANN training loss 0.036101\n",
      ">> Epoch 27 finished \tANN training loss 0.031959\n",
      ">> Epoch 28 finished \tANN training loss 0.033934\n",
      ">> Epoch 29 finished \tANN training loss 0.026952\n",
      ">> Epoch 30 finished \tANN training loss 0.023128\n",
      ">> Epoch 31 finished \tANN training loss 0.026336\n",
      ">> Epoch 32 finished \tANN training loss 0.026706\n",
      ">> Epoch 33 finished \tANN training loss 0.018220\n",
      ">> Epoch 34 finished \tANN training loss 0.015882\n",
      ">> Epoch 35 finished \tANN training loss 0.016173\n",
      ">> Epoch 36 finished \tANN training loss 0.016196\n",
      ">> Epoch 37 finished \tANN training loss 0.013575\n",
      ">> Epoch 38 finished \tANN training loss 0.015625\n",
      ">> Epoch 39 finished \tANN training loss 0.012308\n",
      ">> Epoch 40 finished \tANN training loss 0.010823\n",
      ">> Epoch 41 finished \tANN training loss 0.011302\n",
      ">> Epoch 42 finished \tANN training loss 0.010032\n",
      ">> Epoch 43 finished \tANN training loss 0.011795\n",
      ">> Epoch 44 finished \tANN training loss 0.008577\n",
      ">> Epoch 45 finished \tANN training loss 0.008509\n",
      ">> Epoch 46 finished \tANN training loss 0.007353\n",
      ">> Epoch 47 finished \tANN training loss 0.007781\n",
      ">> Epoch 48 finished \tANN training loss 0.007838\n",
      ">> Epoch 49 finished \tANN training loss 0.006890\n",
      ">> Epoch 50 finished \tANN training loss 0.007011\n",
      ">> Epoch 51 finished \tANN training loss 0.005143\n",
      ">> Epoch 52 finished \tANN training loss 0.005654\n",
      ">> Epoch 53 finished \tANN training loss 0.006066\n",
      ">> Epoch 54 finished \tANN training loss 0.008010\n",
      ">> Epoch 55 finished \tANN training loss 0.005409\n",
      ">> Epoch 56 finished \tANN training loss 0.004631\n",
      ">> Epoch 57 finished \tANN training loss 0.004697\n",
      ">> Epoch 58 finished \tANN training loss 0.004014\n",
      ">> Epoch 59 finished \tANN training loss 0.004065\n",
      ">> Epoch 60 finished \tANN training loss 0.003648\n",
      ">> Epoch 61 finished \tANN training loss 0.003955\n",
      ">> Epoch 62 finished \tANN training loss 0.003463\n",
      ">> Epoch 63 finished \tANN training loss 0.003597\n",
      ">> Epoch 64 finished \tANN training loss 0.003128\n",
      ">> Epoch 65 finished \tANN training loss 0.002815\n",
      ">> Epoch 66 finished \tANN training loss 0.002781\n",
      ">> Epoch 67 finished \tANN training loss 0.003304\n",
      ">> Epoch 68 finished \tANN training loss 0.003071\n",
      ">> Epoch 69 finished \tANN training loss 0.002651\n",
      ">> Epoch 70 finished \tANN training loss 0.002896\n",
      ">> Epoch 71 finished \tANN training loss 0.002207\n",
      ">> Epoch 72 finished \tANN training loss 0.002331\n",
      ">> Epoch 73 finished \tANN training loss 0.002288\n",
      ">> Epoch 74 finished \tANN training loss 0.002224\n",
      ">> Epoch 75 finished \tANN training loss 0.002153\n",
      ">> Epoch 76 finished \tANN training loss 0.002389\n",
      ">> Epoch 77 finished \tANN training loss 0.001850\n",
      ">> Epoch 78 finished \tANN training loss 0.002531\n",
      ">> Epoch 79 finished \tANN training loss 0.001833\n",
      ">> Epoch 80 finished \tANN training loss 0.002138\n",
      ">> Epoch 81 finished \tANN training loss 0.001977\n",
      ">> Epoch 82 finished \tANN training loss 0.001566\n",
      ">> Epoch 83 finished \tANN training loss 0.002432\n",
      ">> Epoch 84 finished \tANN training loss 0.001725\n",
      ">> Epoch 85 finished \tANN training loss 0.001382\n",
      ">> Epoch 86 finished \tANN training loss 0.001534\n",
      ">> Epoch 87 finished \tANN training loss 0.001267\n",
      ">> Epoch 88 finished \tANN training loss 0.001290\n",
      ">> Epoch 89 finished \tANN training loss 0.001479\n",
      ">> Epoch 90 finished \tANN training loss 0.001209\n",
      ">> Epoch 91 finished \tANN training loss 0.001558\n",
      ">> Epoch 92 finished \tANN training loss 0.001149\n",
      ">> Epoch 93 finished \tANN training loss 0.001014\n",
      ">> Epoch 94 finished \tANN training loss 0.002997\n",
      ">> Epoch 95 finished \tANN training loss 0.001005\n",
      ">> Epoch 96 finished \tANN training loss 0.001130\n",
      ">> Epoch 97 finished \tANN training loss 0.001200\n",
      ">> Epoch 98 finished \tANN training loss 0.000986\n",
      ">> Epoch 99 finished \tANN training loss 0.000986\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.895000\n"
     ]
    }
   ],
   "source": [
    "rbm_acc[0] = deep_belief_net(learning_rate_rbm=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.895\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(rbm_acc[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setting 2**\n",
    "\n",
    "    learning_rate_rbm=0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 49.428436\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 38.784439\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 36.508812\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 32.487255\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 30.027876\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 27.512949\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 28.220713\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 36.800270\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 26.080055\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 27.156790\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 127.447762\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 60.762264\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 132.231400\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 136.848663\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 105.110397\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 122.694519\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 115.857346\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 184.296173\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 178.169037\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 167.853561\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.635793\n",
      ">> Epoch 1 finished \tANN training loss 0.481509\n",
      ">> Epoch 2 finished \tANN training loss 0.373607\n",
      ">> Epoch 3 finished \tANN training loss 0.311064\n",
      ">> Epoch 4 finished \tANN training loss 0.288711\n",
      ">> Epoch 5 finished \tANN training loss 0.226046\n",
      ">> Epoch 6 finished \tANN training loss 0.197164\n",
      ">> Epoch 7 finished \tANN training loss 0.192715\n",
      ">> Epoch 8 finished \tANN training loss 0.162547\n",
      ">> Epoch 9 finished \tANN training loss 0.137399\n",
      ">> Epoch 10 finished \tANN training loss 0.124895\n",
      ">> Epoch 11 finished \tANN training loss 0.106781\n",
      ">> Epoch 12 finished \tANN training loss 0.095121\n",
      ">> Epoch 13 finished \tANN training loss 0.085233\n",
      ">> Epoch 14 finished \tANN training loss 0.081535\n",
      ">> Epoch 15 finished \tANN training loss 0.063347\n",
      ">> Epoch 16 finished \tANN training loss 0.055384\n",
      ">> Epoch 17 finished \tANN training loss 0.052163\n",
      ">> Epoch 18 finished \tANN training loss 0.049630\n",
      ">> Epoch 19 finished \tANN training loss 0.042883\n",
      ">> Epoch 20 finished \tANN training loss 0.040378\n",
      ">> Epoch 21 finished \tANN training loss 0.032656\n",
      ">> Epoch 22 finished \tANN training loss 0.029055\n",
      ">> Epoch 23 finished \tANN training loss 0.028338\n",
      ">> Epoch 24 finished \tANN training loss 0.025327\n",
      ">> Epoch 25 finished \tANN training loss 0.023662\n",
      ">> Epoch 26 finished \tANN training loss 0.020544\n",
      ">> Epoch 27 finished \tANN training loss 0.021189\n",
      ">> Epoch 28 finished \tANN training loss 0.017175\n",
      ">> Epoch 29 finished \tANN training loss 0.018268\n",
      ">> Epoch 30 finished \tANN training loss 0.015474\n",
      ">> Epoch 31 finished \tANN training loss 0.015518\n",
      ">> Epoch 32 finished \tANN training loss 0.017359\n",
      ">> Epoch 33 finished \tANN training loss 0.012366\n",
      ">> Epoch 34 finished \tANN training loss 0.010943\n",
      ">> Epoch 35 finished \tANN training loss 0.011026\n",
      ">> Epoch 36 finished \tANN training loss 0.009873\n",
      ">> Epoch 37 finished \tANN training loss 0.009909\n",
      ">> Epoch 38 finished \tANN training loss 0.008420\n",
      ">> Epoch 39 finished \tANN training loss 0.009114\n",
      ">> Epoch 40 finished \tANN training loss 0.007491\n",
      ">> Epoch 41 finished \tANN training loss 0.007807\n",
      ">> Epoch 42 finished \tANN training loss 0.006912\n",
      ">> Epoch 43 finished \tANN training loss 0.006836\n",
      ">> Epoch 44 finished \tANN training loss 0.006192\n",
      ">> Epoch 45 finished \tANN training loss 0.005974\n",
      ">> Epoch 46 finished \tANN training loss 0.005571\n",
      ">> Epoch 47 finished \tANN training loss 0.005055\n",
      ">> Epoch 48 finished \tANN training loss 0.005414\n",
      ">> Epoch 49 finished \tANN training loss 0.004368\n",
      ">> Epoch 50 finished \tANN training loss 0.004063\n",
      ">> Epoch 51 finished \tANN training loss 0.004289\n",
      ">> Epoch 52 finished \tANN training loss 0.004410\n",
      ">> Epoch 53 finished \tANN training loss 0.003731\n",
      ">> Epoch 54 finished \tANN training loss 0.003492\n",
      ">> Epoch 55 finished \tANN training loss 0.003437\n",
      ">> Epoch 56 finished \tANN training loss 0.003974\n",
      ">> Epoch 57 finished \tANN training loss 0.003274\n",
      ">> Epoch 58 finished \tANN training loss 0.003593\n",
      ">> Epoch 59 finished \tANN training loss 0.003117\n",
      ">> Epoch 60 finished \tANN training loss 0.003042\n",
      ">> Epoch 61 finished \tANN training loss 0.002655\n",
      ">> Epoch 62 finished \tANN training loss 0.002706\n",
      ">> Epoch 63 finished \tANN training loss 0.002677\n",
      ">> Epoch 64 finished \tANN training loss 0.002640\n",
      ">> Epoch 65 finished \tANN training loss 0.002286\n",
      ">> Epoch 66 finished \tANN training loss 0.002343\n",
      ">> Epoch 67 finished \tANN training loss 0.002021\n",
      ">> Epoch 68 finished \tANN training loss 0.001908\n",
      ">> Epoch 69 finished \tANN training loss 0.001922\n",
      ">> Epoch 70 finished \tANN training loss 0.002169\n",
      ">> Epoch 71 finished \tANN training loss 0.001971\n",
      ">> Epoch 72 finished \tANN training loss 0.001913\n",
      ">> Epoch 73 finished \tANN training loss 0.001818\n",
      ">> Epoch 74 finished \tANN training loss 0.001914\n",
      ">> Epoch 75 finished \tANN training loss 0.001864\n",
      ">> Epoch 76 finished \tANN training loss 0.001706\n",
      ">> Epoch 77 finished \tANN training loss 0.001824\n",
      ">> Epoch 78 finished \tANN training loss 0.001557\n",
      ">> Epoch 79 finished \tANN training loss 0.001725\n",
      ">> Epoch 80 finished \tANN training loss 0.001677\n",
      ">> Epoch 81 finished \tANN training loss 0.001914\n",
      ">> Epoch 82 finished \tANN training loss 0.001608\n",
      ">> Epoch 83 finished \tANN training loss 0.001835\n",
      ">> Epoch 84 finished \tANN training loss 0.001420\n",
      ">> Epoch 85 finished \tANN training loss 0.001372\n",
      ">> Epoch 86 finished \tANN training loss 0.001213\n",
      ">> Epoch 87 finished \tANN training loss 0.001284\n",
      ">> Epoch 88 finished \tANN training loss 0.001029\n",
      ">> Epoch 89 finished \tANN training loss 0.001252\n",
      ">> Epoch 90 finished \tANN training loss 0.001041\n",
      ">> Epoch 91 finished \tANN training loss 0.001204\n",
      ">> Epoch 92 finished \tANN training loss 0.001129\n",
      ">> Epoch 93 finished \tANN training loss 0.001000\n",
      ">> Epoch 94 finished \tANN training loss 0.001000\n",
      ">> Epoch 95 finished \tANN training loss 0.000882\n",
      ">> Epoch 96 finished \tANN training loss 0.000831\n",
      ">> Epoch 97 finished \tANN training loss 0.000905\n",
      ">> Epoch 98 finished \tANN training loss 0.000854\n",
      ">> Epoch 99 finished \tANN training loss 0.000798\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.910000\n"
     ]
    }
   ],
   "source": [
    "rbm_acc[1] = deep_belief_net(learning_rate_rbm=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.91\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(rbm_acc[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setting 3**\n",
    "\n",
    "    learning_rate_rbm=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 110.904007\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 125.835686\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 164.992279\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 136.079025\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 230.743042\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 214.991425\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 219.420624\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 186.010040\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 267.364807\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 164.378845\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 3380.159424\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 2683.055664\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 4320.045898\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 5420.953125\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 4888.116211\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 8385.436523\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 5375.637695\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 5713.787109\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 6178.853516\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 5392.146973\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.974992\n",
      ">> Epoch 1 finished \tANN training loss 0.661995\n",
      ">> Epoch 2 finished \tANN training loss 0.549957\n",
      ">> Epoch 3 finished \tANN training loss 0.470238\n",
      ">> Epoch 4 finished \tANN training loss 0.430156\n",
      ">> Epoch 5 finished \tANN training loss 0.356329\n",
      ">> Epoch 6 finished \tANN training loss 0.332026\n",
      ">> Epoch 7 finished \tANN training loss 0.296107\n",
      ">> Epoch 8 finished \tANN training loss 0.245873\n",
      ">> Epoch 9 finished \tANN training loss 0.238457\n",
      ">> Epoch 10 finished \tANN training loss 0.200607\n",
      ">> Epoch 11 finished \tANN training loss 0.197177\n",
      ">> Epoch 12 finished \tANN training loss 0.182111\n",
      ">> Epoch 13 finished \tANN training loss 0.160953\n",
      ">> Epoch 14 finished \tANN training loss 0.162905\n",
      ">> Epoch 15 finished \tANN training loss 0.161946\n",
      ">> Epoch 16 finished \tANN training loss 0.133252\n",
      ">> Epoch 17 finished \tANN training loss 0.121509\n",
      ">> Epoch 18 finished \tANN training loss 0.109463\n",
      ">> Epoch 19 finished \tANN training loss 0.106086\n",
      ">> Epoch 20 finished \tANN training loss 0.093620\n",
      ">> Epoch 21 finished \tANN training loss 0.081935\n",
      ">> Epoch 22 finished \tANN training loss 0.081763\n",
      ">> Epoch 23 finished \tANN training loss 0.077425\n",
      ">> Epoch 24 finished \tANN training loss 0.071794\n",
      ">> Epoch 25 finished \tANN training loss 0.065713\n",
      ">> Epoch 26 finished \tANN training loss 0.064029\n",
      ">> Epoch 27 finished \tANN training loss 0.060664\n",
      ">> Epoch 28 finished \tANN training loss 0.056845\n",
      ">> Epoch 29 finished \tANN training loss 0.050273\n",
      ">> Epoch 30 finished \tANN training loss 0.052669\n",
      ">> Epoch 31 finished \tANN training loss 0.047205\n",
      ">> Epoch 32 finished \tANN training loss 0.039564\n",
      ">> Epoch 33 finished \tANN training loss 0.039708\n",
      ">> Epoch 34 finished \tANN training loss 0.040156\n",
      ">> Epoch 35 finished \tANN training loss 0.037226\n",
      ">> Epoch 36 finished \tANN training loss 0.039703\n",
      ">> Epoch 37 finished \tANN training loss 0.036133\n",
      ">> Epoch 38 finished \tANN training loss 0.031029\n",
      ">> Epoch 39 finished \tANN training loss 0.028533\n",
      ">> Epoch 40 finished \tANN training loss 0.027219\n",
      ">> Epoch 41 finished \tANN training loss 0.025730\n",
      ">> Epoch 42 finished \tANN training loss 0.029454\n",
      ">> Epoch 43 finished \tANN training loss 0.023459\n",
      ">> Epoch 44 finished \tANN training loss 0.024340\n",
      ">> Epoch 45 finished \tANN training loss 0.021740\n",
      ">> Epoch 46 finished \tANN training loss 0.025161\n",
      ">> Epoch 47 finished \tANN training loss 0.018554\n",
      ">> Epoch 48 finished \tANN training loss 0.018711\n",
      ">> Epoch 49 finished \tANN training loss 0.019660\n",
      ">> Epoch 50 finished \tANN training loss 0.017450\n",
      ">> Epoch 51 finished \tANN training loss 0.015244\n",
      ">> Epoch 52 finished \tANN training loss 0.014013\n",
      ">> Epoch 53 finished \tANN training loss 0.013459\n",
      ">> Epoch 54 finished \tANN training loss 0.014766\n",
      ">> Epoch 55 finished \tANN training loss 0.012963\n",
      ">> Epoch 56 finished \tANN training loss 0.012350\n",
      ">> Epoch 57 finished \tANN training loss 0.012016\n",
      ">> Epoch 58 finished \tANN training loss 0.012834\n",
      ">> Epoch 59 finished \tANN training loss 0.012906\n",
      ">> Epoch 60 finished \tANN training loss 0.011110\n",
      ">> Epoch 61 finished \tANN training loss 0.011643\n",
      ">> Epoch 62 finished \tANN training loss 0.010738\n",
      ">> Epoch 63 finished \tANN training loss 0.011479\n",
      ">> Epoch 64 finished \tANN training loss 0.010109\n",
      ">> Epoch 65 finished \tANN training loss 0.009724\n",
      ">> Epoch 66 finished \tANN training loss 0.008283\n",
      ">> Epoch 67 finished \tANN training loss 0.008818\n",
      ">> Epoch 68 finished \tANN training loss 0.009003\n",
      ">> Epoch 69 finished \tANN training loss 0.007449\n",
      ">> Epoch 70 finished \tANN training loss 0.007127\n",
      ">> Epoch 71 finished \tANN training loss 0.007380\n",
      ">> Epoch 72 finished \tANN training loss 0.006818\n",
      ">> Epoch 73 finished \tANN training loss 0.005973\n",
      ">> Epoch 74 finished \tANN training loss 0.006184\n",
      ">> Epoch 75 finished \tANN training loss 0.006140\n",
      ">> Epoch 76 finished \tANN training loss 0.005986\n",
      ">> Epoch 77 finished \tANN training loss 0.005218\n",
      ">> Epoch 78 finished \tANN training loss 0.005357\n",
      ">> Epoch 79 finished \tANN training loss 0.005804\n",
      ">> Epoch 80 finished \tANN training loss 0.006060\n",
      ">> Epoch 81 finished \tANN training loss 0.005213\n",
      ">> Epoch 82 finished \tANN training loss 0.005856\n",
      ">> Epoch 83 finished \tANN training loss 0.007979\n",
      ">> Epoch 84 finished \tANN training loss 0.004489\n",
      ">> Epoch 85 finished \tANN training loss 0.004373\n",
      ">> Epoch 86 finished \tANN training loss 0.005216\n",
      ">> Epoch 87 finished \tANN training loss 0.004771\n",
      ">> Epoch 88 finished \tANN training loss 0.005004\n",
      ">> Epoch 89 finished \tANN training loss 0.004507\n",
      ">> Epoch 90 finished \tANN training loss 0.004008\n",
      ">> Epoch 91 finished \tANN training loss 0.004959\n",
      ">> Epoch 92 finished \tANN training loss 0.003502\n",
      ">> Epoch 93 finished \tANN training loss 0.003327\n",
      ">> Epoch 94 finished \tANN training loss 0.003738\n",
      ">> Epoch 95 finished \tANN training loss 0.003221\n",
      ">> Epoch 96 finished \tANN training loss 0.002786\n",
      ">> Epoch 97 finished \tANN training loss 0.002647\n",
      ">> Epoch 98 finished \tANN training loss 0.002955\n",
      ">> Epoch 99 finished \tANN training loss 0.002613\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.885000\n"
     ]
    }
   ],
   "source": [
    "rbm_acc[2] = deep_belief_net(learning_rate_rbm=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.885\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(rbm_acc[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setting 4**\n",
    "\n",
    "    learning_rate_rbm=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 31779.625000\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 189873.421875\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 180143.687500\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 224904.078125\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 170434.359375\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 164641.656250\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 263771.000000\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 274975.156250\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 245861.359375\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 189474.312500\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 846481653760.000000\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 790017802240.000000\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 3146633969664.000000\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 2016595542016.000000\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 1990187155456.000000\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 983176052736.000000\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 2778203160576.000000\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 1564428730368.000000\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 2989057114112.000000\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 1582784708608.000000\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 2.300706\n",
      ">> Epoch 1 finished \tANN training loss 2.298146\n",
      ">> Epoch 2 finished \tANN training loss 2.293770\n",
      ">> Epoch 3 finished \tANN training loss 2.284529\n",
      ">> Epoch 4 finished \tANN training loss 2.262846\n",
      ">> Epoch 5 finished \tANN training loss 2.232202\n",
      ">> Epoch 6 finished \tANN training loss 2.208014\n",
      ">> Epoch 7 finished \tANN training loss 2.189869\n",
      ">> Epoch 8 finished \tANN training loss 2.179913\n",
      ">> Epoch 9 finished \tANN training loss 2.171853\n",
      ">> Epoch 10 finished \tANN training loss 2.166882\n",
      ">> Epoch 11 finished \tANN training loss 2.160546\n",
      ">> Epoch 12 finished \tANN training loss 2.154613\n",
      ">> Epoch 13 finished \tANN training loss 2.150535\n",
      ">> Epoch 14 finished \tANN training loss 2.142323\n",
      ">> Epoch 15 finished \tANN training loss 2.138678\n",
      ">> Epoch 16 finished \tANN training loss 2.133862\n",
      ">> Epoch 17 finished \tANN training loss 2.131676\n",
      ">> Epoch 18 finished \tANN training loss 2.130769\n",
      ">> Epoch 19 finished \tANN training loss 2.129317\n",
      ">> Epoch 20 finished \tANN training loss 2.127237\n",
      ">> Epoch 21 finished \tANN training loss 2.125255\n",
      ">> Epoch 22 finished \tANN training loss 2.123509\n",
      ">> Epoch 23 finished \tANN training loss 2.121984\n",
      ">> Epoch 24 finished \tANN training loss 2.121421\n",
      ">> Epoch 25 finished \tANN training loss 2.118954\n",
      ">> Epoch 26 finished \tANN training loss 2.114867\n",
      ">> Epoch 27 finished \tANN training loss 2.107084\n",
      ">> Epoch 28 finished \tANN training loss 2.096528\n",
      ">> Epoch 29 finished \tANN training loss 2.091860\n",
      ">> Epoch 30 finished \tANN training loss 2.085374\n",
      ">> Epoch 31 finished \tANN training loss 2.083824\n",
      ">> Epoch 32 finished \tANN training loss 2.077761\n",
      ">> Epoch 33 finished \tANN training loss 2.065537\n",
      ">> Epoch 34 finished \tANN training loss 2.055930\n",
      ">> Epoch 35 finished \tANN training loss 2.034392\n",
      ">> Epoch 36 finished \tANN training loss 2.007093\n",
      ">> Epoch 37 finished \tANN training loss 1.969934\n",
      ">> Epoch 38 finished \tANN training loss 1.931400\n",
      ">> Epoch 39 finished \tANN training loss 1.913904\n",
      ">> Epoch 40 finished \tANN training loss 1.857626\n",
      ">> Epoch 41 finished \tANN training loss 1.823647\n",
      ">> Epoch 42 finished \tANN training loss 1.778742\n",
      ">> Epoch 43 finished \tANN training loss 1.753495\n",
      ">> Epoch 44 finished \tANN training loss 1.710771\n",
      ">> Epoch 45 finished \tANN training loss 1.720669\n",
      ">> Epoch 46 finished \tANN training loss 1.646131\n",
      ">> Epoch 47 finished \tANN training loss 1.614182\n",
      ">> Epoch 48 finished \tANN training loss 1.551993\n",
      ">> Epoch 49 finished \tANN training loss 1.526839\n",
      ">> Epoch 50 finished \tANN training loss 1.491676\n",
      ">> Epoch 51 finished \tANN training loss 1.468890\n",
      ">> Epoch 52 finished \tANN training loss 1.458467\n",
      ">> Epoch 53 finished \tANN training loss 1.398917\n",
      ">> Epoch 54 finished \tANN training loss 1.390435\n",
      ">> Epoch 55 finished \tANN training loss 1.355867\n",
      ">> Epoch 56 finished \tANN training loss 1.329674\n",
      ">> Epoch 57 finished \tANN training loss 1.339947\n",
      ">> Epoch 58 finished \tANN training loss 1.304133\n",
      ">> Epoch 59 finished \tANN training loss 1.327821\n",
      ">> Epoch 60 finished \tANN training loss 1.268801\n",
      ">> Epoch 61 finished \tANN training loss 1.241808\n",
      ">> Epoch 62 finished \tANN training loss 1.202110\n",
      ">> Epoch 63 finished \tANN training loss 1.185184\n",
      ">> Epoch 64 finished \tANN training loss 1.178161\n",
      ">> Epoch 65 finished \tANN training loss 1.180296\n",
      ">> Epoch 66 finished \tANN training loss 1.135762\n",
      ">> Epoch 67 finished \tANN training loss 1.127119\n",
      ">> Epoch 68 finished \tANN training loss 1.137036\n",
      ">> Epoch 69 finished \tANN training loss 1.104631\n",
      ">> Epoch 70 finished \tANN training loss 1.115334\n",
      ">> Epoch 71 finished \tANN training loss 1.089910\n",
      ">> Epoch 72 finished \tANN training loss 1.119207\n",
      ">> Epoch 73 finished \tANN training loss 1.065434\n",
      ">> Epoch 74 finished \tANN training loss 0.999465\n",
      ">> Epoch 75 finished \tANN training loss 0.991289\n",
      ">> Epoch 76 finished \tANN training loss 1.002694\n",
      ">> Epoch 77 finished \tANN training loss 1.033328\n",
      ">> Epoch 78 finished \tANN training loss 0.946839\n",
      ">> Epoch 79 finished \tANN training loss 0.912989\n",
      ">> Epoch 80 finished \tANN training loss 0.924845\n",
      ">> Epoch 81 finished \tANN training loss 0.935192\n",
      ">> Epoch 82 finished \tANN training loss 0.874035\n",
      ">> Epoch 83 finished \tANN training loss 0.920369\n",
      ">> Epoch 84 finished \tANN training loss 0.896667\n",
      ">> Epoch 85 finished \tANN training loss 0.860983\n",
      ">> Epoch 86 finished \tANN training loss 0.876610\n",
      ">> Epoch 87 finished \tANN training loss 0.844067\n",
      ">> Epoch 88 finished \tANN training loss 0.871358\n",
      ">> Epoch 89 finished \tANN training loss 0.841526\n",
      ">> Epoch 90 finished \tANN training loss 0.862323\n",
      ">> Epoch 91 finished \tANN training loss 0.847701\n",
      ">> Epoch 92 finished \tANN training loss 0.840739\n",
      ">> Epoch 93 finished \tANN training loss 0.871223\n",
      ">> Epoch 94 finished \tANN training loss 0.849284\n",
      ">> Epoch 95 finished \tANN training loss 0.854567\n",
      ">> Epoch 96 finished \tANN training loss 0.837644\n",
      ">> Epoch 97 finished \tANN training loss 0.807640\n",
      ">> Epoch 98 finished \tANN training loss 0.855461\n",
      ">> Epoch 99 finished \tANN training loss 0.807312\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.595000\n"
     ]
    }
   ],
   "source": [
    "rbm_acc[3] = deep_belief_net(learning_rate_rbm=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.595\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(rbm_acc[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setting 5**\n",
    "\n",
    "    learning_rate_rbm=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 22193574.000000\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 34716188.000000\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 30751990.000000\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 29281052.000000\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 18426390.000000\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 17048326.000000\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 24133926.000000\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 26676414.000000\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 26827366.000000\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 27220286.000000\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 2080750301580623872.000000\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 811598117131517952.000000\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 1798525045622964224.000000\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 3235358044707618816.000000\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 2510459099485831168.000000\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 2100549757217800192.000000\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 2453775701661188096.000000\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 3079025633179729920.000000\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 4631429398971547648.000000\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 2737053877441396736.000000\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 2.297242\n",
      ">> Epoch 1 finished \tANN training loss 2.274268\n",
      ">> Epoch 2 finished \tANN training loss 2.262210\n",
      ">> Epoch 3 finished \tANN training loss 2.240759\n",
      ">> Epoch 4 finished \tANN training loss 2.202708\n",
      ">> Epoch 5 finished \tANN training loss 2.157465\n",
      ">> Epoch 6 finished \tANN training loss 2.086788\n",
      ">> Epoch 7 finished \tANN training loss 2.038932\n",
      ">> Epoch 8 finished \tANN training loss 2.001882\n",
      ">> Epoch 9 finished \tANN training loss 1.974601\n",
      ">> Epoch 10 finished \tANN training loss 1.962472\n",
      ">> Epoch 11 finished \tANN training loss 1.934985\n",
      ">> Epoch 12 finished \tANN training loss 1.925109\n",
      ">> Epoch 13 finished \tANN training loss 1.917198\n",
      ">> Epoch 14 finished \tANN training loss 1.889538\n",
      ">> Epoch 15 finished \tANN training loss 1.873553\n",
      ">> Epoch 16 finished \tANN training loss 1.865580\n",
      ">> Epoch 17 finished \tANN training loss 1.859376\n",
      ">> Epoch 18 finished \tANN training loss 1.865465\n",
      ">> Epoch 19 finished \tANN training loss 1.857389\n",
      ">> Epoch 20 finished \tANN training loss 1.832351\n",
      ">> Epoch 21 finished \tANN training loss 1.842289\n",
      ">> Epoch 22 finished \tANN training loss 1.805307\n",
      ">> Epoch 23 finished \tANN training loss 1.806518\n",
      ">> Epoch 24 finished \tANN training loss 1.818427\n",
      ">> Epoch 25 finished \tANN training loss 1.812820\n",
      ">> Epoch 26 finished \tANN training loss 1.800590\n",
      ">> Epoch 27 finished \tANN training loss 1.821038\n",
      ">> Epoch 28 finished \tANN training loss 1.789669\n",
      ">> Epoch 29 finished \tANN training loss 1.787685\n",
      ">> Epoch 30 finished \tANN training loss 1.762848\n",
      ">> Epoch 31 finished \tANN training loss 1.769935\n",
      ">> Epoch 32 finished \tANN training loss 1.770095\n",
      ">> Epoch 33 finished \tANN training loss 1.772374\n",
      ">> Epoch 34 finished \tANN training loss 1.816199\n",
      ">> Epoch 35 finished \tANN training loss 1.799861\n",
      ">> Epoch 36 finished \tANN training loss 1.791363\n",
      ">> Epoch 37 finished \tANN training loss 1.752728\n",
      ">> Epoch 38 finished \tANN training loss 1.755200\n",
      ">> Epoch 39 finished \tANN training loss 1.746279\n",
      ">> Epoch 40 finished \tANN training loss 1.750928\n",
      ">> Epoch 41 finished \tANN training loss 1.775592\n",
      ">> Epoch 42 finished \tANN training loss 1.726165\n",
      ">> Epoch 43 finished \tANN training loss 1.750028\n",
      ">> Epoch 44 finished \tANN training loss 1.756253\n",
      ">> Epoch 45 finished \tANN training loss 1.736169\n",
      ">> Epoch 46 finished \tANN training loss 1.726813\n",
      ">> Epoch 47 finished \tANN training loss 1.746619\n",
      ">> Epoch 48 finished \tANN training loss 1.724770\n",
      ">> Epoch 49 finished \tANN training loss 1.783655\n",
      ">> Epoch 50 finished \tANN training loss 1.711332\n",
      ">> Epoch 51 finished \tANN training loss 1.733179\n",
      ">> Epoch 52 finished \tANN training loss 1.706090\n",
      ">> Epoch 53 finished \tANN training loss 1.755092\n",
      ">> Epoch 54 finished \tANN training loss 1.724696\n",
      ">> Epoch 55 finished \tANN training loss 1.702846\n",
      ">> Epoch 56 finished \tANN training loss 1.700244\n",
      ">> Epoch 57 finished \tANN training loss 1.698678\n",
      ">> Epoch 58 finished \tANN training loss 1.721877\n",
      ">> Epoch 59 finished \tANN training loss 1.772423\n",
      ">> Epoch 60 finished \tANN training loss 1.685490\n",
      ">> Epoch 61 finished \tANN training loss 1.689682\n",
      ">> Epoch 62 finished \tANN training loss 1.691927\n",
      ">> Epoch 63 finished \tANN training loss 1.701385\n",
      ">> Epoch 64 finished \tANN training loss 1.699016\n",
      ">> Epoch 65 finished \tANN training loss 1.684073\n",
      ">> Epoch 66 finished \tANN training loss 1.704044\n",
      ">> Epoch 67 finished \tANN training loss 1.715825\n",
      ">> Epoch 68 finished \tANN training loss 1.675861\n",
      ">> Epoch 69 finished \tANN training loss 1.701425\n",
      ">> Epoch 70 finished \tANN training loss 1.710229\n",
      ">> Epoch 71 finished \tANN training loss 1.676245\n",
      ">> Epoch 72 finished \tANN training loss 1.726708\n",
      ">> Epoch 73 finished \tANN training loss 1.713694\n",
      ">> Epoch 74 finished \tANN training loss 1.668636\n",
      ">> Epoch 75 finished \tANN training loss 1.685295\n",
      ">> Epoch 76 finished \tANN training loss 1.657891\n",
      ">> Epoch 77 finished \tANN training loss 1.718065\n",
      ">> Epoch 78 finished \tANN training loss 1.752952\n",
      ">> Epoch 79 finished \tANN training loss 1.709124\n",
      ">> Epoch 80 finished \tANN training loss 1.661495\n",
      ">> Epoch 81 finished \tANN training loss 1.670639\n",
      ">> Epoch 82 finished \tANN training loss 1.656527\n",
      ">> Epoch 83 finished \tANN training loss 1.761409\n",
      ">> Epoch 84 finished \tANN training loss 1.702537\n",
      ">> Epoch 85 finished \tANN training loss 1.722679\n",
      ">> Epoch 86 finished \tANN training loss 1.662428\n",
      ">> Epoch 87 finished \tANN training loss 1.792198\n",
      ">> Epoch 88 finished \tANN training loss 1.753308\n",
      ">> Epoch 89 finished \tANN training loss 1.743749\n",
      ">> Epoch 90 finished \tANN training loss 1.797779\n",
      ">> Epoch 91 finished \tANN training loss 1.681832\n",
      ">> Epoch 92 finished \tANN training loss 1.715062\n",
      ">> Epoch 93 finished \tANN training loss 1.711773\n",
      ">> Epoch 94 finished \tANN training loss 1.648828\n",
      ">> Epoch 95 finished \tANN training loss 1.763949\n",
      ">> Epoch 96 finished \tANN training loss 1.747831\n",
      ">> Epoch 97 finished \tANN training loss 1.678146\n",
      ">> Epoch 98 finished \tANN training loss 1.671631\n",
      ">> Epoch 99 finished \tANN training loss 1.641885\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.215000\n"
     ]
    }
   ],
   "source": [
    "rbm_acc[4] = deep_belief_net(learning_rate_rbm=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.215\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(rbm_acc[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.89500000000000002, 0.91000000000000003, 0.88500000000000001, 0.59499999999999997, 0.215]\n",
      "Most accurate rbm learning_rate setting is Setting 2\n"
     ]
    }
   ],
   "source": [
    "# Collated results\n",
    "print(rbm_acc)\n",
    "print('Most accurate rbm learning_rate setting is Setting ' + str(rbm_acc.index(max(rbm_acc)) + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEgCAYAAABb8m8VAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSVLHUJAkdQwFSVLnfwBd7K+ciUwWCAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1cd03cbae10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create bar graph\n",
    "plt.clf()\n",
    "objects = ('0.01', '0.05', '0.10', '0.50', '1.0')\n",
    "y_pos = np.arange(len(objects))\n",
    "performance = [0.895,0.91,0.885,0.595, 0.215]\n",
    "\n",
    "plt.bar(y_pos, performance, align='center', alpha=0.8)\n",
    "plt.xticks(y_pos, objects, rotation=30)\n",
    "plt.xlabel('Settings')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('learning_rate_rbm Settings')\n",
    " \n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different Learning Rates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store accuracies\n",
    "learning_rate_acc = [0, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setting 1**\n",
    "\n",
    "    learning_rate=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 80.508308\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 56.202160\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 76.771797\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 61.637432\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 73.832054\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 68.935799\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 89.181679\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 76.366074\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 89.140671\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 61.519100\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 143.703171\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 171.010208\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 214.397614\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 192.322067\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 199.107819\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 192.237610\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 207.964966\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 225.997635\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 222.943314\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 298.965149\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 1.605582\n",
      ">> Epoch 1 finished \tANN training loss 1.256691\n",
      ">> Epoch 2 finished \tANN training loss 1.041892\n",
      ">> Epoch 3 finished \tANN training loss 0.908176\n",
      ">> Epoch 4 finished \tANN training loss 0.817185\n",
      ">> Epoch 5 finished \tANN training loss 0.746655\n",
      ">> Epoch 6 finished \tANN training loss 0.692895\n",
      ">> Epoch 7 finished \tANN training loss 0.651748\n",
      ">> Epoch 8 finished \tANN training loss 0.615956\n",
      ">> Epoch 9 finished \tANN training loss 0.591655\n",
      ">> Epoch 10 finished \tANN training loss 0.563548\n",
      ">> Epoch 11 finished \tANN training loss 0.540059\n",
      ">> Epoch 12 finished \tANN training loss 0.524468\n",
      ">> Epoch 13 finished \tANN training loss 0.504133\n",
      ">> Epoch 14 finished \tANN training loss 0.490128\n",
      ">> Epoch 15 finished \tANN training loss 0.474626\n",
      ">> Epoch 16 finished \tANN training loss 0.467938\n",
      ">> Epoch 17 finished \tANN training loss 0.452328\n",
      ">> Epoch 18 finished \tANN training loss 0.441225\n",
      ">> Epoch 19 finished \tANN training loss 0.429239\n",
      ">> Epoch 20 finished \tANN training loss 0.423407\n",
      ">> Epoch 21 finished \tANN training loss 0.413563\n",
      ">> Epoch 22 finished \tANN training loss 0.404902\n",
      ">> Epoch 23 finished \tANN training loss 0.395966\n",
      ">> Epoch 24 finished \tANN training loss 0.389757\n",
      ">> Epoch 25 finished \tANN training loss 0.382781\n",
      ">> Epoch 26 finished \tANN training loss 0.375894\n",
      ">> Epoch 27 finished \tANN training loss 0.365179\n",
      ">> Epoch 28 finished \tANN training loss 0.359713\n",
      ">> Epoch 29 finished \tANN training loss 0.352262\n",
      ">> Epoch 30 finished \tANN training loss 0.349488\n",
      ">> Epoch 31 finished \tANN training loss 0.340875\n",
      ">> Epoch 32 finished \tANN training loss 0.338723\n",
      ">> Epoch 33 finished \tANN training loss 0.333318\n",
      ">> Epoch 34 finished \tANN training loss 0.326745\n",
      ">> Epoch 35 finished \tANN training loss 0.322652\n",
      ">> Epoch 36 finished \tANN training loss 0.316992\n",
      ">> Epoch 37 finished \tANN training loss 0.313476\n",
      ">> Epoch 38 finished \tANN training loss 0.306860\n",
      ">> Epoch 39 finished \tANN training loss 0.304439\n",
      ">> Epoch 40 finished \tANN training loss 0.297698\n",
      ">> Epoch 41 finished \tANN training loss 0.295981\n",
      ">> Epoch 42 finished \tANN training loss 0.289961\n",
      ">> Epoch 43 finished \tANN training loss 0.286322\n",
      ">> Epoch 44 finished \tANN training loss 0.280451\n",
      ">> Epoch 45 finished \tANN training loss 0.276129\n",
      ">> Epoch 46 finished \tANN training loss 0.274126\n",
      ">> Epoch 47 finished \tANN training loss 0.268594\n",
      ">> Epoch 48 finished \tANN training loss 0.265549\n",
      ">> Epoch 49 finished \tANN training loss 0.267172\n",
      ">> Epoch 50 finished \tANN training loss 0.259268\n",
      ">> Epoch 51 finished \tANN training loss 0.259059\n",
      ">> Epoch 52 finished \tANN training loss 0.251981\n",
      ">> Epoch 53 finished \tANN training loss 0.256523\n",
      ">> Epoch 54 finished \tANN training loss 0.245687\n",
      ">> Epoch 55 finished \tANN training loss 0.243593\n",
      ">> Epoch 56 finished \tANN training loss 0.241691\n",
      ">> Epoch 57 finished \tANN training loss 0.237249\n",
      ">> Epoch 58 finished \tANN training loss 0.235186\n",
      ">> Epoch 59 finished \tANN training loss 0.233754\n",
      ">> Epoch 60 finished \tANN training loss 0.229291\n",
      ">> Epoch 61 finished \tANN training loss 0.225640\n",
      ">> Epoch 62 finished \tANN training loss 0.223897\n",
      ">> Epoch 63 finished \tANN training loss 0.219965\n",
      ">> Epoch 64 finished \tANN training loss 0.217309\n",
      ">> Epoch 65 finished \tANN training loss 0.214615\n",
      ">> Epoch 66 finished \tANN training loss 0.212048\n",
      ">> Epoch 67 finished \tANN training loss 0.210142\n",
      ">> Epoch 68 finished \tANN training loss 0.206790\n",
      ">> Epoch 69 finished \tANN training loss 0.206273\n",
      ">> Epoch 70 finished \tANN training loss 0.202590\n",
      ">> Epoch 71 finished \tANN training loss 0.198663\n",
      ">> Epoch 72 finished \tANN training loss 0.198622\n",
      ">> Epoch 73 finished \tANN training loss 0.195035\n",
      ">> Epoch 74 finished \tANN training loss 0.192717\n",
      ">> Epoch 75 finished \tANN training loss 0.191702\n",
      ">> Epoch 76 finished \tANN training loss 0.190267\n",
      ">> Epoch 77 finished \tANN training loss 0.186787\n",
      ">> Epoch 78 finished \tANN training loss 0.185742\n",
      ">> Epoch 79 finished \tANN training loss 0.183775\n",
      ">> Epoch 80 finished \tANN training loss 0.181876\n",
      ">> Epoch 81 finished \tANN training loss 0.179821\n",
      ">> Epoch 82 finished \tANN training loss 0.178532\n",
      ">> Epoch 83 finished \tANN training loss 0.174873\n",
      ">> Epoch 84 finished \tANN training loss 0.172816\n",
      ">> Epoch 85 finished \tANN training loss 0.171760\n",
      ">> Epoch 86 finished \tANN training loss 0.169226\n",
      ">> Epoch 87 finished \tANN training loss 0.168151\n",
      ">> Epoch 88 finished \tANN training loss 0.165527\n",
      ">> Epoch 89 finished \tANN training loss 0.164152\n",
      ">> Epoch 90 finished \tANN training loss 0.163154\n",
      ">> Epoch 91 finished \tANN training loss 0.163119\n",
      ">> Epoch 92 finished \tANN training loss 0.159858\n",
      ">> Epoch 93 finished \tANN training loss 0.159550\n",
      ">> Epoch 94 finished \tANN training loss 0.156982\n",
      ">> Epoch 95 finished \tANN training loss 0.155653\n",
      ">> Epoch 96 finished \tANN training loss 0.154482\n",
      ">> Epoch 97 finished \tANN training loss 0.151421\n",
      ">> Epoch 98 finished \tANN training loss 0.148886\n",
      ">> Epoch 99 finished \tANN training loss 0.148983\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.885000\n"
     ]
    }
   ],
   "source": [
    "learning_rate_acc[0] = deep_belief_net(learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.885\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(learning_rate_acc[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setting 2**\n",
    "\n",
    "    learning_rate=0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 81.708733\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 55.849056\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 48.589561\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 45.749756\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 44.211746\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 54.903095\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 42.353642\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 43.882450\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 59.503819\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 50.491993\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 222.933167\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 371.112885\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 420.357605\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 200.313187\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 319.664520\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 271.096985\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 297.548096\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 308.967896\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 279.965790\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 336.806732\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.832086\n",
      ">> Epoch 1 finished \tANN training loss 0.631094\n",
      ">> Epoch 2 finished \tANN training loss 0.499987\n",
      ">> Epoch 3 finished \tANN training loss 0.446935\n",
      ">> Epoch 4 finished \tANN training loss 0.380466\n",
      ">> Epoch 5 finished \tANN training loss 0.343083\n",
      ">> Epoch 6 finished \tANN training loss 0.327225\n",
      ">> Epoch 7 finished \tANN training loss 0.308603\n",
      ">> Epoch 8 finished \tANN training loss 0.269831\n",
      ">> Epoch 9 finished \tANN training loss 0.279059\n",
      ">> Epoch 10 finished \tANN training loss 0.245699\n",
      ">> Epoch 11 finished \tANN training loss 0.228884\n",
      ">> Epoch 12 finished \tANN training loss 0.221781\n",
      ">> Epoch 13 finished \tANN training loss 0.202035\n",
      ">> Epoch 14 finished \tANN training loss 0.191175\n",
      ">> Epoch 15 finished \tANN training loss 0.173313\n",
      ">> Epoch 16 finished \tANN training loss 0.170683\n",
      ">> Epoch 17 finished \tANN training loss 0.160590\n",
      ">> Epoch 18 finished \tANN training loss 0.147598\n",
      ">> Epoch 19 finished \tANN training loss 0.146364\n",
      ">> Epoch 20 finished \tANN training loss 0.136079\n",
      ">> Epoch 21 finished \tANN training loss 0.130020\n",
      ">> Epoch 22 finished \tANN training loss 0.122344\n",
      ">> Epoch 23 finished \tANN training loss 0.106600\n",
      ">> Epoch 24 finished \tANN training loss 0.101626\n",
      ">> Epoch 25 finished \tANN training loss 0.095792\n",
      ">> Epoch 26 finished \tANN training loss 0.091160\n",
      ">> Epoch 27 finished \tANN training loss 0.084276\n",
      ">> Epoch 28 finished \tANN training loss 0.080266\n",
      ">> Epoch 29 finished \tANN training loss 0.081004\n",
      ">> Epoch 30 finished \tANN training loss 0.074666\n",
      ">> Epoch 31 finished \tANN training loss 0.071260\n",
      ">> Epoch 32 finished \tANN training loss 0.064608\n",
      ">> Epoch 33 finished \tANN training loss 0.063873\n",
      ">> Epoch 34 finished \tANN training loss 0.062643\n",
      ">> Epoch 35 finished \tANN training loss 0.056112\n",
      ">> Epoch 36 finished \tANN training loss 0.055827\n",
      ">> Epoch 37 finished \tANN training loss 0.052442\n",
      ">> Epoch 38 finished \tANN training loss 0.046361\n",
      ">> Epoch 39 finished \tANN training loss 0.044391\n",
      ">> Epoch 40 finished \tANN training loss 0.044546\n",
      ">> Epoch 41 finished \tANN training loss 0.046930\n",
      ">> Epoch 42 finished \tANN training loss 0.041922\n",
      ">> Epoch 43 finished \tANN training loss 0.038258\n",
      ">> Epoch 44 finished \tANN training loss 0.036549\n",
      ">> Epoch 45 finished \tANN training loss 0.034326\n",
      ">> Epoch 46 finished \tANN training loss 0.035937\n",
      ">> Epoch 47 finished \tANN training loss 0.034378\n",
      ">> Epoch 48 finished \tANN training loss 0.030205\n",
      ">> Epoch 49 finished \tANN training loss 0.030137\n",
      ">> Epoch 50 finished \tANN training loss 0.027596\n",
      ">> Epoch 51 finished \tANN training loss 0.026496\n",
      ">> Epoch 52 finished \tANN training loss 0.026889\n",
      ">> Epoch 53 finished \tANN training loss 0.023991\n",
      ">> Epoch 54 finished \tANN training loss 0.023917\n",
      ">> Epoch 55 finished \tANN training loss 0.022650\n",
      ">> Epoch 56 finished \tANN training loss 0.023188\n",
      ">> Epoch 57 finished \tANN training loss 0.021756\n",
      ">> Epoch 58 finished \tANN training loss 0.021397\n",
      ">> Epoch 59 finished \tANN training loss 0.021832\n",
      ">> Epoch 60 finished \tANN training loss 0.018921\n",
      ">> Epoch 61 finished \tANN training loss 0.018720\n",
      ">> Epoch 62 finished \tANN training loss 0.017264\n",
      ">> Epoch 63 finished \tANN training loss 0.017214\n",
      ">> Epoch 64 finished \tANN training loss 0.015186\n",
      ">> Epoch 65 finished \tANN training loss 0.016196\n",
      ">> Epoch 66 finished \tANN training loss 0.015223\n",
      ">> Epoch 67 finished \tANN training loss 0.013936\n",
      ">> Epoch 68 finished \tANN training loss 0.014372\n",
      ">> Epoch 69 finished \tANN training loss 0.013984\n",
      ">> Epoch 70 finished \tANN training loss 0.013033\n",
      ">> Epoch 71 finished \tANN training loss 0.014657\n",
      ">> Epoch 72 finished \tANN training loss 0.011918\n",
      ">> Epoch 73 finished \tANN training loss 0.011618\n",
      ">> Epoch 74 finished \tANN training loss 0.012326\n",
      ">> Epoch 75 finished \tANN training loss 0.010860\n",
      ">> Epoch 76 finished \tANN training loss 0.011137\n",
      ">> Epoch 77 finished \tANN training loss 0.010364\n",
      ">> Epoch 78 finished \tANN training loss 0.009984\n",
      ">> Epoch 79 finished \tANN training loss 0.009691\n",
      ">> Epoch 80 finished \tANN training loss 0.009287\n",
      ">> Epoch 81 finished \tANN training loss 0.009500\n",
      ">> Epoch 82 finished \tANN training loss 0.008652\n",
      ">> Epoch 83 finished \tANN training loss 0.009191\n",
      ">> Epoch 84 finished \tANN training loss 0.008879\n",
      ">> Epoch 85 finished \tANN training loss 0.007862\n",
      ">> Epoch 86 finished \tANN training loss 0.007762\n",
      ">> Epoch 87 finished \tANN training loss 0.007817\n",
      ">> Epoch 88 finished \tANN training loss 0.007047\n",
      ">> Epoch 89 finished \tANN training loss 0.006941\n",
      ">> Epoch 90 finished \tANN training loss 0.006881\n",
      ">> Epoch 91 finished \tANN training loss 0.006879\n",
      ">> Epoch 92 finished \tANN training loss 0.006168\n",
      ">> Epoch 93 finished \tANN training loss 0.006342\n",
      ">> Epoch 94 finished \tANN training loss 0.006365\n",
      ">> Epoch 95 finished \tANN training loss 0.005720\n",
      ">> Epoch 96 finished \tANN training loss 0.005604\n",
      ">> Epoch 97 finished \tANN training loss 0.005754\n",
      ">> Epoch 98 finished \tANN training loss 0.005939\n",
      ">> Epoch 99 finished \tANN training loss 0.005114\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.895000\n"
     ]
    }
   ],
   "source": [
    "learning_rate_acc[1] = deep_belief_net(learning_rate=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.895\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(learning_rate_acc[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setting 3**\n",
    "\n",
    "    learning_rate=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 69.982780\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 36.274624\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 28.071461\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 46.687866\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 21.562672\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 54.380157\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 31.012707\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 36.234512\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 40.408497\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 34.521046\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 232.677383\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 128.166565\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 133.991409\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 131.393631\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 141.272736\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 180.160980\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 186.990906\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 69.389183\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 124.187363\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 177.669907\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.635615\n",
      ">> Epoch 1 finished \tANN training loss 0.444051\n",
      ">> Epoch 2 finished \tANN training loss 0.358084\n",
      ">> Epoch 3 finished \tANN training loss 0.313933\n",
      ">> Epoch 4 finished \tANN training loss 0.280643\n",
      ">> Epoch 5 finished \tANN training loss 0.218436\n",
      ">> Epoch 6 finished \tANN training loss 0.206894\n",
      ">> Epoch 7 finished \tANN training loss 0.173023\n",
      ">> Epoch 8 finished \tANN training loss 0.158146\n",
      ">> Epoch 9 finished \tANN training loss 0.136123\n",
      ">> Epoch 10 finished \tANN training loss 0.120227\n",
      ">> Epoch 11 finished \tANN training loss 0.113445\n",
      ">> Epoch 12 finished \tANN training loss 0.105395\n",
      ">> Epoch 13 finished \tANN training loss 0.080144\n",
      ">> Epoch 14 finished \tANN training loss 0.076876\n",
      ">> Epoch 15 finished \tANN training loss 0.070847\n",
      ">> Epoch 16 finished \tANN training loss 0.060769\n",
      ">> Epoch 17 finished \tANN training loss 0.054252\n",
      ">> Epoch 18 finished \tANN training loss 0.047125\n",
      ">> Epoch 19 finished \tANN training loss 0.039209\n",
      ">> Epoch 20 finished \tANN training loss 0.037317\n",
      ">> Epoch 21 finished \tANN training loss 0.037025\n",
      ">> Epoch 22 finished \tANN training loss 0.031349\n",
      ">> Epoch 23 finished \tANN training loss 0.029862\n",
      ">> Epoch 24 finished \tANN training loss 0.025357\n",
      ">> Epoch 25 finished \tANN training loss 0.022290\n",
      ">> Epoch 26 finished \tANN training loss 0.022562\n",
      ">> Epoch 27 finished \tANN training loss 0.018725\n",
      ">> Epoch 28 finished \tANN training loss 0.017980\n",
      ">> Epoch 29 finished \tANN training loss 0.015338\n",
      ">> Epoch 30 finished \tANN training loss 0.014259\n",
      ">> Epoch 31 finished \tANN training loss 0.012136\n",
      ">> Epoch 32 finished \tANN training loss 0.012440\n",
      ">> Epoch 33 finished \tANN training loss 0.011384\n",
      ">> Epoch 34 finished \tANN training loss 0.010551\n",
      ">> Epoch 35 finished \tANN training loss 0.011239\n",
      ">> Epoch 36 finished \tANN training loss 0.009546\n",
      ">> Epoch 37 finished \tANN training loss 0.008967\n",
      ">> Epoch 38 finished \tANN training loss 0.007785\n",
      ">> Epoch 39 finished \tANN training loss 0.007889\n",
      ">> Epoch 40 finished \tANN training loss 0.007497\n",
      ">> Epoch 41 finished \tANN training loss 0.006230\n",
      ">> Epoch 42 finished \tANN training loss 0.005336\n",
      ">> Epoch 43 finished \tANN training loss 0.005462\n",
      ">> Epoch 44 finished \tANN training loss 0.005591\n",
      ">> Epoch 45 finished \tANN training loss 0.006107\n",
      ">> Epoch 46 finished \tANN training loss 0.004970\n",
      ">> Epoch 47 finished \tANN training loss 0.004611\n",
      ">> Epoch 48 finished \tANN training loss 0.004077\n",
      ">> Epoch 49 finished \tANN training loss 0.004403\n",
      ">> Epoch 50 finished \tANN training loss 0.004114\n",
      ">> Epoch 51 finished \tANN training loss 0.004387\n",
      ">> Epoch 52 finished \tANN training loss 0.003674\n",
      ">> Epoch 53 finished \tANN training loss 0.003555\n",
      ">> Epoch 54 finished \tANN training loss 0.003395\n",
      ">> Epoch 55 finished \tANN training loss 0.003470\n",
      ">> Epoch 56 finished \tANN training loss 0.002933\n",
      ">> Epoch 57 finished \tANN training loss 0.002721\n",
      ">> Epoch 58 finished \tANN training loss 0.002661\n",
      ">> Epoch 59 finished \tANN training loss 0.002753\n",
      ">> Epoch 60 finished \tANN training loss 0.002514\n",
      ">> Epoch 61 finished \tANN training loss 0.002459\n",
      ">> Epoch 62 finished \tANN training loss 0.002155\n",
      ">> Epoch 63 finished \tANN training loss 0.002020\n",
      ">> Epoch 64 finished \tANN training loss 0.002050\n",
      ">> Epoch 65 finished \tANN training loss 0.002497\n",
      ">> Epoch 66 finished \tANN training loss 0.002113\n",
      ">> Epoch 67 finished \tANN training loss 0.001868\n",
      ">> Epoch 68 finished \tANN training loss 0.001735\n",
      ">> Epoch 69 finished \tANN training loss 0.001974\n",
      ">> Epoch 70 finished \tANN training loss 0.001601\n",
      ">> Epoch 71 finished \tANN training loss 0.001603\n",
      ">> Epoch 72 finished \tANN training loss 0.001723\n",
      ">> Epoch 73 finished \tANN training loss 0.001551\n",
      ">> Epoch 74 finished \tANN training loss 0.001695\n",
      ">> Epoch 75 finished \tANN training loss 0.001497\n",
      ">> Epoch 76 finished \tANN training loss 0.001321\n",
      ">> Epoch 77 finished \tANN training loss 0.001402\n",
      ">> Epoch 78 finished \tANN training loss 0.001113\n",
      ">> Epoch 79 finished \tANN training loss 0.001264\n",
      ">> Epoch 80 finished \tANN training loss 0.001136\n",
      ">> Epoch 81 finished \tANN training loss 0.001103\n",
      ">> Epoch 82 finished \tANN training loss 0.000961\n",
      ">> Epoch 83 finished \tANN training loss 0.001010\n",
      ">> Epoch 84 finished \tANN training loss 0.001055\n",
      ">> Epoch 85 finished \tANN training loss 0.000914\n",
      ">> Epoch 86 finished \tANN training loss 0.000943\n",
      ">> Epoch 87 finished \tANN training loss 0.000872\n",
      ">> Epoch 88 finished \tANN training loss 0.001109\n",
      ">> Epoch 89 finished \tANN training loss 0.001122\n",
      ">> Epoch 90 finished \tANN training loss 0.000959\n",
      ">> Epoch 91 finished \tANN training loss 0.000929\n",
      ">> Epoch 92 finished \tANN training loss 0.000905\n",
      ">> Epoch 93 finished \tANN training loss 0.000796\n",
      ">> Epoch 94 finished \tANN training loss 0.000740\n",
      ">> Epoch 95 finished \tANN training loss 0.000660\n",
      ">> Epoch 96 finished \tANN training loss 0.000747\n",
      ">> Epoch 97 finished \tANN training loss 0.000708\n",
      ">> Epoch 98 finished \tANN training loss 0.000778\n",
      ">> Epoch 99 finished \tANN training loss 0.000611\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.915000\n"
     ]
    }
   ],
   "source": [
    "learning_rate_acc[2] = deep_belief_net(learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.915\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(learning_rate_acc[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setting 4**\n",
    "\n",
    "    learning_rate=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 79.112305\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 96.725349\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 51.052197\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 37.876381\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 47.518787\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 33.752762\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 48.474228\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 44.482853\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 37.027935\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 42.175068\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 133.939896\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 197.219604\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 240.424820\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 256.749054\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 258.981415\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 168.490448\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 153.094330\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 211.722504\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 189.015991\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 214.493042\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.592979\n",
      ">> Epoch 1 finished \tANN training loss 0.919712\n",
      ">> Epoch 2 finished \tANN training loss 0.269604\n",
      ">> Epoch 3 finished \tANN training loss 0.243406\n",
      ">> Epoch 4 finished \tANN training loss 0.144731\n",
      ">> Epoch 5 finished \tANN training loss 0.106821\n",
      ">> Epoch 6 finished \tANN training loss 0.155397\n",
      ">> Epoch 7 finished \tANN training loss 0.118717\n",
      ">> Epoch 8 finished \tANN training loss 0.066168\n",
      ">> Epoch 9 finished \tANN training loss 0.050119\n",
      ">> Epoch 10 finished \tANN training loss 0.055784\n",
      ">> Epoch 11 finished \tANN training loss 0.116684\n",
      ">> Epoch 12 finished \tANN training loss 0.018682\n",
      ">> Epoch 13 finished \tANN training loss 0.027114\n",
      ">> Epoch 14 finished \tANN training loss 0.038821\n",
      ">> Epoch 15 finished \tANN training loss 0.019516\n",
      ">> Epoch 16 finished \tANN training loss 0.043683\n",
      ">> Epoch 17 finished \tANN training loss 0.017078\n",
      ">> Epoch 18 finished \tANN training loss 0.013328\n",
      ">> Epoch 19 finished \tANN training loss 0.008714\n",
      ">> Epoch 20 finished \tANN training loss 0.007808\n",
      ">> Epoch 21 finished \tANN training loss 0.009127\n",
      ">> Epoch 22 finished \tANN training loss 0.008772\n",
      ">> Epoch 23 finished \tANN training loss 0.015420\n",
      ">> Epoch 24 finished \tANN training loss 0.006906\n",
      ">> Epoch 25 finished \tANN training loss 0.002489\n",
      ">> Epoch 26 finished \tANN training loss 0.002595\n",
      ">> Epoch 27 finished \tANN training loss 0.016892\n",
      ">> Epoch 28 finished \tANN training loss 0.005684\n",
      ">> Epoch 29 finished \tANN training loss 0.002046\n",
      ">> Epoch 30 finished \tANN training loss 0.009243\n",
      ">> Epoch 31 finished \tANN training loss 0.003108\n",
      ">> Epoch 32 finished \tANN training loss 0.004623\n",
      ">> Epoch 33 finished \tANN training loss 0.004727\n",
      ">> Epoch 34 finished \tANN training loss 0.001026\n",
      ">> Epoch 35 finished \tANN training loss 0.002531\n",
      ">> Epoch 36 finished \tANN training loss 0.010728\n",
      ">> Epoch 37 finished \tANN training loss 0.008343\n",
      ">> Epoch 38 finished \tANN training loss 0.009291\n",
      ">> Epoch 39 finished \tANN training loss 0.001726\n",
      ">> Epoch 40 finished \tANN training loss 0.000759\n",
      ">> Epoch 41 finished \tANN training loss 0.000726\n",
      ">> Epoch 42 finished \tANN training loss 0.002183\n",
      ">> Epoch 43 finished \tANN training loss 0.003649\n",
      ">> Epoch 44 finished \tANN training loss 0.001090\n",
      ">> Epoch 45 finished \tANN training loss 0.000794\n",
      ">> Epoch 46 finished \tANN training loss 0.001917\n",
      ">> Epoch 47 finished \tANN training loss 0.001134\n",
      ">> Epoch 48 finished \tANN training loss 0.002529\n",
      ">> Epoch 49 finished \tANN training loss 0.000704\n",
      ">> Epoch 50 finished \tANN training loss 0.000694\n",
      ">> Epoch 51 finished \tANN training loss 0.000179\n",
      ">> Epoch 52 finished \tANN training loss 0.000652\n",
      ">> Epoch 53 finished \tANN training loss 0.000684\n",
      ">> Epoch 54 finished \tANN training loss 0.000338\n",
      ">> Epoch 55 finished \tANN training loss 0.000210\n",
      ">> Epoch 56 finished \tANN training loss 0.000185\n",
      ">> Epoch 57 finished \tANN training loss 0.002425\n",
      ">> Epoch 58 finished \tANN training loss 0.000948\n",
      ">> Epoch 59 finished \tANN training loss 0.000192\n",
      ">> Epoch 60 finished \tANN training loss 0.000189\n",
      ">> Epoch 61 finished \tANN training loss 0.001760\n",
      ">> Epoch 62 finished \tANN training loss 0.000620\n",
      ">> Epoch 63 finished \tANN training loss 0.000317\n",
      ">> Epoch 64 finished \tANN training loss 0.000141\n",
      ">> Epoch 65 finished \tANN training loss 0.000130\n",
      ">> Epoch 66 finished \tANN training loss 0.000140\n",
      ">> Epoch 67 finished \tANN training loss 0.000074\n",
      ">> Epoch 68 finished \tANN training loss 0.000058\n",
      ">> Epoch 69 finished \tANN training loss 0.000044\n",
      ">> Epoch 70 finished \tANN training loss 0.000267\n",
      ">> Epoch 71 finished \tANN training loss 0.000171\n",
      ">> Epoch 72 finished \tANN training loss 0.001560\n",
      ">> Epoch 73 finished \tANN training loss 0.000123\n",
      ">> Epoch 74 finished \tANN training loss 0.000052\n",
      ">> Epoch 75 finished \tANN training loss 0.000608\n",
      ">> Epoch 76 finished \tANN training loss 0.000136\n",
      ">> Epoch 77 finished \tANN training loss 0.000064\n",
      ">> Epoch 78 finished \tANN training loss 0.000091\n",
      ">> Epoch 79 finished \tANN training loss 0.000185\n",
      ">> Epoch 80 finished \tANN training loss 0.000050\n",
      ">> Epoch 81 finished \tANN training loss 0.000043\n",
      ">> Epoch 82 finished \tANN training loss 0.000252\n",
      ">> Epoch 83 finished \tANN training loss 0.000112\n",
      ">> Epoch 84 finished \tANN training loss 0.000828\n",
      ">> Epoch 85 finished \tANN training loss 0.000062\n",
      ">> Epoch 86 finished \tANN training loss 0.000045\n",
      ">> Epoch 87 finished \tANN training loss 0.000065\n",
      ">> Epoch 88 finished \tANN training loss 0.000088\n",
      ">> Epoch 89 finished \tANN training loss 0.000945\n",
      ">> Epoch 90 finished \tANN training loss 0.000660\n",
      ">> Epoch 91 finished \tANN training loss 0.000056\n",
      ">> Epoch 92 finished \tANN training loss 0.000073\n",
      ">> Epoch 93 finished \tANN training loss 0.000320\n",
      ">> Epoch 94 finished \tANN training loss 0.000045\n",
      ">> Epoch 95 finished \tANN training loss 0.000358\n",
      ">> Epoch 96 finished \tANN training loss 0.000064\n",
      ">> Epoch 97 finished \tANN training loss 0.000205\n",
      ">> Epoch 98 finished \tANN training loss 0.000103\n",
      ">> Epoch 99 finished \tANN training loss 0.000056\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.895000\n"
     ]
    }
   ],
   "source": [
    "learning_rate_acc[3] = deep_belief_net(learning_rate=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.895\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(learning_rate_acc[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setting 5**\n",
    "\n",
    "    learning_rate=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 76.779762\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 47.087139\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 59.271000\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 54.856850\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 72.953751\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 34.348270\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 50.360817\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 35.676952\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 40.834705\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 47.222740\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 266.484985\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 177.374817\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 254.176849\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 316.424438\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 188.242752\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 231.718887\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 162.049530\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 193.549850\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 268.331512\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 206.428787\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 1.864518\n",
      ">> Epoch 1 finished \tANN training loss 2.007123\n",
      ">> Epoch 2 finished \tANN training loss 1.281737\n",
      ">> Epoch 3 finished \tANN training loss 1.052272\n",
      ">> Epoch 4 finished \tANN training loss 0.871565\n",
      ">> Epoch 5 finished \tANN training loss 0.983157\n",
      ">> Epoch 6 finished \tANN training loss 0.714842\n",
      ">> Epoch 7 finished \tANN training loss 1.038296\n",
      ">> Epoch 8 finished \tANN training loss 1.266252\n",
      ">> Epoch 9 finished \tANN training loss 0.776818\n",
      ">> Epoch 10 finished \tANN training loss 0.741439\n",
      ">> Epoch 11 finished \tANN training loss 0.486001\n",
      ">> Epoch 12 finished \tANN training loss 0.419827\n",
      ">> Epoch 13 finished \tANN training loss 0.361042\n",
      ">> Epoch 14 finished \tANN training loss 0.367671\n",
      ">> Epoch 15 finished \tANN training loss 0.268364\n",
      ">> Epoch 16 finished \tANN training loss 0.405128\n",
      ">> Epoch 17 finished \tANN training loss 0.327894\n",
      ">> Epoch 18 finished \tANN training loss 0.199816\n",
      ">> Epoch 19 finished \tANN training loss 0.264040\n",
      ">> Epoch 20 finished \tANN training loss 0.164132\n",
      ">> Epoch 21 finished \tANN training loss 0.158136\n",
      ">> Epoch 22 finished \tANN training loss 0.280490\n",
      ">> Epoch 23 finished \tANN training loss 0.179689\n",
      ">> Epoch 24 finished \tANN training loss 0.166684\n",
      ">> Epoch 25 finished \tANN training loss 0.220441\n",
      ">> Epoch 26 finished \tANN training loss 0.176274\n",
      ">> Epoch 27 finished \tANN training loss 0.108506\n",
      ">> Epoch 28 finished \tANN training loss 0.157198\n",
      ">> Epoch 29 finished \tANN training loss 0.082274\n",
      ">> Epoch 30 finished \tANN training loss 0.101209\n",
      ">> Epoch 31 finished \tANN training loss 0.188011\n",
      ">> Epoch 32 finished \tANN training loss 0.073861\n",
      ">> Epoch 33 finished \tANN training loss 0.106709\n",
      ">> Epoch 34 finished \tANN training loss 0.077232\n",
      ">> Epoch 35 finished \tANN training loss 0.084905\n",
      ">> Epoch 36 finished \tANN training loss 0.072080\n",
      ">> Epoch 37 finished \tANN training loss 0.100271\n",
      ">> Epoch 38 finished \tANN training loss 0.070289\n",
      ">> Epoch 39 finished \tANN training loss 0.078162\n",
      ">> Epoch 40 finished \tANN training loss 0.079343\n",
      ">> Epoch 41 finished \tANN training loss 0.041180\n",
      ">> Epoch 42 finished \tANN training loss 0.068703\n",
      ">> Epoch 43 finished \tANN training loss 0.042101\n",
      ">> Epoch 44 finished \tANN training loss 0.086688\n",
      ">> Epoch 45 finished \tANN training loss 0.070873\n",
      ">> Epoch 46 finished \tANN training loss 0.048416\n",
      ">> Epoch 47 finished \tANN training loss 0.080061\n",
      ">> Epoch 48 finished \tANN training loss 0.063316\n",
      ">> Epoch 49 finished \tANN training loss 0.047654\n",
      ">> Epoch 50 finished \tANN training loss 0.038760\n",
      ">> Epoch 51 finished \tANN training loss 0.064794\n",
      ">> Epoch 52 finished \tANN training loss 0.064305\n",
      ">> Epoch 53 finished \tANN training loss 0.045841\n",
      ">> Epoch 54 finished \tANN training loss 0.058909\n",
      ">> Epoch 55 finished \tANN training loss 0.055491\n",
      ">> Epoch 56 finished \tANN training loss 0.033093\n",
      ">> Epoch 57 finished \tANN training loss 0.028660\n",
      ">> Epoch 58 finished \tANN training loss 0.036853\n",
      ">> Epoch 59 finished \tANN training loss 0.029701\n",
      ">> Epoch 60 finished \tANN training loss 0.035146\n",
      ">> Epoch 61 finished \tANN training loss 0.030586\n",
      ">> Epoch 62 finished \tANN training loss 0.018859\n",
      ">> Epoch 63 finished \tANN training loss 0.025407\n",
      ">> Epoch 64 finished \tANN training loss 0.026353\n",
      ">> Epoch 65 finished \tANN training loss 0.032093\n",
      ">> Epoch 66 finished \tANN training loss 0.015542\n",
      ">> Epoch 67 finished \tANN training loss 0.028916\n",
      ">> Epoch 68 finished \tANN training loss 0.051072\n",
      ">> Epoch 69 finished \tANN training loss 0.032595\n",
      ">> Epoch 70 finished \tANN training loss 0.038493\n",
      ">> Epoch 71 finished \tANN training loss 0.026300\n",
      ">> Epoch 72 finished \tANN training loss 0.029317\n",
      ">> Epoch 73 finished \tANN training loss 0.039796\n",
      ">> Epoch 74 finished \tANN training loss 0.036456\n",
      ">> Epoch 75 finished \tANN training loss 0.043984\n",
      ">> Epoch 76 finished \tANN training loss 0.030501\n",
      ">> Epoch 77 finished \tANN training loss 0.025206\n",
      ">> Epoch 78 finished \tANN training loss 0.021465\n",
      ">> Epoch 79 finished \tANN training loss 0.015890\n",
      ">> Epoch 80 finished \tANN training loss 0.026133\n",
      ">> Epoch 81 finished \tANN training loss 0.030957\n",
      ">> Epoch 82 finished \tANN training loss 0.030298\n",
      ">> Epoch 83 finished \tANN training loss 0.035556\n",
      ">> Epoch 84 finished \tANN training loss 0.026714\n",
      ">> Epoch 85 finished \tANN training loss 0.031732\n",
      ">> Epoch 86 finished \tANN training loss 0.028883\n",
      ">> Epoch 87 finished \tANN training loss 0.025399\n",
      ">> Epoch 88 finished \tANN training loss 0.025603\n",
      ">> Epoch 89 finished \tANN training loss 0.021756\n",
      ">> Epoch 90 finished \tANN training loss 0.021983\n",
      ">> Epoch 91 finished \tANN training loss 0.028638\n",
      ">> Epoch 92 finished \tANN training loss 0.026257\n",
      ">> Epoch 93 finished \tANN training loss 0.020985\n",
      ">> Epoch 94 finished \tANN training loss 0.024991\n",
      ">> Epoch 95 finished \tANN training loss 0.020566\n",
      ">> Epoch 96 finished \tANN training loss 0.032148\n",
      ">> Epoch 97 finished \tANN training loss 0.019458\n",
      ">> Epoch 98 finished \tANN training loss 0.029209\n",
      ">> Epoch 99 finished \tANN training loss 0.022101\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.875000\n"
     ]
    }
   ],
   "source": [
    "learning_rate_acc[4] = deep_belief_net(learning_rate=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.875\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(learning_rate_acc[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.88500000000000001, 0.89500000000000002, 0.91500000000000004, 0.89500000000000002, 0.875]\n",
      "Most accurate learning_rate setting is Setting 3\n"
     ]
    }
   ],
   "source": [
    "# Collated results\n",
    "print(learning_rate_acc)\n",
    "print('Most accurate learning_rate setting is Setting ' + str(learning_rate_acc.index(max(learning_rate_acc)) + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEgCAYAAABb8m8VAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1cd04018080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create bar graph\n",
    "plt.clf()\n",
    "objects = ('0.01', '0.05', '0.10', '0.50', '1.0')\n",
    "y_pos = np.arange(len(objects))\n",
    "performance = [0.885,0.895,0.915,0.895, 0.875]\n",
    "\n",
    "plt.bar(y_pos, performance, align='center', alpha=1.)\n",
    "plt.xticks(y_pos, objects, rotation=30)\n",
    "plt.xlabel('Settings')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('learning_rate Settings')\n",
    " \n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different Pre-training Lengths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store accuracies\n",
    "epoch_rbm_acc = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setting 1**\n",
    "\n",
    "    n_epochs_rbm=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 58.882492\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 278.842926\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 2.223761\n",
      ">> Epoch 1 finished \tANN training loss 2.037116\n",
      ">> Epoch 2 finished \tANN training loss 1.761922\n",
      ">> Epoch 3 finished \tANN training loss 1.454467\n",
      ">> Epoch 4 finished \tANN training loss 1.263631\n",
      ">> Epoch 5 finished \tANN training loss 1.113083\n",
      ">> Epoch 6 finished \tANN training loss 0.964771\n",
      ">> Epoch 7 finished \tANN training loss 0.840998\n",
      ">> Epoch 8 finished \tANN training loss 0.768864\n",
      ">> Epoch 9 finished \tANN training loss 0.729350\n",
      ">> Epoch 10 finished \tANN training loss 0.596092\n",
      ">> Epoch 11 finished \tANN training loss 0.531411\n",
      ">> Epoch 12 finished \tANN training loss 0.463428\n",
      ">> Epoch 13 finished \tANN training loss 0.412320\n",
      ">> Epoch 14 finished \tANN training loss 0.361198\n",
      ">> Epoch 15 finished \tANN training loss 0.324128\n",
      ">> Epoch 16 finished \tANN training loss 0.300418\n",
      ">> Epoch 17 finished \tANN training loss 0.270532\n",
      ">> Epoch 18 finished \tANN training loss 0.237967\n",
      ">> Epoch 19 finished \tANN training loss 0.242986\n",
      ">> Epoch 20 finished \tANN training loss 0.193844\n",
      ">> Epoch 21 finished \tANN training loss 0.189994\n",
      ">> Epoch 22 finished \tANN training loss 0.164269\n",
      ">> Epoch 23 finished \tANN training loss 0.170173\n",
      ">> Epoch 24 finished \tANN training loss 0.146996\n",
      ">> Epoch 25 finished \tANN training loss 0.128791\n",
      ">> Epoch 26 finished \tANN training loss 0.124010\n",
      ">> Epoch 27 finished \tANN training loss 0.115098\n",
      ">> Epoch 28 finished \tANN training loss 0.101233\n",
      ">> Epoch 29 finished \tANN training loss 0.084624\n",
      ">> Epoch 30 finished \tANN training loss 0.089629\n",
      ">> Epoch 31 finished \tANN training loss 0.079986\n",
      ">> Epoch 32 finished \tANN training loss 0.072556\n",
      ">> Epoch 33 finished \tANN training loss 0.071409\n",
      ">> Epoch 34 finished \tANN training loss 0.056656\n",
      ">> Epoch 35 finished \tANN training loss 0.064879\n",
      ">> Epoch 36 finished \tANN training loss 0.055338\n",
      ">> Epoch 37 finished \tANN training loss 0.053665\n",
      ">> Epoch 38 finished \tANN training loss 0.047763\n",
      ">> Epoch 39 finished \tANN training loss 0.039060\n",
      ">> Epoch 40 finished \tANN training loss 0.038810\n",
      ">> Epoch 41 finished \tANN training loss 0.031391\n",
      ">> Epoch 42 finished \tANN training loss 0.028825\n",
      ">> Epoch 43 finished \tANN training loss 0.026952\n",
      ">> Epoch 44 finished \tANN training loss 0.026540\n",
      ">> Epoch 45 finished \tANN training loss 0.023928\n",
      ">> Epoch 46 finished \tANN training loss 0.025771\n",
      ">> Epoch 47 finished \tANN training loss 0.018797\n",
      ">> Epoch 48 finished \tANN training loss 0.020072\n",
      ">> Epoch 49 finished \tANN training loss 0.017710\n",
      ">> Epoch 50 finished \tANN training loss 0.019654\n",
      ">> Epoch 51 finished \tANN training loss 0.022472\n",
      ">> Epoch 52 finished \tANN training loss 0.016233\n",
      ">> Epoch 53 finished \tANN training loss 0.017495\n",
      ">> Epoch 54 finished \tANN training loss 0.012503\n",
      ">> Epoch 55 finished \tANN training loss 0.011917\n",
      ">> Epoch 56 finished \tANN training loss 0.013414\n",
      ">> Epoch 57 finished \tANN training loss 0.011644\n",
      ">> Epoch 58 finished \tANN training loss 0.010111\n",
      ">> Epoch 59 finished \tANN training loss 0.010870\n",
      ">> Epoch 60 finished \tANN training loss 0.011353\n",
      ">> Epoch 61 finished \tANN training loss 0.008745\n",
      ">> Epoch 62 finished \tANN training loss 0.007789\n",
      ">> Epoch 63 finished \tANN training loss 0.008426\n",
      ">> Epoch 64 finished \tANN training loss 0.006816\n",
      ">> Epoch 65 finished \tANN training loss 0.008983\n",
      ">> Epoch 66 finished \tANN training loss 0.009432\n",
      ">> Epoch 67 finished \tANN training loss 0.008194\n",
      ">> Epoch 68 finished \tANN training loss 0.007353\n",
      ">> Epoch 69 finished \tANN training loss 0.005795\n",
      ">> Epoch 70 finished \tANN training loss 0.006238\n",
      ">> Epoch 71 finished \tANN training loss 0.005558\n",
      ">> Epoch 72 finished \tANN training loss 0.005462\n",
      ">> Epoch 73 finished \tANN training loss 0.005598\n",
      ">> Epoch 74 finished \tANN training loss 0.006482\n",
      ">> Epoch 75 finished \tANN training loss 0.004952\n",
      ">> Epoch 76 finished \tANN training loss 0.006256\n",
      ">> Epoch 77 finished \tANN training loss 0.007180\n",
      ">> Epoch 78 finished \tANN training loss 0.005956\n",
      ">> Epoch 79 finished \tANN training loss 0.005255\n",
      ">> Epoch 80 finished \tANN training loss 0.005433\n",
      ">> Epoch 81 finished \tANN training loss 0.004257\n",
      ">> Epoch 82 finished \tANN training loss 0.004993\n",
      ">> Epoch 83 finished \tANN training loss 0.006366\n",
      ">> Epoch 84 finished \tANN training loss 0.003958\n",
      ">> Epoch 85 finished \tANN training loss 0.004470\n",
      ">> Epoch 86 finished \tANN training loss 0.003970\n",
      ">> Epoch 87 finished \tANN training loss 0.003097\n",
      ">> Epoch 88 finished \tANN training loss 0.003155\n",
      ">> Epoch 89 finished \tANN training loss 0.003054\n",
      ">> Epoch 90 finished \tANN training loss 0.003835\n",
      ">> Epoch 91 finished \tANN training loss 0.003231\n",
      ">> Epoch 92 finished \tANN training loss 0.004185\n",
      ">> Epoch 93 finished \tANN training loss 0.002709\n",
      ">> Epoch 94 finished \tANN training loss 0.002217\n",
      ">> Epoch 95 finished \tANN training loss 0.002616\n",
      ">> Epoch 96 finished \tANN training loss 0.002371\n",
      ">> Epoch 97 finished \tANN training loss 0.002233\n",
      ">> Epoch 98 finished \tANN training loss 0.002299\n",
      ">> Epoch 99 finished \tANN training loss 0.002585\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.880000\n"
     ]
    }
   ],
   "source": [
    "epoch_rbm_acc[0] = deep_belief_net(n_epochs_rbm=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.88\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(epoch_rbm_acc[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setting 2**\n",
    "\n",
    "    n_epochs_rbm=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 64.689949\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 59.904064\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 411.289185\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 575.223877\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 1.419303\n",
      ">> Epoch 1 finished \tANN training loss 1.063062\n",
      ">> Epoch 2 finished \tANN training loss 0.856164\n",
      ">> Epoch 3 finished \tANN training loss 0.754857\n",
      ">> Epoch 4 finished \tANN training loss 0.681133\n",
      ">> Epoch 5 finished \tANN training loss 0.563045\n",
      ">> Epoch 6 finished \tANN training loss 0.497078\n",
      ">> Epoch 7 finished \tANN training loss 0.439034\n",
      ">> Epoch 8 finished \tANN training loss 0.410330\n",
      ">> Epoch 9 finished \tANN training loss 0.372165\n",
      ">> Epoch 10 finished \tANN training loss 0.331818\n",
      ">> Epoch 11 finished \tANN training loss 0.277258\n",
      ">> Epoch 12 finished \tANN training loss 0.247271\n",
      ">> Epoch 13 finished \tANN training loss 0.233883\n",
      ">> Epoch 14 finished \tANN training loss 0.213489\n",
      ">> Epoch 15 finished \tANN training loss 0.200255\n",
      ">> Epoch 16 finished \tANN training loss 0.187413\n",
      ">> Epoch 17 finished \tANN training loss 0.151062\n",
      ">> Epoch 18 finished \tANN training loss 0.147466\n",
      ">> Epoch 19 finished \tANN training loss 0.132926\n",
      ">> Epoch 20 finished \tANN training loss 0.111680\n",
      ">> Epoch 21 finished \tANN training loss 0.102049\n",
      ">> Epoch 22 finished \tANN training loss 0.107214\n",
      ">> Epoch 23 finished \tANN training loss 0.087160\n",
      ">> Epoch 24 finished \tANN training loss 0.079395\n",
      ">> Epoch 25 finished \tANN training loss 0.067591\n",
      ">> Epoch 26 finished \tANN training loss 0.060134\n",
      ">> Epoch 27 finished \tANN training loss 0.060750\n",
      ">> Epoch 28 finished \tANN training loss 0.048710\n",
      ">> Epoch 29 finished \tANN training loss 0.043150\n",
      ">> Epoch 30 finished \tANN training loss 0.047188\n",
      ">> Epoch 31 finished \tANN training loss 0.040832\n",
      ">> Epoch 32 finished \tANN training loss 0.036856\n",
      ">> Epoch 33 finished \tANN training loss 0.029638\n",
      ">> Epoch 34 finished \tANN training loss 0.025295\n",
      ">> Epoch 35 finished \tANN training loss 0.030186\n",
      ">> Epoch 36 finished \tANN training loss 0.029155\n",
      ">> Epoch 37 finished \tANN training loss 0.032035\n",
      ">> Epoch 38 finished \tANN training loss 0.022894\n",
      ">> Epoch 39 finished \tANN training loss 0.021914\n",
      ">> Epoch 40 finished \tANN training loss 0.019929\n",
      ">> Epoch 41 finished \tANN training loss 0.016096\n",
      ">> Epoch 42 finished \tANN training loss 0.016274\n",
      ">> Epoch 43 finished \tANN training loss 0.013958\n",
      ">> Epoch 44 finished \tANN training loss 0.015070\n",
      ">> Epoch 45 finished \tANN training loss 0.013994\n",
      ">> Epoch 46 finished \tANN training loss 0.013595\n",
      ">> Epoch 47 finished \tANN training loss 0.010287\n",
      ">> Epoch 48 finished \tANN training loss 0.012040\n",
      ">> Epoch 49 finished \tANN training loss 0.010471\n",
      ">> Epoch 50 finished \tANN training loss 0.010486\n",
      ">> Epoch 51 finished \tANN training loss 0.009014\n",
      ">> Epoch 52 finished \tANN training loss 0.008870\n",
      ">> Epoch 53 finished \tANN training loss 0.015122\n",
      ">> Epoch 54 finished \tANN training loss 0.007489\n",
      ">> Epoch 55 finished \tANN training loss 0.007910\n",
      ">> Epoch 56 finished \tANN training loss 0.006729\n",
      ">> Epoch 57 finished \tANN training loss 0.007217\n",
      ">> Epoch 58 finished \tANN training loss 0.006205\n",
      ">> Epoch 59 finished \tANN training loss 0.005724\n",
      ">> Epoch 60 finished \tANN training loss 0.005335\n",
      ">> Epoch 61 finished \tANN training loss 0.004808\n",
      ">> Epoch 62 finished \tANN training loss 0.005119\n",
      ">> Epoch 63 finished \tANN training loss 0.005627\n",
      ">> Epoch 64 finished \tANN training loss 0.004749\n",
      ">> Epoch 65 finished \tANN training loss 0.005440\n",
      ">> Epoch 66 finished \tANN training loss 0.004403\n",
      ">> Epoch 67 finished \tANN training loss 0.004492\n",
      ">> Epoch 68 finished \tANN training loss 0.004223\n",
      ">> Epoch 69 finished \tANN training loss 0.004254\n",
      ">> Epoch 70 finished \tANN training loss 0.003173\n",
      ">> Epoch 71 finished \tANN training loss 0.003260\n",
      ">> Epoch 72 finished \tANN training loss 0.003513\n",
      ">> Epoch 73 finished \tANN training loss 0.003192\n",
      ">> Epoch 74 finished \tANN training loss 0.003297\n",
      ">> Epoch 75 finished \tANN training loss 0.002554\n",
      ">> Epoch 76 finished \tANN training loss 0.002418\n",
      ">> Epoch 77 finished \tANN training loss 0.002834\n",
      ">> Epoch 78 finished \tANN training loss 0.002748\n",
      ">> Epoch 79 finished \tANN training loss 0.004030\n",
      ">> Epoch 80 finished \tANN training loss 0.003492\n",
      ">> Epoch 81 finished \tANN training loss 0.002854\n",
      ">> Epoch 82 finished \tANN training loss 0.002339\n",
      ">> Epoch 83 finished \tANN training loss 0.002591\n",
      ">> Epoch 84 finished \tANN training loss 0.002238\n",
      ">> Epoch 85 finished \tANN training loss 0.002122\n",
      ">> Epoch 86 finished \tANN training loss 0.002470\n",
      ">> Epoch 87 finished \tANN training loss 0.002199\n",
      ">> Epoch 88 finished \tANN training loss 0.002292\n",
      ">> Epoch 89 finished \tANN training loss 0.002280\n",
      ">> Epoch 90 finished \tANN training loss 0.001784\n",
      ">> Epoch 91 finished \tANN training loss 0.001722\n",
      ">> Epoch 92 finished \tANN training loss 0.002249\n",
      ">> Epoch 93 finished \tANN training loss 0.001568\n",
      ">> Epoch 94 finished \tANN training loss 0.001680\n",
      ">> Epoch 95 finished \tANN training loss 0.001634\n",
      ">> Epoch 96 finished \tANN training loss 0.001589\n",
      ">> Epoch 97 finished \tANN training loss 0.002590\n",
      ">> Epoch 98 finished \tANN training loss 0.001645\n",
      ">> Epoch 99 finished \tANN training loss 0.001494\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.880000\n"
     ]
    }
   ],
   "source": [
    "epoch_rbm_acc[1] = deep_belief_net(n_epochs_rbm=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.88\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(epoch_rbm_acc[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setting 3**\n",
    "\n",
    "    n_epochs_rbm=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 56.931183\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 77.546394\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 65.859505\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 51.727715\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 30.440014\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 59.351768\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 79.193237\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 44.656254\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 90.029076\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 60.892635\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.755383\n",
      ">> Epoch 1 finished \tANN training loss 0.544380\n",
      ">> Epoch 2 finished \tANN training loss 0.465932\n",
      ">> Epoch 3 finished \tANN training loss 0.375281\n",
      ">> Epoch 4 finished \tANN training loss 0.321060\n",
      ">> Epoch 5 finished \tANN training loss 0.283924\n",
      ">> Epoch 6 finished \tANN training loss 0.263247\n",
      ">> Epoch 7 finished \tANN training loss 0.250553\n",
      ">> Epoch 8 finished \tANN training loss 0.237075\n",
      ">> Epoch 9 finished \tANN training loss 0.180646\n",
      ">> Epoch 10 finished \tANN training loss 0.170824\n",
      ">> Epoch 11 finished \tANN training loss 0.146446\n",
      ">> Epoch 12 finished \tANN training loss 0.148486\n",
      ">> Epoch 13 finished \tANN training loss 0.121909\n",
      ">> Epoch 14 finished \tANN training loss 0.130320\n",
      ">> Epoch 15 finished \tANN training loss 0.101270\n",
      ">> Epoch 16 finished \tANN training loss 0.095869\n",
      ">> Epoch 17 finished \tANN training loss 0.081758\n",
      ">> Epoch 18 finished \tANN training loss 0.072612\n",
      ">> Epoch 19 finished \tANN training loss 0.066798\n",
      ">> Epoch 20 finished \tANN training loss 0.060896\n",
      ">> Epoch 21 finished \tANN training loss 0.052189\n",
      ">> Epoch 22 finished \tANN training loss 0.049018\n",
      ">> Epoch 23 finished \tANN training loss 0.047216\n",
      ">> Epoch 24 finished \tANN training loss 0.043193\n",
      ">> Epoch 25 finished \tANN training loss 0.037723\n",
      ">> Epoch 26 finished \tANN training loss 0.035975\n",
      ">> Epoch 27 finished \tANN training loss 0.031218\n",
      ">> Epoch 28 finished \tANN training loss 0.031397\n",
      ">> Epoch 29 finished \tANN training loss 0.031476\n",
      ">> Epoch 30 finished \tANN training loss 0.024891\n",
      ">> Epoch 31 finished \tANN training loss 0.022046\n",
      ">> Epoch 32 finished \tANN training loss 0.020564\n",
      ">> Epoch 33 finished \tANN training loss 0.018182\n",
      ">> Epoch 34 finished \tANN training loss 0.016064\n",
      ">> Epoch 35 finished \tANN training loss 0.016191\n",
      ">> Epoch 36 finished \tANN training loss 0.015226\n",
      ">> Epoch 37 finished \tANN training loss 0.015814\n",
      ">> Epoch 38 finished \tANN training loss 0.014541\n",
      ">> Epoch 39 finished \tANN training loss 0.012586\n",
      ">> Epoch 40 finished \tANN training loss 0.012461\n",
      ">> Epoch 41 finished \tANN training loss 0.011501\n",
      ">> Epoch 42 finished \tANN training loss 0.011941\n",
      ">> Epoch 43 finished \tANN training loss 0.009156\n",
      ">> Epoch 44 finished \tANN training loss 0.010101\n",
      ">> Epoch 45 finished \tANN training loss 0.010293\n",
      ">> Epoch 46 finished \tANN training loss 0.008471\n",
      ">> Epoch 47 finished \tANN training loss 0.008303\n",
      ">> Epoch 48 finished \tANN training loss 0.007711\n",
      ">> Epoch 49 finished \tANN training loss 0.008316\n",
      ">> Epoch 50 finished \tANN training loss 0.006659\n",
      ">> Epoch 51 finished \tANN training loss 0.007318\n",
      ">> Epoch 52 finished \tANN training loss 0.006796\n",
      ">> Epoch 53 finished \tANN training loss 0.005895\n",
      ">> Epoch 54 finished \tANN training loss 0.006493\n",
      ">> Epoch 55 finished \tANN training loss 0.005107\n",
      ">> Epoch 56 finished \tANN training loss 0.005322\n",
      ">> Epoch 57 finished \tANN training loss 0.004942\n",
      ">> Epoch 58 finished \tANN training loss 0.004013\n",
      ">> Epoch 59 finished \tANN training loss 0.003807\n",
      ">> Epoch 60 finished \tANN training loss 0.004168\n",
      ">> Epoch 61 finished \tANN training loss 0.004650\n",
      ">> Epoch 62 finished \tANN training loss 0.004459\n",
      ">> Epoch 63 finished \tANN training loss 0.003537\n",
      ">> Epoch 64 finished \tANN training loss 0.003326\n",
      ">> Epoch 65 finished \tANN training loss 0.003472\n",
      ">> Epoch 66 finished \tANN training loss 0.004851\n",
      ">> Epoch 67 finished \tANN training loss 0.003296\n",
      ">> Epoch 68 finished \tANN training loss 0.002763\n",
      ">> Epoch 69 finished \tANN training loss 0.003118\n",
      ">> Epoch 70 finished \tANN training loss 0.002633\n",
      ">> Epoch 71 finished \tANN training loss 0.002410\n",
      ">> Epoch 72 finished \tANN training loss 0.002526\n",
      ">> Epoch 73 finished \tANN training loss 0.002244\n",
      ">> Epoch 74 finished \tANN training loss 0.002375\n",
      ">> Epoch 75 finished \tANN training loss 0.002440\n",
      ">> Epoch 76 finished \tANN training loss 0.002048\n",
      ">> Epoch 77 finished \tANN training loss 0.001956\n",
      ">> Epoch 78 finished \tANN training loss 0.001893\n",
      ">> Epoch 79 finished \tANN training loss 0.001904\n",
      ">> Epoch 80 finished \tANN training loss 0.002147\n",
      ">> Epoch 81 finished \tANN training loss 0.001924\n",
      ">> Epoch 82 finished \tANN training loss 0.001826\n",
      ">> Epoch 83 finished \tANN training loss 0.001678\n",
      ">> Epoch 84 finished \tANN training loss 0.001584\n",
      ">> Epoch 85 finished \tANN training loss 0.001587\n",
      ">> Epoch 86 finished \tANN training loss 0.001834\n",
      ">> Epoch 87 finished \tANN training loss 0.001483\n",
      ">> Epoch 88 finished \tANN training loss 0.001813\n",
      ">> Epoch 89 finished \tANN training loss 0.001701\n",
      ">> Epoch 90 finished \tANN training loss 0.001782\n",
      ">> Epoch 91 finished \tANN training loss 0.001362\n",
      ">> Epoch 92 finished \tANN training loss 0.001288\n",
      ">> Epoch 93 finished \tANN training loss 0.001483\n",
      ">> Epoch 94 finished \tANN training loss 0.001155\n",
      ">> Epoch 95 finished \tANN training loss 0.001198\n",
      ">> Epoch 96 finished \tANN training loss 0.001290\n",
      ">> Epoch 97 finished \tANN training loss 0.001352\n",
      ">> Epoch 98 finished \tANN training loss 0.001136\n",
      ">> Epoch 99 finished \tANN training loss 0.001038\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.910000\n"
     ]
    }
   ],
   "source": [
    "epoch_rbm_acc[2] = deep_belief_net(n_epochs_rbm=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.91\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(epoch_rbm_acc[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setting 4 (Default)**\n",
    "\n",
    "    n_epochs_rbm=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.91\n"
     ]
    }
   ],
   "source": [
    "epoch_rbm_acc[3] = default_acc\n",
    "print('ACCURACY: ' + str(default_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setting 5**\n",
    "\n",
    "    n_epochs_rbm=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 56.427452\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 71.606277\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 55.203838\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 49.950108\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 43.959949\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 35.955669\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 31.652964\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 46.792759\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 32.297512\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 37.488388\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 35.783787\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 33.071846\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 42.293262\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 42.861328\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 44.246651\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 42.367062\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 45.918037\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 36.767139\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 39.103394\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 42.151669\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 187.944672\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 178.430161\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 161.261093\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 86.875801\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 153.360779\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 205.123718\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 179.342361\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 149.012955\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 172.777756\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 163.147797\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 161.883987\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 159.376404\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 194.027191\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 163.270905\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 144.504623\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 158.026321\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 147.669937\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 140.680222\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 137.220871\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 192.522034\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.597145\n",
      ">> Epoch 1 finished \tANN training loss 0.475535\n",
      ">> Epoch 2 finished \tANN training loss 0.324435\n",
      ">> Epoch 3 finished \tANN training loss 0.298316\n",
      ">> Epoch 4 finished \tANN training loss 0.248335\n",
      ">> Epoch 5 finished \tANN training loss 0.209339\n",
      ">> Epoch 6 finished \tANN training loss 0.184778\n",
      ">> Epoch 7 finished \tANN training loss 0.158373\n",
      ">> Epoch 8 finished \tANN training loss 0.139129\n",
      ">> Epoch 9 finished \tANN training loss 0.114644\n",
      ">> Epoch 10 finished \tANN training loss 0.111366\n",
      ">> Epoch 11 finished \tANN training loss 0.098022\n",
      ">> Epoch 12 finished \tANN training loss 0.083005\n",
      ">> Epoch 13 finished \tANN training loss 0.068859\n",
      ">> Epoch 14 finished \tANN training loss 0.075899\n",
      ">> Epoch 15 finished \tANN training loss 0.069448\n",
      ">> Epoch 16 finished \tANN training loss 0.053975\n",
      ">> Epoch 17 finished \tANN training loss 0.050634\n",
      ">> Epoch 18 finished \tANN training loss 0.041964\n",
      ">> Epoch 19 finished \tANN training loss 0.035965\n",
      ">> Epoch 20 finished \tANN training loss 0.039626\n",
      ">> Epoch 21 finished \tANN training loss 0.029006\n",
      ">> Epoch 22 finished \tANN training loss 0.027374\n",
      ">> Epoch 23 finished \tANN training loss 0.026816\n",
      ">> Epoch 24 finished \tANN training loss 0.025267\n",
      ">> Epoch 25 finished \tANN training loss 0.024297\n",
      ">> Epoch 26 finished \tANN training loss 0.018325\n",
      ">> Epoch 27 finished \tANN training loss 0.019077\n",
      ">> Epoch 28 finished \tANN training loss 0.017407\n",
      ">> Epoch 29 finished \tANN training loss 0.014804\n",
      ">> Epoch 30 finished \tANN training loss 0.013759\n",
      ">> Epoch 31 finished \tANN training loss 0.012384\n",
      ">> Epoch 32 finished \tANN training loss 0.013008\n",
      ">> Epoch 33 finished \tANN training loss 0.012786\n",
      ">> Epoch 34 finished \tANN training loss 0.014741\n",
      ">> Epoch 35 finished \tANN training loss 0.009634\n",
      ">> Epoch 36 finished \tANN training loss 0.009522\n",
      ">> Epoch 37 finished \tANN training loss 0.008735\n",
      ">> Epoch 38 finished \tANN training loss 0.007373\n",
      ">> Epoch 39 finished \tANN training loss 0.007362\n",
      ">> Epoch 40 finished \tANN training loss 0.006693\n",
      ">> Epoch 41 finished \tANN training loss 0.007376\n",
      ">> Epoch 42 finished \tANN training loss 0.006166\n",
      ">> Epoch 43 finished \tANN training loss 0.006046\n",
      ">> Epoch 44 finished \tANN training loss 0.005588\n",
      ">> Epoch 45 finished \tANN training loss 0.004943\n",
      ">> Epoch 46 finished \tANN training loss 0.005082\n",
      ">> Epoch 47 finished \tANN training loss 0.004644\n",
      ">> Epoch 48 finished \tANN training loss 0.004235\n",
      ">> Epoch 49 finished \tANN training loss 0.003759\n",
      ">> Epoch 50 finished \tANN training loss 0.003394\n",
      ">> Epoch 51 finished \tANN training loss 0.003706\n",
      ">> Epoch 52 finished \tANN training loss 0.003375\n",
      ">> Epoch 53 finished \tANN training loss 0.003192\n",
      ">> Epoch 54 finished \tANN training loss 0.003678\n",
      ">> Epoch 55 finished \tANN training loss 0.003450\n",
      ">> Epoch 56 finished \tANN training loss 0.002996\n",
      ">> Epoch 57 finished \tANN training loss 0.003453\n",
      ">> Epoch 58 finished \tANN training loss 0.003489\n",
      ">> Epoch 59 finished \tANN training loss 0.002746\n",
      ">> Epoch 60 finished \tANN training loss 0.002690\n",
      ">> Epoch 61 finished \tANN training loss 0.002511\n",
      ">> Epoch 62 finished \tANN training loss 0.002690\n",
      ">> Epoch 63 finished \tANN training loss 0.002292\n",
      ">> Epoch 64 finished \tANN training loss 0.002188\n",
      ">> Epoch 65 finished \tANN training loss 0.001994\n",
      ">> Epoch 66 finished \tANN training loss 0.002216\n",
      ">> Epoch 67 finished \tANN training loss 0.001872\n",
      ">> Epoch 68 finished \tANN training loss 0.001980\n",
      ">> Epoch 69 finished \tANN training loss 0.001816\n",
      ">> Epoch 70 finished \tANN training loss 0.001969\n",
      ">> Epoch 71 finished \tANN training loss 0.002076\n",
      ">> Epoch 72 finished \tANN training loss 0.002132\n",
      ">> Epoch 73 finished \tANN training loss 0.001786\n",
      ">> Epoch 74 finished \tANN training loss 0.001732\n",
      ">> Epoch 75 finished \tANN training loss 0.001871\n",
      ">> Epoch 76 finished \tANN training loss 0.001601\n",
      ">> Epoch 77 finished \tANN training loss 0.001554\n",
      ">> Epoch 78 finished \tANN training loss 0.001788\n",
      ">> Epoch 79 finished \tANN training loss 0.001425\n",
      ">> Epoch 80 finished \tANN training loss 0.001425\n",
      ">> Epoch 81 finished \tANN training loss 0.001311\n",
      ">> Epoch 82 finished \tANN training loss 0.001223\n",
      ">> Epoch 83 finished \tANN training loss 0.001107\n",
      ">> Epoch 84 finished \tANN training loss 0.001179\n",
      ">> Epoch 85 finished \tANN training loss 0.001530\n",
      ">> Epoch 86 finished \tANN training loss 0.001150\n",
      ">> Epoch 87 finished \tANN training loss 0.001061\n",
      ">> Epoch 88 finished \tANN training loss 0.001028\n",
      ">> Epoch 89 finished \tANN training loss 0.001123\n",
      ">> Epoch 90 finished \tANN training loss 0.001113\n",
      ">> Epoch 91 finished \tANN training loss 0.001308\n",
      ">> Epoch 92 finished \tANN training loss 0.001027\n",
      ">> Epoch 93 finished \tANN training loss 0.000953\n",
      ">> Epoch 94 finished \tANN training loss 0.000967\n",
      ">> Epoch 95 finished \tANN training loss 0.000763\n",
      ">> Epoch 96 finished \tANN training loss 0.000894\n",
      ">> Epoch 97 finished \tANN training loss 0.001017\n",
      ">> Epoch 98 finished \tANN training loss 0.000718\n",
      ">> Epoch 99 finished \tANN training loss 0.000792\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.905000\n"
     ]
    }
   ],
   "source": [
    "epoch_rbm_acc[4] = deep_belief_net(n_epochs_rbm=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.905\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(epoch_rbm_acc[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setting 6**\n",
    "\n",
    "    n_epochs_rbm=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 92.470978\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 85.301331\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 42.295254\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 49.635422\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 35.004749\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 54.408066\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 28.305761\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 34.892990\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 29.667559\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 45.455029\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 37.146450\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 38.424442\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 23.905962\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 27.231716\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 36.267136\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 36.911022\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 29.398628\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 29.262503\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 60.107067\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 31.638620\n",
      ">> Epoch 21 finished \tRBM Reconstruction error 47.515308\n",
      ">> Epoch 22 finished \tRBM Reconstruction error 26.997276\n",
      ">> Epoch 23 finished \tRBM Reconstruction error 41.345928\n",
      ">> Epoch 24 finished \tRBM Reconstruction error 31.000488\n",
      ">> Epoch 25 finished \tRBM Reconstruction error 28.862558\n",
      ">> Epoch 26 finished \tRBM Reconstruction error 48.974628\n",
      ">> Epoch 27 finished \tRBM Reconstruction error 26.598774\n",
      ">> Epoch 28 finished \tRBM Reconstruction error 38.259293\n",
      ">> Epoch 29 finished \tRBM Reconstruction error 32.167492\n",
      ">> Epoch 30 finished \tRBM Reconstruction error 38.163063\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 156.527618\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 170.634262\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 131.709961\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 84.261086\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 110.089180\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 133.341827\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 149.274155\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 169.515671\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 162.134476\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 134.458298\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 167.629059\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 76.496315\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 136.987457\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 94.472458\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 118.150604\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 105.890312\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 100.782692\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 117.506126\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 142.523926\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 130.437424\n",
      ">> Epoch 21 finished \tRBM Reconstruction error 139.270248\n",
      ">> Epoch 22 finished \tRBM Reconstruction error 129.217331\n",
      ">> Epoch 23 finished \tRBM Reconstruction error 167.897339\n",
      ">> Epoch 24 finished \tRBM Reconstruction error 129.140259\n",
      ">> Epoch 25 finished \tRBM Reconstruction error 141.708908\n",
      ">> Epoch 26 finished \tRBM Reconstruction error 101.961205\n",
      ">> Epoch 27 finished \tRBM Reconstruction error 114.496872\n",
      ">> Epoch 28 finished \tRBM Reconstruction error 160.465378\n",
      ">> Epoch 29 finished \tRBM Reconstruction error 110.425629\n",
      ">> Epoch 30 finished \tRBM Reconstruction error 123.323578\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.572528\n",
      ">> Epoch 1 finished \tANN training loss 0.395362\n",
      ">> Epoch 2 finished \tANN training loss 0.310572\n",
      ">> Epoch 3 finished \tANN training loss 0.239717\n",
      ">> Epoch 4 finished \tANN training loss 0.241273\n",
      ">> Epoch 5 finished \tANN training loss 0.175358\n",
      ">> Epoch 6 finished \tANN training loss 0.148248\n",
      ">> Epoch 7 finished \tANN training loss 0.142117\n",
      ">> Epoch 8 finished \tANN training loss 0.113329\n",
      ">> Epoch 9 finished \tANN training loss 0.106426\n",
      ">> Epoch 10 finished \tANN training loss 0.092405\n",
      ">> Epoch 11 finished \tANN training loss 0.072843\n",
      ">> Epoch 12 finished \tANN training loss 0.077364\n",
      ">> Epoch 13 finished \tANN training loss 0.056641\n",
      ">> Epoch 14 finished \tANN training loss 0.053819\n",
      ">> Epoch 15 finished \tANN training loss 0.050993\n",
      ">> Epoch 16 finished \tANN training loss 0.044622\n",
      ">> Epoch 17 finished \tANN training loss 0.037784\n",
      ">> Epoch 18 finished \tANN training loss 0.035975\n",
      ">> Epoch 19 finished \tANN training loss 0.030879\n",
      ">> Epoch 20 finished \tANN training loss 0.028483\n",
      ">> Epoch 21 finished \tANN training loss 0.023566\n",
      ">> Epoch 22 finished \tANN training loss 0.023037\n",
      ">> Epoch 23 finished \tANN training loss 0.018066\n",
      ">> Epoch 24 finished \tANN training loss 0.016938\n",
      ">> Epoch 25 finished \tANN training loss 0.016175\n",
      ">> Epoch 26 finished \tANN training loss 0.016632\n",
      ">> Epoch 27 finished \tANN training loss 0.016116\n",
      ">> Epoch 28 finished \tANN training loss 0.013732\n",
      ">> Epoch 29 finished \tANN training loss 0.012201\n",
      ">> Epoch 30 finished \tANN training loss 0.012063\n",
      ">> Epoch 31 finished \tANN training loss 0.009918\n",
      ">> Epoch 32 finished \tANN training loss 0.009608\n",
      ">> Epoch 33 finished \tANN training loss 0.008646\n",
      ">> Epoch 34 finished \tANN training loss 0.008744\n",
      ">> Epoch 35 finished \tANN training loss 0.007667\n",
      ">> Epoch 36 finished \tANN training loss 0.011065\n",
      ">> Epoch 37 finished \tANN training loss 0.006568\n",
      ">> Epoch 38 finished \tANN training loss 0.006130\n",
      ">> Epoch 39 finished \tANN training loss 0.006220\n",
      ">> Epoch 40 finished \tANN training loss 0.005824\n",
      ">> Epoch 41 finished \tANN training loss 0.004509\n",
      ">> Epoch 42 finished \tANN training loss 0.004289\n",
      ">> Epoch 43 finished \tANN training loss 0.004463\n",
      ">> Epoch 44 finished \tANN training loss 0.003908\n",
      ">> Epoch 45 finished \tANN training loss 0.003970\n",
      ">> Epoch 46 finished \tANN training loss 0.003736\n",
      ">> Epoch 47 finished \tANN training loss 0.004104\n",
      ">> Epoch 48 finished \tANN training loss 0.004193\n",
      ">> Epoch 49 finished \tANN training loss 0.003379\n",
      ">> Epoch 50 finished \tANN training loss 0.003105\n",
      ">> Epoch 51 finished \tANN training loss 0.003201\n",
      ">> Epoch 52 finished \tANN training loss 0.002963\n",
      ">> Epoch 53 finished \tANN training loss 0.003433\n",
      ">> Epoch 54 finished \tANN training loss 0.002954\n",
      ">> Epoch 55 finished \tANN training loss 0.003347\n",
      ">> Epoch 56 finished \tANN training loss 0.003261\n",
      ">> Epoch 57 finished \tANN training loss 0.002726\n",
      ">> Epoch 58 finished \tANN training loss 0.002441\n",
      ">> Epoch 59 finished \tANN training loss 0.004095\n",
      ">> Epoch 60 finished \tANN training loss 0.002078\n",
      ">> Epoch 61 finished \tANN training loss 0.002155\n",
      ">> Epoch 62 finished \tANN training loss 0.001965\n",
      ">> Epoch 63 finished \tANN training loss 0.002055\n",
      ">> Epoch 64 finished \tANN training loss 0.002164\n",
      ">> Epoch 65 finished \tANN training loss 0.002029\n",
      ">> Epoch 66 finished \tANN training loss 0.001824\n",
      ">> Epoch 67 finished \tANN training loss 0.001766\n",
      ">> Epoch 68 finished \tANN training loss 0.001924\n",
      ">> Epoch 69 finished \tANN training loss 0.001645\n",
      ">> Epoch 70 finished \tANN training loss 0.001501\n",
      ">> Epoch 71 finished \tANN training loss 0.001504\n",
      ">> Epoch 72 finished \tANN training loss 0.001443\n",
      ">> Epoch 73 finished \tANN training loss 0.001615\n",
      ">> Epoch 74 finished \tANN training loss 0.001390\n",
      ">> Epoch 75 finished \tANN training loss 0.001322\n",
      ">> Epoch 76 finished \tANN training loss 0.001251\n",
      ">> Epoch 77 finished \tANN training loss 0.001251\n",
      ">> Epoch 78 finished \tANN training loss 0.001105\n",
      ">> Epoch 79 finished \tANN training loss 0.001350\n",
      ">> Epoch 80 finished \tANN training loss 0.001378\n",
      ">> Epoch 81 finished \tANN training loss 0.001187\n",
      ">> Epoch 82 finished \tANN training loss 0.001070\n",
      ">> Epoch 83 finished \tANN training loss 0.001095\n",
      ">> Epoch 84 finished \tANN training loss 0.001141\n",
      ">> Epoch 85 finished \tANN training loss 0.001168\n",
      ">> Epoch 86 finished \tANN training loss 0.001146\n",
      ">> Epoch 87 finished \tANN training loss 0.001034\n",
      ">> Epoch 88 finished \tANN training loss 0.001084\n",
      ">> Epoch 89 finished \tANN training loss 0.001132\n",
      ">> Epoch 90 finished \tANN training loss 0.001036\n",
      ">> Epoch 91 finished \tANN training loss 0.000937\n",
      ">> Epoch 92 finished \tANN training loss 0.000908\n",
      ">> Epoch 93 finished \tANN training loss 0.000818\n",
      ">> Epoch 94 finished \tANN training loss 0.000884\n",
      ">> Epoch 95 finished \tANN training loss 0.000947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 96 finished \tANN training loss 0.000796\n",
      ">> Epoch 97 finished \tANN training loss 0.000713\n",
      ">> Epoch 98 finished \tANN training loss 0.001003\n",
      ">> Epoch 99 finished \tANN training loss 0.000714\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.920000\n"
     ]
    }
   ],
   "source": [
    "epoch_rbm_acc[5] = deep_belief_net(n_epochs_rbm=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.92\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(epoch_rbm_acc[5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setting 7**\n",
    "\n",
    "    n_epochs_rbm=40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 43.358288\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 56.363575\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 53.744125\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 51.788158\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 38.050350\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 33.176849\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 34.224003\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 21.834377\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 45.354679\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 22.315962\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 35.863102\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 29.260483\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 27.823875\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 29.280943\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 21.086147\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 20.398621\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 48.363113\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 36.256248\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 23.029125\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 36.814152\n",
      ">> Epoch 21 finished \tRBM Reconstruction error 28.656294\n",
      ">> Epoch 22 finished \tRBM Reconstruction error 33.848091\n",
      ">> Epoch 23 finished \tRBM Reconstruction error 23.485928\n",
      ">> Epoch 24 finished \tRBM Reconstruction error 36.647797\n",
      ">> Epoch 25 finished \tRBM Reconstruction error 40.407646\n",
      ">> Epoch 26 finished \tRBM Reconstruction error 34.541111\n",
      ">> Epoch 27 finished \tRBM Reconstruction error 41.327587\n",
      ">> Epoch 28 finished \tRBM Reconstruction error 40.208481\n",
      ">> Epoch 29 finished \tRBM Reconstruction error 28.690596\n",
      ">> Epoch 30 finished \tRBM Reconstruction error 40.044468\n",
      ">> Epoch 31 finished \tRBM Reconstruction error 34.405594\n",
      ">> Epoch 32 finished \tRBM Reconstruction error 30.950840\n",
      ">> Epoch 33 finished \tRBM Reconstruction error 33.167236\n",
      ">> Epoch 34 finished \tRBM Reconstruction error 38.003414\n",
      ">> Epoch 35 finished \tRBM Reconstruction error 42.338654\n",
      ">> Epoch 36 finished \tRBM Reconstruction error 40.864254\n",
      ">> Epoch 37 finished \tRBM Reconstruction error 41.628902\n",
      ">> Epoch 38 finished \tRBM Reconstruction error 27.510088\n",
      ">> Epoch 39 finished \tRBM Reconstruction error 30.467539\n",
      ">> Epoch 40 finished \tRBM Reconstruction error 36.698532\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 96.253242\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 183.684174\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 78.674377\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 91.714569\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 110.993431\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 138.788040\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 89.061523\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 136.134369\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 114.479111\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 108.112259\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 98.016914\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 104.318283\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 100.402695\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 139.686798\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 115.318695\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 102.192581\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 122.168610\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 124.092590\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 88.544075\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 120.426048\n",
      ">> Epoch 21 finished \tRBM Reconstruction error 129.410858\n",
      ">> Epoch 22 finished \tRBM Reconstruction error 98.306053\n",
      ">> Epoch 23 finished \tRBM Reconstruction error 136.514603\n",
      ">> Epoch 24 finished \tRBM Reconstruction error 98.119904\n",
      ">> Epoch 25 finished \tRBM Reconstruction error 100.555511\n",
      ">> Epoch 26 finished \tRBM Reconstruction error 143.315979\n",
      ">> Epoch 27 finished \tRBM Reconstruction error 142.413483\n",
      ">> Epoch 28 finished \tRBM Reconstruction error 119.005058\n",
      ">> Epoch 29 finished \tRBM Reconstruction error 95.662399\n",
      ">> Epoch 30 finished \tRBM Reconstruction error 132.270020\n",
      ">> Epoch 31 finished \tRBM Reconstruction error 118.468826\n",
      ">> Epoch 32 finished \tRBM Reconstruction error 136.883179\n",
      ">> Epoch 33 finished \tRBM Reconstruction error 123.653419\n",
      ">> Epoch 34 finished \tRBM Reconstruction error 153.304733\n",
      ">> Epoch 35 finished \tRBM Reconstruction error 115.269630\n",
      ">> Epoch 36 finished \tRBM Reconstruction error 136.281601\n",
      ">> Epoch 37 finished \tRBM Reconstruction error 122.730011\n",
      ">> Epoch 38 finished \tRBM Reconstruction error 119.815926\n",
      ">> Epoch 39 finished \tRBM Reconstruction error 147.712051\n",
      ">> Epoch 40 finished \tRBM Reconstruction error 97.136719\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.575559\n",
      ">> Epoch 1 finished \tANN training loss 0.405406\n",
      ">> Epoch 2 finished \tANN training loss 0.308998\n",
      ">> Epoch 3 finished \tANN training loss 0.247995\n",
      ">> Epoch 4 finished \tANN training loss 0.228701\n",
      ">> Epoch 5 finished \tANN training loss 0.175525\n",
      ">> Epoch 6 finished \tANN training loss 0.154795\n",
      ">> Epoch 7 finished \tANN training loss 0.200008\n",
      ">> Epoch 8 finished \tANN training loss 0.124494\n",
      ">> Epoch 9 finished \tANN training loss 0.096474\n",
      ">> Epoch 10 finished \tANN training loss 0.087040\n",
      ">> Epoch 11 finished \tANN training loss 0.073994\n",
      ">> Epoch 12 finished \tANN training loss 0.068757\n",
      ">> Epoch 13 finished \tANN training loss 0.060631\n",
      ">> Epoch 14 finished \tANN training loss 0.054501\n",
      ">> Epoch 15 finished \tANN training loss 0.047281\n",
      ">> Epoch 16 finished \tANN training loss 0.052018\n",
      ">> Epoch 17 finished \tANN training loss 0.041238\n",
      ">> Epoch 18 finished \tANN training loss 0.032027\n",
      ">> Epoch 19 finished \tANN training loss 0.029175\n",
      ">> Epoch 20 finished \tANN training loss 0.030474\n",
      ">> Epoch 21 finished \tANN training loss 0.026703\n",
      ">> Epoch 22 finished \tANN training loss 0.023318\n",
      ">> Epoch 23 finished \tANN training loss 0.022907\n",
      ">> Epoch 24 finished \tANN training loss 0.018816\n",
      ">> Epoch 25 finished \tANN training loss 0.017781\n",
      ">> Epoch 26 finished \tANN training loss 0.015022\n",
      ">> Epoch 27 finished \tANN training loss 0.014593\n",
      ">> Epoch 28 finished \tANN training loss 0.012941\n",
      ">> Epoch 29 finished \tANN training loss 0.012645\n",
      ">> Epoch 30 finished \tANN training loss 0.010471\n",
      ">> Epoch 31 finished \tANN training loss 0.010958\n",
      ">> Epoch 32 finished \tANN training loss 0.009815\n",
      ">> Epoch 33 finished \tANN training loss 0.009746\n",
      ">> Epoch 34 finished \tANN training loss 0.007565\n",
      ">> Epoch 35 finished \tANN training loss 0.008848\n",
      ">> Epoch 36 finished \tANN training loss 0.007828\n",
      ">> Epoch 37 finished \tANN training loss 0.006719\n",
      ">> Epoch 38 finished \tANN training loss 0.006489\n",
      ">> Epoch 39 finished \tANN training loss 0.006735\n",
      ">> Epoch 40 finished \tANN training loss 0.005985\n",
      ">> Epoch 41 finished \tANN training loss 0.005306\n",
      ">> Epoch 42 finished \tANN training loss 0.005140\n",
      ">> Epoch 43 finished \tANN training loss 0.005064\n",
      ">> Epoch 44 finished \tANN training loss 0.005068\n",
      ">> Epoch 45 finished \tANN training loss 0.004523\n",
      ">> Epoch 46 finished \tANN training loss 0.004456\n",
      ">> Epoch 47 finished \tANN training loss 0.004155\n",
      ">> Epoch 48 finished \tANN training loss 0.003685\n",
      ">> Epoch 49 finished \tANN training loss 0.003845\n",
      ">> Epoch 50 finished \tANN training loss 0.003182\n",
      ">> Epoch 51 finished \tANN training loss 0.003157\n",
      ">> Epoch 52 finished \tANN training loss 0.002802\n",
      ">> Epoch 53 finished \tANN training loss 0.002670\n",
      ">> Epoch 54 finished \tANN training loss 0.002610\n",
      ">> Epoch 55 finished \tANN training loss 0.002858\n",
      ">> Epoch 56 finished \tANN training loss 0.002601\n",
      ">> Epoch 57 finished \tANN training loss 0.002551\n",
      ">> Epoch 58 finished \tANN training loss 0.002157\n",
      ">> Epoch 59 finished \tANN training loss 0.002258\n",
      ">> Epoch 60 finished \tANN training loss 0.001994\n",
      ">> Epoch 61 finished \tANN training loss 0.002260\n",
      ">> Epoch 62 finished \tANN training loss 0.001872\n",
      ">> Epoch 63 finished \tANN training loss 0.001822\n",
      ">> Epoch 64 finished \tANN training loss 0.001723\n",
      ">> Epoch 65 finished \tANN training loss 0.001777\n",
      ">> Epoch 66 finished \tANN training loss 0.001591\n",
      ">> Epoch 67 finished \tANN training loss 0.001657\n",
      ">> Epoch 68 finished \tANN training loss 0.001491\n",
      ">> Epoch 69 finished \tANN training loss 0.001864\n",
      ">> Epoch 70 finished \tANN training loss 0.001798\n",
      ">> Epoch 71 finished \tANN training loss 0.001346\n",
      ">> Epoch 72 finished \tANN training loss 0.001330\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 73 finished \tANN training loss 0.001384\n",
      ">> Epoch 74 finished \tANN training loss 0.001189\n",
      ">> Epoch 75 finished \tANN training loss 0.001220\n",
      ">> Epoch 76 finished \tANN training loss 0.001221\n",
      ">> Epoch 77 finished \tANN training loss 0.001270\n",
      ">> Epoch 78 finished \tANN training loss 0.001121\n",
      ">> Epoch 79 finished \tANN training loss 0.001050\n",
      ">> Epoch 80 finished \tANN training loss 0.000943\n",
      ">> Epoch 81 finished \tANN training loss 0.001007\n",
      ">> Epoch 82 finished \tANN training loss 0.001005\n",
      ">> Epoch 83 finished \tANN training loss 0.000976\n",
      ">> Epoch 84 finished \tANN training loss 0.000965\n",
      ">> Epoch 85 finished \tANN training loss 0.001021\n",
      ">> Epoch 86 finished \tANN training loss 0.000942\n",
      ">> Epoch 87 finished \tANN training loss 0.000949\n",
      ">> Epoch 88 finished \tANN training loss 0.000796\n",
      ">> Epoch 89 finished \tANN training loss 0.000785\n",
      ">> Epoch 90 finished \tANN training loss 0.000735\n",
      ">> Epoch 91 finished \tANN training loss 0.000766\n",
      ">> Epoch 92 finished \tANN training loss 0.000729\n",
      ">> Epoch 93 finished \tANN training loss 0.000678\n",
      ">> Epoch 94 finished \tANN training loss 0.000698\n",
      ">> Epoch 95 finished \tANN training loss 0.000730\n",
      ">> Epoch 96 finished \tANN training loss 0.000700\n",
      ">> Epoch 97 finished \tANN training loss 0.000633\n",
      ">> Epoch 98 finished \tANN training loss 0.000574\n",
      ">> Epoch 99 finished \tANN training loss 0.000587\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.930000\n"
     ]
    }
   ],
   "source": [
    "epoch_rbm_acc[6] = deep_belief_net(n_epochs_rbm=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.93\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(epoch_rbm_acc[6]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setting 8**\n",
    "\n",
    "    n_epochs_rbm=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 54.985214\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 62.096172\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 45.251816\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 43.081493\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 29.928345\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 28.466885\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 36.912857\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 45.177746\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 49.283730\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 46.618320\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 35.642426\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 30.457457\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 31.268808\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 45.809601\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 44.016468\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 31.719570\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 33.370472\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 40.761951\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 31.600405\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 32.468315\n",
      ">> Epoch 21 finished \tRBM Reconstruction error 31.679224\n",
      ">> Epoch 22 finished \tRBM Reconstruction error 32.492111\n",
      ">> Epoch 23 finished \tRBM Reconstruction error 35.449799\n",
      ">> Epoch 24 finished \tRBM Reconstruction error 43.675613\n",
      ">> Epoch 25 finished \tRBM Reconstruction error 33.906666\n",
      ">> Epoch 26 finished \tRBM Reconstruction error 40.161636\n",
      ">> Epoch 27 finished \tRBM Reconstruction error 28.779116\n",
      ">> Epoch 28 finished \tRBM Reconstruction error 34.979935\n",
      ">> Epoch 29 finished \tRBM Reconstruction error 28.730215\n",
      ">> Epoch 30 finished \tRBM Reconstruction error 31.814503\n",
      ">> Epoch 31 finished \tRBM Reconstruction error 42.189648\n",
      ">> Epoch 32 finished \tRBM Reconstruction error 35.736607\n",
      ">> Epoch 33 finished \tRBM Reconstruction error 36.740726\n",
      ">> Epoch 34 finished \tRBM Reconstruction error 30.774963\n",
      ">> Epoch 35 finished \tRBM Reconstruction error 38.731300\n",
      ">> Epoch 36 finished \tRBM Reconstruction error 36.828842\n",
      ">> Epoch 37 finished \tRBM Reconstruction error 40.284721\n",
      ">> Epoch 38 finished \tRBM Reconstruction error 33.431606\n",
      ">> Epoch 39 finished \tRBM Reconstruction error 53.262314\n",
      ">> Epoch 40 finished \tRBM Reconstruction error 33.641586\n",
      ">> Epoch 41 finished \tRBM Reconstruction error 45.688915\n",
      ">> Epoch 42 finished \tRBM Reconstruction error 37.920570\n",
      ">> Epoch 43 finished \tRBM Reconstruction error 34.334785\n",
      ">> Epoch 44 finished \tRBM Reconstruction error 29.965361\n",
      ">> Epoch 45 finished \tRBM Reconstruction error 35.227638\n",
      ">> Epoch 46 finished \tRBM Reconstruction error 32.324764\n",
      ">> Epoch 47 finished \tRBM Reconstruction error 36.219288\n",
      ">> Epoch 48 finished \tRBM Reconstruction error 38.799026\n",
      ">> Epoch 49 finished \tRBM Reconstruction error 39.128159\n",
      ">> Epoch 50 finished \tRBM Reconstruction error 32.884838\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 125.804100\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 77.486778\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 93.401787\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 99.944412\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 78.066780\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 99.720978\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 43.306313\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 82.794495\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 98.328117\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 86.998634\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 86.324905\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 76.527878\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 59.903404\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 88.480637\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 86.590782\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 102.271576\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 87.106445\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 99.319748\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 111.583473\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 83.346527\n",
      ">> Epoch 21 finished \tRBM Reconstruction error 86.381500\n",
      ">> Epoch 22 finished \tRBM Reconstruction error 106.694183\n",
      ">> Epoch 23 finished \tRBM Reconstruction error 102.709122\n",
      ">> Epoch 24 finished \tRBM Reconstruction error 124.166740\n",
      ">> Epoch 25 finished \tRBM Reconstruction error 85.671097\n",
      ">> Epoch 26 finished \tRBM Reconstruction error 93.935783\n",
      ">> Epoch 27 finished \tRBM Reconstruction error 81.691200\n",
      ">> Epoch 28 finished \tRBM Reconstruction error 85.807098\n",
      ">> Epoch 29 finished \tRBM Reconstruction error 77.376297\n",
      ">> Epoch 30 finished \tRBM Reconstruction error 93.682793\n",
      ">> Epoch 31 finished \tRBM Reconstruction error 159.651398\n",
      ">> Epoch 32 finished \tRBM Reconstruction error 90.419952\n",
      ">> Epoch 33 finished \tRBM Reconstruction error 94.738319\n",
      ">> Epoch 34 finished \tRBM Reconstruction error 67.020050\n",
      ">> Epoch 35 finished \tRBM Reconstruction error 84.792435\n",
      ">> Epoch 36 finished \tRBM Reconstruction error 96.659531\n",
      ">> Epoch 37 finished \tRBM Reconstruction error 105.481972\n",
      ">> Epoch 38 finished \tRBM Reconstruction error 114.097282\n",
      ">> Epoch 39 finished \tRBM Reconstruction error 76.890915\n",
      ">> Epoch 40 finished \tRBM Reconstruction error 81.217575\n",
      ">> Epoch 41 finished \tRBM Reconstruction error 97.449081\n",
      ">> Epoch 42 finished \tRBM Reconstruction error 75.984642\n",
      ">> Epoch 43 finished \tRBM Reconstruction error 79.706650\n",
      ">> Epoch 44 finished \tRBM Reconstruction error 96.392365\n",
      ">> Epoch 45 finished \tRBM Reconstruction error 114.727180\n",
      ">> Epoch 46 finished \tRBM Reconstruction error 104.383186\n",
      ">> Epoch 47 finished \tRBM Reconstruction error 95.007362\n",
      ">> Epoch 48 finished \tRBM Reconstruction error 91.924377\n",
      ">> Epoch 49 finished \tRBM Reconstruction error 92.933937\n",
      ">> Epoch 50 finished \tRBM Reconstruction error 87.460838\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.610499\n",
      ">> Epoch 1 finished \tANN training loss 0.395352\n",
      ">> Epoch 2 finished \tANN training loss 0.286621\n",
      ">> Epoch 3 finished \tANN training loss 0.246337\n",
      ">> Epoch 4 finished \tANN training loss 0.223768\n",
      ">> Epoch 5 finished \tANN training loss 0.197915\n",
      ">> Epoch 6 finished \tANN training loss 0.145112\n",
      ">> Epoch 7 finished \tANN training loss 0.133209\n",
      ">> Epoch 8 finished \tANN training loss 0.112177\n",
      ">> Epoch 9 finished \tANN training loss 0.086660\n",
      ">> Epoch 10 finished \tANN training loss 0.084649\n",
      ">> Epoch 11 finished \tANN training loss 0.068575\n",
      ">> Epoch 12 finished \tANN training loss 0.077028\n",
      ">> Epoch 13 finished \tANN training loss 0.061931\n",
      ">> Epoch 14 finished \tANN training loss 0.056546\n",
      ">> Epoch 15 finished \tANN training loss 0.042343\n",
      ">> Epoch 16 finished \tANN training loss 0.040199\n",
      ">> Epoch 17 finished \tANN training loss 0.037044\n",
      ">> Epoch 18 finished \tANN training loss 0.030045\n",
      ">> Epoch 19 finished \tANN training loss 0.031748\n",
      ">> Epoch 20 finished \tANN training loss 0.028787\n",
      ">> Epoch 21 finished \tANN training loss 0.023705\n",
      ">> Epoch 22 finished \tANN training loss 0.021906\n",
      ">> Epoch 23 finished \tANN training loss 0.020605\n",
      ">> Epoch 24 finished \tANN training loss 0.017037\n",
      ">> Epoch 25 finished \tANN training loss 0.018187\n",
      ">> Epoch 26 finished \tANN training loss 0.015041\n",
      ">> Epoch 27 finished \tANN training loss 0.015201\n",
      ">> Epoch 28 finished \tANN training loss 0.012150\n",
      ">> Epoch 29 finished \tANN training loss 0.011889\n",
      ">> Epoch 30 finished \tANN training loss 0.010933\n",
      ">> Epoch 31 finished \tANN training loss 0.010373\n",
      ">> Epoch 32 finished \tANN training loss 0.009542\n",
      ">> Epoch 33 finished \tANN training loss 0.009623\n",
      ">> Epoch 34 finished \tANN training loss 0.009694\n",
      ">> Epoch 35 finished \tANN training loss 0.008224\n",
      ">> Epoch 36 finished \tANN training loss 0.007116\n",
      ">> Epoch 37 finished \tANN training loss 0.007418\n",
      ">> Epoch 38 finished \tANN training loss 0.005990\n",
      ">> Epoch 39 finished \tANN training loss 0.006221\n",
      ">> Epoch 40 finished \tANN training loss 0.005930\n",
      ">> Epoch 41 finished \tANN training loss 0.005008\n",
      ">> Epoch 42 finished \tANN training loss 0.004821\n",
      ">> Epoch 43 finished \tANN training loss 0.004608\n",
      ">> Epoch 44 finished \tANN training loss 0.004338\n",
      ">> Epoch 45 finished \tANN training loss 0.004340\n",
      ">> Epoch 46 finished \tANN training loss 0.004184\n",
      ">> Epoch 47 finished \tANN training loss 0.004392\n",
      ">> Epoch 48 finished \tANN training loss 0.003624\n",
      ">> Epoch 49 finished \tANN training loss 0.003609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 50 finished \tANN training loss 0.003329\n",
      ">> Epoch 51 finished \tANN training loss 0.003367\n",
      ">> Epoch 52 finished \tANN training loss 0.003713\n",
      ">> Epoch 53 finished \tANN training loss 0.003610\n",
      ">> Epoch 54 finished \tANN training loss 0.003883\n",
      ">> Epoch 55 finished \tANN training loss 0.002767\n",
      ">> Epoch 56 finished \tANN training loss 0.003932\n",
      ">> Epoch 57 finished \tANN training loss 0.002878\n",
      ">> Epoch 58 finished \tANN training loss 0.002623\n",
      ">> Epoch 59 finished \tANN training loss 0.002451\n",
      ">> Epoch 60 finished \tANN training loss 0.002198\n",
      ">> Epoch 61 finished \tANN training loss 0.002257\n",
      ">> Epoch 62 finished \tANN training loss 0.002150\n",
      ">> Epoch 63 finished \tANN training loss 0.001997\n",
      ">> Epoch 64 finished \tANN training loss 0.002560\n",
      ">> Epoch 65 finished \tANN training loss 0.002030\n",
      ">> Epoch 66 finished \tANN training loss 0.001902\n",
      ">> Epoch 67 finished \tANN training loss 0.001634\n",
      ">> Epoch 68 finished \tANN training loss 0.001685\n",
      ">> Epoch 69 finished \tANN training loss 0.001626\n",
      ">> Epoch 70 finished \tANN training loss 0.001636\n",
      ">> Epoch 71 finished \tANN training loss 0.001502\n",
      ">> Epoch 72 finished \tANN training loss 0.001697\n",
      ">> Epoch 73 finished \tANN training loss 0.001512\n",
      ">> Epoch 74 finished \tANN training loss 0.001432\n",
      ">> Epoch 75 finished \tANN training loss 0.001206\n",
      ">> Epoch 76 finished \tANN training loss 0.001236\n",
      ">> Epoch 77 finished \tANN training loss 0.001196\n",
      ">> Epoch 78 finished \tANN training loss 0.001236\n",
      ">> Epoch 79 finished \tANN training loss 0.001169\n",
      ">> Epoch 80 finished \tANN training loss 0.001196\n",
      ">> Epoch 81 finished \tANN training loss 0.001166\n",
      ">> Epoch 82 finished \tANN training loss 0.001105\n",
      ">> Epoch 83 finished \tANN training loss 0.001154\n",
      ">> Epoch 84 finished \tANN training loss 0.001090\n",
      ">> Epoch 85 finished \tANN training loss 0.001076\n",
      ">> Epoch 86 finished \tANN training loss 0.001028\n",
      ">> Epoch 87 finished \tANN training loss 0.000973\n",
      ">> Epoch 88 finished \tANN training loss 0.000949\n",
      ">> Epoch 89 finished \tANN training loss 0.000939\n",
      ">> Epoch 90 finished \tANN training loss 0.000887\n",
      ">> Epoch 91 finished \tANN training loss 0.000964\n",
      ">> Epoch 92 finished \tANN training loss 0.000898\n",
      ">> Epoch 93 finished \tANN training loss 0.000889\n",
      ">> Epoch 94 finished \tANN training loss 0.000803\n",
      ">> Epoch 95 finished \tANN training loss 0.000858\n",
      ">> Epoch 96 finished \tANN training loss 0.000817\n",
      ">> Epoch 97 finished \tANN training loss 0.000752\n",
      ">> Epoch 98 finished \tANN training loss 0.000755\n",
      ">> Epoch 99 finished \tANN training loss 0.000734\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.900000\n"
     ]
    }
   ],
   "source": [
    "epoch_rbm_acc[7] = deep_belief_net(n_epochs_rbm=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.9\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(epoch_rbm_acc[7]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setting 9**\n",
    "\n",
    "    n_epochs_rbm=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 102.369392\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 39.580040\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 58.195435\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 42.953144\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 30.074026\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 25.546534\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 32.715004\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 28.658079\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 27.248291\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 21.905323\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 20.972374\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 30.766115\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 43.735683\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 27.307983\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 33.528534\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 18.731007\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 27.331324\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 38.402706\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 29.862865\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 40.268932\n",
      ">> Epoch 21 finished \tRBM Reconstruction error 32.184887\n",
      ">> Epoch 22 finished \tRBM Reconstruction error 39.825115\n",
      ">> Epoch 23 finished \tRBM Reconstruction error 26.806177\n",
      ">> Epoch 24 finished \tRBM Reconstruction error 29.430204\n",
      ">> Epoch 25 finished \tRBM Reconstruction error 23.967192\n",
      ">> Epoch 26 finished \tRBM Reconstruction error 23.805079\n",
      ">> Epoch 27 finished \tRBM Reconstruction error 34.138329\n",
      ">> Epoch 28 finished \tRBM Reconstruction error 36.367519\n",
      ">> Epoch 29 finished \tRBM Reconstruction error 32.880257\n",
      ">> Epoch 30 finished \tRBM Reconstruction error 43.483456\n",
      ">> Epoch 31 finished \tRBM Reconstruction error 27.266542\n",
      ">> Epoch 32 finished \tRBM Reconstruction error 29.320518\n",
      ">> Epoch 33 finished \tRBM Reconstruction error 37.196865\n",
      ">> Epoch 34 finished \tRBM Reconstruction error 27.303625\n",
      ">> Epoch 35 finished \tRBM Reconstruction error 26.077913\n",
      ">> Epoch 36 finished \tRBM Reconstruction error 50.016006\n",
      ">> Epoch 37 finished \tRBM Reconstruction error 27.148087\n",
      ">> Epoch 38 finished \tRBM Reconstruction error 23.290499\n",
      ">> Epoch 39 finished \tRBM Reconstruction error 32.554523\n",
      ">> Epoch 40 finished \tRBM Reconstruction error 30.474489\n",
      ">> Epoch 41 finished \tRBM Reconstruction error 30.306696\n",
      ">> Epoch 42 finished \tRBM Reconstruction error 36.048634\n",
      ">> Epoch 43 finished \tRBM Reconstruction error 29.288630\n",
      ">> Epoch 44 finished \tRBM Reconstruction error 40.861832\n",
      ">> Epoch 45 finished \tRBM Reconstruction error 30.047388\n",
      ">> Epoch 46 finished \tRBM Reconstruction error 26.523979\n",
      ">> Epoch 47 finished \tRBM Reconstruction error 33.720680\n",
      ">> Epoch 48 finished \tRBM Reconstruction error 40.593685\n",
      ">> Epoch 49 finished \tRBM Reconstruction error 37.461300\n",
      ">> Epoch 50 finished \tRBM Reconstruction error 35.021915\n",
      ">> Epoch 51 finished \tRBM Reconstruction error 33.216915\n",
      ">> Epoch 52 finished \tRBM Reconstruction error 31.770569\n",
      ">> Epoch 53 finished \tRBM Reconstruction error 42.008743\n",
      ">> Epoch 54 finished \tRBM Reconstruction error 34.392147\n",
      ">> Epoch 55 finished \tRBM Reconstruction error 35.436268\n",
      ">> Epoch 56 finished \tRBM Reconstruction error 39.889271\n",
      ">> Epoch 57 finished \tRBM Reconstruction error 36.003551\n",
      ">> Epoch 58 finished \tRBM Reconstruction error 40.840450\n",
      ">> Epoch 59 finished \tRBM Reconstruction error 25.438360\n",
      ">> Epoch 60 finished \tRBM Reconstruction error 38.855698\n",
      ">> Epoch 61 finished \tRBM Reconstruction error 34.215500\n",
      ">> Epoch 62 finished \tRBM Reconstruction error 33.483765\n",
      ">> Epoch 63 finished \tRBM Reconstruction error 56.107422\n",
      ">> Epoch 64 finished \tRBM Reconstruction error 41.467388\n",
      ">> Epoch 65 finished \tRBM Reconstruction error 30.269831\n",
      ">> Epoch 66 finished \tRBM Reconstruction error 38.067558\n",
      ">> Epoch 67 finished \tRBM Reconstruction error 37.634613\n",
      ">> Epoch 68 finished \tRBM Reconstruction error 48.928848\n",
      ">> Epoch 69 finished \tRBM Reconstruction error 48.783077\n",
      ">> Epoch 70 finished \tRBM Reconstruction error 42.592354\n",
      ">> Epoch 71 finished \tRBM Reconstruction error 33.132034\n",
      ">> Epoch 72 finished \tRBM Reconstruction error 36.890575\n",
      ">> Epoch 73 finished \tRBM Reconstruction error 33.157074\n",
      ">> Epoch 74 finished \tRBM Reconstruction error 33.242275\n",
      ">> Epoch 75 finished \tRBM Reconstruction error 43.174435\n",
      ">> Epoch 76 finished \tRBM Reconstruction error 27.996876\n",
      ">> Epoch 77 finished \tRBM Reconstruction error 36.515167\n",
      ">> Epoch 78 finished \tRBM Reconstruction error 39.309006\n",
      ">> Epoch 79 finished \tRBM Reconstruction error 33.881725\n",
      ">> Epoch 80 finished \tRBM Reconstruction error 37.876244\n",
      ">> Epoch 81 finished \tRBM Reconstruction error 36.871010\n",
      ">> Epoch 82 finished \tRBM Reconstruction error 27.419258\n",
      ">> Epoch 83 finished \tRBM Reconstruction error 38.832897\n",
      ">> Epoch 84 finished \tRBM Reconstruction error 36.318336\n",
      ">> Epoch 85 finished \tRBM Reconstruction error 36.781708\n",
      ">> Epoch 86 finished \tRBM Reconstruction error 46.488720\n",
      ">> Epoch 87 finished \tRBM Reconstruction error 26.829906\n",
      ">> Epoch 88 finished \tRBM Reconstruction error 31.571228\n",
      ">> Epoch 89 finished \tRBM Reconstruction error 47.068878\n",
      ">> Epoch 90 finished \tRBM Reconstruction error 48.483788\n",
      ">> Epoch 91 finished \tRBM Reconstruction error 25.893618\n",
      ">> Epoch 92 finished \tRBM Reconstruction error 28.653183\n",
      ">> Epoch 93 finished \tRBM Reconstruction error 35.576008\n",
      ">> Epoch 94 finished \tRBM Reconstruction error 30.349709\n",
      ">> Epoch 95 finished \tRBM Reconstruction error 36.958496\n",
      ">> Epoch 96 finished \tRBM Reconstruction error 40.241615\n",
      ">> Epoch 97 finished \tRBM Reconstruction error 46.928314\n",
      ">> Epoch 98 finished \tRBM Reconstruction error 41.136864\n",
      ">> Epoch 99 finished \tRBM Reconstruction error 36.787724\n",
      ">> Epoch 100 finished \tRBM Reconstruction error 37.594799\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 111.787949\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 113.721779\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 59.413731\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 114.905830\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 105.282150\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 142.728775\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 106.057503\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 86.573914\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 111.232780\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 88.242256\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 83.717697\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 87.396034\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 68.227180\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 58.917084\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 94.475250\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 77.703255\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 64.360077\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 82.129433\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 79.948997\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 119.310173\n",
      ">> Epoch 21 finished \tRBM Reconstruction error 85.296211\n",
      ">> Epoch 22 finished \tRBM Reconstruction error 100.491905\n",
      ">> Epoch 23 finished \tRBM Reconstruction error 79.802795\n",
      ">> Epoch 24 finished \tRBM Reconstruction error 98.110596\n",
      ">> Epoch 25 finished \tRBM Reconstruction error 83.812149\n",
      ">> Epoch 26 finished \tRBM Reconstruction error 93.394333\n",
      ">> Epoch 27 finished \tRBM Reconstruction error 83.264099\n",
      ">> Epoch 28 finished \tRBM Reconstruction error 114.985497\n",
      ">> Epoch 29 finished \tRBM Reconstruction error 110.320137\n",
      ">> Epoch 30 finished \tRBM Reconstruction error 122.557419\n",
      ">> Epoch 31 finished \tRBM Reconstruction error 101.644287\n",
      ">> Epoch 32 finished \tRBM Reconstruction error 106.259033\n",
      ">> Epoch 33 finished \tRBM Reconstruction error 115.398964\n",
      ">> Epoch 34 finished \tRBM Reconstruction error 92.343513\n",
      ">> Epoch 35 finished \tRBM Reconstruction error 94.540451\n",
      ">> Epoch 36 finished \tRBM Reconstruction error 104.574768\n",
      ">> Epoch 37 finished \tRBM Reconstruction error 125.920013\n",
      ">> Epoch 38 finished \tRBM Reconstruction error 103.285080\n",
      ">> Epoch 39 finished \tRBM Reconstruction error 120.245918\n",
      ">> Epoch 40 finished \tRBM Reconstruction error 118.585564\n",
      ">> Epoch 41 finished \tRBM Reconstruction error 81.128998\n",
      ">> Epoch 42 finished \tRBM Reconstruction error 129.073593\n",
      ">> Epoch 43 finished \tRBM Reconstruction error 97.606560\n",
      ">> Epoch 44 finished \tRBM Reconstruction error 121.162376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 45 finished \tRBM Reconstruction error 105.466980\n",
      ">> Epoch 46 finished \tRBM Reconstruction error 101.058105\n",
      ">> Epoch 47 finished \tRBM Reconstruction error 86.785782\n",
      ">> Epoch 48 finished \tRBM Reconstruction error 93.663162\n",
      ">> Epoch 49 finished \tRBM Reconstruction error 106.378914\n",
      ">> Epoch 50 finished \tRBM Reconstruction error 103.022987\n",
      ">> Epoch 51 finished \tRBM Reconstruction error 102.834541\n",
      ">> Epoch 52 finished \tRBM Reconstruction error 107.451698\n",
      ">> Epoch 53 finished \tRBM Reconstruction error 100.687538\n",
      ">> Epoch 54 finished \tRBM Reconstruction error 97.238434\n",
      ">> Epoch 55 finished \tRBM Reconstruction error 98.880295\n",
      ">> Epoch 56 finished \tRBM Reconstruction error 111.265289\n",
      ">> Epoch 57 finished \tRBM Reconstruction error 97.847542\n",
      ">> Epoch 58 finished \tRBM Reconstruction error 107.616867\n",
      ">> Epoch 59 finished \tRBM Reconstruction error 103.735153\n",
      ">> Epoch 60 finished \tRBM Reconstruction error 83.978668\n",
      ">> Epoch 61 finished \tRBM Reconstruction error 127.891464\n",
      ">> Epoch 62 finished \tRBM Reconstruction error 103.064163\n",
      ">> Epoch 63 finished \tRBM Reconstruction error 106.983223\n",
      ">> Epoch 64 finished \tRBM Reconstruction error 90.207481\n",
      ">> Epoch 65 finished \tRBM Reconstruction error 103.057190\n",
      ">> Epoch 66 finished \tRBM Reconstruction error 157.250000\n",
      ">> Epoch 67 finished \tRBM Reconstruction error 118.494110\n",
      ">> Epoch 68 finished \tRBM Reconstruction error 90.348381\n",
      ">> Epoch 69 finished \tRBM Reconstruction error 94.758858\n",
      ">> Epoch 70 finished \tRBM Reconstruction error 98.701248\n",
      ">> Epoch 71 finished \tRBM Reconstruction error 86.486549\n",
      ">> Epoch 72 finished \tRBM Reconstruction error 90.717346\n",
      ">> Epoch 73 finished \tRBM Reconstruction error 112.103401\n",
      ">> Epoch 74 finished \tRBM Reconstruction error 107.603416\n",
      ">> Epoch 75 finished \tRBM Reconstruction error 122.361267\n",
      ">> Epoch 76 finished \tRBM Reconstruction error 114.831375\n",
      ">> Epoch 77 finished \tRBM Reconstruction error 98.925232\n",
      ">> Epoch 78 finished \tRBM Reconstruction error 102.833397\n",
      ">> Epoch 79 finished \tRBM Reconstruction error 108.348137\n",
      ">> Epoch 80 finished \tRBM Reconstruction error 93.257637\n",
      ">> Epoch 81 finished \tRBM Reconstruction error 101.417900\n",
      ">> Epoch 82 finished \tRBM Reconstruction error 104.676659\n",
      ">> Epoch 83 finished \tRBM Reconstruction error 94.100647\n",
      ">> Epoch 84 finished \tRBM Reconstruction error 112.348595\n",
      ">> Epoch 85 finished \tRBM Reconstruction error 89.634171\n",
      ">> Epoch 86 finished \tRBM Reconstruction error 101.568123\n",
      ">> Epoch 87 finished \tRBM Reconstruction error 115.196754\n",
      ">> Epoch 88 finished \tRBM Reconstruction error 107.449051\n",
      ">> Epoch 89 finished \tRBM Reconstruction error 94.930321\n",
      ">> Epoch 90 finished \tRBM Reconstruction error 105.782578\n",
      ">> Epoch 91 finished \tRBM Reconstruction error 139.425262\n",
      ">> Epoch 92 finished \tRBM Reconstruction error 85.520081\n",
      ">> Epoch 93 finished \tRBM Reconstruction error 131.636383\n",
      ">> Epoch 94 finished \tRBM Reconstruction error 122.221222\n",
      ">> Epoch 95 finished \tRBM Reconstruction error 114.124588\n",
      ">> Epoch 96 finished \tRBM Reconstruction error 104.876503\n",
      ">> Epoch 97 finished \tRBM Reconstruction error 99.367996\n",
      ">> Epoch 98 finished \tRBM Reconstruction error 98.790466\n",
      ">> Epoch 99 finished \tRBM Reconstruction error 83.223122\n",
      ">> Epoch 100 finished \tRBM Reconstruction error 103.494896\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.559153\n",
      ">> Epoch 1 finished \tANN training loss 0.423848\n",
      ">> Epoch 2 finished \tANN training loss 0.286882\n",
      ">> Epoch 3 finished \tANN training loss 0.247302\n",
      ">> Epoch 4 finished \tANN training loss 0.203537\n",
      ">> Epoch 5 finished \tANN training loss 0.164076\n",
      ">> Epoch 6 finished \tANN training loss 0.146482\n",
      ">> Epoch 7 finished \tANN training loss 0.126781\n",
      ">> Epoch 8 finished \tANN training loss 0.096451\n",
      ">> Epoch 9 finished \tANN training loss 0.084875\n",
      ">> Epoch 10 finished \tANN training loss 0.075282\n",
      ">> Epoch 11 finished \tANN training loss 0.066013\n",
      ">> Epoch 12 finished \tANN training loss 0.054026\n",
      ">> Epoch 13 finished \tANN training loss 0.056047\n",
      ">> Epoch 14 finished \tANN training loss 0.043809\n",
      ">> Epoch 15 finished \tANN training loss 0.036771\n",
      ">> Epoch 16 finished \tANN training loss 0.034257\n",
      ">> Epoch 17 finished \tANN training loss 0.029328\n",
      ">> Epoch 18 finished \tANN training loss 0.024533\n",
      ">> Epoch 19 finished \tANN training loss 0.023145\n",
      ">> Epoch 20 finished \tANN training loss 0.021318\n",
      ">> Epoch 21 finished \tANN training loss 0.021223\n",
      ">> Epoch 22 finished \tANN training loss 0.020408\n",
      ">> Epoch 23 finished \tANN training loss 0.015606\n",
      ">> Epoch 24 finished \tANN training loss 0.014383\n",
      ">> Epoch 25 finished \tANN training loss 0.012754\n",
      ">> Epoch 26 finished \tANN training loss 0.014770\n",
      ">> Epoch 27 finished \tANN training loss 0.012297\n",
      ">> Epoch 28 finished \tANN training loss 0.010822\n",
      ">> Epoch 29 finished \tANN training loss 0.010341\n",
      ">> Epoch 30 finished \tANN training loss 0.009971\n",
      ">> Epoch 31 finished \tANN training loss 0.008069\n",
      ">> Epoch 32 finished \tANN training loss 0.008010\n",
      ">> Epoch 33 finished \tANN training loss 0.006893\n",
      ">> Epoch 34 finished \tANN training loss 0.007492\n",
      ">> Epoch 35 finished \tANN training loss 0.005975\n",
      ">> Epoch 36 finished \tANN training loss 0.006351\n",
      ">> Epoch 37 finished \tANN training loss 0.005977\n",
      ">> Epoch 38 finished \tANN training loss 0.005839\n",
      ">> Epoch 39 finished \tANN training loss 0.004995\n",
      ">> Epoch 40 finished \tANN training loss 0.004933\n",
      ">> Epoch 41 finished \tANN training loss 0.005292\n",
      ">> Epoch 42 finished \tANN training loss 0.003973\n",
      ">> Epoch 43 finished \tANN training loss 0.003726\n",
      ">> Epoch 44 finished \tANN training loss 0.004571\n",
      ">> Epoch 45 finished \tANN training loss 0.004490\n",
      ">> Epoch 46 finished \tANN training loss 0.003382\n",
      ">> Epoch 47 finished \tANN training loss 0.003802\n",
      ">> Epoch 48 finished \tANN training loss 0.003105\n",
      ">> Epoch 49 finished \tANN training loss 0.002749\n",
      ">> Epoch 50 finished \tANN training loss 0.003888\n",
      ">> Epoch 51 finished \tANN training loss 0.002902\n",
      ">> Epoch 52 finished \tANN training loss 0.002542\n",
      ">> Epoch 53 finished \tANN training loss 0.002921\n",
      ">> Epoch 54 finished \tANN training loss 0.002682\n",
      ">> Epoch 55 finished \tANN training loss 0.003040\n",
      ">> Epoch 56 finished \tANN training loss 0.002450\n",
      ">> Epoch 57 finished \tANN training loss 0.002197\n",
      ">> Epoch 58 finished \tANN training loss 0.002013\n",
      ">> Epoch 59 finished \tANN training loss 0.002560\n",
      ">> Epoch 60 finished \tANN training loss 0.002253\n",
      ">> Epoch 61 finished \tANN training loss 0.002195\n",
      ">> Epoch 62 finished \tANN training loss 0.001861\n",
      ">> Epoch 63 finished \tANN training loss 0.001955\n",
      ">> Epoch 64 finished \tANN training loss 0.001645\n",
      ">> Epoch 65 finished \tANN training loss 0.001881\n",
      ">> Epoch 66 finished \tANN training loss 0.001828\n",
      ">> Epoch 67 finished \tANN training loss 0.001551\n",
      ">> Epoch 68 finished \tANN training loss 0.001548\n",
      ">> Epoch 69 finished \tANN training loss 0.002024\n",
      ">> Epoch 70 finished \tANN training loss 0.001331\n",
      ">> Epoch 71 finished \tANN training loss 0.001269\n",
      ">> Epoch 72 finished \tANN training loss 0.001172\n",
      ">> Epoch 73 finished \tANN training loss 0.001160\n",
      ">> Epoch 74 finished \tANN training loss 0.001090\n",
      ">> Epoch 75 finished \tANN training loss 0.001022\n",
      ">> Epoch 76 finished \tANN training loss 0.000925\n",
      ">> Epoch 77 finished \tANN training loss 0.000902\n",
      ">> Epoch 78 finished \tANN training loss 0.000879\n",
      ">> Epoch 79 finished \tANN training loss 0.000904\n",
      ">> Epoch 80 finished \tANN training loss 0.000900\n",
      ">> Epoch 81 finished \tANN training loss 0.001140\n",
      ">> Epoch 82 finished \tANN training loss 0.000874\n",
      ">> Epoch 83 finished \tANN training loss 0.000823\n",
      ">> Epoch 84 finished \tANN training loss 0.000886\n",
      ">> Epoch 85 finished \tANN training loss 0.000766\n",
      ">> Epoch 86 finished \tANN training loss 0.000763\n",
      ">> Epoch 87 finished \tANN training loss 0.000728\n",
      ">> Epoch 88 finished \tANN training loss 0.000686\n",
      ">> Epoch 89 finished \tANN training loss 0.000714\n",
      ">> Epoch 90 finished \tANN training loss 0.000649\n",
      ">> Epoch 91 finished \tANN training loss 0.000681\n",
      ">> Epoch 92 finished \tANN training loss 0.000651\n",
      ">> Epoch 93 finished \tANN training loss 0.000573\n",
      ">> Epoch 94 finished \tANN training loss 0.000613\n",
      ">> Epoch 95 finished \tANN training loss 0.000744\n",
      ">> Epoch 96 finished \tANN training loss 0.000732\n",
      ">> Epoch 97 finished \tANN training loss 0.000748\n",
      ">> Epoch 98 finished \tANN training loss 0.000601\n",
      ">> Epoch 99 finished \tANN training loss 0.000600\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.900000\n"
     ]
    }
   ],
   "source": [
    "epoch_rbm_acc[8] = deep_belief_net(n_epochs_rbm=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.9\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(epoch_rbm_acc[8]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setting 10**\n",
    "\n",
    "    n_epochs_rbm=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 57.116241\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 58.623379\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 47.995167\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 46.244217\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 58.939911\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 44.408817\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 42.027470\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 43.308262\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 32.640736\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 24.688789\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 36.358410\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 29.182411\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 33.299675\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 27.233667\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 53.459248\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 37.537029\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 31.129135\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 30.803146\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 38.337326\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 38.912827\n",
      ">> Epoch 21 finished \tRBM Reconstruction error 31.926487\n",
      ">> Epoch 22 finished \tRBM Reconstruction error 45.075577\n",
      ">> Epoch 23 finished \tRBM Reconstruction error 36.450039\n",
      ">> Epoch 24 finished \tRBM Reconstruction error 45.967323\n",
      ">> Epoch 25 finished \tRBM Reconstruction error 40.121899\n",
      ">> Epoch 26 finished \tRBM Reconstruction error 45.008419\n",
      ">> Epoch 27 finished \tRBM Reconstruction error 44.445240\n",
      ">> Epoch 28 finished \tRBM Reconstruction error 41.197735\n",
      ">> Epoch 29 finished \tRBM Reconstruction error 38.312874\n",
      ">> Epoch 30 finished \tRBM Reconstruction error 32.829533\n",
      ">> Epoch 31 finished \tRBM Reconstruction error 30.037983\n",
      ">> Epoch 32 finished \tRBM Reconstruction error 37.973209\n",
      ">> Epoch 33 finished \tRBM Reconstruction error 40.791405\n",
      ">> Epoch 34 finished \tRBM Reconstruction error 35.718197\n",
      ">> Epoch 35 finished \tRBM Reconstruction error 25.079138\n",
      ">> Epoch 36 finished \tRBM Reconstruction error 50.273823\n",
      ">> Epoch 37 finished \tRBM Reconstruction error 33.513794\n",
      ">> Epoch 38 finished \tRBM Reconstruction error 37.381702\n",
      ">> Epoch 39 finished \tRBM Reconstruction error 39.995655\n",
      ">> Epoch 40 finished \tRBM Reconstruction error 42.303234\n",
      ">> Epoch 41 finished \tRBM Reconstruction error 39.147671\n",
      ">> Epoch 42 finished \tRBM Reconstruction error 34.810463\n",
      ">> Epoch 43 finished \tRBM Reconstruction error 54.893890\n",
      ">> Epoch 44 finished \tRBM Reconstruction error 56.195595\n",
      ">> Epoch 45 finished \tRBM Reconstruction error 37.099545\n",
      ">> Epoch 46 finished \tRBM Reconstruction error 43.047619\n",
      ">> Epoch 47 finished \tRBM Reconstruction error 47.525684\n",
      ">> Epoch 48 finished \tRBM Reconstruction error 39.647533\n",
      ">> Epoch 49 finished \tRBM Reconstruction error 38.821484\n",
      ">> Epoch 50 finished \tRBM Reconstruction error 39.319244\n",
      ">> Epoch 51 finished \tRBM Reconstruction error 54.708984\n",
      ">> Epoch 52 finished \tRBM Reconstruction error 35.621536\n",
      ">> Epoch 53 finished \tRBM Reconstruction error 40.572105\n",
      ">> Epoch 54 finished \tRBM Reconstruction error 31.320713\n",
      ">> Epoch 55 finished \tRBM Reconstruction error 38.724178\n",
      ">> Epoch 56 finished \tRBM Reconstruction error 55.324631\n",
      ">> Epoch 57 finished \tRBM Reconstruction error 41.388615\n",
      ">> Epoch 58 finished \tRBM Reconstruction error 45.927841\n",
      ">> Epoch 59 finished \tRBM Reconstruction error 40.084740\n",
      ">> Epoch 60 finished \tRBM Reconstruction error 31.872160\n",
      ">> Epoch 61 finished \tRBM Reconstruction error 46.405949\n",
      ">> Epoch 62 finished \tRBM Reconstruction error 32.117935\n",
      ">> Epoch 63 finished \tRBM Reconstruction error 48.683487\n",
      ">> Epoch 64 finished \tRBM Reconstruction error 42.904560\n",
      ">> Epoch 65 finished \tRBM Reconstruction error 38.039425\n",
      ">> Epoch 66 finished \tRBM Reconstruction error 30.659721\n",
      ">> Epoch 67 finished \tRBM Reconstruction error 52.985546\n",
      ">> Epoch 68 finished \tRBM Reconstruction error 39.618866\n",
      ">> Epoch 69 finished \tRBM Reconstruction error 43.825546\n",
      ">> Epoch 70 finished \tRBM Reconstruction error 37.541351\n",
      ">> Epoch 71 finished \tRBM Reconstruction error 46.164940\n",
      ">> Epoch 72 finished \tRBM Reconstruction error 43.311016\n",
      ">> Epoch 73 finished \tRBM Reconstruction error 51.480202\n",
      ">> Epoch 74 finished \tRBM Reconstruction error 49.289940\n",
      ">> Epoch 75 finished \tRBM Reconstruction error 52.675526\n",
      ">> Epoch 76 finished \tRBM Reconstruction error 51.576584\n",
      ">> Epoch 77 finished \tRBM Reconstruction error 44.223202\n",
      ">> Epoch 78 finished \tRBM Reconstruction error 41.007568\n",
      ">> Epoch 79 finished \tRBM Reconstruction error 45.420475\n",
      ">> Epoch 80 finished \tRBM Reconstruction error 48.857426\n",
      ">> Epoch 81 finished \tRBM Reconstruction error 52.293354\n",
      ">> Epoch 82 finished \tRBM Reconstruction error 37.909679\n",
      ">> Epoch 83 finished \tRBM Reconstruction error 40.521431\n",
      ">> Epoch 84 finished \tRBM Reconstruction error 48.091846\n",
      ">> Epoch 85 finished \tRBM Reconstruction error 50.331036\n",
      ">> Epoch 86 finished \tRBM Reconstruction error 39.579212\n",
      ">> Epoch 87 finished \tRBM Reconstruction error 40.900753\n",
      ">> Epoch 88 finished \tRBM Reconstruction error 47.013878\n",
      ">> Epoch 89 finished \tRBM Reconstruction error 35.184589\n",
      ">> Epoch 90 finished \tRBM Reconstruction error 44.269997\n",
      ">> Epoch 91 finished \tRBM Reconstruction error 49.158096\n",
      ">> Epoch 92 finished \tRBM Reconstruction error 43.611385\n",
      ">> Epoch 93 finished \tRBM Reconstruction error 60.741196\n",
      ">> Epoch 94 finished \tRBM Reconstruction error 47.691978\n",
      ">> Epoch 95 finished \tRBM Reconstruction error 44.343307\n",
      ">> Epoch 96 finished \tRBM Reconstruction error 53.239323\n",
      ">> Epoch 97 finished \tRBM Reconstruction error 41.688107\n",
      ">> Epoch 98 finished \tRBM Reconstruction error 48.550625\n",
      ">> Epoch 99 finished \tRBM Reconstruction error 47.987186\n",
      ">> Epoch 100 finished \tRBM Reconstruction error 34.288380\n",
      ">> Epoch 101 finished \tRBM Reconstruction error 32.221981\n",
      ">> Epoch 102 finished \tRBM Reconstruction error 45.224442\n",
      ">> Epoch 103 finished \tRBM Reconstruction error 43.469223\n",
      ">> Epoch 104 finished \tRBM Reconstruction error 48.746647\n",
      ">> Epoch 105 finished \tRBM Reconstruction error 61.833721\n",
      ">> Epoch 106 finished \tRBM Reconstruction error 41.287601\n",
      ">> Epoch 107 finished \tRBM Reconstruction error 59.132675\n",
      ">> Epoch 108 finished \tRBM Reconstruction error 43.732594\n",
      ">> Epoch 109 finished \tRBM Reconstruction error 49.017735\n",
      ">> Epoch 110 finished \tRBM Reconstruction error 43.722050\n",
      ">> Epoch 111 finished \tRBM Reconstruction error 39.138744\n",
      ">> Epoch 112 finished \tRBM Reconstruction error 45.585972\n",
      ">> Epoch 113 finished \tRBM Reconstruction error 48.090126\n",
      ">> Epoch 114 finished \tRBM Reconstruction error 57.641083\n",
      ">> Epoch 115 finished \tRBM Reconstruction error 44.586620\n",
      ">> Epoch 116 finished \tRBM Reconstruction error 59.575802\n",
      ">> Epoch 117 finished \tRBM Reconstruction error 42.615536\n",
      ">> Epoch 118 finished \tRBM Reconstruction error 51.957764\n",
      ">> Epoch 119 finished \tRBM Reconstruction error 44.619247\n",
      ">> Epoch 120 finished \tRBM Reconstruction error 51.005386\n",
      ">> Epoch 121 finished \tRBM Reconstruction error 42.891911\n",
      ">> Epoch 122 finished \tRBM Reconstruction error 42.635792\n",
      ">> Epoch 123 finished \tRBM Reconstruction error 57.663155\n",
      ">> Epoch 124 finished \tRBM Reconstruction error 44.235672\n",
      ">> Epoch 125 finished \tRBM Reconstruction error 48.467464\n",
      ">> Epoch 126 finished \tRBM Reconstruction error 57.959911\n",
      ">> Epoch 127 finished \tRBM Reconstruction error 47.443966\n",
      ">> Epoch 128 finished \tRBM Reconstruction error 43.067379\n",
      ">> Epoch 129 finished \tRBM Reconstruction error 47.289131\n",
      ">> Epoch 130 finished \tRBM Reconstruction error 47.855812\n",
      ">> Epoch 131 finished \tRBM Reconstruction error 49.009102\n",
      ">> Epoch 132 finished \tRBM Reconstruction error 58.451550\n",
      ">> Epoch 133 finished \tRBM Reconstruction error 52.637852\n",
      ">> Epoch 134 finished \tRBM Reconstruction error 55.585186\n",
      ">> Epoch 135 finished \tRBM Reconstruction error 59.746181\n",
      ">> Epoch 136 finished \tRBM Reconstruction error 45.799423\n",
      ">> Epoch 137 finished \tRBM Reconstruction error 58.681973\n",
      ">> Epoch 138 finished \tRBM Reconstruction error 49.083973\n",
      ">> Epoch 139 finished \tRBM Reconstruction error 44.902637\n",
      ">> Epoch 140 finished \tRBM Reconstruction error 45.707970\n",
      ">> Epoch 141 finished \tRBM Reconstruction error 48.067883\n",
      ">> Epoch 142 finished \tRBM Reconstruction error 54.794247\n",
      ">> Epoch 143 finished \tRBM Reconstruction error 50.475479\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 144 finished \tRBM Reconstruction error 61.509777\n",
      ">> Epoch 145 finished \tRBM Reconstruction error 45.346893\n",
      ">> Epoch 146 finished \tRBM Reconstruction error 52.008034\n",
      ">> Epoch 147 finished \tRBM Reconstruction error 53.157570\n",
      ">> Epoch 148 finished \tRBM Reconstruction error 49.277363\n",
      ">> Epoch 149 finished \tRBM Reconstruction error 65.432045\n",
      ">> Epoch 150 finished \tRBM Reconstruction error 44.341007\n",
      ">> Epoch 151 finished \tRBM Reconstruction error 53.542480\n",
      ">> Epoch 152 finished \tRBM Reconstruction error 50.519344\n",
      ">> Epoch 153 finished \tRBM Reconstruction error 54.931206\n",
      ">> Epoch 154 finished \tRBM Reconstruction error 51.126076\n",
      ">> Epoch 155 finished \tRBM Reconstruction error 56.965164\n",
      ">> Epoch 156 finished \tRBM Reconstruction error 51.243221\n",
      ">> Epoch 157 finished \tRBM Reconstruction error 51.636642\n",
      ">> Epoch 158 finished \tRBM Reconstruction error 46.180283\n",
      ">> Epoch 159 finished \tRBM Reconstruction error 45.546783\n",
      ">> Epoch 160 finished \tRBM Reconstruction error 55.428909\n",
      ">> Epoch 161 finished \tRBM Reconstruction error 45.517998\n",
      ">> Epoch 162 finished \tRBM Reconstruction error 67.060349\n",
      ">> Epoch 163 finished \tRBM Reconstruction error 52.193935\n",
      ">> Epoch 164 finished \tRBM Reconstruction error 51.672436\n",
      ">> Epoch 165 finished \tRBM Reconstruction error 48.624073\n",
      ">> Epoch 166 finished \tRBM Reconstruction error 56.212635\n",
      ">> Epoch 167 finished \tRBM Reconstruction error 46.617111\n",
      ">> Epoch 168 finished \tRBM Reconstruction error 56.426338\n",
      ">> Epoch 169 finished \tRBM Reconstruction error 50.296825\n",
      ">> Epoch 170 finished \tRBM Reconstruction error 60.637421\n",
      ">> Epoch 171 finished \tRBM Reconstruction error 49.461910\n",
      ">> Epoch 172 finished \tRBM Reconstruction error 55.824753\n",
      ">> Epoch 173 finished \tRBM Reconstruction error 45.668785\n",
      ">> Epoch 174 finished \tRBM Reconstruction error 54.185070\n",
      ">> Epoch 175 finished \tRBM Reconstruction error 56.532116\n",
      ">> Epoch 176 finished \tRBM Reconstruction error 55.155918\n",
      ">> Epoch 177 finished \tRBM Reconstruction error 64.086472\n",
      ">> Epoch 178 finished \tRBM Reconstruction error 43.452862\n",
      ">> Epoch 179 finished \tRBM Reconstruction error 49.613544\n",
      ">> Epoch 180 finished \tRBM Reconstruction error 65.419159\n",
      ">> Epoch 181 finished \tRBM Reconstruction error 49.341522\n",
      ">> Epoch 182 finished \tRBM Reconstruction error 45.563507\n",
      ">> Epoch 183 finished \tRBM Reconstruction error 56.294167\n",
      ">> Epoch 184 finished \tRBM Reconstruction error 56.491844\n",
      ">> Epoch 185 finished \tRBM Reconstruction error 56.194443\n",
      ">> Epoch 186 finished \tRBM Reconstruction error 55.997520\n",
      ">> Epoch 187 finished \tRBM Reconstruction error 55.332954\n",
      ">> Epoch 188 finished \tRBM Reconstruction error 53.397930\n",
      ">> Epoch 189 finished \tRBM Reconstruction error 54.762638\n",
      ">> Epoch 190 finished \tRBM Reconstruction error 48.943295\n",
      ">> Epoch 191 finished \tRBM Reconstruction error 57.285973\n",
      ">> Epoch 192 finished \tRBM Reconstruction error 47.129757\n",
      ">> Epoch 193 finished \tRBM Reconstruction error 54.448589\n",
      ">> Epoch 194 finished \tRBM Reconstruction error 58.092270\n",
      ">> Epoch 195 finished \tRBM Reconstruction error 61.148083\n",
      ">> Epoch 196 finished \tRBM Reconstruction error 51.700756\n",
      ">> Epoch 197 finished \tRBM Reconstruction error 46.888985\n",
      ">> Epoch 198 finished \tRBM Reconstruction error 56.939980\n",
      ">> Epoch 199 finished \tRBM Reconstruction error 58.377651\n",
      ">> Epoch 200 finished \tRBM Reconstruction error 53.385487\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 200.181366\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 242.774338\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 275.711334\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 201.150269\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 186.731674\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 168.622955\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 175.088379\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 223.309570\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 197.670670\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 160.714081\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 162.731384\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 177.109116\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 280.858490\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 185.056870\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 149.487122\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 131.544662\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 183.991882\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 255.154068\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 177.688324\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 174.188171\n",
      ">> Epoch 21 finished \tRBM Reconstruction error 118.692497\n",
      ">> Epoch 22 finished \tRBM Reconstruction error 189.338089\n",
      ">> Epoch 23 finished \tRBM Reconstruction error 149.635941\n",
      ">> Epoch 24 finished \tRBM Reconstruction error 147.106979\n",
      ">> Epoch 25 finished \tRBM Reconstruction error 185.047897\n",
      ">> Epoch 26 finished \tRBM Reconstruction error 166.772186\n",
      ">> Epoch 27 finished \tRBM Reconstruction error 210.826218\n",
      ">> Epoch 28 finished \tRBM Reconstruction error 226.418152\n",
      ">> Epoch 29 finished \tRBM Reconstruction error 235.436722\n",
      ">> Epoch 30 finished \tRBM Reconstruction error 189.990982\n",
      ">> Epoch 31 finished \tRBM Reconstruction error 193.610413\n",
      ">> Epoch 32 finished \tRBM Reconstruction error 213.243942\n",
      ">> Epoch 33 finished \tRBM Reconstruction error 169.294922\n",
      ">> Epoch 34 finished \tRBM Reconstruction error 213.200058\n",
      ">> Epoch 35 finished \tRBM Reconstruction error 192.671524\n",
      ">> Epoch 36 finished \tRBM Reconstruction error 191.311646\n",
      ">> Epoch 37 finished \tRBM Reconstruction error 180.954803\n",
      ">> Epoch 38 finished \tRBM Reconstruction error 203.571152\n",
      ">> Epoch 39 finished \tRBM Reconstruction error 235.308517\n",
      ">> Epoch 40 finished \tRBM Reconstruction error 205.586380\n",
      ">> Epoch 41 finished \tRBM Reconstruction error 201.645111\n",
      ">> Epoch 42 finished \tRBM Reconstruction error 224.614685\n",
      ">> Epoch 43 finished \tRBM Reconstruction error 217.219162\n",
      ">> Epoch 44 finished \tRBM Reconstruction error 188.733200\n",
      ">> Epoch 45 finished \tRBM Reconstruction error 232.208588\n",
      ">> Epoch 46 finished \tRBM Reconstruction error 189.405548\n",
      ">> Epoch 47 finished \tRBM Reconstruction error 195.155945\n",
      ">> Epoch 48 finished \tRBM Reconstruction error 186.081284\n",
      ">> Epoch 49 finished \tRBM Reconstruction error 224.220718\n",
      ">> Epoch 50 finished \tRBM Reconstruction error 200.341934\n",
      ">> Epoch 51 finished \tRBM Reconstruction error 214.661972\n",
      ">> Epoch 52 finished \tRBM Reconstruction error 195.970551\n",
      ">> Epoch 53 finished \tRBM Reconstruction error 207.705765\n",
      ">> Epoch 54 finished \tRBM Reconstruction error 208.093246\n",
      ">> Epoch 55 finished \tRBM Reconstruction error 208.947311\n",
      ">> Epoch 56 finished \tRBM Reconstruction error 196.998749\n",
      ">> Epoch 57 finished \tRBM Reconstruction error 220.380508\n",
      ">> Epoch 58 finished \tRBM Reconstruction error 217.909576\n",
      ">> Epoch 59 finished \tRBM Reconstruction error 202.082886\n",
      ">> Epoch 60 finished \tRBM Reconstruction error 196.337616\n",
      ">> Epoch 61 finished \tRBM Reconstruction error 221.512329\n",
      ">> Epoch 62 finished \tRBM Reconstruction error 188.130463\n",
      ">> Epoch 63 finished \tRBM Reconstruction error 198.272324\n",
      ">> Epoch 64 finished \tRBM Reconstruction error 195.477737\n",
      ">> Epoch 65 finished \tRBM Reconstruction error 240.258347\n",
      ">> Epoch 66 finished \tRBM Reconstruction error 198.034927\n",
      ">> Epoch 67 finished \tRBM Reconstruction error 196.602890\n",
      ">> Epoch 68 finished \tRBM Reconstruction error 195.838470\n",
      ">> Epoch 69 finished \tRBM Reconstruction error 224.448196\n",
      ">> Epoch 70 finished \tRBM Reconstruction error 184.104370\n",
      ">> Epoch 71 finished \tRBM Reconstruction error 218.700699\n",
      ">> Epoch 72 finished \tRBM Reconstruction error 206.159103\n",
      ">> Epoch 73 finished \tRBM Reconstruction error 176.194336\n",
      ">> Epoch 74 finished \tRBM Reconstruction error 237.567230\n",
      ">> Epoch 75 finished \tRBM Reconstruction error 213.303970\n",
      ">> Epoch 76 finished \tRBM Reconstruction error 225.758438\n",
      ">> Epoch 77 finished \tRBM Reconstruction error 155.049759\n",
      ">> Epoch 78 finished \tRBM Reconstruction error 212.505310\n",
      ">> Epoch 79 finished \tRBM Reconstruction error 241.326096\n",
      ">> Epoch 80 finished \tRBM Reconstruction error 206.550858\n",
      ">> Epoch 81 finished \tRBM Reconstruction error 194.035782\n",
      ">> Epoch 82 finished \tRBM Reconstruction error 167.370239\n",
      ">> Epoch 83 finished \tRBM Reconstruction error 205.280258\n",
      ">> Epoch 84 finished \tRBM Reconstruction error 206.364807\n",
      ">> Epoch 85 finished \tRBM Reconstruction error 196.309525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 86 finished \tRBM Reconstruction error 196.848480\n",
      ">> Epoch 87 finished \tRBM Reconstruction error 197.108459\n",
      ">> Epoch 88 finished \tRBM Reconstruction error 214.650269\n",
      ">> Epoch 89 finished \tRBM Reconstruction error 250.796478\n",
      ">> Epoch 90 finished \tRBM Reconstruction error 145.884583\n",
      ">> Epoch 91 finished \tRBM Reconstruction error 247.266068\n",
      ">> Epoch 92 finished \tRBM Reconstruction error 202.665390\n",
      ">> Epoch 93 finished \tRBM Reconstruction error 218.380386\n",
      ">> Epoch 94 finished \tRBM Reconstruction error 220.665466\n",
      ">> Epoch 95 finished \tRBM Reconstruction error 221.336609\n",
      ">> Epoch 96 finished \tRBM Reconstruction error 267.542877\n",
      ">> Epoch 97 finished \tRBM Reconstruction error 224.327896\n",
      ">> Epoch 98 finished \tRBM Reconstruction error 198.308685\n",
      ">> Epoch 99 finished \tRBM Reconstruction error 233.491577\n",
      ">> Epoch 100 finished \tRBM Reconstruction error 204.297836\n",
      ">> Epoch 101 finished \tRBM Reconstruction error 174.189606\n",
      ">> Epoch 102 finished \tRBM Reconstruction error 223.431763\n",
      ">> Epoch 103 finished \tRBM Reconstruction error 252.579056\n",
      ">> Epoch 104 finished \tRBM Reconstruction error 213.763168\n",
      ">> Epoch 105 finished \tRBM Reconstruction error 228.726059\n",
      ">> Epoch 106 finished \tRBM Reconstruction error 225.025574\n",
      ">> Epoch 107 finished \tRBM Reconstruction error 211.435074\n",
      ">> Epoch 108 finished \tRBM Reconstruction error 226.241760\n",
      ">> Epoch 109 finished \tRBM Reconstruction error 193.766479\n",
      ">> Epoch 110 finished \tRBM Reconstruction error 213.593704\n",
      ">> Epoch 111 finished \tRBM Reconstruction error 216.383499\n",
      ">> Epoch 112 finished \tRBM Reconstruction error 207.028732\n",
      ">> Epoch 113 finished \tRBM Reconstruction error 214.052521\n",
      ">> Epoch 114 finished \tRBM Reconstruction error 221.775192\n",
      ">> Epoch 115 finished \tRBM Reconstruction error 231.043106\n",
      ">> Epoch 116 finished \tRBM Reconstruction error 219.929962\n",
      ">> Epoch 117 finished \tRBM Reconstruction error 204.093277\n",
      ">> Epoch 118 finished \tRBM Reconstruction error 214.711868\n",
      ">> Epoch 119 finished \tRBM Reconstruction error 219.799301\n",
      ">> Epoch 120 finished \tRBM Reconstruction error 243.114960\n",
      ">> Epoch 121 finished \tRBM Reconstruction error 206.204163\n",
      ">> Epoch 122 finished \tRBM Reconstruction error 229.379578\n",
      ">> Epoch 123 finished \tRBM Reconstruction error 217.622833\n",
      ">> Epoch 124 finished \tRBM Reconstruction error 197.841522\n",
      ">> Epoch 125 finished \tRBM Reconstruction error 226.795425\n",
      ">> Epoch 126 finished \tRBM Reconstruction error 223.074844\n",
      ">> Epoch 127 finished \tRBM Reconstruction error 248.984253\n",
      ">> Epoch 128 finished \tRBM Reconstruction error 225.798874\n",
      ">> Epoch 129 finished \tRBM Reconstruction error 227.191635\n",
      ">> Epoch 130 finished \tRBM Reconstruction error 244.432755\n",
      ">> Epoch 131 finished \tRBM Reconstruction error 231.049026\n",
      ">> Epoch 132 finished \tRBM Reconstruction error 239.001099\n",
      ">> Epoch 133 finished \tRBM Reconstruction error 217.556198\n",
      ">> Epoch 134 finished \tRBM Reconstruction error 239.072388\n",
      ">> Epoch 135 finished \tRBM Reconstruction error 214.383209\n",
      ">> Epoch 136 finished \tRBM Reconstruction error 213.258438\n",
      ">> Epoch 137 finished \tRBM Reconstruction error 221.597946\n",
      ">> Epoch 138 finished \tRBM Reconstruction error 213.698151\n",
      ">> Epoch 139 finished \tRBM Reconstruction error 219.739609\n",
      ">> Epoch 140 finished \tRBM Reconstruction error 219.074921\n",
      ">> Epoch 141 finished \tRBM Reconstruction error 207.256119\n",
      ">> Epoch 142 finished \tRBM Reconstruction error 206.058792\n",
      ">> Epoch 143 finished \tRBM Reconstruction error 192.350708\n",
      ">> Epoch 144 finished \tRBM Reconstruction error 240.975357\n",
      ">> Epoch 145 finished \tRBM Reconstruction error 210.719955\n",
      ">> Epoch 146 finished \tRBM Reconstruction error 203.691910\n",
      ">> Epoch 147 finished \tRBM Reconstruction error 209.966293\n",
      ">> Epoch 148 finished \tRBM Reconstruction error 204.440033\n",
      ">> Epoch 149 finished \tRBM Reconstruction error 226.639145\n",
      ">> Epoch 150 finished \tRBM Reconstruction error 240.488159\n",
      ">> Epoch 151 finished \tRBM Reconstruction error 230.223984\n",
      ">> Epoch 152 finished \tRBM Reconstruction error 235.704926\n",
      ">> Epoch 153 finished \tRBM Reconstruction error 233.968353\n",
      ">> Epoch 154 finished \tRBM Reconstruction error 220.618591\n",
      ">> Epoch 155 finished \tRBM Reconstruction error 205.623123\n",
      ">> Epoch 156 finished \tRBM Reconstruction error 264.416382\n",
      ">> Epoch 157 finished \tRBM Reconstruction error 205.860489\n",
      ">> Epoch 158 finished \tRBM Reconstruction error 220.431763\n",
      ">> Epoch 159 finished \tRBM Reconstruction error 215.606873\n",
      ">> Epoch 160 finished \tRBM Reconstruction error 208.144028\n",
      ">> Epoch 161 finished \tRBM Reconstruction error 223.293655\n",
      ">> Epoch 162 finished \tRBM Reconstruction error 218.477493\n",
      ">> Epoch 163 finished \tRBM Reconstruction error 229.473755\n",
      ">> Epoch 164 finished \tRBM Reconstruction error 240.140137\n",
      ">> Epoch 165 finished \tRBM Reconstruction error 207.617264\n",
      ">> Epoch 166 finished \tRBM Reconstruction error 218.916916\n",
      ">> Epoch 167 finished \tRBM Reconstruction error 229.642319\n",
      ">> Epoch 168 finished \tRBM Reconstruction error 227.399216\n",
      ">> Epoch 169 finished \tRBM Reconstruction error 199.985855\n",
      ">> Epoch 170 finished \tRBM Reconstruction error 233.785904\n",
      ">> Epoch 171 finished \tRBM Reconstruction error 220.627808\n",
      ">> Epoch 172 finished \tRBM Reconstruction error 222.667191\n",
      ">> Epoch 173 finished \tRBM Reconstruction error 221.127594\n",
      ">> Epoch 174 finished \tRBM Reconstruction error 212.889420\n",
      ">> Epoch 175 finished \tRBM Reconstruction error 254.490738\n",
      ">> Epoch 176 finished \tRBM Reconstruction error 234.652771\n",
      ">> Epoch 177 finished \tRBM Reconstruction error 219.652542\n",
      ">> Epoch 178 finished \tRBM Reconstruction error 242.585602\n",
      ">> Epoch 179 finished \tRBM Reconstruction error 255.395081\n",
      ">> Epoch 180 finished \tRBM Reconstruction error 229.205032\n",
      ">> Epoch 181 finished \tRBM Reconstruction error 252.319427\n",
      ">> Epoch 182 finished \tRBM Reconstruction error 243.640900\n",
      ">> Epoch 183 finished \tRBM Reconstruction error 206.429260\n",
      ">> Epoch 184 finished \tRBM Reconstruction error 222.625778\n",
      ">> Epoch 185 finished \tRBM Reconstruction error 246.307343\n",
      ">> Epoch 186 finished \tRBM Reconstruction error 220.067245\n",
      ">> Epoch 187 finished \tRBM Reconstruction error 226.229187\n",
      ">> Epoch 188 finished \tRBM Reconstruction error 208.193420\n",
      ">> Epoch 189 finished \tRBM Reconstruction error 231.715118\n",
      ">> Epoch 190 finished \tRBM Reconstruction error 229.132385\n",
      ">> Epoch 191 finished \tRBM Reconstruction error 227.938568\n",
      ">> Epoch 192 finished \tRBM Reconstruction error 227.725891\n",
      ">> Epoch 193 finished \tRBM Reconstruction error 220.878632\n",
      ">> Epoch 194 finished \tRBM Reconstruction error 222.349106\n",
      ">> Epoch 195 finished \tRBM Reconstruction error 241.142532\n",
      ">> Epoch 196 finished \tRBM Reconstruction error 222.700241\n",
      ">> Epoch 197 finished \tRBM Reconstruction error 211.630432\n",
      ">> Epoch 198 finished \tRBM Reconstruction error 229.865845\n",
      ">> Epoch 199 finished \tRBM Reconstruction error 214.611023\n",
      ">> Epoch 200 finished \tRBM Reconstruction error 252.983765\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.580909\n",
      ">> Epoch 1 finished \tANN training loss 0.348968\n",
      ">> Epoch 2 finished \tANN training loss 0.293230\n",
      ">> Epoch 3 finished \tANN training loss 0.229737\n",
      ">> Epoch 4 finished \tANN training loss 0.211241\n",
      ">> Epoch 5 finished \tANN training loss 0.163032\n",
      ">> Epoch 6 finished \tANN training loss 0.135197\n",
      ">> Epoch 7 finished \tANN training loss 0.107732\n",
      ">> Epoch 8 finished \tANN training loss 0.097198\n",
      ">> Epoch 9 finished \tANN training loss 0.084452\n",
      ">> Epoch 10 finished \tANN training loss 0.079183\n",
      ">> Epoch 11 finished \tANN training loss 0.062415\n",
      ">> Epoch 12 finished \tANN training loss 0.056549\n",
      ">> Epoch 13 finished \tANN training loss 0.054099\n",
      ">> Epoch 14 finished \tANN training loss 0.042692\n",
      ">> Epoch 15 finished \tANN training loss 0.037218\n",
      ">> Epoch 16 finished \tANN training loss 0.034735\n",
      ">> Epoch 17 finished \tANN training loss 0.030426\n",
      ">> Epoch 18 finished \tANN training loss 0.029074\n",
      ">> Epoch 19 finished \tANN training loss 0.025898\n",
      ">> Epoch 20 finished \tANN training loss 0.023864\n",
      ">> Epoch 21 finished \tANN training loss 0.022095\n",
      ">> Epoch 22 finished \tANN training loss 0.017579\n",
      ">> Epoch 23 finished \tANN training loss 0.015936\n",
      ">> Epoch 24 finished \tANN training loss 0.017030\n",
      ">> Epoch 25 finished \tANN training loss 0.014391\n",
      ">> Epoch 26 finished \tANN training loss 0.013288\n",
      ">> Epoch 27 finished \tANN training loss 0.012022\n",
      ">> Epoch 28 finished \tANN training loss 0.012725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 29 finished \tANN training loss 0.011279\n",
      ">> Epoch 30 finished \tANN training loss 0.010585\n",
      ">> Epoch 31 finished \tANN training loss 0.008383\n",
      ">> Epoch 32 finished \tANN training loss 0.008156\n",
      ">> Epoch 33 finished \tANN training loss 0.007844\n",
      ">> Epoch 34 finished \tANN training loss 0.007210\n",
      ">> Epoch 35 finished \tANN training loss 0.007595\n",
      ">> Epoch 36 finished \tANN training loss 0.006130\n",
      ">> Epoch 37 finished \tANN training loss 0.006037\n",
      ">> Epoch 38 finished \tANN training loss 0.005578\n",
      ">> Epoch 39 finished \tANN training loss 0.004982\n",
      ">> Epoch 40 finished \tANN training loss 0.004639\n",
      ">> Epoch 41 finished \tANN training loss 0.004240\n",
      ">> Epoch 42 finished \tANN training loss 0.004408\n",
      ">> Epoch 43 finished \tANN training loss 0.004036\n",
      ">> Epoch 44 finished \tANN training loss 0.003707\n",
      ">> Epoch 45 finished \tANN training loss 0.004197\n",
      ">> Epoch 46 finished \tANN training loss 0.003487\n",
      ">> Epoch 47 finished \tANN training loss 0.003265\n",
      ">> Epoch 48 finished \tANN training loss 0.002998\n",
      ">> Epoch 49 finished \tANN training loss 0.003865\n",
      ">> Epoch 50 finished \tANN training loss 0.003190\n",
      ">> Epoch 51 finished \tANN training loss 0.002724\n",
      ">> Epoch 52 finished \tANN training loss 0.002447\n",
      ">> Epoch 53 finished \tANN training loss 0.002349\n",
      ">> Epoch 54 finished \tANN training loss 0.002605\n",
      ">> Epoch 55 finished \tANN training loss 0.002337\n",
      ">> Epoch 56 finished \tANN training loss 0.002328\n",
      ">> Epoch 57 finished \tANN training loss 0.002467\n",
      ">> Epoch 58 finished \tANN training loss 0.001995\n",
      ">> Epoch 59 finished \tANN training loss 0.001728\n",
      ">> Epoch 60 finished \tANN training loss 0.002199\n",
      ">> Epoch 61 finished \tANN training loss 0.001832\n",
      ">> Epoch 62 finished \tANN training loss 0.002042\n",
      ">> Epoch 63 finished \tANN training loss 0.001837\n",
      ">> Epoch 64 finished \tANN training loss 0.001811\n",
      ">> Epoch 65 finished \tANN training loss 0.001750\n",
      ">> Epoch 66 finished \tANN training loss 0.001519\n",
      ">> Epoch 67 finished \tANN training loss 0.001650\n",
      ">> Epoch 68 finished \tANN training loss 0.001392\n",
      ">> Epoch 69 finished \tANN training loss 0.001390\n",
      ">> Epoch 70 finished \tANN training loss 0.001337\n",
      ">> Epoch 71 finished \tANN training loss 0.001254\n",
      ">> Epoch 72 finished \tANN training loss 0.001549\n",
      ">> Epoch 73 finished \tANN training loss 0.001348\n",
      ">> Epoch 74 finished \tANN training loss 0.001286\n",
      ">> Epoch 75 finished \tANN training loss 0.001154\n",
      ">> Epoch 76 finished \tANN training loss 0.001271\n",
      ">> Epoch 77 finished \tANN training loss 0.001495\n",
      ">> Epoch 78 finished \tANN training loss 0.001031\n",
      ">> Epoch 79 finished \tANN training loss 0.001087\n",
      ">> Epoch 80 finished \tANN training loss 0.001100\n",
      ">> Epoch 81 finished \tANN training loss 0.000989\n",
      ">> Epoch 82 finished \tANN training loss 0.000921\n",
      ">> Epoch 83 finished \tANN training loss 0.000923\n",
      ">> Epoch 84 finished \tANN training loss 0.001123\n",
      ">> Epoch 85 finished \tANN training loss 0.001008\n",
      ">> Epoch 86 finished \tANN training loss 0.001041\n",
      ">> Epoch 87 finished \tANN training loss 0.000961\n",
      ">> Epoch 88 finished \tANN training loss 0.001001\n",
      ">> Epoch 89 finished \tANN training loss 0.000905\n",
      ">> Epoch 90 finished \tANN training loss 0.000852\n",
      ">> Epoch 91 finished \tANN training loss 0.000830\n",
      ">> Epoch 92 finished \tANN training loss 0.000839\n",
      ">> Epoch 93 finished \tANN training loss 0.000802\n",
      ">> Epoch 94 finished \tANN training loss 0.000783\n",
      ">> Epoch 95 finished \tANN training loss 0.000803\n",
      ">> Epoch 96 finished \tANN training loss 0.000765\n",
      ">> Epoch 97 finished \tANN training loss 0.000748\n",
      ">> Epoch 98 finished \tANN training loss 0.000769\n",
      ">> Epoch 99 finished \tANN training loss 0.000660\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.890000\n"
     ]
    }
   ],
   "source": [
    "epoch_rbm_acc[9] = deep_belief_net(n_epochs_rbm=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.89\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(epoch_rbm_acc[9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.88, 0.88, 0.91000000000000003, 0.91000000000000003, 0.90500000000000003, 0.92000000000000004, 0.93000000000000005, 0.90000000000000002, 0.90000000000000002, 0.89000000000000001]\n",
      "Most accurate n_epochs_rbm setting is Setting 7\n"
     ]
    }
   ],
   "source": [
    "# Collated results\n",
    "print(epoch_rbm_acc)\n",
    "print('Most accurate n_epochs_rbm setting is Setting ' + str(epoch_rbm_acc.index(max(epoch_rbm_acc)) + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEeCAYAAABlggnIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGAqSpI6hIEnqGAqSpM7/B4Zhm6J/wwoYAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1cd03eec5f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create bar graph\n",
    "plt.clf()\n",
    "objects = ('1', '2', '5', '10', '20', '30', '40', '50', '100', '200')\n",
    "y_pos = np.arange(len(objects))\n",
    "performance = [0.88, 0.88, 0.91000000000000003, 0.91000000000000003, 0.90500000000000003, 0.92000000000000004, 0.93000000000000005, 0.90000000000000002, 0.90000000000000002, 0.89000000000000001]\n",
    "\n",
    "plt.bar(y_pos, performance, align='center', alpha=0.8)\n",
    "plt.xticks(y_pos, objects, rotation=30)\n",
    "plt.xlabel('Settings')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('n_epochs_rbm Settings')\n",
    " \n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different Fine-tuning / Backprop Lengths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store accuracies\n",
    "backprop_acc = [0, 0, 0, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setting 1**\n",
    "\n",
    "    n_iter_backprop=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 55.817745\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 69.508820\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 48.128082\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 46.322353\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 39.850433\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 27.097710\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 33.053356\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 35.346004\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 35.167938\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 33.056343\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 145.429611\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 134.780823\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 96.752975\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 97.752327\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 124.652870\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 62.641727\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 132.078110\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 139.843246\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 149.523788\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 71.515213\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.670907\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.785000\n"
     ]
    }
   ],
   "source": [
    "backprop_acc[0] = deep_belief_net(n_iter_backprop=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.785\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(backprop_acc[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setting 2**\n",
    "\n",
    "    n_iter_backprop=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 56.174522\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 37.896420\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 32.486263\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 55.078617\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 31.739643\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 52.946396\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 27.569790\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 30.449902\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 37.940350\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 36.021034\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 165.765762\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 148.581039\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 89.941345\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 126.390038\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 106.427254\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 142.782364\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 102.480690\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 145.183884\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 92.604729\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 141.417679\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.702097\n",
      ">> Epoch 1 finished \tANN training loss 0.569819\n",
      ">> Epoch 2 finished \tANN training loss 0.361315\n",
      ">> Epoch 3 finished \tANN training loss 0.324648\n",
      ">> Epoch 4 finished \tANN training loss 0.272950\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.845000\n"
     ]
    }
   ],
   "source": [
    "backprop_acc[1] = deep_belief_net(n_iter_backprop=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.845\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(backprop_acc[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setting 3**\n",
    "\n",
    "    n_iter_backprop=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 65.974220\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 53.847179\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 51.343510\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 65.502045\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 50.545322\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 37.524105\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 39.445473\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 43.069767\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 35.095516\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 60.808556\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 86.329010\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 210.858826\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 214.150864\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 200.452820\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 238.501770\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 236.603210\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 200.314056\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 373.685852\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 199.616257\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 157.811234\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.648157\n",
      ">> Epoch 1 finished \tANN training loss 0.503972\n",
      ">> Epoch 2 finished \tANN training loss 0.361671\n",
      ">> Epoch 3 finished \tANN training loss 0.321565\n",
      ">> Epoch 4 finished \tANN training loss 0.304847\n",
      ">> Epoch 5 finished \tANN training loss 0.242860\n",
      ">> Epoch 6 finished \tANN training loss 0.212873\n",
      ">> Epoch 7 finished \tANN training loss 0.192529\n",
      ">> Epoch 8 finished \tANN training loss 0.169078\n",
      ">> Epoch 9 finished \tANN training loss 0.159680\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.870000\n"
     ]
    }
   ],
   "source": [
    "backprop_acc[2] = deep_belief_net(n_iter_backprop=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.87\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(backprop_acc[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setting 4**\n",
    "\n",
    "    n_iter_backprop=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 79.951347\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 64.819191\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 60.852627\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 63.349876\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 34.447636\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 43.758175\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 39.598602\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 24.591450\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 48.674816\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 24.759068\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 77.523529\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 91.118660\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 84.581970\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 61.938057\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 86.447746\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 92.810883\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 89.089783\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 89.824196\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 96.242249\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 90.174896\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.670005\n",
      ">> Epoch 1 finished \tANN training loss 0.455668\n",
      ">> Epoch 2 finished \tANN training loss 0.385733\n",
      ">> Epoch 3 finished \tANN training loss 0.316249\n",
      ">> Epoch 4 finished \tANN training loss 0.298953\n",
      ">> Epoch 5 finished \tANN training loss 0.232435\n",
      ">> Epoch 6 finished \tANN training loss 0.205809\n",
      ">> Epoch 7 finished \tANN training loss 0.185147\n",
      ">> Epoch 8 finished \tANN training loss 0.172254\n",
      ">> Epoch 9 finished \tANN training loss 0.174326\n",
      ">> Epoch 10 finished \tANN training loss 0.118040\n",
      ">> Epoch 11 finished \tANN training loss 0.121350\n",
      ">> Epoch 12 finished \tANN training loss 0.097296\n",
      ">> Epoch 13 finished \tANN training loss 0.098613\n",
      ">> Epoch 14 finished \tANN training loss 0.080863\n",
      ">> Epoch 15 finished \tANN training loss 0.079201\n",
      ">> Epoch 16 finished \tANN training loss 0.065509\n",
      ">> Epoch 17 finished \tANN training loss 0.060506\n",
      ">> Epoch 18 finished \tANN training loss 0.050646\n",
      ">> Epoch 19 finished \tANN training loss 0.044502\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.890000\n"
     ]
    }
   ],
   "source": [
    "backprop_acc[3] = deep_belief_net(n_iter_backprop=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.89\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(backprop_acc[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setting 5**\n",
    "\n",
    "    n_iter_backprop=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 47.966709\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 45.884468\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 41.949032\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 51.684757\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 54.949287\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 35.029099\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 47.786827\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 47.673374\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 51.909813\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 39.520073\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 224.726135\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 144.034760\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 202.503006\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 146.634048\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 132.441116\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 153.882095\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 146.382202\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 122.807571\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 128.477036\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 153.221161\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.656392\n",
      ">> Epoch 1 finished \tANN training loss 0.511056\n",
      ">> Epoch 2 finished \tANN training loss 0.419523\n",
      ">> Epoch 3 finished \tANN training loss 0.345274\n",
      ">> Epoch 4 finished \tANN training loss 0.291805\n",
      ">> Epoch 5 finished \tANN training loss 0.234449\n",
      ">> Epoch 6 finished \tANN training loss 0.202438\n",
      ">> Epoch 7 finished \tANN training loss 0.189213\n",
      ">> Epoch 8 finished \tANN training loss 0.161982\n",
      ">> Epoch 9 finished \tANN training loss 0.142671\n",
      ">> Epoch 10 finished \tANN training loss 0.125817\n",
      ">> Epoch 11 finished \tANN training loss 0.116951\n",
      ">> Epoch 12 finished \tANN training loss 0.118129\n",
      ">> Epoch 13 finished \tANN training loss 0.094424\n",
      ">> Epoch 14 finished \tANN training loss 0.100267\n",
      ">> Epoch 15 finished \tANN training loss 0.081743\n",
      ">> Epoch 16 finished \tANN training loss 0.087958\n",
      ">> Epoch 17 finished \tANN training loss 0.059302\n",
      ">> Epoch 18 finished \tANN training loss 0.053444\n",
      ">> Epoch 19 finished \tANN training loss 0.048428\n",
      ">> Epoch 20 finished \tANN training loss 0.042279\n",
      ">> Epoch 21 finished \tANN training loss 0.040057\n",
      ">> Epoch 22 finished \tANN training loss 0.036358\n",
      ">> Epoch 23 finished \tANN training loss 0.038913\n",
      ">> Epoch 24 finished \tANN training loss 0.032932\n",
      ">> Epoch 25 finished \tANN training loss 0.029065\n",
      ">> Epoch 26 finished \tANN training loss 0.026651\n",
      ">> Epoch 27 finished \tANN training loss 0.026513\n",
      ">> Epoch 28 finished \tANN training loss 0.023277\n",
      ">> Epoch 29 finished \tANN training loss 0.020492\n",
      ">> Epoch 30 finished \tANN training loss 0.023263\n",
      ">> Epoch 31 finished \tANN training loss 0.017441\n",
      ">> Epoch 32 finished \tANN training loss 0.019259\n",
      ">> Epoch 33 finished \tANN training loss 0.014802\n",
      ">> Epoch 34 finished \tANN training loss 0.015365\n",
      ">> Epoch 35 finished \tANN training loss 0.012633\n",
      ">> Epoch 36 finished \tANN training loss 0.012282\n",
      ">> Epoch 37 finished \tANN training loss 0.013217\n",
      ">> Epoch 38 finished \tANN training loss 0.011150\n",
      ">> Epoch 39 finished \tANN training loss 0.009588\n",
      ">> Epoch 40 finished \tANN training loss 0.010154\n",
      ">> Epoch 41 finished \tANN training loss 0.008799\n",
      ">> Epoch 42 finished \tANN training loss 0.008715\n",
      ">> Epoch 43 finished \tANN training loss 0.008538\n",
      ">> Epoch 44 finished \tANN training loss 0.007858\n",
      ">> Epoch 45 finished \tANN training loss 0.007424\n",
      ">> Epoch 46 finished \tANN training loss 0.006622\n",
      ">> Epoch 47 finished \tANN training loss 0.006787\n",
      ">> Epoch 48 finished \tANN training loss 0.006269\n",
      ">> Epoch 49 finished \tANN training loss 0.006362\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.920000\n"
     ]
    }
   ],
   "source": [
    "backprop_acc[4] = deep_belief_net(n_iter_backprop=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.92\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(backprop_acc[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setting 6 (Default)**\n",
    "\n",
    "    n_iter_backprop=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "backprop_acc[5] = default_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.91\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(backprop_acc[5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setting 7**\n",
    "\n",
    "    n_iter_backprop=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 55.013329\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 48.502865\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 59.771660\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 53.611660\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 41.888935\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 33.063171\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 39.538239\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 65.689186\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 32.433292\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 32.080685\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 82.105980\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 51.835796\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 61.509434\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 72.298897\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 93.334030\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 126.274864\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 145.872345\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 99.858513\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 124.187401\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 147.684265\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.664079\n",
      ">> Epoch 1 finished \tANN training loss 0.479317\n",
      ">> Epoch 2 finished \tANN training loss 0.408934\n",
      ">> Epoch 3 finished \tANN training loss 0.317145\n",
      ">> Epoch 4 finished \tANN training loss 0.277289\n",
      ">> Epoch 5 finished \tANN training loss 0.272827\n",
      ">> Epoch 6 finished \tANN training loss 0.227836\n",
      ">> Epoch 7 finished \tANN training loss 0.199976\n",
      ">> Epoch 8 finished \tANN training loss 0.185234\n",
      ">> Epoch 9 finished \tANN training loss 0.158111\n",
      ">> Epoch 10 finished \tANN training loss 0.134525\n",
      ">> Epoch 11 finished \tANN training loss 0.122839\n",
      ">> Epoch 12 finished \tANN training loss 0.117652\n",
      ">> Epoch 13 finished \tANN training loss 0.104176\n",
      ">> Epoch 14 finished \tANN training loss 0.091423\n",
      ">> Epoch 15 finished \tANN training loss 0.095539\n",
      ">> Epoch 16 finished \tANN training loss 0.072739\n",
      ">> Epoch 17 finished \tANN training loss 0.066299\n",
      ">> Epoch 18 finished \tANN training loss 0.072635\n",
      ">> Epoch 19 finished \tANN training loss 0.058822\n",
      ">> Epoch 20 finished \tANN training loss 0.049980\n",
      ">> Epoch 21 finished \tANN training loss 0.042655\n",
      ">> Epoch 22 finished \tANN training loss 0.043048\n",
      ">> Epoch 23 finished \tANN training loss 0.040025\n",
      ">> Epoch 24 finished \tANN training loss 0.037563\n",
      ">> Epoch 25 finished \tANN training loss 0.032806\n",
      ">> Epoch 26 finished \tANN training loss 0.028028\n",
      ">> Epoch 27 finished \tANN training loss 0.025567\n",
      ">> Epoch 28 finished \tANN training loss 0.023030\n",
      ">> Epoch 29 finished \tANN training loss 0.022123\n",
      ">> Epoch 30 finished \tANN training loss 0.020134\n",
      ">> Epoch 31 finished \tANN training loss 0.019087\n",
      ">> Epoch 32 finished \tANN training loss 0.020946\n",
      ">> Epoch 33 finished \tANN training loss 0.016696\n",
      ">> Epoch 34 finished \tANN training loss 0.017109\n",
      ">> Epoch 35 finished \tANN training loss 0.014412\n",
      ">> Epoch 36 finished \tANN training loss 0.012136\n",
      ">> Epoch 37 finished \tANN training loss 0.012011\n",
      ">> Epoch 38 finished \tANN training loss 0.010189\n",
      ">> Epoch 39 finished \tANN training loss 0.010192\n",
      ">> Epoch 40 finished \tANN training loss 0.011124\n",
      ">> Epoch 41 finished \tANN training loss 0.008018\n",
      ">> Epoch 42 finished \tANN training loss 0.009929\n",
      ">> Epoch 43 finished \tANN training loss 0.008672\n",
      ">> Epoch 44 finished \tANN training loss 0.007237\n",
      ">> Epoch 45 finished \tANN training loss 0.006594\n",
      ">> Epoch 46 finished \tANN training loss 0.006595\n",
      ">> Epoch 47 finished \tANN training loss 0.005762\n",
      ">> Epoch 48 finished \tANN training loss 0.005818\n",
      ">> Epoch 49 finished \tANN training loss 0.005897\n",
      ">> Epoch 50 finished \tANN training loss 0.005935\n",
      ">> Epoch 51 finished \tANN training loss 0.005573\n",
      ">> Epoch 52 finished \tANN training loss 0.005448\n",
      ">> Epoch 53 finished \tANN training loss 0.005461\n",
      ">> Epoch 54 finished \tANN training loss 0.004520\n",
      ">> Epoch 55 finished \tANN training loss 0.004873\n",
      ">> Epoch 56 finished \tANN training loss 0.004795\n",
      ">> Epoch 57 finished \tANN training loss 0.004075\n",
      ">> Epoch 58 finished \tANN training loss 0.004024\n",
      ">> Epoch 59 finished \tANN training loss 0.003248\n",
      ">> Epoch 60 finished \tANN training loss 0.003506\n",
      ">> Epoch 61 finished \tANN training loss 0.004726\n",
      ">> Epoch 62 finished \tANN training loss 0.003061\n",
      ">> Epoch 63 finished \tANN training loss 0.002853\n",
      ">> Epoch 64 finished \tANN training loss 0.002889\n",
      ">> Epoch 65 finished \tANN training loss 0.003064\n",
      ">> Epoch 66 finished \tANN training loss 0.002212\n",
      ">> Epoch 67 finished \tANN training loss 0.002568\n",
      ">> Epoch 68 finished \tANN training loss 0.002115\n",
      ">> Epoch 69 finished \tANN training loss 0.002289\n",
      ">> Epoch 70 finished \tANN training loss 0.002122\n",
      ">> Epoch 71 finished \tANN training loss 0.002099\n",
      ">> Epoch 72 finished \tANN training loss 0.001884\n",
      ">> Epoch 73 finished \tANN training loss 0.001751\n",
      ">> Epoch 74 finished \tANN training loss 0.001685\n",
      ">> Epoch 75 finished \tANN training loss 0.001586\n",
      ">> Epoch 76 finished \tANN training loss 0.001635\n",
      ">> Epoch 77 finished \tANN training loss 0.001536\n",
      ">> Epoch 78 finished \tANN training loss 0.001605\n",
      ">> Epoch 79 finished \tANN training loss 0.001707\n",
      ">> Epoch 80 finished \tANN training loss 0.001363\n",
      ">> Epoch 81 finished \tANN training loss 0.001408\n",
      ">> Epoch 82 finished \tANN training loss 0.001385\n",
      ">> Epoch 83 finished \tANN training loss 0.001387\n",
      ">> Epoch 84 finished \tANN training loss 0.001440\n",
      ">> Epoch 85 finished \tANN training loss 0.001480\n",
      ">> Epoch 86 finished \tANN training loss 0.001481\n",
      ">> Epoch 87 finished \tANN training loss 0.001435\n",
      ">> Epoch 88 finished \tANN training loss 0.001389\n",
      ">> Epoch 89 finished \tANN training loss 0.001259\n",
      ">> Epoch 90 finished \tANN training loss 0.001328\n",
      ">> Epoch 91 finished \tANN training loss 0.001192\n",
      ">> Epoch 92 finished \tANN training loss 0.001289\n",
      ">> Epoch 93 finished \tANN training loss 0.001206\n",
      ">> Epoch 94 finished \tANN training loss 0.001175\n",
      ">> Epoch 95 finished \tANN training loss 0.001065\n",
      ">> Epoch 96 finished \tANN training loss 0.001057\n",
      ">> Epoch 97 finished \tANN training loss 0.000936\n",
      ">> Epoch 98 finished \tANN training loss 0.000948\n",
      ">> Epoch 99 finished \tANN training loss 0.000945\n",
      ">> Epoch 100 finished \tANN training loss 0.000825\n",
      ">> Epoch 101 finished \tANN training loss 0.001418\n",
      ">> Epoch 102 finished \tANN training loss 0.000817\n",
      ">> Epoch 103 finished \tANN training loss 0.000747\n",
      ">> Epoch 104 finished \tANN training loss 0.000851\n",
      ">> Epoch 105 finished \tANN training loss 0.000741\n",
      ">> Epoch 106 finished \tANN training loss 0.000805\n",
      ">> Epoch 107 finished \tANN training loss 0.000930\n",
      ">> Epoch 108 finished \tANN training loss 0.000816\n",
      ">> Epoch 109 finished \tANN training loss 0.000909\n",
      ">> Epoch 110 finished \tANN training loss 0.001038\n",
      ">> Epoch 111 finished \tANN training loss 0.000716\n",
      ">> Epoch 112 finished \tANN training loss 0.000699\n",
      ">> Epoch 113 finished \tANN training loss 0.000722\n",
      ">> Epoch 114 finished \tANN training loss 0.000797\n",
      ">> Epoch 115 finished \tANN training loss 0.000785\n",
      ">> Epoch 116 finished \tANN training loss 0.000767\n",
      ">> Epoch 117 finished \tANN training loss 0.000665\n",
      ">> Epoch 118 finished \tANN training loss 0.000673\n",
      ">> Epoch 119 finished \tANN training loss 0.000680\n",
      ">> Epoch 120 finished \tANN training loss 0.000577\n",
      ">> Epoch 121 finished \tANN training loss 0.000502\n",
      ">> Epoch 122 finished \tANN training loss 0.000570\n",
      ">> Epoch 123 finished \tANN training loss 0.000556\n",
      ">> Epoch 124 finished \tANN training loss 0.000497\n",
      ">> Epoch 125 finished \tANN training loss 0.000563\n",
      ">> Epoch 126 finished \tANN training loss 0.000554\n",
      ">> Epoch 127 finished \tANN training loss 0.000470\n",
      ">> Epoch 128 finished \tANN training loss 0.000462\n",
      ">> Epoch 129 finished \tANN training loss 0.000384\n",
      ">> Epoch 130 finished \tANN training loss 0.000390\n",
      ">> Epoch 131 finished \tANN training loss 0.000342\n",
      ">> Epoch 132 finished \tANN training loss 0.000722\n",
      ">> Epoch 133 finished \tANN training loss 0.000378\n",
      ">> Epoch 134 finished \tANN training loss 0.000430\n",
      ">> Epoch 135 finished \tANN training loss 0.000490\n",
      ">> Epoch 136 finished \tANN training loss 0.000416\n",
      ">> Epoch 137 finished \tANN training loss 0.000397\n",
      ">> Epoch 138 finished \tANN training loss 0.000467\n",
      ">> Epoch 139 finished \tANN training loss 0.000413\n",
      ">> Epoch 140 finished \tANN training loss 0.000400\n",
      ">> Epoch 141 finished \tANN training loss 0.000416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 142 finished \tANN training loss 0.000476\n",
      ">> Epoch 143 finished \tANN training loss 0.000475\n",
      ">> Epoch 144 finished \tANN training loss 0.000576\n",
      ">> Epoch 145 finished \tANN training loss 0.000468\n",
      ">> Epoch 146 finished \tANN training loss 0.000416\n",
      ">> Epoch 147 finished \tANN training loss 0.000362\n",
      ">> Epoch 148 finished \tANN training loss 0.000440\n",
      ">> Epoch 149 finished \tANN training loss 0.000433\n",
      ">> Epoch 150 finished \tANN training loss 0.000334\n",
      ">> Epoch 151 finished \tANN training loss 0.000325\n",
      ">> Epoch 152 finished \tANN training loss 0.000354\n",
      ">> Epoch 153 finished \tANN training loss 0.000328\n",
      ">> Epoch 154 finished \tANN training loss 0.000353\n",
      ">> Epoch 155 finished \tANN training loss 0.000335\n",
      ">> Epoch 156 finished \tANN training loss 0.000308\n",
      ">> Epoch 157 finished \tANN training loss 0.000300\n",
      ">> Epoch 158 finished \tANN training loss 0.000375\n",
      ">> Epoch 159 finished \tANN training loss 0.000329\n",
      ">> Epoch 160 finished \tANN training loss 0.000290\n",
      ">> Epoch 161 finished \tANN training loss 0.000288\n",
      ">> Epoch 162 finished \tANN training loss 0.000283\n",
      ">> Epoch 163 finished \tANN training loss 0.000279\n",
      ">> Epoch 164 finished \tANN training loss 0.000279\n",
      ">> Epoch 165 finished \tANN training loss 0.000261\n",
      ">> Epoch 166 finished \tANN training loss 0.000226\n",
      ">> Epoch 167 finished \tANN training loss 0.000221\n",
      ">> Epoch 168 finished \tANN training loss 0.000218\n",
      ">> Epoch 169 finished \tANN training loss 0.000215\n",
      ">> Epoch 170 finished \tANN training loss 0.000274\n",
      ">> Epoch 171 finished \tANN training loss 0.000204\n",
      ">> Epoch 172 finished \tANN training loss 0.000258\n",
      ">> Epoch 173 finished \tANN training loss 0.000208\n",
      ">> Epoch 174 finished \tANN training loss 0.000188\n",
      ">> Epoch 175 finished \tANN training loss 0.000196\n",
      ">> Epoch 176 finished \tANN training loss 0.000186\n",
      ">> Epoch 177 finished \tANN training loss 0.000187\n",
      ">> Epoch 178 finished \tANN training loss 0.000217\n",
      ">> Epoch 179 finished \tANN training loss 0.000236\n",
      ">> Epoch 180 finished \tANN training loss 0.000253\n",
      ">> Epoch 181 finished \tANN training loss 0.000222\n",
      ">> Epoch 182 finished \tANN training loss 0.000223\n",
      ">> Epoch 183 finished \tANN training loss 0.000217\n",
      ">> Epoch 184 finished \tANN training loss 0.000199\n",
      ">> Epoch 185 finished \tANN training loss 0.000241\n",
      ">> Epoch 186 finished \tANN training loss 0.000200\n",
      ">> Epoch 187 finished \tANN training loss 0.000198\n",
      ">> Epoch 188 finished \tANN training loss 0.000182\n",
      ">> Epoch 189 finished \tANN training loss 0.000161\n",
      ">> Epoch 190 finished \tANN training loss 0.000156\n",
      ">> Epoch 191 finished \tANN training loss 0.000153\n",
      ">> Epoch 192 finished \tANN training loss 0.000147\n",
      ">> Epoch 193 finished \tANN training loss 0.000182\n",
      ">> Epoch 194 finished \tANN training loss 0.000158\n",
      ">> Epoch 195 finished \tANN training loss 0.000146\n",
      ">> Epoch 196 finished \tANN training loss 0.000180\n",
      ">> Epoch 197 finished \tANN training loss 0.000152\n",
      ">> Epoch 198 finished \tANN training loss 0.000146\n",
      ">> Epoch 199 finished \tANN training loss 0.000159\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.930000\n"
     ]
    }
   ],
   "source": [
    "backprop_acc[6] = deep_belief_net(n_iter_backprop=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.93\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(backprop_acc[6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.78500000000000003, 0.84499999999999997, 0.87, 0.89000000000000001, 0.92000000000000004, 0.91000000000000003, 0.93000000000000005]\n",
      "Most accurate backprop iteration setting is Setting 7\n"
     ]
    }
   ],
   "source": [
    "# Collated results\n",
    "print(backprop_acc)\n",
    "print('Most accurate backprop iteration setting is Setting ' + str(backprop_acc.index(max(backprop_acc)) + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEeCAYAAABlggnIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ny7B0gboAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1cd03db2b00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create bar graph\n",
    "plt.clf()\n",
    "objects = ('1', '5', '10', '20', '50', '100', '200')\n",
    "y_pos = np.arange(len(objects))\n",
    "performance = [0.78500000000000003, 0.84499999999999997, 0.87, 0.89000000000000001, 0.92000000000000004, 0.91000000000000003, 0.93000000000000005]\n",
    "\n",
    "plt.bar(y_pos, performance, align='center', alpha=0.3)\n",
    "plt.xticks(y_pos, objects, rotation=30)\n",
    "plt.xlabel('Settings')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('n_iter_backprop Settings')\n",
    " \n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark 1:\n",
    "\n",
    "Take the best setting for each n-layer setup and use the best performing parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 48.168800\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 60.400215\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 63.935898\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 39.533291\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 32.563824\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 37.166458\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 26.511868\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 39.058151\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 37.210552\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 43.879555\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 55.275127\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 37.254604\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 37.860001\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 37.345356\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 34.719990\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 36.081120\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 32.172657\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 33.625538\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 34.060772\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 33.108856\n",
      ">> Epoch 21 finished \tRBM Reconstruction error 35.387943\n",
      ">> Epoch 22 finished \tRBM Reconstruction error 36.899574\n",
      ">> Epoch 23 finished \tRBM Reconstruction error 43.065262\n",
      ">> Epoch 24 finished \tRBM Reconstruction error 49.713409\n",
      ">> Epoch 25 finished \tRBM Reconstruction error 30.912466\n",
      ">> Epoch 26 finished \tRBM Reconstruction error 39.174515\n",
      ">> Epoch 27 finished \tRBM Reconstruction error 37.267910\n",
      ">> Epoch 28 finished \tRBM Reconstruction error 39.332657\n",
      ">> Epoch 29 finished \tRBM Reconstruction error 36.845791\n",
      ">> Epoch 30 finished \tRBM Reconstruction error 38.480324\n",
      ">> Epoch 31 finished \tRBM Reconstruction error 29.171761\n",
      ">> Epoch 32 finished \tRBM Reconstruction error 40.481262\n",
      ">> Epoch 33 finished \tRBM Reconstruction error 51.669361\n",
      ">> Epoch 34 finished \tRBM Reconstruction error 58.144600\n",
      ">> Epoch 35 finished \tRBM Reconstruction error 43.789810\n",
      ">> Epoch 36 finished \tRBM Reconstruction error 37.336426\n",
      ">> Epoch 37 finished \tRBM Reconstruction error 37.533741\n",
      ">> Epoch 38 finished \tRBM Reconstruction error 50.071625\n",
      ">> Epoch 39 finished \tRBM Reconstruction error 52.193783\n",
      ">> Epoch 40 finished \tRBM Reconstruction error 43.204815\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.746216\n",
      ">> Epoch 1 finished \tANN training loss 0.512527\n",
      ">> Epoch 2 finished \tANN training loss 0.407565\n",
      ">> Epoch 3 finished \tANN training loss 0.344573\n",
      ">> Epoch 4 finished \tANN training loss 0.307931\n",
      ">> Epoch 5 finished \tANN training loss 0.261170\n",
      ">> Epoch 6 finished \tANN training loss 0.232531\n",
      ">> Epoch 7 finished \tANN training loss 0.213916\n",
      ">> Epoch 8 finished \tANN training loss 0.189582\n",
      ">> Epoch 9 finished \tANN training loss 0.187780\n",
      ">> Epoch 10 finished \tANN training loss 0.159324\n",
      ">> Epoch 11 finished \tANN training loss 0.149528\n",
      ">> Epoch 12 finished \tANN training loss 0.137469\n",
      ">> Epoch 13 finished \tANN training loss 0.126154\n",
      ">> Epoch 14 finished \tANN training loss 0.109239\n",
      ">> Epoch 15 finished \tANN training loss 0.111346\n",
      ">> Epoch 16 finished \tANN training loss 0.097815\n",
      ">> Epoch 17 finished \tANN training loss 0.095576\n",
      ">> Epoch 18 finished \tANN training loss 0.080653\n",
      ">> Epoch 19 finished \tANN training loss 0.075779\n",
      ">> Epoch 20 finished \tANN training loss 0.067027\n",
      ">> Epoch 21 finished \tANN training loss 0.062688\n",
      ">> Epoch 22 finished \tANN training loss 0.060348\n",
      ">> Epoch 23 finished \tANN training loss 0.065758\n",
      ">> Epoch 24 finished \tANN training loss 0.057402\n",
      ">> Epoch 25 finished \tANN training loss 0.047824\n",
      ">> Epoch 26 finished \tANN training loss 0.043683\n",
      ">> Epoch 27 finished \tANN training loss 0.042293\n",
      ">> Epoch 28 finished \tANN training loss 0.039879\n",
      ">> Epoch 29 finished \tANN training loss 0.036098\n",
      ">> Epoch 30 finished \tANN training loss 0.034039\n",
      ">> Epoch 31 finished \tANN training loss 0.033823\n",
      ">> Epoch 32 finished \tANN training loss 0.029938\n",
      ">> Epoch 33 finished \tANN training loss 0.033278\n",
      ">> Epoch 34 finished \tANN training loss 0.027886\n",
      ">> Epoch 35 finished \tANN training loss 0.025632\n",
      ">> Epoch 36 finished \tANN training loss 0.024100\n",
      ">> Epoch 37 finished \tANN training loss 0.023274\n",
      ">> Epoch 38 finished \tANN training loss 0.022080\n",
      ">> Epoch 39 finished \tANN training loss 0.021919\n",
      ">> Epoch 40 finished \tANN training loss 0.022422\n",
      ">> Epoch 41 finished \tANN training loss 0.019018\n",
      ">> Epoch 42 finished \tANN training loss 0.018405\n",
      ">> Epoch 43 finished \tANN training loss 0.016863\n",
      ">> Epoch 44 finished \tANN training loss 0.017013\n",
      ">> Epoch 45 finished \tANN training loss 0.015901\n",
      ">> Epoch 46 finished \tANN training loss 0.015003\n",
      ">> Epoch 47 finished \tANN training loss 0.015038\n",
      ">> Epoch 48 finished \tANN training loss 0.013627\n",
      ">> Epoch 49 finished \tANN training loss 0.013029\n",
      ">> Epoch 50 finished \tANN training loss 0.012874\n",
      ">> Epoch 51 finished \tANN training loss 0.012056\n",
      ">> Epoch 52 finished \tANN training loss 0.011890\n",
      ">> Epoch 53 finished \tANN training loss 0.010734\n",
      ">> Epoch 54 finished \tANN training loss 0.011105\n",
      ">> Epoch 55 finished \tANN training loss 0.010316\n",
      ">> Epoch 56 finished \tANN training loss 0.009811\n",
      ">> Epoch 57 finished \tANN training loss 0.009628\n",
      ">> Epoch 58 finished \tANN training loss 0.009383\n",
      ">> Epoch 59 finished \tANN training loss 0.008875\n",
      ">> Epoch 60 finished \tANN training loss 0.008443\n",
      ">> Epoch 61 finished \tANN training loss 0.009048\n",
      ">> Epoch 62 finished \tANN training loss 0.008089\n",
      ">> Epoch 63 finished \tANN training loss 0.007561\n",
      ">> Epoch 64 finished \tANN training loss 0.007407\n",
      ">> Epoch 65 finished \tANN training loss 0.007198\n",
      ">> Epoch 66 finished \tANN training loss 0.007208\n",
      ">> Epoch 67 finished \tANN training loss 0.006687\n",
      ">> Epoch 68 finished \tANN training loss 0.006617\n",
      ">> Epoch 69 finished \tANN training loss 0.006374\n",
      ">> Epoch 70 finished \tANN training loss 0.006170\n",
      ">> Epoch 71 finished \tANN training loss 0.006176\n",
      ">> Epoch 72 finished \tANN training loss 0.005998\n",
      ">> Epoch 73 finished \tANN training loss 0.005869\n",
      ">> Epoch 74 finished \tANN training loss 0.005771\n",
      ">> Epoch 75 finished \tANN training loss 0.005277\n",
      ">> Epoch 76 finished \tANN training loss 0.005309\n",
      ">> Epoch 77 finished \tANN training loss 0.005109\n",
      ">> Epoch 78 finished \tANN training loss 0.005513\n",
      ">> Epoch 79 finished \tANN training loss 0.004751\n",
      ">> Epoch 80 finished \tANN training loss 0.004898\n",
      ">> Epoch 81 finished \tANN training loss 0.005011\n",
      ">> Epoch 82 finished \tANN training loss 0.004672\n",
      ">> Epoch 83 finished \tANN training loss 0.004685\n",
      ">> Epoch 84 finished \tANN training loss 0.004802\n",
      ">> Epoch 85 finished \tANN training loss 0.004400\n",
      ">> Epoch 86 finished \tANN training loss 0.004481\n",
      ">> Epoch 87 finished \tANN training loss 0.004204\n",
      ">> Epoch 88 finished \tANN training loss 0.004285\n",
      ">> Epoch 89 finished \tANN training loss 0.004075\n",
      ">> Epoch 90 finished \tANN training loss 0.004067\n",
      ">> Epoch 91 finished \tANN training loss 0.003937\n",
      ">> Epoch 92 finished \tANN training loss 0.003868\n",
      ">> Epoch 93 finished \tANN training loss 0.003730\n",
      ">> Epoch 94 finished \tANN training loss 0.003618\n",
      ">> Epoch 95 finished \tANN training loss 0.003564\n",
      ">> Epoch 96 finished \tANN training loss 0.003621\n",
      ">> Epoch 97 finished \tANN training loss 0.003333\n",
      ">> Epoch 98 finished \tANN training loss 0.003281\n",
      ">> Epoch 99 finished \tANN training loss 0.003266\n",
      ">> Epoch 100 finished \tANN training loss 0.003322\n",
      ">> Epoch 101 finished \tANN training loss 0.003187\n",
      ">> Epoch 102 finished \tANN training loss 0.003153\n",
      ">> Epoch 103 finished \tANN training loss 0.003042\n",
      ">> Epoch 104 finished \tANN training loss 0.002924\n",
      ">> Epoch 105 finished \tANN training loss 0.003084\n",
      ">> Epoch 106 finished \tANN training loss 0.002815\n",
      ">> Epoch 107 finished \tANN training loss 0.002869\n",
      ">> Epoch 108 finished \tANN training loss 0.002703\n",
      ">> Epoch 109 finished \tANN training loss 0.002769\n",
      ">> Epoch 110 finished \tANN training loss 0.002646\n",
      ">> Epoch 111 finished \tANN training loss 0.002603\n",
      ">> Epoch 112 finished \tANN training loss 0.002593\n",
      ">> Epoch 113 finished \tANN training loss 0.002497\n",
      ">> Epoch 114 finished \tANN training loss 0.002500\n",
      ">> Epoch 115 finished \tANN training loss 0.002625\n",
      ">> Epoch 116 finished \tANN training loss 0.002350\n",
      ">> Epoch 117 finished \tANN training loss 0.002422\n",
      ">> Epoch 118 finished \tANN training loss 0.002280\n",
      ">> Epoch 119 finished \tANN training loss 0.002365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 120 finished \tANN training loss 0.002161\n",
      ">> Epoch 121 finished \tANN training loss 0.002177\n",
      ">> Epoch 122 finished \tANN training loss 0.002173\n",
      ">> Epoch 123 finished \tANN training loss 0.002193\n",
      ">> Epoch 124 finished \tANN training loss 0.002075\n",
      ">> Epoch 125 finished \tANN training loss 0.002106\n",
      ">> Epoch 126 finished \tANN training loss 0.002071\n",
      ">> Epoch 127 finished \tANN training loss 0.001971\n",
      ">> Epoch 128 finished \tANN training loss 0.002027\n",
      ">> Epoch 129 finished \tANN training loss 0.001945\n",
      ">> Epoch 130 finished \tANN training loss 0.001889\n",
      ">> Epoch 131 finished \tANN training loss 0.001941\n",
      ">> Epoch 132 finished \tANN training loss 0.001796\n",
      ">> Epoch 133 finished \tANN training loss 0.001808\n",
      ">> Epoch 134 finished \tANN training loss 0.001759\n",
      ">> Epoch 135 finished \tANN training loss 0.001947\n",
      ">> Epoch 136 finished \tANN training loss 0.001682\n",
      ">> Epoch 137 finished \tANN training loss 0.001930\n",
      ">> Epoch 138 finished \tANN training loss 0.001909\n",
      ">> Epoch 139 finished \tANN training loss 0.001750\n",
      ">> Epoch 140 finished \tANN training loss 0.001728\n",
      ">> Epoch 141 finished \tANN training loss 0.001677\n",
      ">> Epoch 142 finished \tANN training loss 0.001666\n",
      ">> Epoch 143 finished \tANN training loss 0.001662\n",
      ">> Epoch 144 finished \tANN training loss 0.001768\n",
      ">> Epoch 145 finished \tANN training loss 0.001849\n",
      ">> Epoch 146 finished \tANN training loss 0.001542\n",
      ">> Epoch 147 finished \tANN training loss 0.001520\n",
      ">> Epoch 148 finished \tANN training loss 0.001529\n",
      ">> Epoch 149 finished \tANN training loss 0.001437\n",
      ">> Epoch 150 finished \tANN training loss 0.001457\n",
      ">> Epoch 151 finished \tANN training loss 0.001440\n",
      ">> Epoch 152 finished \tANN training loss 0.001432\n",
      ">> Epoch 153 finished \tANN training loss 0.001411\n",
      ">> Epoch 154 finished \tANN training loss 0.001426\n",
      ">> Epoch 155 finished \tANN training loss 0.001417\n",
      ">> Epoch 156 finished \tANN training loss 0.001393\n",
      ">> Epoch 157 finished \tANN training loss 0.001388\n",
      ">> Epoch 158 finished \tANN training loss 0.001304\n",
      ">> Epoch 159 finished \tANN training loss 0.001268\n",
      ">> Epoch 160 finished \tANN training loss 0.001241\n",
      ">> Epoch 161 finished \tANN training loss 0.001285\n",
      ">> Epoch 162 finished \tANN training loss 0.001192\n",
      ">> Epoch 163 finished \tANN training loss 0.001189\n",
      ">> Epoch 164 finished \tANN training loss 0.001181\n",
      ">> Epoch 165 finished \tANN training loss 0.001169\n",
      ">> Epoch 166 finished \tANN training loss 0.001175\n",
      ">> Epoch 167 finished \tANN training loss 0.001183\n",
      ">> Epoch 168 finished \tANN training loss 0.001137\n",
      ">> Epoch 169 finished \tANN training loss 0.001075\n",
      ">> Epoch 170 finished \tANN training loss 0.001086\n",
      ">> Epoch 171 finished \tANN training loss 0.001091\n",
      ">> Epoch 172 finished \tANN training loss 0.001077\n",
      ">> Epoch 173 finished \tANN training loss 0.001035\n",
      ">> Epoch 174 finished \tANN training loss 0.001144\n",
      ">> Epoch 175 finished \tANN training loss 0.001037\n",
      ">> Epoch 176 finished \tANN training loss 0.001068\n",
      ">> Epoch 177 finished \tANN training loss 0.001070\n",
      ">> Epoch 178 finished \tANN training loss 0.001022\n",
      ">> Epoch 179 finished \tANN training loss 0.001009\n",
      ">> Epoch 180 finished \tANN training loss 0.001062\n",
      ">> Epoch 181 finished \tANN training loss 0.000995\n",
      ">> Epoch 182 finished \tANN training loss 0.001005\n",
      ">> Epoch 183 finished \tANN training loss 0.001026\n",
      ">> Epoch 184 finished \tANN training loss 0.000988\n",
      ">> Epoch 185 finished \tANN training loss 0.001043\n",
      ">> Epoch 186 finished \tANN training loss 0.000989\n",
      ">> Epoch 187 finished \tANN training loss 0.000963\n",
      ">> Epoch 188 finished \tANN training loss 0.001189\n",
      ">> Epoch 189 finished \tANN training loss 0.001081\n",
      ">> Epoch 190 finished \tANN training loss 0.001003\n",
      ">> Epoch 191 finished \tANN training loss 0.001001\n",
      ">> Epoch 192 finished \tANN training loss 0.000973\n",
      ">> Epoch 193 finished \tANN training loss 0.000970\n",
      ">> Epoch 194 finished \tANN training loss 0.000913\n",
      ">> Epoch 195 finished \tANN training loss 0.000908\n",
      ">> Epoch 196 finished \tANN training loss 0.000873\n",
      ">> Epoch 197 finished \tANN training loss 0.000865\n",
      ">> Epoch 198 finished \tANN training loss 0.000858\n",
      ">> Epoch 199 finished \tANN training loss 0.000947\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.900000\n"
     ]
    }
   ],
   "source": [
    "# 1-layer\n",
    "acc1 = deep_belief_net(hidden_layers_structure=[200], \n",
    "                       learning_rate_rbm=0.05,\n",
    "                       learning_rate=0.1,\n",
    "                       n_epochs_rbm=40,\n",
    "                       n_iter_backprop=200,\n",
    "                       batch_size=32,\n",
    "                       activation_function='relu',\n",
    "                       dropout_p=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.9\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(acc1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 52.362724\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 67.402321\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 55.501465\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 55.674171\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 51.376099\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 39.820408\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 49.372471\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 42.299225\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 43.152420\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 44.879902\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 44.187225\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 38.721596\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 44.987022\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 34.595253\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 38.884151\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 45.426708\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 42.453545\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 39.000511\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 39.822796\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 44.699356\n",
      ">> Epoch 21 finished \tRBM Reconstruction error 42.462193\n",
      ">> Epoch 22 finished \tRBM Reconstruction error 51.749691\n",
      ">> Epoch 23 finished \tRBM Reconstruction error 54.024845\n",
      ">> Epoch 24 finished \tRBM Reconstruction error 36.917061\n",
      ">> Epoch 25 finished \tRBM Reconstruction error 35.519672\n",
      ">> Epoch 26 finished \tRBM Reconstruction error 57.290401\n",
      ">> Epoch 27 finished \tRBM Reconstruction error 47.573944\n",
      ">> Epoch 28 finished \tRBM Reconstruction error 46.187237\n",
      ">> Epoch 29 finished \tRBM Reconstruction error 37.448875\n",
      ">> Epoch 30 finished \tRBM Reconstruction error 43.712196\n",
      ">> Epoch 31 finished \tRBM Reconstruction error 35.429165\n",
      ">> Epoch 32 finished \tRBM Reconstruction error 59.610336\n",
      ">> Epoch 33 finished \tRBM Reconstruction error 40.510052\n",
      ">> Epoch 34 finished \tRBM Reconstruction error 41.287785\n",
      ">> Epoch 35 finished \tRBM Reconstruction error 39.892479\n",
      ">> Epoch 36 finished \tRBM Reconstruction error 44.331387\n",
      ">> Epoch 37 finished \tRBM Reconstruction error 41.451927\n",
      ">> Epoch 38 finished \tRBM Reconstruction error 33.561676\n",
      ">> Epoch 39 finished \tRBM Reconstruction error 51.854393\n",
      ">> Epoch 40 finished \tRBM Reconstruction error 48.544121\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.737389\n",
      ">> Epoch 1 finished \tANN training loss 0.510684\n",
      ">> Epoch 2 finished \tANN training loss 0.416880\n",
      ">> Epoch 3 finished \tANN training loss 0.363944\n",
      ">> Epoch 4 finished \tANN training loss 0.300254\n",
      ">> Epoch 5 finished \tANN training loss 0.278735\n",
      ">> Epoch 6 finished \tANN training loss 0.240704\n",
      ">> Epoch 7 finished \tANN training loss 0.215216\n",
      ">> Epoch 8 finished \tANN training loss 0.193875\n",
      ">> Epoch 9 finished \tANN training loss 0.200080\n",
      ">> Epoch 10 finished \tANN training loss 0.164040\n",
      ">> Epoch 11 finished \tANN training loss 0.151093\n",
      ">> Epoch 12 finished \tANN training loss 0.135506\n",
      ">> Epoch 13 finished \tANN training loss 0.129488\n",
      ">> Epoch 14 finished \tANN training loss 0.118267\n",
      ">> Epoch 15 finished \tANN training loss 0.108225\n",
      ">> Epoch 16 finished \tANN training loss 0.097833\n",
      ">> Epoch 17 finished \tANN training loss 0.088887\n",
      ">> Epoch 18 finished \tANN training loss 0.086095\n",
      ">> Epoch 19 finished \tANN training loss 0.076195\n",
      ">> Epoch 20 finished \tANN training loss 0.069642\n",
      ">> Epoch 21 finished \tANN training loss 0.063800\n",
      ">> Epoch 22 finished \tANN training loss 0.060976\n",
      ">> Epoch 23 finished \tANN training loss 0.055572\n",
      ">> Epoch 24 finished \tANN training loss 0.050424\n",
      ">> Epoch 25 finished \tANN training loss 0.054087\n",
      ">> Epoch 26 finished \tANN training loss 0.050476\n",
      ">> Epoch 27 finished \tANN training loss 0.042417\n",
      ">> Epoch 28 finished \tANN training loss 0.041815\n",
      ">> Epoch 29 finished \tANN training loss 0.037267\n",
      ">> Epoch 30 finished \tANN training loss 0.035003\n",
      ">> Epoch 31 finished \tANN training loss 0.035003\n",
      ">> Epoch 32 finished \tANN training loss 0.032139\n",
      ">> Epoch 33 finished \tANN training loss 0.030261\n",
      ">> Epoch 34 finished \tANN training loss 0.028318\n",
      ">> Epoch 35 finished \tANN training loss 0.028338\n",
      ">> Epoch 36 finished \tANN training loss 0.027152\n",
      ">> Epoch 37 finished \tANN training loss 0.026517\n",
      ">> Epoch 38 finished \tANN training loss 0.022998\n",
      ">> Epoch 39 finished \tANN training loss 0.022701\n",
      ">> Epoch 40 finished \tANN training loss 0.020563\n",
      ">> Epoch 41 finished \tANN training loss 0.018844\n",
      ">> Epoch 42 finished \tANN training loss 0.018133\n",
      ">> Epoch 43 finished \tANN training loss 0.018060\n",
      ">> Epoch 44 finished \tANN training loss 0.017219\n",
      ">> Epoch 45 finished \tANN training loss 0.015878\n",
      ">> Epoch 46 finished \tANN training loss 0.015737\n",
      ">> Epoch 47 finished \tANN training loss 0.014439\n",
      ">> Epoch 48 finished \tANN training loss 0.015131\n",
      ">> Epoch 49 finished \tANN training loss 0.013918\n",
      ">> Epoch 50 finished \tANN training loss 0.013113\n",
      ">> Epoch 51 finished \tANN training loss 0.013314\n",
      ">> Epoch 52 finished \tANN training loss 0.012216\n",
      ">> Epoch 53 finished \tANN training loss 0.011207\n",
      ">> Epoch 54 finished \tANN training loss 0.010975\n",
      ">> Epoch 55 finished \tANN training loss 0.010746\n",
      ">> Epoch 56 finished \tANN training loss 0.010390\n",
      ">> Epoch 57 finished \tANN training loss 0.010228\n",
      ">> Epoch 58 finished \tANN training loss 0.010118\n",
      ">> Epoch 59 finished \tANN training loss 0.009898\n",
      ">> Epoch 60 finished \tANN training loss 0.009475\n",
      ">> Epoch 61 finished \tANN training loss 0.009077\n",
      ">> Epoch 62 finished \tANN training loss 0.008506\n",
      ">> Epoch 63 finished \tANN training loss 0.008389\n",
      ">> Epoch 64 finished \tANN training loss 0.008302\n",
      ">> Epoch 65 finished \tANN training loss 0.007664\n",
      ">> Epoch 66 finished \tANN training loss 0.007477\n",
      ">> Epoch 67 finished \tANN training loss 0.007072\n",
      ">> Epoch 68 finished \tANN training loss 0.007408\n",
      ">> Epoch 69 finished \tANN training loss 0.006817\n",
      ">> Epoch 70 finished \tANN training loss 0.006450\n",
      ">> Epoch 71 finished \tANN training loss 0.006885\n",
      ">> Epoch 72 finished \tANN training loss 0.006061\n",
      ">> Epoch 73 finished \tANN training loss 0.005946\n",
      ">> Epoch 74 finished \tANN training loss 0.005856\n",
      ">> Epoch 75 finished \tANN training loss 0.005532\n",
      ">> Epoch 76 finished \tANN training loss 0.005609\n",
      ">> Epoch 77 finished \tANN training loss 0.005455\n",
      ">> Epoch 78 finished \tANN training loss 0.005257\n",
      ">> Epoch 79 finished \tANN training loss 0.004813\n",
      ">> Epoch 80 finished \tANN training loss 0.004732\n",
      ">> Epoch 81 finished \tANN training loss 0.004738\n",
      ">> Epoch 82 finished \tANN training loss 0.004537\n",
      ">> Epoch 83 finished \tANN training loss 0.005078\n",
      ">> Epoch 84 finished \tANN training loss 0.004463\n",
      ">> Epoch 85 finished \tANN training loss 0.004523\n",
      ">> Epoch 86 finished \tANN training loss 0.004206\n",
      ">> Epoch 87 finished \tANN training loss 0.004330\n",
      ">> Epoch 88 finished \tANN training loss 0.004132\n",
      ">> Epoch 89 finished \tANN training loss 0.003999\n",
      ">> Epoch 90 finished \tANN training loss 0.003782\n",
      ">> Epoch 91 finished \tANN training loss 0.003765\n",
      ">> Epoch 92 finished \tANN training loss 0.003818\n",
      ">> Epoch 93 finished \tANN training loss 0.003652\n",
      ">> Epoch 94 finished \tANN training loss 0.003607\n",
      ">> Epoch 95 finished \tANN training loss 0.003499\n",
      ">> Epoch 96 finished \tANN training loss 0.003454\n",
      ">> Epoch 97 finished \tANN training loss 0.003335\n",
      ">> Epoch 98 finished \tANN training loss 0.003360\n",
      ">> Epoch 99 finished \tANN training loss 0.003448\n",
      ">> Epoch 100 finished \tANN training loss 0.003380\n",
      ">> Epoch 101 finished \tANN training loss 0.003367\n",
      ">> Epoch 102 finished \tANN training loss 0.003077\n",
      ">> Epoch 103 finished \tANN training loss 0.003057\n",
      ">> Epoch 104 finished \tANN training loss 0.002935\n",
      ">> Epoch 105 finished \tANN training loss 0.003174\n",
      ">> Epoch 106 finished \tANN training loss 0.002816\n",
      ">> Epoch 107 finished \tANN training loss 0.002890\n",
      ">> Epoch 108 finished \tANN training loss 0.002659\n",
      ">> Epoch 109 finished \tANN training loss 0.002688\n",
      ">> Epoch 110 finished \tANN training loss 0.002526\n",
      ">> Epoch 111 finished \tANN training loss 0.002529\n",
      ">> Epoch 112 finished \tANN training loss 0.002553\n",
      ">> Epoch 113 finished \tANN training loss 0.002484\n",
      ">> Epoch 114 finished \tANN training loss 0.002312\n",
      ">> Epoch 115 finished \tANN training loss 0.002354\n",
      ">> Epoch 116 finished \tANN training loss 0.002415\n",
      ">> Epoch 117 finished \tANN training loss 0.002437\n",
      ">> Epoch 118 finished \tANN training loss 0.002383\n",
      ">> Epoch 119 finished \tANN training loss 0.002175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 120 finished \tANN training loss 0.002120\n",
      ">> Epoch 121 finished \tANN training loss 0.002113\n",
      ">> Epoch 122 finished \tANN training loss 0.002175\n",
      ">> Epoch 123 finished \tANN training loss 0.002189\n",
      ">> Epoch 124 finished \tANN training loss 0.002079\n",
      ">> Epoch 125 finished \tANN training loss 0.002005\n",
      ">> Epoch 126 finished \tANN training loss 0.001969\n",
      ">> Epoch 127 finished \tANN training loss 0.002142\n",
      ">> Epoch 128 finished \tANN training loss 0.001984\n",
      ">> Epoch 129 finished \tANN training loss 0.001972\n",
      ">> Epoch 130 finished \tANN training loss 0.001884\n",
      ">> Epoch 131 finished \tANN training loss 0.001838\n",
      ">> Epoch 132 finished \tANN training loss 0.001891\n",
      ">> Epoch 133 finished \tANN training loss 0.001746\n",
      ">> Epoch 134 finished \tANN training loss 0.001782\n",
      ">> Epoch 135 finished \tANN training loss 0.001758\n",
      ">> Epoch 136 finished \tANN training loss 0.001703\n",
      ">> Epoch 137 finished \tANN training loss 0.001698\n",
      ">> Epoch 138 finished \tANN training loss 0.001643\n",
      ">> Epoch 139 finished \tANN training loss 0.001669\n",
      ">> Epoch 140 finished \tANN training loss 0.001719\n",
      ">> Epoch 141 finished \tANN training loss 0.001643\n",
      ">> Epoch 142 finished \tANN training loss 0.001615\n",
      ">> Epoch 143 finished \tANN training loss 0.001607\n",
      ">> Epoch 144 finished \tANN training loss 0.001587\n",
      ">> Epoch 145 finished \tANN training loss 0.001580\n",
      ">> Epoch 146 finished \tANN training loss 0.001538\n",
      ">> Epoch 147 finished \tANN training loss 0.001552\n",
      ">> Epoch 148 finished \tANN training loss 0.001547\n",
      ">> Epoch 149 finished \tANN training loss 0.001549\n",
      ">> Epoch 150 finished \tANN training loss 0.001478\n",
      ">> Epoch 151 finished \tANN training loss 0.001405\n",
      ">> Epoch 152 finished \tANN training loss 0.001398\n",
      ">> Epoch 153 finished \tANN training loss 0.001515\n",
      ">> Epoch 154 finished \tANN training loss 0.001330\n",
      ">> Epoch 155 finished \tANN training loss 0.001339\n",
      ">> Epoch 156 finished \tANN training loss 0.001325\n",
      ">> Epoch 157 finished \tANN training loss 0.001312\n",
      ">> Epoch 158 finished \tANN training loss 0.001292\n",
      ">> Epoch 159 finished \tANN training loss 0.001291\n",
      ">> Epoch 160 finished \tANN training loss 0.001314\n",
      ">> Epoch 161 finished \tANN training loss 0.001263\n",
      ">> Epoch 162 finished \tANN training loss 0.001240\n",
      ">> Epoch 163 finished \tANN training loss 0.001305\n",
      ">> Epoch 164 finished \tANN training loss 0.001269\n",
      ">> Epoch 165 finished \tANN training loss 0.001226\n",
      ">> Epoch 166 finished \tANN training loss 0.001308\n",
      ">> Epoch 167 finished \tANN training loss 0.001164\n",
      ">> Epoch 168 finished \tANN training loss 0.001201\n",
      ">> Epoch 169 finished \tANN training loss 0.001224\n",
      ">> Epoch 170 finished \tANN training loss 0.001134\n",
      ">> Epoch 171 finished \tANN training loss 0.001156\n",
      ">> Epoch 172 finished \tANN training loss 0.001219\n",
      ">> Epoch 173 finished \tANN training loss 0.001138\n",
      ">> Epoch 174 finished \tANN training loss 0.001191\n",
      ">> Epoch 175 finished \tANN training loss 0.001072\n",
      ">> Epoch 176 finished \tANN training loss 0.001021\n",
      ">> Epoch 177 finished \tANN training loss 0.001048\n",
      ">> Epoch 178 finished \tANN training loss 0.001036\n",
      ">> Epoch 179 finished \tANN training loss 0.001074\n",
      ">> Epoch 180 finished \tANN training loss 0.001007\n",
      ">> Epoch 181 finished \tANN training loss 0.001034\n",
      ">> Epoch 182 finished \tANN training loss 0.001050\n",
      ">> Epoch 183 finished \tANN training loss 0.000974\n",
      ">> Epoch 184 finished \tANN training loss 0.000935\n",
      ">> Epoch 185 finished \tANN training loss 0.000912\n",
      ">> Epoch 186 finished \tANN training loss 0.000919\n",
      ">> Epoch 187 finished \tANN training loss 0.000944\n",
      ">> Epoch 188 finished \tANN training loss 0.000946\n",
      ">> Epoch 189 finished \tANN training loss 0.000926\n",
      ">> Epoch 190 finished \tANN training loss 0.000878\n",
      ">> Epoch 191 finished \tANN training loss 0.000943\n",
      ">> Epoch 192 finished \tANN training loss 0.000918\n",
      ">> Epoch 193 finished \tANN training loss 0.000912\n",
      ">> Epoch 194 finished \tANN training loss 0.000890\n",
      ">> Epoch 195 finished \tANN training loss 0.000913\n",
      ">> Epoch 196 finished \tANN training loss 0.000890\n",
      ">> Epoch 197 finished \tANN training loss 0.000882\n",
      ">> Epoch 198 finished \tANN training loss 0.000913\n",
      ">> Epoch 199 finished \tANN training loss 0.000929\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.920000\n"
     ]
    }
   ],
   "source": [
    "# 1-layer (2nd one tied)\n",
    "acc2 = deep_belief_net(hidden_layers_structure=[300], \n",
    "                       learning_rate_rbm=0.05,\n",
    "                       learning_rate=0.1,\n",
    "                       n_epochs_rbm=40,\n",
    "                       n_iter_backprop=200,\n",
    "                       batch_size=32,\n",
    "                       activation_function='relu',\n",
    "                       dropout_p=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.92\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(acc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 61.901642\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 60.275616\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 87.619865\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 56.597225\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 45.425381\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 46.222080\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 69.010742\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 45.242207\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 50.292126\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 57.479366\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 73.998779\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 68.098679\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 83.567871\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 75.080116\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 100.359024\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 72.527649\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 80.413300\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 85.674294\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 79.484909\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 107.319412\n",
      ">> Epoch 21 finished \tRBM Reconstruction error 85.888161\n",
      ">> Epoch 22 finished \tRBM Reconstruction error 84.266258\n",
      ">> Epoch 23 finished \tRBM Reconstruction error 78.896393\n",
      ">> Epoch 24 finished \tRBM Reconstruction error 78.359489\n",
      ">> Epoch 25 finished \tRBM Reconstruction error 114.280861\n",
      ">> Epoch 26 finished \tRBM Reconstruction error 104.662018\n",
      ">> Epoch 27 finished \tRBM Reconstruction error 96.259026\n",
      ">> Epoch 28 finished \tRBM Reconstruction error 88.241425\n",
      ">> Epoch 29 finished \tRBM Reconstruction error 92.975906\n",
      ">> Epoch 30 finished \tRBM Reconstruction error 99.582123\n",
      ">> Epoch 31 finished \tRBM Reconstruction error 92.787491\n",
      ">> Epoch 32 finished \tRBM Reconstruction error 99.423019\n",
      ">> Epoch 33 finished \tRBM Reconstruction error 105.046349\n",
      ">> Epoch 34 finished \tRBM Reconstruction error 104.780960\n",
      ">> Epoch 35 finished \tRBM Reconstruction error 99.292625\n",
      ">> Epoch 36 finished \tRBM Reconstruction error 115.861115\n",
      ">> Epoch 37 finished \tRBM Reconstruction error 105.354256\n",
      ">> Epoch 38 finished \tRBM Reconstruction error 112.433105\n",
      ">> Epoch 39 finished \tRBM Reconstruction error 118.529953\n",
      ">> Epoch 40 finished \tRBM Reconstruction error 98.022736\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 343.910980\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 387.981964\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 312.443115\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 352.085144\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 320.552429\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 405.200012\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 371.407196\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 343.438293\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 371.127533\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 397.807495\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 362.312927\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 397.225220\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 384.043701\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 467.161560\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 469.198364\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 419.519287\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 494.007996\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 452.421082\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 477.412628\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 430.911041\n",
      ">> Epoch 21 finished \tRBM Reconstruction error 365.288452\n",
      ">> Epoch 22 finished \tRBM Reconstruction error 557.352783\n",
      ">> Epoch 23 finished \tRBM Reconstruction error 478.143463\n",
      ">> Epoch 24 finished \tRBM Reconstruction error 381.100677\n",
      ">> Epoch 25 finished \tRBM Reconstruction error 415.115631\n",
      ">> Epoch 26 finished \tRBM Reconstruction error 400.681610\n",
      ">> Epoch 27 finished \tRBM Reconstruction error 484.287567\n",
      ">> Epoch 28 finished \tRBM Reconstruction error 417.944702\n",
      ">> Epoch 29 finished \tRBM Reconstruction error 394.090271\n",
      ">> Epoch 30 finished \tRBM Reconstruction error 499.719910\n",
      ">> Epoch 31 finished \tRBM Reconstruction error 389.697815\n",
      ">> Epoch 32 finished \tRBM Reconstruction error 521.718140\n",
      ">> Epoch 33 finished \tRBM Reconstruction error 527.772644\n",
      ">> Epoch 34 finished \tRBM Reconstruction error 402.980469\n",
      ">> Epoch 35 finished \tRBM Reconstruction error 465.946320\n",
      ">> Epoch 36 finished \tRBM Reconstruction error 477.205780\n",
      ">> Epoch 37 finished \tRBM Reconstruction error 451.609131\n",
      ">> Epoch 38 finished \tRBM Reconstruction error 404.333740\n",
      ">> Epoch 39 finished \tRBM Reconstruction error 411.326874\n",
      ">> Epoch 40 finished \tRBM Reconstruction error 417.262665\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.568238\n",
      ">> Epoch 1 finished \tANN training loss 0.408312\n",
      ">> Epoch 2 finished \tANN training loss 0.342579\n",
      ">> Epoch 3 finished \tANN training loss 0.283675\n",
      ">> Epoch 4 finished \tANN training loss 0.287408\n",
      ">> Epoch 5 finished \tANN training loss 0.211970\n",
      ">> Epoch 6 finished \tANN training loss 0.191535\n",
      ">> Epoch 7 finished \tANN training loss 0.162043\n",
      ">> Epoch 8 finished \tANN training loss 0.161872\n",
      ">> Epoch 9 finished \tANN training loss 0.120234\n",
      ">> Epoch 10 finished \tANN training loss 0.112354\n",
      ">> Epoch 11 finished \tANN training loss 0.099087\n",
      ">> Epoch 12 finished \tANN training loss 0.088026\n",
      ">> Epoch 13 finished \tANN training loss 0.088477\n",
      ">> Epoch 14 finished \tANN training loss 0.073664\n",
      ">> Epoch 15 finished \tANN training loss 0.072788\n",
      ">> Epoch 16 finished \tANN training loss 0.052852\n",
      ">> Epoch 17 finished \tANN training loss 0.055829\n",
      ">> Epoch 18 finished \tANN training loss 0.044760\n",
      ">> Epoch 19 finished \tANN training loss 0.042253\n",
      ">> Epoch 20 finished \tANN training loss 0.037475\n",
      ">> Epoch 21 finished \tANN training loss 0.034571\n",
      ">> Epoch 22 finished \tANN training loss 0.037231\n",
      ">> Epoch 23 finished \tANN training loss 0.032096\n",
      ">> Epoch 24 finished \tANN training loss 0.027734\n",
      ">> Epoch 25 finished \tANN training loss 0.027753\n",
      ">> Epoch 26 finished \tANN training loss 0.024580\n",
      ">> Epoch 27 finished \tANN training loss 0.023303\n",
      ">> Epoch 28 finished \tANN training loss 0.021136\n",
      ">> Epoch 29 finished \tANN training loss 0.015942\n",
      ">> Epoch 30 finished \tANN training loss 0.016998\n",
      ">> Epoch 31 finished \tANN training loss 0.014881\n",
      ">> Epoch 32 finished \tANN training loss 0.015414\n",
      ">> Epoch 33 finished \tANN training loss 0.012773\n",
      ">> Epoch 34 finished \tANN training loss 0.014762\n",
      ">> Epoch 35 finished \tANN training loss 0.011117\n",
      ">> Epoch 36 finished \tANN training loss 0.012771\n",
      ">> Epoch 37 finished \tANN training loss 0.011061\n",
      ">> Epoch 38 finished \tANN training loss 0.010126\n",
      ">> Epoch 39 finished \tANN training loss 0.009290\n",
      ">> Epoch 40 finished \tANN training loss 0.007884\n",
      ">> Epoch 41 finished \tANN training loss 0.008647\n",
      ">> Epoch 42 finished \tANN training loss 0.007533\n",
      ">> Epoch 43 finished \tANN training loss 0.007249\n",
      ">> Epoch 44 finished \tANN training loss 0.006757\n",
      ">> Epoch 45 finished \tANN training loss 0.006556\n",
      ">> Epoch 46 finished \tANN training loss 0.006021\n",
      ">> Epoch 47 finished \tANN training loss 0.005268\n",
      ">> Epoch 48 finished \tANN training loss 0.006024\n",
      ">> Epoch 49 finished \tANN training loss 0.005204\n",
      ">> Epoch 50 finished \tANN training loss 0.006755\n",
      ">> Epoch 51 finished \tANN training loss 0.005459\n",
      ">> Epoch 52 finished \tANN training loss 0.004151\n",
      ">> Epoch 53 finished \tANN training loss 0.004102\n",
      ">> Epoch 54 finished \tANN training loss 0.004380\n",
      ">> Epoch 55 finished \tANN training loss 0.004356\n",
      ">> Epoch 56 finished \tANN training loss 0.003965\n",
      ">> Epoch 57 finished \tANN training loss 0.003204\n",
      ">> Epoch 58 finished \tANN training loss 0.004867\n",
      ">> Epoch 59 finished \tANN training loss 0.003666\n",
      ">> Epoch 60 finished \tANN training loss 0.003274\n",
      ">> Epoch 61 finished \tANN training loss 0.003063\n",
      ">> Epoch 62 finished \tANN training loss 0.002788\n",
      ">> Epoch 63 finished \tANN training loss 0.002697\n",
      ">> Epoch 64 finished \tANN training loss 0.002677\n",
      ">> Epoch 65 finished \tANN training loss 0.002473\n",
      ">> Epoch 66 finished \tANN training loss 0.002594\n",
      ">> Epoch 67 finished \tANN training loss 0.002305\n",
      ">> Epoch 68 finished \tANN training loss 0.002568\n",
      ">> Epoch 69 finished \tANN training loss 0.002258\n",
      ">> Epoch 70 finished \tANN training loss 0.002216\n",
      ">> Epoch 71 finished \tANN training loss 0.002410\n",
      ">> Epoch 72 finished \tANN training loss 0.002367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 73 finished \tANN training loss 0.001925\n",
      ">> Epoch 74 finished \tANN training loss 0.002563\n",
      ">> Epoch 75 finished \tANN training loss 0.003048\n",
      ">> Epoch 76 finished \tANN training loss 0.001842\n",
      ">> Epoch 77 finished \tANN training loss 0.001902\n",
      ">> Epoch 78 finished \tANN training loss 0.001958\n",
      ">> Epoch 79 finished \tANN training loss 0.001821\n",
      ">> Epoch 80 finished \tANN training loss 0.002197\n",
      ">> Epoch 81 finished \tANN training loss 0.002116\n",
      ">> Epoch 82 finished \tANN training loss 0.001709\n",
      ">> Epoch 83 finished \tANN training loss 0.001723\n",
      ">> Epoch 84 finished \tANN training loss 0.001719\n",
      ">> Epoch 85 finished \tANN training loss 0.001590\n",
      ">> Epoch 86 finished \tANN training loss 0.001469\n",
      ">> Epoch 87 finished \tANN training loss 0.001514\n",
      ">> Epoch 88 finished \tANN training loss 0.001274\n",
      ">> Epoch 89 finished \tANN training loss 0.001641\n",
      ">> Epoch 90 finished \tANN training loss 0.001597\n",
      ">> Epoch 91 finished \tANN training loss 0.001392\n",
      ">> Epoch 92 finished \tANN training loss 0.001239\n",
      ">> Epoch 93 finished \tANN training loss 0.001163\n",
      ">> Epoch 94 finished \tANN training loss 0.001130\n",
      ">> Epoch 95 finished \tANN training loss 0.001050\n",
      ">> Epoch 96 finished \tANN training loss 0.001007\n",
      ">> Epoch 97 finished \tANN training loss 0.000900\n",
      ">> Epoch 98 finished \tANN training loss 0.000936\n",
      ">> Epoch 99 finished \tANN training loss 0.000859\n",
      ">> Epoch 100 finished \tANN training loss 0.000805\n",
      ">> Epoch 101 finished \tANN training loss 0.000932\n",
      ">> Epoch 102 finished \tANN training loss 0.000821\n",
      ">> Epoch 103 finished \tANN training loss 0.000816\n",
      ">> Epoch 104 finished \tANN training loss 0.000858\n",
      ">> Epoch 105 finished \tANN training loss 0.000936\n",
      ">> Epoch 106 finished \tANN training loss 0.000810\n",
      ">> Epoch 107 finished \tANN training loss 0.000759\n",
      ">> Epoch 108 finished \tANN training loss 0.000790\n",
      ">> Epoch 109 finished \tANN training loss 0.000723\n",
      ">> Epoch 110 finished \tANN training loss 0.000656\n",
      ">> Epoch 111 finished \tANN training loss 0.000597\n",
      ">> Epoch 112 finished \tANN training loss 0.000620\n",
      ">> Epoch 113 finished \tANN training loss 0.000619\n",
      ">> Epoch 114 finished \tANN training loss 0.000569\n",
      ">> Epoch 115 finished \tANN training loss 0.000587\n",
      ">> Epoch 116 finished \tANN training loss 0.000567\n",
      ">> Epoch 117 finished \tANN training loss 0.000659\n",
      ">> Epoch 118 finished \tANN training loss 0.000684\n",
      ">> Epoch 119 finished \tANN training loss 0.000604\n",
      ">> Epoch 120 finished \tANN training loss 0.000655\n",
      ">> Epoch 121 finished \tANN training loss 0.000630\n",
      ">> Epoch 122 finished \tANN training loss 0.000547\n",
      ">> Epoch 123 finished \tANN training loss 0.000639\n",
      ">> Epoch 124 finished \tANN training loss 0.000573\n",
      ">> Epoch 125 finished \tANN training loss 0.000506\n",
      ">> Epoch 126 finished \tANN training loss 0.000500\n",
      ">> Epoch 127 finished \tANN training loss 0.000538\n",
      ">> Epoch 128 finished \tANN training loss 0.000463\n",
      ">> Epoch 129 finished \tANN training loss 0.000441\n",
      ">> Epoch 130 finished \tANN training loss 0.000470\n",
      ">> Epoch 131 finished \tANN training loss 0.000741\n",
      ">> Epoch 132 finished \tANN training loss 0.000440\n",
      ">> Epoch 133 finished \tANN training loss 0.000426\n",
      ">> Epoch 134 finished \tANN training loss 0.000410\n",
      ">> Epoch 135 finished \tANN training loss 0.000448\n",
      ">> Epoch 136 finished \tANN training loss 0.000442\n",
      ">> Epoch 137 finished \tANN training loss 0.000487\n",
      ">> Epoch 138 finished \tANN training loss 0.000508\n",
      ">> Epoch 139 finished \tANN training loss 0.000427\n",
      ">> Epoch 140 finished \tANN training loss 0.000454\n",
      ">> Epoch 141 finished \tANN training loss 0.000385\n",
      ">> Epoch 142 finished \tANN training loss 0.000360\n",
      ">> Epoch 143 finished \tANN training loss 0.000393\n",
      ">> Epoch 144 finished \tANN training loss 0.000398\n",
      ">> Epoch 145 finished \tANN training loss 0.000385\n",
      ">> Epoch 146 finished \tANN training loss 0.000384\n",
      ">> Epoch 147 finished \tANN training loss 0.000336\n",
      ">> Epoch 148 finished \tANN training loss 0.000328\n",
      ">> Epoch 149 finished \tANN training loss 0.000326\n",
      ">> Epoch 150 finished \tANN training loss 0.000377\n",
      ">> Epoch 151 finished \tANN training loss 0.000335\n",
      ">> Epoch 152 finished \tANN training loss 0.000338\n",
      ">> Epoch 153 finished \tANN training loss 0.000292\n",
      ">> Epoch 154 finished \tANN training loss 0.000377\n",
      ">> Epoch 155 finished \tANN training loss 0.000363\n",
      ">> Epoch 156 finished \tANN training loss 0.000306\n",
      ">> Epoch 157 finished \tANN training loss 0.000299\n",
      ">> Epoch 158 finished \tANN training loss 0.000316\n",
      ">> Epoch 159 finished \tANN training loss 0.000353\n",
      ">> Epoch 160 finished \tANN training loss 0.000344\n",
      ">> Epoch 161 finished \tANN training loss 0.000374\n",
      ">> Epoch 162 finished \tANN training loss 0.000384\n",
      ">> Epoch 163 finished \tANN training loss 0.000331\n",
      ">> Epoch 164 finished \tANN training loss 0.000278\n",
      ">> Epoch 165 finished \tANN training loss 0.000302\n",
      ">> Epoch 166 finished \tANN training loss 0.000273\n",
      ">> Epoch 167 finished \tANN training loss 0.000305\n",
      ">> Epoch 168 finished \tANN training loss 0.000289\n",
      ">> Epoch 169 finished \tANN training loss 0.000311\n",
      ">> Epoch 170 finished \tANN training loss 0.000329\n",
      ">> Epoch 171 finished \tANN training loss 0.000300\n",
      ">> Epoch 172 finished \tANN training loss 0.000392\n",
      ">> Epoch 173 finished \tANN training loss 0.000311\n",
      ">> Epoch 174 finished \tANN training loss 0.000458\n",
      ">> Epoch 175 finished \tANN training loss 0.000305\n",
      ">> Epoch 176 finished \tANN training loss 0.000290\n",
      ">> Epoch 177 finished \tANN training loss 0.000531\n",
      ">> Epoch 178 finished \tANN training loss 0.000354\n",
      ">> Epoch 179 finished \tANN training loss 0.000298\n",
      ">> Epoch 180 finished \tANN training loss 0.000301\n",
      ">> Epoch 181 finished \tANN training loss 0.000269\n",
      ">> Epoch 182 finished \tANN training loss 0.000252\n",
      ">> Epoch 183 finished \tANN training loss 0.000263\n",
      ">> Epoch 184 finished \tANN training loss 0.000263\n",
      ">> Epoch 185 finished \tANN training loss 0.000303\n",
      ">> Epoch 186 finished \tANN training loss 0.000354\n",
      ">> Epoch 187 finished \tANN training loss 0.000286\n",
      ">> Epoch 188 finished \tANN training loss 0.000282\n",
      ">> Epoch 189 finished \tANN training loss 0.000311\n",
      ">> Epoch 190 finished \tANN training loss 0.000273\n",
      ">> Epoch 191 finished \tANN training loss 0.000215\n",
      ">> Epoch 192 finished \tANN training loss 0.000232\n",
      ">> Epoch 193 finished \tANN training loss 0.000232\n",
      ">> Epoch 194 finished \tANN training loss 0.000214\n",
      ">> Epoch 195 finished \tANN training loss 0.000209\n",
      ">> Epoch 196 finished \tANN training loss 0.000210\n",
      ">> Epoch 197 finished \tANN training loss 0.000193\n",
      ">> Epoch 198 finished \tANN training loss 0.000199\n",
      ">> Epoch 199 finished \tANN training loss 0.000174\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.905000\n"
     ]
    }
   ],
   "source": [
    "# 2-layer\n",
    "acc3 = deep_belief_net(hidden_layers_structure=[100, 500], \n",
    "                       learning_rate_rbm=0.05,\n",
    "                       learning_rate=0.1,\n",
    "                       n_epochs_rbm=40,\n",
    "                       n_iter_backprop=200,\n",
    "                       batch_size=32,\n",
    "                       activation_function='relu',\n",
    "                       dropout_p=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.905\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(acc3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 44.434200\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 36.059303\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 27.978998\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 43.884365\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 27.703917\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 40.638386\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 22.463474\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 20.800531\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 28.822454\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 32.591835\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 41.460468\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 29.742527\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 37.216793\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 27.116709\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 34.042049\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 17.215670\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 29.130920\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 28.987253\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 40.967419\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 19.373569\n",
      ">> Epoch 21 finished \tRBM Reconstruction error 39.018585\n",
      ">> Epoch 22 finished \tRBM Reconstruction error 21.660276\n",
      ">> Epoch 23 finished \tRBM Reconstruction error 26.941730\n",
      ">> Epoch 24 finished \tRBM Reconstruction error 25.511240\n",
      ">> Epoch 25 finished \tRBM Reconstruction error 25.745319\n",
      ">> Epoch 26 finished \tRBM Reconstruction error 22.503906\n",
      ">> Epoch 27 finished \tRBM Reconstruction error 26.412804\n",
      ">> Epoch 28 finished \tRBM Reconstruction error 22.252998\n",
      ">> Epoch 29 finished \tRBM Reconstruction error 28.351004\n",
      ">> Epoch 30 finished \tRBM Reconstruction error 31.632248\n",
      ">> Epoch 31 finished \tRBM Reconstruction error 25.962896\n",
      ">> Epoch 32 finished \tRBM Reconstruction error 17.551035\n",
      ">> Epoch 33 finished \tRBM Reconstruction error 26.432123\n",
      ">> Epoch 34 finished \tRBM Reconstruction error 26.573092\n",
      ">> Epoch 35 finished \tRBM Reconstruction error 22.466236\n",
      ">> Epoch 36 finished \tRBM Reconstruction error 25.236826\n",
      ">> Epoch 37 finished \tRBM Reconstruction error 24.970171\n",
      ">> Epoch 38 finished \tRBM Reconstruction error 29.371695\n",
      ">> Epoch 39 finished \tRBM Reconstruction error 25.026304\n",
      ">> Epoch 40 finished \tRBM Reconstruction error 46.429874\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 190.199234\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 273.150726\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 317.882355\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 187.512558\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 203.559860\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 259.353577\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 247.309570\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 204.795410\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 172.767212\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 200.218307\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 382.932251\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 298.597290\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 287.038605\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 232.804962\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 291.033691\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 272.512756\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 242.022385\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 262.276886\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 303.549805\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 284.918823\n",
      ">> Epoch 21 finished \tRBM Reconstruction error 260.383209\n",
      ">> Epoch 22 finished \tRBM Reconstruction error 302.546722\n",
      ">> Epoch 23 finished \tRBM Reconstruction error 246.841385\n",
      ">> Epoch 24 finished \tRBM Reconstruction error 273.669617\n",
      ">> Epoch 25 finished \tRBM Reconstruction error 295.891388\n",
      ">> Epoch 26 finished \tRBM Reconstruction error 310.213867\n",
      ">> Epoch 27 finished \tRBM Reconstruction error 257.509644\n",
      ">> Epoch 28 finished \tRBM Reconstruction error 270.249023\n",
      ">> Epoch 29 finished \tRBM Reconstruction error 310.550232\n",
      ">> Epoch 30 finished \tRBM Reconstruction error 316.861298\n",
      ">> Epoch 31 finished \tRBM Reconstruction error 272.223724\n",
      ">> Epoch 32 finished \tRBM Reconstruction error 297.711884\n",
      ">> Epoch 33 finished \tRBM Reconstruction error 316.275909\n",
      ">> Epoch 34 finished \tRBM Reconstruction error 308.092712\n",
      ">> Epoch 35 finished \tRBM Reconstruction error 289.956726\n",
      ">> Epoch 36 finished \tRBM Reconstruction error 301.680237\n",
      ">> Epoch 37 finished \tRBM Reconstruction error 276.194153\n",
      ">> Epoch 38 finished \tRBM Reconstruction error 296.185822\n",
      ">> Epoch 39 finished \tRBM Reconstruction error 286.745422\n",
      ">> Epoch 40 finished \tRBM Reconstruction error 273.325165\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 2320.280273\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 2860.946777\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 4900.419922\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 5983.877930\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 6004.254883\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 5797.275391\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 7914.038086\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 8993.140625\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 7948.834961\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 8781.235352\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 9260.076172\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 10654.962891\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 10094.716797\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 11494.802734\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 12828.037109\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 13806.967773\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 13699.952148\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 14856.209961\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 15005.352539\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 15271.562500\n",
      ">> Epoch 21 finished \tRBM Reconstruction error 15914.365234\n",
      ">> Epoch 22 finished \tRBM Reconstruction error 16437.126953\n",
      ">> Epoch 23 finished \tRBM Reconstruction error 14874.750977\n",
      ">> Epoch 24 finished \tRBM Reconstruction error 19135.523438\n",
      ">> Epoch 25 finished \tRBM Reconstruction error 18322.642578\n",
      ">> Epoch 26 finished \tRBM Reconstruction error 16946.261719\n",
      ">> Epoch 27 finished \tRBM Reconstruction error 18924.007812\n",
      ">> Epoch 28 finished \tRBM Reconstruction error 18296.394531\n",
      ">> Epoch 29 finished \tRBM Reconstruction error 18818.375000\n",
      ">> Epoch 30 finished \tRBM Reconstruction error 18824.070312\n",
      ">> Epoch 31 finished \tRBM Reconstruction error 17882.273438\n",
      ">> Epoch 32 finished \tRBM Reconstruction error 20025.429688\n",
      ">> Epoch 33 finished \tRBM Reconstruction error 19703.408203\n",
      ">> Epoch 34 finished \tRBM Reconstruction error 18131.257812\n",
      ">> Epoch 35 finished \tRBM Reconstruction error 19559.080078\n",
      ">> Epoch 36 finished \tRBM Reconstruction error 17912.009766\n",
      ">> Epoch 37 finished \tRBM Reconstruction error 18638.230469\n",
      ">> Epoch 38 finished \tRBM Reconstruction error 20198.675781\n",
      ">> Epoch 39 finished \tRBM Reconstruction error 20073.988281\n",
      ">> Epoch 40 finished \tRBM Reconstruction error 19722.828125\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.781871\n",
      ">> Epoch 1 finished \tANN training loss 0.473890\n",
      ">> Epoch 2 finished \tANN training loss 0.332426\n",
      ">> Epoch 3 finished \tANN training loss 0.272003\n",
      ">> Epoch 4 finished \tANN training loss 0.211460\n",
      ">> Epoch 5 finished \tANN training loss 0.172745\n",
      ">> Epoch 6 finished \tANN training loss 0.159768\n",
      ">> Epoch 7 finished \tANN training loss 0.114711\n",
      ">> Epoch 8 finished \tANN training loss 0.096943\n",
      ">> Epoch 9 finished \tANN training loss 0.083168\n",
      ">> Epoch 10 finished \tANN training loss 0.082297\n",
      ">> Epoch 11 finished \tANN training loss 0.064451\n",
      ">> Epoch 12 finished \tANN training loss 0.053686\n",
      ">> Epoch 13 finished \tANN training loss 0.056996\n",
      ">> Epoch 14 finished \tANN training loss 0.046761\n",
      ">> Epoch 15 finished \tANN training loss 0.035447\n",
      ">> Epoch 16 finished \tANN training loss 0.025796\n",
      ">> Epoch 17 finished \tANN training loss 0.026041\n",
      ">> Epoch 18 finished \tANN training loss 0.032909\n",
      ">> Epoch 19 finished \tANN training loss 0.025882\n",
      ">> Epoch 20 finished \tANN training loss 0.019785\n",
      ">> Epoch 21 finished \tANN training loss 0.017260\n",
      ">> Epoch 22 finished \tANN training loss 0.015189\n",
      ">> Epoch 23 finished \tANN training loss 0.011439\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 24 finished \tANN training loss 0.010435\n",
      ">> Epoch 25 finished \tANN training loss 0.009148\n",
      ">> Epoch 26 finished \tANN training loss 0.009593\n",
      ">> Epoch 27 finished \tANN training loss 0.007510\n",
      ">> Epoch 28 finished \tANN training loss 0.006532\n",
      ">> Epoch 29 finished \tANN training loss 0.006323\n",
      ">> Epoch 30 finished \tANN training loss 0.006246\n",
      ">> Epoch 31 finished \tANN training loss 0.005392\n",
      ">> Epoch 32 finished \tANN training loss 0.007369\n",
      ">> Epoch 33 finished \tANN training loss 0.004952\n",
      ">> Epoch 34 finished \tANN training loss 0.004019\n",
      ">> Epoch 35 finished \tANN training loss 0.003908\n",
      ">> Epoch 36 finished \tANN training loss 0.002986\n",
      ">> Epoch 37 finished \tANN training loss 0.002723\n",
      ">> Epoch 38 finished \tANN training loss 0.002689\n",
      ">> Epoch 39 finished \tANN training loss 0.003579\n",
      ">> Epoch 40 finished \tANN training loss 0.003775\n",
      ">> Epoch 41 finished \tANN training loss 0.002453\n",
      ">> Epoch 42 finished \tANN training loss 0.002384\n",
      ">> Epoch 43 finished \tANN training loss 0.001628\n",
      ">> Epoch 44 finished \tANN training loss 0.001892\n",
      ">> Epoch 45 finished \tANN training loss 0.002331\n",
      ">> Epoch 46 finished \tANN training loss 0.001743\n",
      ">> Epoch 47 finished \tANN training loss 0.001330\n",
      ">> Epoch 48 finished \tANN training loss 0.001611\n",
      ">> Epoch 49 finished \tANN training loss 0.001564\n",
      ">> Epoch 50 finished \tANN training loss 0.001503\n",
      ">> Epoch 51 finished \tANN training loss 0.001696\n",
      ">> Epoch 52 finished \tANN training loss 0.001428\n",
      ">> Epoch 53 finished \tANN training loss 0.001068\n",
      ">> Epoch 54 finished \tANN training loss 0.001225\n",
      ">> Epoch 55 finished \tANN training loss 0.001250\n",
      ">> Epoch 56 finished \tANN training loss 0.000799\n",
      ">> Epoch 57 finished \tANN training loss 0.001030\n",
      ">> Epoch 58 finished \tANN training loss 0.000706\n",
      ">> Epoch 59 finished \tANN training loss 0.001596\n",
      ">> Epoch 60 finished \tANN training loss 0.000798\n",
      ">> Epoch 61 finished \tANN training loss 0.000904\n",
      ">> Epoch 62 finished \tANN training loss 0.000607\n",
      ">> Epoch 63 finished \tANN training loss 0.000501\n",
      ">> Epoch 64 finished \tANN training loss 0.000571\n",
      ">> Epoch 65 finished \tANN training loss 0.000558\n",
      ">> Epoch 66 finished \tANN training loss 0.000803\n",
      ">> Epoch 67 finished \tANN training loss 0.000618\n",
      ">> Epoch 68 finished \tANN training loss 0.000715\n",
      ">> Epoch 69 finished \tANN training loss 0.000610\n",
      ">> Epoch 70 finished \tANN training loss 0.000558\n",
      ">> Epoch 71 finished \tANN training loss 0.004027\n",
      ">> Epoch 72 finished \tANN training loss 0.000548\n",
      ">> Epoch 73 finished \tANN training loss 0.000539\n",
      ">> Epoch 74 finished \tANN training loss 0.000364\n",
      ">> Epoch 75 finished \tANN training loss 0.000523\n",
      ">> Epoch 76 finished \tANN training loss 0.000355\n",
      ">> Epoch 77 finished \tANN training loss 0.000377\n",
      ">> Epoch 78 finished \tANN training loss 0.000585\n",
      ">> Epoch 79 finished \tANN training loss 0.000468\n",
      ">> Epoch 80 finished \tANN training loss 0.000459\n",
      ">> Epoch 81 finished \tANN training loss 0.000390\n",
      ">> Epoch 82 finished \tANN training loss 0.000309\n",
      ">> Epoch 83 finished \tANN training loss 0.000235\n",
      ">> Epoch 84 finished \tANN training loss 0.000185\n",
      ">> Epoch 85 finished \tANN training loss 0.000270\n",
      ">> Epoch 86 finished \tANN training loss 0.000414\n",
      ">> Epoch 87 finished \tANN training loss 0.000283\n",
      ">> Epoch 88 finished \tANN training loss 0.000221\n",
      ">> Epoch 89 finished \tANN training loss 0.000968\n",
      ">> Epoch 90 finished \tANN training loss 0.000152\n",
      ">> Epoch 91 finished \tANN training loss 0.000132\n",
      ">> Epoch 92 finished \tANN training loss 0.000157\n",
      ">> Epoch 93 finished \tANN training loss 0.000203\n",
      ">> Epoch 94 finished \tANN training loss 0.000193\n",
      ">> Epoch 95 finished \tANN training loss 0.000223\n",
      ">> Epoch 96 finished \tANN training loss 0.000148\n",
      ">> Epoch 97 finished \tANN training loss 0.000173\n",
      ">> Epoch 98 finished \tANN training loss 0.000326\n",
      ">> Epoch 99 finished \tANN training loss 0.000149\n",
      ">> Epoch 100 finished \tANN training loss 0.000151\n",
      ">> Epoch 101 finished \tANN training loss 0.000207\n",
      ">> Epoch 102 finished \tANN training loss 0.000122\n",
      ">> Epoch 103 finished \tANN training loss 0.000200\n",
      ">> Epoch 104 finished \tANN training loss 0.000382\n",
      ">> Epoch 105 finished \tANN training loss 0.000209\n",
      ">> Epoch 106 finished \tANN training loss 0.000221\n",
      ">> Epoch 107 finished \tANN training loss 0.000218\n",
      ">> Epoch 108 finished \tANN training loss 0.000260\n",
      ">> Epoch 109 finished \tANN training loss 0.000125\n",
      ">> Epoch 110 finished \tANN training loss 0.000343\n",
      ">> Epoch 111 finished \tANN training loss 0.000623\n",
      ">> Epoch 112 finished \tANN training loss 0.000095\n",
      ">> Epoch 113 finished \tANN training loss 0.000105\n",
      ">> Epoch 114 finished \tANN training loss 0.000071\n",
      ">> Epoch 115 finished \tANN training loss 0.000062\n",
      ">> Epoch 116 finished \tANN training loss 0.000169\n",
      ">> Epoch 117 finished \tANN training loss 0.000145\n",
      ">> Epoch 118 finished \tANN training loss 0.000192\n",
      ">> Epoch 119 finished \tANN training loss 0.000117\n",
      ">> Epoch 120 finished \tANN training loss 0.000171\n",
      ">> Epoch 121 finished \tANN training loss 0.000191\n",
      ">> Epoch 122 finished \tANN training loss 0.000118\n",
      ">> Epoch 123 finished \tANN training loss 0.000094\n",
      ">> Epoch 124 finished \tANN training loss 0.000074\n",
      ">> Epoch 125 finished \tANN training loss 0.000070\n",
      ">> Epoch 126 finished \tANN training loss 0.000067\n",
      ">> Epoch 127 finished \tANN training loss 0.000064\n",
      ">> Epoch 128 finished \tANN training loss 0.000055\n",
      ">> Epoch 129 finished \tANN training loss 0.000061\n",
      ">> Epoch 130 finished \tANN training loss 0.000671\n",
      ">> Epoch 131 finished \tANN training loss 0.000077\n",
      ">> Epoch 132 finished \tANN training loss 0.000054\n",
      ">> Epoch 133 finished \tANN training loss 0.000041\n",
      ">> Epoch 134 finished \tANN training loss 0.000039\n",
      ">> Epoch 135 finished \tANN training loss 0.000039\n",
      ">> Epoch 136 finished \tANN training loss 0.000039\n",
      ">> Epoch 137 finished \tANN training loss 0.000039\n",
      ">> Epoch 138 finished \tANN training loss 0.000046\n",
      ">> Epoch 139 finished \tANN training loss 0.000062\n",
      ">> Epoch 140 finished \tANN training loss 0.000059\n",
      ">> Epoch 141 finished \tANN training loss 0.000047\n",
      ">> Epoch 142 finished \tANN training loss 0.000041\n",
      ">> Epoch 143 finished \tANN training loss 0.000043\n",
      ">> Epoch 144 finished \tANN training loss 0.000034\n",
      ">> Epoch 145 finished \tANN training loss 0.000048\n",
      ">> Epoch 146 finished \tANN training loss 0.000047\n",
      ">> Epoch 147 finished \tANN training loss 0.000040\n",
      ">> Epoch 148 finished \tANN training loss 0.000031\n",
      ">> Epoch 149 finished \tANN training loss 0.000038\n",
      ">> Epoch 150 finished \tANN training loss 0.000040\n",
      ">> Epoch 151 finished \tANN training loss 0.000058\n",
      ">> Epoch 152 finished \tANN training loss 0.000082\n",
      ">> Epoch 153 finished \tANN training loss 0.000039\n",
      ">> Epoch 154 finished \tANN training loss 0.000050\n",
      ">> Epoch 155 finished \tANN training loss 0.000029\n",
      ">> Epoch 156 finished \tANN training loss 0.000069\n",
      ">> Epoch 157 finished \tANN training loss 0.000037\n",
      ">> Epoch 158 finished \tANN training loss 0.000034\n",
      ">> Epoch 159 finished \tANN training loss 0.000037\n",
      ">> Epoch 160 finished \tANN training loss 0.000052\n",
      ">> Epoch 161 finished \tANN training loss 0.000047\n",
      ">> Epoch 162 finished \tANN training loss 0.000044\n",
      ">> Epoch 163 finished \tANN training loss 0.000115\n",
      ">> Epoch 164 finished \tANN training loss 0.000112\n",
      ">> Epoch 165 finished \tANN training loss 0.000042\n",
      ">> Epoch 166 finished \tANN training loss 0.000036\n",
      ">> Epoch 167 finished \tANN training loss 0.000032\n",
      ">> Epoch 168 finished \tANN training loss 0.000038\n",
      ">> Epoch 169 finished \tANN training loss 0.000037\n",
      ">> Epoch 170 finished \tANN training loss 0.000031\n",
      ">> Epoch 171 finished \tANN training loss 0.000026\n",
      ">> Epoch 172 finished \tANN training loss 0.000032\n",
      ">> Epoch 173 finished \tANN training loss 0.000026\n",
      ">> Epoch 174 finished \tANN training loss 0.000021\n",
      ">> Epoch 175 finished \tANN training loss 0.000030\n",
      ">> Epoch 176 finished \tANN training loss 0.000026\n",
      ">> Epoch 177 finished \tANN training loss 0.000041\n",
      ">> Epoch 178 finished \tANN training loss 0.000048\n",
      ">> Epoch 179 finished \tANN training loss 0.000026\n",
      ">> Epoch 180 finished \tANN training loss 0.000031\n",
      ">> Epoch 181 finished \tANN training loss 0.000032\n",
      ">> Epoch 182 finished \tANN training loss 0.000028\n",
      ">> Epoch 183 finished \tANN training loss 0.000043\n",
      ">> Epoch 184 finished \tANN training loss 0.000038\n",
      ">> Epoch 185 finished \tANN training loss 0.000035\n",
      ">> Epoch 186 finished \tANN training loss 0.000058\n",
      ">> Epoch 187 finished \tANN training loss 0.000059\n",
      ">> Epoch 188 finished \tANN training loss 0.000053\n",
      ">> Epoch 189 finished \tANN training loss 0.000039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 190 finished \tANN training loss 0.000032\n",
      ">> Epoch 191 finished \tANN training loss 0.000029\n",
      ">> Epoch 192 finished \tANN training loss 0.000027\n",
      ">> Epoch 193 finished \tANN training loss 0.000025\n",
      ">> Epoch 194 finished \tANN training loss 0.000025\n",
      ">> Epoch 195 finished \tANN training loss 0.000021\n",
      ">> Epoch 196 finished \tANN training loss 0.000019\n",
      ">> Epoch 197 finished \tANN training loss 0.000022\n",
      ">> Epoch 198 finished \tANN training loss 0.000020\n",
      ">> Epoch 199 finished \tANN training loss 0.000027\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.905000\n"
     ]
    }
   ],
   "source": [
    "# 3-layer\n",
    "acc4 = deep_belief_net(hidden_layers_structure=[500, 200, 300], \n",
    "                       learning_rate_rbm=0.05,\n",
    "                       learning_rate=0.1,\n",
    "                       n_epochs_rbm=40,\n",
    "                       n_iter_backprop=200,\n",
    "                       batch_size=32,\n",
    "                       activation_function='relu',\n",
    "                       dropout_p=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.905\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(acc4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAE7CAYAAAAy451NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1cd03f7fc88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create bar graph\n",
    "plt.clf()\n",
    "objects = ('[200]', '[300]', '[100, 500]', '[500, 200, 300]')\n",
    "y_pos = np.arange(len(objects))\n",
    "performance = [0.9, 0.92, 0.905, 0.905]\n",
    "plt.bar(y_pos, performance, align='center', alpha=0.5)\n",
    "plt.xticks(y_pos, objects, rotation=30)\n",
    "plt.xlabel('Settings')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Benchmark 1 Settings')\n",
    " \n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark 2:\n",
    "Take the best performing one from benchmark 1 and test with the sigmoidal activation function and various dropouts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 44.978886\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 35.064987\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 29.802525\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 26.875952\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 24.820986\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 23.147341\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 21.914331\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 20.743584\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 19.739283\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 18.733677\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 18.087568\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 17.153889\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 16.515764\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 15.871891\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 15.334622\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 14.836199\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 14.399968\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 13.953169\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 13.539706\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 13.197330\n",
      ">> Epoch 21 finished \tRBM Reconstruction error 12.844109\n",
      ">> Epoch 22 finished \tRBM Reconstruction error 12.559888\n",
      ">> Epoch 23 finished \tRBM Reconstruction error 12.235383\n",
      ">> Epoch 24 finished \tRBM Reconstruction error 11.954041\n",
      ">> Epoch 25 finished \tRBM Reconstruction error 11.678492\n",
      ">> Epoch 26 finished \tRBM Reconstruction error 11.447932\n",
      ">> Epoch 27 finished \tRBM Reconstruction error 11.173759\n",
      ">> Epoch 28 finished \tRBM Reconstruction error 10.916043\n",
      ">> Epoch 29 finished \tRBM Reconstruction error 10.677363\n",
      ">> Epoch 30 finished \tRBM Reconstruction error 10.474979\n",
      ">> Epoch 31 finished \tRBM Reconstruction error 10.296218\n",
      ">> Epoch 32 finished \tRBM Reconstruction error 10.132094\n",
      ">> Epoch 33 finished \tRBM Reconstruction error 9.909482\n",
      ">> Epoch 34 finished \tRBM Reconstruction error 9.733149\n",
      ">> Epoch 35 finished \tRBM Reconstruction error 9.567850\n",
      ">> Epoch 36 finished \tRBM Reconstruction error 9.448261\n",
      ">> Epoch 37 finished \tRBM Reconstruction error 9.236425\n",
      ">> Epoch 38 finished \tRBM Reconstruction error 9.135662\n",
      ">> Epoch 39 finished \tRBM Reconstruction error 8.978279\n",
      ">> Epoch 40 finished \tRBM Reconstruction error 8.830650\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 1.059366\n",
      ">> Epoch 1 finished \tANN training loss 0.720585\n",
      ">> Epoch 2 finished \tANN training loss 0.576323\n",
      ">> Epoch 3 finished \tANN training loss 0.508379\n",
      ">> Epoch 4 finished \tANN training loss 0.452819\n",
      ">> Epoch 5 finished \tANN training loss 0.419483\n",
      ">> Epoch 6 finished \tANN training loss 0.389207\n",
      ">> Epoch 7 finished \tANN training loss 0.368384\n",
      ">> Epoch 8 finished \tANN training loss 0.353782\n",
      ">> Epoch 9 finished \tANN training loss 0.328215\n",
      ">> Epoch 10 finished \tANN training loss 0.317579\n",
      ">> Epoch 11 finished \tANN training loss 0.299478\n",
      ">> Epoch 12 finished \tANN training loss 0.286153\n",
      ">> Epoch 13 finished \tANN training loss 0.277577\n",
      ">> Epoch 14 finished \tANN training loss 0.269549\n",
      ">> Epoch 15 finished \tANN training loss 0.258548\n",
      ">> Epoch 16 finished \tANN training loss 0.252736\n",
      ">> Epoch 17 finished \tANN training loss 0.240714\n",
      ">> Epoch 18 finished \tANN training loss 0.235240\n",
      ">> Epoch 19 finished \tANN training loss 0.228007\n",
      ">> Epoch 20 finished \tANN training loss 0.221333\n",
      ">> Epoch 21 finished \tANN training loss 0.215079\n",
      ">> Epoch 22 finished \tANN training loss 0.208948\n",
      ">> Epoch 23 finished \tANN training loss 0.204645\n",
      ">> Epoch 24 finished \tANN training loss 0.195886\n",
      ">> Epoch 25 finished \tANN training loss 0.191143\n",
      ">> Epoch 26 finished \tANN training loss 0.186023\n",
      ">> Epoch 27 finished \tANN training loss 0.180871\n",
      ">> Epoch 28 finished \tANN training loss 0.177058\n",
      ">> Epoch 29 finished \tANN training loss 0.173154\n",
      ">> Epoch 30 finished \tANN training loss 0.168348\n",
      ">> Epoch 31 finished \tANN training loss 0.163330\n",
      ">> Epoch 32 finished \tANN training loss 0.159497\n",
      ">> Epoch 33 finished \tANN training loss 0.156670\n",
      ">> Epoch 34 finished \tANN training loss 0.152804\n",
      ">> Epoch 35 finished \tANN training loss 0.149355\n",
      ">> Epoch 36 finished \tANN training loss 0.144886\n",
      ">> Epoch 37 finished \tANN training loss 0.141691\n",
      ">> Epoch 38 finished \tANN training loss 0.138142\n",
      ">> Epoch 39 finished \tANN training loss 0.135929\n",
      ">> Epoch 40 finished \tANN training loss 0.132970\n",
      ">> Epoch 41 finished \tANN training loss 0.130139\n",
      ">> Epoch 42 finished \tANN training loss 0.126875\n",
      ">> Epoch 43 finished \tANN training loss 0.124062\n",
      ">> Epoch 44 finished \tANN training loss 0.121516\n",
      ">> Epoch 45 finished \tANN training loss 0.119083\n",
      ">> Epoch 46 finished \tANN training loss 0.115736\n",
      ">> Epoch 47 finished \tANN training loss 0.113316\n",
      ">> Epoch 48 finished \tANN training loss 0.111461\n",
      ">> Epoch 49 finished \tANN training loss 0.108518\n",
      ">> Epoch 50 finished \tANN training loss 0.107530\n",
      ">> Epoch 51 finished \tANN training loss 0.104256\n",
      ">> Epoch 52 finished \tANN training loss 0.101806\n",
      ">> Epoch 53 finished \tANN training loss 0.100084\n",
      ">> Epoch 54 finished \tANN training loss 0.097725\n",
      ">> Epoch 55 finished \tANN training loss 0.096299\n",
      ">> Epoch 56 finished \tANN training loss 0.095102\n",
      ">> Epoch 57 finished \tANN training loss 0.091962\n",
      ">> Epoch 58 finished \tANN training loss 0.090533\n",
      ">> Epoch 59 finished \tANN training loss 0.088415\n",
      ">> Epoch 60 finished \tANN training loss 0.087197\n",
      ">> Epoch 61 finished \tANN training loss 0.085696\n",
      ">> Epoch 62 finished \tANN training loss 0.083831\n",
      ">> Epoch 63 finished \tANN training loss 0.082041\n",
      ">> Epoch 64 finished \tANN training loss 0.080171\n",
      ">> Epoch 65 finished \tANN training loss 0.078968\n",
      ">> Epoch 66 finished \tANN training loss 0.077286\n",
      ">> Epoch 67 finished \tANN training loss 0.076095\n",
      ">> Epoch 68 finished \tANN training loss 0.074650\n",
      ">> Epoch 69 finished \tANN training loss 0.073128\n",
      ">> Epoch 70 finished \tANN training loss 0.071851\n",
      ">> Epoch 71 finished \tANN training loss 0.070724\n",
      ">> Epoch 72 finished \tANN training loss 0.069507\n",
      ">> Epoch 73 finished \tANN training loss 0.068230\n",
      ">> Epoch 74 finished \tANN training loss 0.067374\n",
      ">> Epoch 75 finished \tANN training loss 0.065934\n",
      ">> Epoch 76 finished \tANN training loss 0.064883\n",
      ">> Epoch 77 finished \tANN training loss 0.063910\n",
      ">> Epoch 78 finished \tANN training loss 0.062483\n",
      ">> Epoch 79 finished \tANN training loss 0.061581\n",
      ">> Epoch 80 finished \tANN training loss 0.060599\n",
      ">> Epoch 81 finished \tANN training loss 0.059575\n",
      ">> Epoch 82 finished \tANN training loss 0.058504\n",
      ">> Epoch 83 finished \tANN training loss 0.057707\n",
      ">> Epoch 84 finished \tANN training loss 0.056563\n",
      ">> Epoch 85 finished \tANN training loss 0.055735\n",
      ">> Epoch 86 finished \tANN training loss 0.054942\n",
      ">> Epoch 87 finished \tANN training loss 0.054162\n",
      ">> Epoch 88 finished \tANN training loss 0.053100\n",
      ">> Epoch 89 finished \tANN training loss 0.052387\n",
      ">> Epoch 90 finished \tANN training loss 0.051655\n",
      ">> Epoch 91 finished \tANN training loss 0.051044\n",
      ">> Epoch 92 finished \tANN training loss 0.050097\n",
      ">> Epoch 93 finished \tANN training loss 0.049394\n",
      ">> Epoch 94 finished \tANN training loss 0.048670\n",
      ">> Epoch 95 finished \tANN training loss 0.048230\n",
      ">> Epoch 96 finished \tANN training loss 0.047345\n",
      ">> Epoch 97 finished \tANN training loss 0.046534\n",
      ">> Epoch 98 finished \tANN training loss 0.046075\n",
      ">> Epoch 99 finished \tANN training loss 0.045411\n",
      ">> Epoch 100 finished \tANN training loss 0.044641\n",
      ">> Epoch 101 finished \tANN training loss 0.043997\n",
      ">> Epoch 102 finished \tANN training loss 0.043410\n",
      ">> Epoch 103 finished \tANN training loss 0.042910\n",
      ">> Epoch 104 finished \tANN training loss 0.042227\n",
      ">> Epoch 105 finished \tANN training loss 0.041661\n",
      ">> Epoch 106 finished \tANN training loss 0.041168\n",
      ">> Epoch 107 finished \tANN training loss 0.040615\n",
      ">> Epoch 108 finished \tANN training loss 0.040181\n",
      ">> Epoch 109 finished \tANN training loss 0.039569\n",
      ">> Epoch 110 finished \tANN training loss 0.039080\n",
      ">> Epoch 111 finished \tANN training loss 0.038646\n",
      ">> Epoch 112 finished \tANN training loss 0.038152\n",
      ">> Epoch 113 finished \tANN training loss 0.037687\n",
      ">> Epoch 114 finished \tANN training loss 0.037301\n",
      ">> Epoch 115 finished \tANN training loss 0.036786\n",
      ">> Epoch 116 finished \tANN training loss 0.036402\n",
      ">> Epoch 117 finished \tANN training loss 0.035859\n",
      ">> Epoch 118 finished \tANN training loss 0.035426\n",
      ">> Epoch 119 finished \tANN training loss 0.034989\n",
      ">> Epoch 120 finished \tANN training loss 0.034605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 121 finished \tANN training loss 0.034182\n",
      ">> Epoch 122 finished \tANN training loss 0.033898\n",
      ">> Epoch 123 finished \tANN training loss 0.033460\n",
      ">> Epoch 124 finished \tANN training loss 0.033117\n",
      ">> Epoch 125 finished \tANN training loss 0.032694\n",
      ">> Epoch 126 finished \tANN training loss 0.032297\n",
      ">> Epoch 127 finished \tANN training loss 0.031986\n",
      ">> Epoch 128 finished \tANN training loss 0.031597\n",
      ">> Epoch 129 finished \tANN training loss 0.031262\n",
      ">> Epoch 130 finished \tANN training loss 0.030999\n",
      ">> Epoch 131 finished \tANN training loss 0.030615\n",
      ">> Epoch 132 finished \tANN training loss 0.030283\n",
      ">> Epoch 133 finished \tANN training loss 0.029961\n",
      ">> Epoch 134 finished \tANN training loss 0.029701\n",
      ">> Epoch 135 finished \tANN training loss 0.029337\n",
      ">> Epoch 136 finished \tANN training loss 0.029031\n",
      ">> Epoch 137 finished \tANN training loss 0.028730\n",
      ">> Epoch 138 finished \tANN training loss 0.028474\n",
      ">> Epoch 139 finished \tANN training loss 0.028186\n",
      ">> Epoch 140 finished \tANN training loss 0.027884\n",
      ">> Epoch 141 finished \tANN training loss 0.027608\n",
      ">> Epoch 142 finished \tANN training loss 0.027324\n",
      ">> Epoch 143 finished \tANN training loss 0.027062\n",
      ">> Epoch 144 finished \tANN training loss 0.026830\n",
      ">> Epoch 145 finished \tANN training loss 0.026540\n",
      ">> Epoch 146 finished \tANN training loss 0.026304\n",
      ">> Epoch 147 finished \tANN training loss 0.026049\n",
      ">> Epoch 148 finished \tANN training loss 0.025802\n",
      ">> Epoch 149 finished \tANN training loss 0.025571\n",
      ">> Epoch 150 finished \tANN training loss 0.025335\n",
      ">> Epoch 151 finished \tANN training loss 0.025099\n",
      ">> Epoch 152 finished \tANN training loss 0.024860\n",
      ">> Epoch 153 finished \tANN training loss 0.024638\n",
      ">> Epoch 154 finished \tANN training loss 0.024423\n",
      ">> Epoch 155 finished \tANN training loss 0.024196\n",
      ">> Epoch 156 finished \tANN training loss 0.023975\n",
      ">> Epoch 157 finished \tANN training loss 0.023751\n",
      ">> Epoch 158 finished \tANN training loss 0.023558\n",
      ">> Epoch 159 finished \tANN training loss 0.023334\n",
      ">> Epoch 160 finished \tANN training loss 0.023134\n",
      ">> Epoch 161 finished \tANN training loss 0.022939\n",
      ">> Epoch 162 finished \tANN training loss 0.022745\n",
      ">> Epoch 163 finished \tANN training loss 0.022560\n",
      ">> Epoch 164 finished \tANN training loss 0.022366\n",
      ">> Epoch 165 finished \tANN training loss 0.022168\n",
      ">> Epoch 166 finished \tANN training loss 0.021991\n",
      ">> Epoch 167 finished \tANN training loss 0.021808\n",
      ">> Epoch 168 finished \tANN training loss 0.021625\n",
      ">> Epoch 169 finished \tANN training loss 0.021464\n",
      ">> Epoch 170 finished \tANN training loss 0.021280\n",
      ">> Epoch 171 finished \tANN training loss 0.021098\n",
      ">> Epoch 172 finished \tANN training loss 0.020937\n",
      ">> Epoch 173 finished \tANN training loss 0.020760\n",
      ">> Epoch 174 finished \tANN training loss 0.020600\n",
      ">> Epoch 175 finished \tANN training loss 0.020432\n",
      ">> Epoch 176 finished \tANN training loss 0.020273\n",
      ">> Epoch 177 finished \tANN training loss 0.020119\n",
      ">> Epoch 178 finished \tANN training loss 0.019959\n",
      ">> Epoch 179 finished \tANN training loss 0.019806\n",
      ">> Epoch 180 finished \tANN training loss 0.019655\n",
      ">> Epoch 181 finished \tANN training loss 0.019507\n",
      ">> Epoch 182 finished \tANN training loss 0.019356\n",
      ">> Epoch 183 finished \tANN training loss 0.019211\n",
      ">> Epoch 184 finished \tANN training loss 0.019066\n",
      ">> Epoch 185 finished \tANN training loss 0.018923\n",
      ">> Epoch 186 finished \tANN training loss 0.018785\n",
      ">> Epoch 187 finished \tANN training loss 0.018649\n",
      ">> Epoch 188 finished \tANN training loss 0.018511\n",
      ">> Epoch 189 finished \tANN training loss 0.018385\n",
      ">> Epoch 190 finished \tANN training loss 0.018245\n",
      ">> Epoch 191 finished \tANN training loss 0.018110\n",
      ">> Epoch 192 finished \tANN training loss 0.017983\n",
      ">> Epoch 193 finished \tANN training loss 0.017857\n",
      ">> Epoch 194 finished \tANN training loss 0.017730\n",
      ">> Epoch 195 finished \tANN training loss 0.017604\n",
      ">> Epoch 196 finished \tANN training loss 0.017482\n",
      ">> Epoch 197 finished \tANN training loss 0.017361\n",
      ">> Epoch 198 finished \tANN training loss 0.017240\n",
      ">> Epoch 199 finished \tANN training loss 0.017121\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.885000\n"
     ]
    }
   ],
   "source": [
    "# Sigmoidal activation, dropout = 0\n",
    "acc1 = deep_belief_net(hidden_layers_structure=[300], \n",
    "                       learning_rate_rbm=0.05,\n",
    "                       learning_rate=0.1,\n",
    "                       n_epochs_rbm=40,\n",
    "                       n_iter_backprop=200,\n",
    "                       batch_size=32,\n",
    "                       activation_function='sigmoid',\n",
    "                       dropout_p=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.885\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(acc1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 44.260117\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 35.192554\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 29.961706\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 26.800102\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 24.817398\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 23.394810\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 22.047796\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 20.963203\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 19.795736\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 18.846254\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 17.976402\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 17.134426\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 16.498646\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 15.851318\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 15.397351\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 14.870476\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 14.406140\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 13.980465\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 13.518435\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 13.194644\n",
      ">> Epoch 21 finished \tRBM Reconstruction error 12.848903\n",
      ">> Epoch 22 finished \tRBM Reconstruction error 12.519985\n",
      ">> Epoch 23 finished \tRBM Reconstruction error 12.247717\n",
      ">> Epoch 24 finished \tRBM Reconstruction error 11.962986\n",
      ">> Epoch 25 finished \tRBM Reconstruction error 11.697039\n",
      ">> Epoch 26 finished \tRBM Reconstruction error 11.470684\n",
      ">> Epoch 27 finished \tRBM Reconstruction error 11.205069\n",
      ">> Epoch 28 finished \tRBM Reconstruction error 10.994288\n",
      ">> Epoch 29 finished \tRBM Reconstruction error 10.739779\n",
      ">> Epoch 30 finished \tRBM Reconstruction error 10.553040\n",
      ">> Epoch 31 finished \tRBM Reconstruction error 10.393013\n",
      ">> Epoch 32 finished \tRBM Reconstruction error 10.134296\n",
      ">> Epoch 33 finished \tRBM Reconstruction error 9.982868\n",
      ">> Epoch 34 finished \tRBM Reconstruction error 9.832887\n",
      ">> Epoch 35 finished \tRBM Reconstruction error 9.677349\n",
      ">> Epoch 36 finished \tRBM Reconstruction error 9.445460\n",
      ">> Epoch 37 finished \tRBM Reconstruction error 9.336734\n",
      ">> Epoch 38 finished \tRBM Reconstruction error 9.187797\n",
      ">> Epoch 39 finished \tRBM Reconstruction error 9.029409\n",
      ">> Epoch 40 finished \tRBM Reconstruction error 8.905331\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 1.007746\n",
      ">> Epoch 1 finished \tANN training loss 0.713660\n",
      ">> Epoch 2 finished \tANN training loss 0.589967\n",
      ">> Epoch 3 finished \tANN training loss 0.507092\n",
      ">> Epoch 4 finished \tANN training loss 0.456064\n",
      ">> Epoch 5 finished \tANN training loss 0.423914\n",
      ">> Epoch 6 finished \tANN training loss 0.399210\n",
      ">> Epoch 7 finished \tANN training loss 0.371243\n",
      ">> Epoch 8 finished \tANN training loss 0.354643\n",
      ">> Epoch 9 finished \tANN training loss 0.335047\n",
      ">> Epoch 10 finished \tANN training loss 0.316869\n",
      ">> Epoch 11 finished \tANN training loss 0.303064\n",
      ">> Epoch 12 finished \tANN training loss 0.294619\n",
      ">> Epoch 13 finished \tANN training loss 0.287201\n",
      ">> Epoch 14 finished \tANN training loss 0.270468\n",
      ">> Epoch 15 finished \tANN training loss 0.261257\n",
      ">> Epoch 16 finished \tANN training loss 0.252111\n",
      ">> Epoch 17 finished \tANN training loss 0.254067\n",
      ">> Epoch 18 finished \tANN training loss 0.239964\n",
      ">> Epoch 19 finished \tANN training loss 0.233298\n",
      ">> Epoch 20 finished \tANN training loss 0.224330\n",
      ">> Epoch 21 finished \tANN training loss 0.218712\n",
      ">> Epoch 22 finished \tANN training loss 0.212346\n",
      ">> Epoch 23 finished \tANN training loss 0.205572\n",
      ">> Epoch 24 finished \tANN training loss 0.201879\n",
      ">> Epoch 25 finished \tANN training loss 0.195010\n",
      ">> Epoch 26 finished \tANN training loss 0.190675\n",
      ">> Epoch 27 finished \tANN training loss 0.185076\n",
      ">> Epoch 28 finished \tANN training loss 0.180749\n",
      ">> Epoch 29 finished \tANN training loss 0.175511\n",
      ">> Epoch 30 finished \tANN training loss 0.170807\n",
      ">> Epoch 31 finished \tANN training loss 0.167809\n",
      ">> Epoch 32 finished \tANN training loss 0.163036\n",
      ">> Epoch 33 finished \tANN training loss 0.159895\n",
      ">> Epoch 34 finished \tANN training loss 0.156336\n",
      ">> Epoch 35 finished \tANN training loss 0.152482\n",
      ">> Epoch 36 finished \tANN training loss 0.149658\n",
      ">> Epoch 37 finished \tANN training loss 0.146000\n",
      ">> Epoch 38 finished \tANN training loss 0.141716\n",
      ">> Epoch 39 finished \tANN training loss 0.141648\n",
      ">> Epoch 40 finished \tANN training loss 0.135041\n",
      ">> Epoch 41 finished \tANN training loss 0.131795\n",
      ">> Epoch 42 finished \tANN training loss 0.129641\n",
      ">> Epoch 43 finished \tANN training loss 0.127404\n",
      ">> Epoch 44 finished \tANN training loss 0.125876\n",
      ">> Epoch 45 finished \tANN training loss 0.121129\n",
      ">> Epoch 46 finished \tANN training loss 0.119839\n",
      ">> Epoch 47 finished \tANN training loss 0.117801\n",
      ">> Epoch 48 finished \tANN training loss 0.115641\n",
      ">> Epoch 49 finished \tANN training loss 0.111761\n",
      ">> Epoch 50 finished \tANN training loss 0.109673\n",
      ">> Epoch 51 finished \tANN training loss 0.106887\n",
      ">> Epoch 52 finished \tANN training loss 0.104325\n",
      ">> Epoch 53 finished \tANN training loss 0.103820\n",
      ">> Epoch 54 finished \tANN training loss 0.102717\n",
      ">> Epoch 55 finished \tANN training loss 0.097918\n",
      ">> Epoch 56 finished \tANN training loss 0.096818\n",
      ">> Epoch 57 finished \tANN training loss 0.094471\n",
      ">> Epoch 58 finished \tANN training loss 0.092703\n",
      ">> Epoch 59 finished \tANN training loss 0.091341\n",
      ">> Epoch 60 finished \tANN training loss 0.088968\n",
      ">> Epoch 61 finished \tANN training loss 0.086384\n",
      ">> Epoch 62 finished \tANN training loss 0.085044\n",
      ">> Epoch 63 finished \tANN training loss 0.084166\n",
      ">> Epoch 64 finished \tANN training loss 0.082316\n",
      ">> Epoch 65 finished \tANN training loss 0.080008\n",
      ">> Epoch 66 finished \tANN training loss 0.078601\n",
      ">> Epoch 67 finished \tANN training loss 0.077741\n",
      ">> Epoch 68 finished \tANN training loss 0.075995\n",
      ">> Epoch 69 finished \tANN training loss 0.073865\n",
      ">> Epoch 70 finished \tANN training loss 0.072436\n",
      ">> Epoch 71 finished \tANN training loss 0.071137\n",
      ">> Epoch 72 finished \tANN training loss 0.070434\n",
      ">> Epoch 73 finished \tANN training loss 0.069817\n",
      ">> Epoch 74 finished \tANN training loss 0.067969\n",
      ">> Epoch 75 finished \tANN training loss 0.066689\n",
      ">> Epoch 76 finished \tANN training loss 0.065018\n",
      ">> Epoch 77 finished \tANN training loss 0.063822\n",
      ">> Epoch 78 finished \tANN training loss 0.062932\n",
      ">> Epoch 79 finished \tANN training loss 0.061710\n",
      ">> Epoch 80 finished \tANN training loss 0.060961\n",
      ">> Epoch 81 finished \tANN training loss 0.059906\n",
      ">> Epoch 82 finished \tANN training loss 0.058800\n",
      ">> Epoch 83 finished \tANN training loss 0.057310\n",
      ">> Epoch 84 finished \tANN training loss 0.056255\n",
      ">> Epoch 85 finished \tANN training loss 0.058016\n",
      ">> Epoch 86 finished \tANN training loss 0.054740\n",
      ">> Epoch 87 finished \tANN training loss 0.053552\n",
      ">> Epoch 88 finished \tANN training loss 0.052823\n",
      ">> Epoch 89 finished \tANN training loss 0.051639\n",
      ">> Epoch 90 finished \tANN training loss 0.051049\n",
      ">> Epoch 91 finished \tANN training loss 0.050649\n",
      ">> Epoch 92 finished \tANN training loss 0.049994\n",
      ">> Epoch 93 finished \tANN training loss 0.048658\n",
      ">> Epoch 94 finished \tANN training loss 0.047739\n",
      ">> Epoch 95 finished \tANN training loss 0.047055\n",
      ">> Epoch 96 finished \tANN training loss 0.046979\n",
      ">> Epoch 97 finished \tANN training loss 0.045918\n",
      ">> Epoch 98 finished \tANN training loss 0.044987\n",
      ">> Epoch 99 finished \tANN training loss 0.045042\n",
      ">> Epoch 100 finished \tANN training loss 0.043815\n",
      ">> Epoch 101 finished \tANN training loss 0.043272\n",
      ">> Epoch 102 finished \tANN training loss 0.042332\n",
      ">> Epoch 103 finished \tANN training loss 0.041709\n",
      ">> Epoch 104 finished \tANN training loss 0.041419\n",
      ">> Epoch 105 finished \tANN training loss 0.040349\n",
      ">> Epoch 106 finished \tANN training loss 0.040287\n",
      ">> Epoch 107 finished \tANN training loss 0.039639\n",
      ">> Epoch 108 finished \tANN training loss 0.038552\n",
      ">> Epoch 109 finished \tANN training loss 0.038260\n",
      ">> Epoch 110 finished \tANN training loss 0.037713\n",
      ">> Epoch 111 finished \tANN training loss 0.037437\n",
      ">> Epoch 112 finished \tANN training loss 0.036814\n",
      ">> Epoch 113 finished \tANN training loss 0.036073\n",
      ">> Epoch 114 finished \tANN training loss 0.035448\n",
      ">> Epoch 115 finished \tANN training loss 0.035394\n",
      ">> Epoch 116 finished \tANN training loss 0.034451\n",
      ">> Epoch 117 finished \tANN training loss 0.033980\n",
      ">> Epoch 118 finished \tANN training loss 0.033906\n",
      ">> Epoch 119 finished \tANN training loss 0.033660\n",
      ">> Epoch 120 finished \tANN training loss 0.032951\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 121 finished \tANN training loss 0.032556\n",
      ">> Epoch 122 finished \tANN training loss 0.032018\n",
      ">> Epoch 123 finished \tANN training loss 0.031477\n",
      ">> Epoch 124 finished \tANN training loss 0.030894\n",
      ">> Epoch 125 finished \tANN training loss 0.030610\n",
      ">> Epoch 126 finished \tANN training loss 0.030331\n",
      ">> Epoch 127 finished \tANN training loss 0.029928\n",
      ">> Epoch 128 finished \tANN training loss 0.029626\n",
      ">> Epoch 129 finished \tANN training loss 0.029394\n",
      ">> Epoch 130 finished \tANN training loss 0.028863\n",
      ">> Epoch 131 finished \tANN training loss 0.028396\n",
      ">> Epoch 132 finished \tANN training loss 0.028059\n",
      ">> Epoch 133 finished \tANN training loss 0.027879\n",
      ">> Epoch 134 finished \tANN training loss 0.027217\n",
      ">> Epoch 135 finished \tANN training loss 0.026838\n",
      ">> Epoch 136 finished \tANN training loss 0.026872\n",
      ">> Epoch 137 finished \tANN training loss 0.026302\n",
      ">> Epoch 138 finished \tANN training loss 0.025856\n",
      ">> Epoch 139 finished \tANN training loss 0.025802\n",
      ">> Epoch 140 finished \tANN training loss 0.025997\n",
      ">> Epoch 141 finished \tANN training loss 0.025253\n",
      ">> Epoch 142 finished \tANN training loss 0.025097\n",
      ">> Epoch 143 finished \tANN training loss 0.024676\n",
      ">> Epoch 144 finished \tANN training loss 0.024377\n",
      ">> Epoch 145 finished \tANN training loss 0.023999\n",
      ">> Epoch 146 finished \tANN training loss 0.023892\n",
      ">> Epoch 147 finished \tANN training loss 0.023500\n",
      ">> Epoch 148 finished \tANN training loss 0.023084\n",
      ">> Epoch 149 finished \tANN training loss 0.022898\n",
      ">> Epoch 150 finished \tANN training loss 0.022741\n",
      ">> Epoch 151 finished \tANN training loss 0.022486\n",
      ">> Epoch 152 finished \tANN training loss 0.022017\n",
      ">> Epoch 153 finished \tANN training loss 0.021829\n",
      ">> Epoch 154 finished \tANN training loss 0.021505\n",
      ">> Epoch 155 finished \tANN training loss 0.021365\n",
      ">> Epoch 156 finished \tANN training loss 0.021389\n",
      ">> Epoch 157 finished \tANN training loss 0.021049\n",
      ">> Epoch 158 finished \tANN training loss 0.020777\n",
      ">> Epoch 159 finished \tANN training loss 0.020614\n",
      ">> Epoch 160 finished \tANN training loss 0.020214\n",
      ">> Epoch 161 finished \tANN training loss 0.020221\n",
      ">> Epoch 162 finished \tANN training loss 0.019769\n",
      ">> Epoch 163 finished \tANN training loss 0.019639\n",
      ">> Epoch 164 finished \tANN training loss 0.019366\n",
      ">> Epoch 165 finished \tANN training loss 0.019223\n",
      ">> Epoch 166 finished \tANN training loss 0.019122\n",
      ">> Epoch 167 finished \tANN training loss 0.018791\n",
      ">> Epoch 168 finished \tANN training loss 0.018652\n",
      ">> Epoch 169 finished \tANN training loss 0.018470\n",
      ">> Epoch 170 finished \tANN training loss 0.018272\n",
      ">> Epoch 171 finished \tANN training loss 0.018165\n",
      ">> Epoch 172 finished \tANN training loss 0.017990\n",
      ">> Epoch 173 finished \tANN training loss 0.018009\n",
      ">> Epoch 174 finished \tANN training loss 0.017530\n",
      ">> Epoch 175 finished \tANN training loss 0.017341\n",
      ">> Epoch 176 finished \tANN training loss 0.017174\n",
      ">> Epoch 177 finished \tANN training loss 0.017167\n",
      ">> Epoch 178 finished \tANN training loss 0.016956\n",
      ">> Epoch 179 finished \tANN training loss 0.016857\n",
      ">> Epoch 180 finished \tANN training loss 0.016559\n",
      ">> Epoch 181 finished \tANN training loss 0.016506\n",
      ">> Epoch 182 finished \tANN training loss 0.016211\n",
      ">> Epoch 183 finished \tANN training loss 0.016135\n",
      ">> Epoch 184 finished \tANN training loss 0.015914\n",
      ">> Epoch 185 finished \tANN training loss 0.015859\n",
      ">> Epoch 186 finished \tANN training loss 0.015627\n",
      ">> Epoch 187 finished \tANN training loss 0.015765\n",
      ">> Epoch 188 finished \tANN training loss 0.015330\n",
      ">> Epoch 189 finished \tANN training loss 0.015262\n",
      ">> Epoch 190 finished \tANN training loss 0.015225\n",
      ">> Epoch 191 finished \tANN training loss 0.015007\n",
      ">> Epoch 192 finished \tANN training loss 0.014917\n",
      ">> Epoch 193 finished \tANN training loss 0.014645\n",
      ">> Epoch 194 finished \tANN training loss 0.014746\n",
      ">> Epoch 195 finished \tANN training loss 0.014429\n",
      ">> Epoch 196 finished \tANN training loss 0.014312\n",
      ">> Epoch 197 finished \tANN training loss 0.014340\n",
      ">> Epoch 198 finished \tANN training loss 0.014098\n",
      ">> Epoch 199 finished \tANN training loss 0.014554\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.920000\n"
     ]
    }
   ],
   "source": [
    "# Sigmoidal activation, dropout = 0.1\n",
    "acc2 = deep_belief_net(hidden_layers_structure=[300], \n",
    "                       learning_rate_rbm=0.05,\n",
    "                       learning_rate=0.1,\n",
    "                       n_epochs_rbm=40,\n",
    "                       n_iter_backprop=200,\n",
    "                       batch_size=32,\n",
    "                       activation_function='sigmoid',\n",
    "                       dropout_p=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.92\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(acc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 43.777512\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 34.819828\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 29.929005\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 26.783167\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 24.906950\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 23.168297\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 21.832165\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 20.717415\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 19.738537\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 18.720924\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 17.846592\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 17.117878\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 16.533508\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 15.922471\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 15.373255\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 14.934807\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 14.433249\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 13.970982\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 13.633357\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 13.290371\n",
      ">> Epoch 21 finished \tRBM Reconstruction error 12.880840\n",
      ">> Epoch 22 finished \tRBM Reconstruction error 12.614532\n",
      ">> Epoch 23 finished \tRBM Reconstruction error 12.266057\n",
      ">> Epoch 24 finished \tRBM Reconstruction error 11.948356\n",
      ">> Epoch 25 finished \tRBM Reconstruction error 11.696796\n",
      ">> Epoch 26 finished \tRBM Reconstruction error 11.445100\n",
      ">> Epoch 27 finished \tRBM Reconstruction error 11.176467\n",
      ">> Epoch 28 finished \tRBM Reconstruction error 11.006009\n",
      ">> Epoch 29 finished \tRBM Reconstruction error 10.782668\n",
      ">> Epoch 30 finished \tRBM Reconstruction error 10.539347\n",
      ">> Epoch 31 finished \tRBM Reconstruction error 10.344507\n",
      ">> Epoch 32 finished \tRBM Reconstruction error 10.168985\n",
      ">> Epoch 33 finished \tRBM Reconstruction error 9.972208\n",
      ">> Epoch 34 finished \tRBM Reconstruction error 9.822224\n",
      ">> Epoch 35 finished \tRBM Reconstruction error 9.626230\n",
      ">> Epoch 36 finished \tRBM Reconstruction error 9.475251\n",
      ">> Epoch 37 finished \tRBM Reconstruction error 9.329857\n",
      ">> Epoch 38 finished \tRBM Reconstruction error 9.136558\n",
      ">> Epoch 39 finished \tRBM Reconstruction error 9.002902\n",
      ">> Epoch 40 finished \tRBM Reconstruction error 8.850791\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 1.065697\n",
      ">> Epoch 1 finished \tANN training loss 0.717583\n",
      ">> Epoch 2 finished \tANN training loss 0.600794\n",
      ">> Epoch 3 finished \tANN training loss 0.527482\n",
      ">> Epoch 4 finished \tANN training loss 0.467975\n",
      ">> Epoch 5 finished \tANN training loss 0.450443\n",
      ">> Epoch 6 finished \tANN training loss 0.405933\n",
      ">> Epoch 7 finished \tANN training loss 0.383285\n",
      ">> Epoch 8 finished \tANN training loss 0.361676\n",
      ">> Epoch 9 finished \tANN training loss 0.348030\n",
      ">> Epoch 10 finished \tANN training loss 0.329410\n",
      ">> Epoch 11 finished \tANN training loss 0.318654\n",
      ">> Epoch 12 finished \tANN training loss 0.301345\n",
      ">> Epoch 13 finished \tANN training loss 0.296071\n",
      ">> Epoch 14 finished \tANN training loss 0.281912\n",
      ">> Epoch 15 finished \tANN training loss 0.277744\n",
      ">> Epoch 16 finished \tANN training loss 0.262597\n",
      ">> Epoch 17 finished \tANN training loss 0.260922\n",
      ">> Epoch 18 finished \tANN training loss 0.247801\n",
      ">> Epoch 19 finished \tANN training loss 0.241431\n",
      ">> Epoch 20 finished \tANN training loss 0.232367\n",
      ">> Epoch 21 finished \tANN training loss 0.229119\n",
      ">> Epoch 22 finished \tANN training loss 0.224992\n",
      ">> Epoch 23 finished \tANN training loss 0.221587\n",
      ">> Epoch 24 finished \tANN training loss 0.210380\n",
      ">> Epoch 25 finished \tANN training loss 0.206023\n",
      ">> Epoch 26 finished \tANN training loss 0.201842\n",
      ">> Epoch 27 finished \tANN training loss 0.197195\n",
      ">> Epoch 28 finished \tANN training loss 0.193826\n",
      ">> Epoch 29 finished \tANN training loss 0.191098\n",
      ">> Epoch 30 finished \tANN training loss 0.180707\n",
      ">> Epoch 31 finished \tANN training loss 0.178848\n",
      ">> Epoch 32 finished \tANN training loss 0.175316\n",
      ">> Epoch 33 finished \tANN training loss 0.169990\n",
      ">> Epoch 34 finished \tANN training loss 0.168140\n",
      ">> Epoch 35 finished \tANN training loss 0.162141\n",
      ">> Epoch 36 finished \tANN training loss 0.158436\n",
      ">> Epoch 37 finished \tANN training loss 0.159946\n",
      ">> Epoch 38 finished \tANN training loss 0.152144\n",
      ">> Epoch 39 finished \tANN training loss 0.151135\n",
      ">> Epoch 40 finished \tANN training loss 0.146563\n",
      ">> Epoch 41 finished \tANN training loss 0.143737\n",
      ">> Epoch 42 finished \tANN training loss 0.140931\n",
      ">> Epoch 43 finished \tANN training loss 0.136631\n",
      ">> Epoch 44 finished \tANN training loss 0.133715\n",
      ">> Epoch 45 finished \tANN training loss 0.132241\n",
      ">> Epoch 46 finished \tANN training loss 0.129652\n",
      ">> Epoch 47 finished \tANN training loss 0.127489\n",
      ">> Epoch 48 finished \tANN training loss 0.125692\n",
      ">> Epoch 49 finished \tANN training loss 0.120991\n",
      ">> Epoch 50 finished \tANN training loss 0.119548\n",
      ">> Epoch 51 finished \tANN training loss 0.117144\n",
      ">> Epoch 52 finished \tANN training loss 0.115995\n",
      ">> Epoch 53 finished \tANN training loss 0.116354\n",
      ">> Epoch 54 finished \tANN training loss 0.109835\n",
      ">> Epoch 55 finished \tANN training loss 0.109501\n",
      ">> Epoch 56 finished \tANN training loss 0.104453\n",
      ">> Epoch 57 finished \tANN training loss 0.102877\n",
      ">> Epoch 58 finished \tANN training loss 0.101962\n",
      ">> Epoch 59 finished \tANN training loss 0.098809\n",
      ">> Epoch 60 finished \tANN training loss 0.098519\n",
      ">> Epoch 61 finished \tANN training loss 0.094914\n",
      ">> Epoch 62 finished \tANN training loss 0.093527\n",
      ">> Epoch 63 finished \tANN training loss 0.091179\n",
      ">> Epoch 64 finished \tANN training loss 0.089851\n",
      ">> Epoch 65 finished \tANN training loss 0.088348\n",
      ">> Epoch 66 finished \tANN training loss 0.086305\n",
      ">> Epoch 67 finished \tANN training loss 0.085820\n",
      ">> Epoch 68 finished \tANN training loss 0.083887\n",
      ">> Epoch 69 finished \tANN training loss 0.083223\n",
      ">> Epoch 70 finished \tANN training loss 0.081161\n",
      ">> Epoch 71 finished \tANN training loss 0.080457\n",
      ">> Epoch 72 finished \tANN training loss 0.080812\n",
      ">> Epoch 73 finished \tANN training loss 0.077779\n",
      ">> Epoch 74 finished \tANN training loss 0.076037\n",
      ">> Epoch 75 finished \tANN training loss 0.073930\n",
      ">> Epoch 76 finished \tANN training loss 0.073253\n",
      ">> Epoch 77 finished \tANN training loss 0.071949\n",
      ">> Epoch 78 finished \tANN training loss 0.071994\n",
      ">> Epoch 79 finished \tANN training loss 0.071813\n",
      ">> Epoch 80 finished \tANN training loss 0.068681\n",
      ">> Epoch 81 finished \tANN training loss 0.066972\n",
      ">> Epoch 82 finished \tANN training loss 0.066640\n",
      ">> Epoch 83 finished \tANN training loss 0.065313\n",
      ">> Epoch 84 finished \tANN training loss 0.064598\n",
      ">> Epoch 85 finished \tANN training loss 0.062437\n",
      ">> Epoch 86 finished \tANN training loss 0.062538\n",
      ">> Epoch 87 finished \tANN training loss 0.061102\n",
      ">> Epoch 88 finished \tANN training loss 0.059184\n",
      ">> Epoch 89 finished \tANN training loss 0.058145\n",
      ">> Epoch 90 finished \tANN training loss 0.058763\n",
      ">> Epoch 91 finished \tANN training loss 0.056060\n",
      ">> Epoch 92 finished \tANN training loss 0.055468\n",
      ">> Epoch 93 finished \tANN training loss 0.054332\n",
      ">> Epoch 94 finished \tANN training loss 0.054812\n",
      ">> Epoch 95 finished \tANN training loss 0.052975\n",
      ">> Epoch 96 finished \tANN training loss 0.051502\n",
      ">> Epoch 97 finished \tANN training loss 0.050517\n",
      ">> Epoch 98 finished \tANN training loss 0.049684\n",
      ">> Epoch 99 finished \tANN training loss 0.049138\n",
      ">> Epoch 100 finished \tANN training loss 0.049206\n",
      ">> Epoch 101 finished \tANN training loss 0.047021\n",
      ">> Epoch 102 finished \tANN training loss 0.046450\n",
      ">> Epoch 103 finished \tANN training loss 0.045885\n",
      ">> Epoch 104 finished \tANN training loss 0.045520\n",
      ">> Epoch 105 finished \tANN training loss 0.044633\n",
      ">> Epoch 106 finished \tANN training loss 0.044794\n",
      ">> Epoch 107 finished \tANN training loss 0.043565\n",
      ">> Epoch 108 finished \tANN training loss 0.042984\n",
      ">> Epoch 109 finished \tANN training loss 0.043377\n",
      ">> Epoch 110 finished \tANN training loss 0.041503\n",
      ">> Epoch 111 finished \tANN training loss 0.041402\n",
      ">> Epoch 112 finished \tANN training loss 0.040439\n",
      ">> Epoch 113 finished \tANN training loss 0.040754\n",
      ">> Epoch 114 finished \tANN training loss 0.039418\n",
      ">> Epoch 115 finished \tANN training loss 0.038984\n",
      ">> Epoch 116 finished \tANN training loss 0.038889\n",
      ">> Epoch 117 finished \tANN training loss 0.038442\n",
      ">> Epoch 118 finished \tANN training loss 0.037643\n",
      ">> Epoch 119 finished \tANN training loss 0.036852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 120 finished \tANN training loss 0.036258\n",
      ">> Epoch 121 finished \tANN training loss 0.035302\n",
      ">> Epoch 122 finished \tANN training loss 0.034379\n",
      ">> Epoch 123 finished \tANN training loss 0.034103\n",
      ">> Epoch 124 finished \tANN training loss 0.034058\n",
      ">> Epoch 125 finished \tANN training loss 0.033410\n",
      ">> Epoch 126 finished \tANN training loss 0.033458\n",
      ">> Epoch 127 finished \tANN training loss 0.033981\n",
      ">> Epoch 128 finished \tANN training loss 0.032542\n",
      ">> Epoch 129 finished \tANN training loss 0.032299\n",
      ">> Epoch 130 finished \tANN training loss 0.031160\n",
      ">> Epoch 131 finished \tANN training loss 0.030333\n",
      ">> Epoch 132 finished \tANN training loss 0.030050\n",
      ">> Epoch 133 finished \tANN training loss 0.030630\n",
      ">> Epoch 134 finished \tANN training loss 0.029221\n",
      ">> Epoch 135 finished \tANN training loss 0.029662\n",
      ">> Epoch 136 finished \tANN training loss 0.028692\n",
      ">> Epoch 137 finished \tANN training loss 0.028272\n",
      ">> Epoch 138 finished \tANN training loss 0.028279\n",
      ">> Epoch 139 finished \tANN training loss 0.028901\n",
      ">> Epoch 140 finished \tANN training loss 0.027454\n",
      ">> Epoch 141 finished \tANN training loss 0.026754\n",
      ">> Epoch 142 finished \tANN training loss 0.027001\n",
      ">> Epoch 143 finished \tANN training loss 0.026237\n",
      ">> Epoch 144 finished \tANN training loss 0.025722\n",
      ">> Epoch 145 finished \tANN training loss 0.025650\n",
      ">> Epoch 146 finished \tANN training loss 0.025234\n",
      ">> Epoch 147 finished \tANN training loss 0.025666\n",
      ">> Epoch 148 finished \tANN training loss 0.024611\n",
      ">> Epoch 149 finished \tANN training loss 0.025244\n",
      ">> Epoch 150 finished \tANN training loss 0.024567\n",
      ">> Epoch 151 finished \tANN training loss 0.023598\n",
      ">> Epoch 152 finished \tANN training loss 0.023260\n",
      ">> Epoch 153 finished \tANN training loss 0.023109\n",
      ">> Epoch 154 finished \tANN training loss 0.022550\n",
      ">> Epoch 155 finished \tANN training loss 0.023260\n",
      ">> Epoch 156 finished \tANN training loss 0.022206\n",
      ">> Epoch 157 finished \tANN training loss 0.021841\n",
      ">> Epoch 158 finished \tANN training loss 0.021922\n",
      ">> Epoch 159 finished \tANN training loss 0.021165\n",
      ">> Epoch 160 finished \tANN training loss 0.021195\n",
      ">> Epoch 161 finished \tANN training loss 0.020878\n",
      ">> Epoch 162 finished \tANN training loss 0.020912\n",
      ">> Epoch 163 finished \tANN training loss 0.020570\n",
      ">> Epoch 164 finished \tANN training loss 0.020186\n",
      ">> Epoch 165 finished \tANN training loss 0.020545\n",
      ">> Epoch 166 finished \tANN training loss 0.020354\n",
      ">> Epoch 167 finished \tANN training loss 0.019553\n",
      ">> Epoch 168 finished \tANN training loss 0.019099\n",
      ">> Epoch 169 finished \tANN training loss 0.019211\n",
      ">> Epoch 170 finished \tANN training loss 0.019486\n",
      ">> Epoch 171 finished \tANN training loss 0.018621\n",
      ">> Epoch 172 finished \tANN training loss 0.018918\n",
      ">> Epoch 173 finished \tANN training loss 0.018962\n",
      ">> Epoch 174 finished \tANN training loss 0.017976\n",
      ">> Epoch 175 finished \tANN training loss 0.018078\n",
      ">> Epoch 176 finished \tANN training loss 0.017955\n",
      ">> Epoch 177 finished \tANN training loss 0.017690\n",
      ">> Epoch 178 finished \tANN training loss 0.017473\n",
      ">> Epoch 179 finished \tANN training loss 0.017312\n",
      ">> Epoch 180 finished \tANN training loss 0.017008\n",
      ">> Epoch 181 finished \tANN training loss 0.016748\n",
      ">> Epoch 182 finished \tANN training loss 0.016467\n",
      ">> Epoch 183 finished \tANN training loss 0.016673\n",
      ">> Epoch 184 finished \tANN training loss 0.016249\n",
      ">> Epoch 185 finished \tANN training loss 0.016325\n",
      ">> Epoch 186 finished \tANN training loss 0.016157\n",
      ">> Epoch 187 finished \tANN training loss 0.015667\n",
      ">> Epoch 188 finished \tANN training loss 0.015258\n",
      ">> Epoch 189 finished \tANN training loss 0.015330\n",
      ">> Epoch 190 finished \tANN training loss 0.014973\n",
      ">> Epoch 191 finished \tANN training loss 0.014862\n",
      ">> Epoch 192 finished \tANN training loss 0.014803\n",
      ">> Epoch 193 finished \tANN training loss 0.014689\n",
      ">> Epoch 194 finished \tANN training loss 0.014628\n",
      ">> Epoch 195 finished \tANN training loss 0.014397\n",
      ">> Epoch 196 finished \tANN training loss 0.014091\n",
      ">> Epoch 197 finished \tANN training loss 0.013958\n",
      ">> Epoch 198 finished \tANN training loss 0.014125\n",
      ">> Epoch 199 finished \tANN training loss 0.014154\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.895000\n"
     ]
    }
   ],
   "source": [
    "# Sigmoidal activation, dropout = 0.2\n",
    "acc3 = deep_belief_net(hidden_layers_structure=[300], \n",
    "                       learning_rate_rbm=0.05,\n",
    "                       learning_rate=0.1,\n",
    "                       n_epochs_rbm=40,\n",
    "                       n_iter_backprop=200,\n",
    "                       batch_size=32,\n",
    "                       activation_function='sigmoid',\n",
    "                       dropout_p=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.895\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(acc3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 44.225895\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 34.718281\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 29.788921\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 26.949614\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 24.911928\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 23.260574\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 21.936123\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 20.846138\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 19.758648\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 18.762615\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 17.996824\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 17.189198\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 16.503515\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 15.971636\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 15.345450\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 14.842959\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 14.420224\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 14.055085\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 13.568018\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 13.243212\n",
      ">> Epoch 21 finished \tRBM Reconstruction error 12.880724\n",
      ">> Epoch 22 finished \tRBM Reconstruction error 12.603806\n",
      ">> Epoch 23 finished \tRBM Reconstruction error 12.256539\n",
      ">> Epoch 24 finished \tRBM Reconstruction error 12.033975\n",
      ">> Epoch 25 finished \tRBM Reconstruction error 11.768235\n",
      ">> Epoch 26 finished \tRBM Reconstruction error 11.515949\n",
      ">> Epoch 27 finished \tRBM Reconstruction error 11.250635\n",
      ">> Epoch 28 finished \tRBM Reconstruction error 11.042171\n",
      ">> Epoch 29 finished \tRBM Reconstruction error 10.783718\n",
      ">> Epoch 30 finished \tRBM Reconstruction error 10.576234\n",
      ">> Epoch 31 finished \tRBM Reconstruction error 10.382644\n",
      ">> Epoch 32 finished \tRBM Reconstruction error 10.254204\n",
      ">> Epoch 33 finished \tRBM Reconstruction error 10.007308\n",
      ">> Epoch 34 finished \tRBM Reconstruction error 9.881972\n",
      ">> Epoch 35 finished \tRBM Reconstruction error 9.700727\n",
      ">> Epoch 36 finished \tRBM Reconstruction error 9.511007\n",
      ">> Epoch 37 finished \tRBM Reconstruction error 9.342621\n",
      ">> Epoch 38 finished \tRBM Reconstruction error 9.188943\n",
      ">> Epoch 39 finished \tRBM Reconstruction error 9.156702\n",
      ">> Epoch 40 finished \tRBM Reconstruction error 8.973461\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 1.062474\n",
      ">> Epoch 1 finished \tANN training loss 0.760325\n",
      ">> Epoch 2 finished \tANN training loss 0.615764\n",
      ">> Epoch 3 finished \tANN training loss 0.523059\n",
      ">> Epoch 4 finished \tANN training loss 0.485734\n",
      ">> Epoch 5 finished \tANN training loss 0.436973\n",
      ">> Epoch 6 finished \tANN training loss 0.420036\n",
      ">> Epoch 7 finished \tANN training loss 0.385624\n",
      ">> Epoch 8 finished \tANN training loss 0.373335\n",
      ">> Epoch 9 finished \tANN training loss 0.348785\n",
      ">> Epoch 10 finished \tANN training loss 0.339566\n",
      ">> Epoch 11 finished \tANN training loss 0.327256\n",
      ">> Epoch 12 finished \tANN training loss 0.311299\n",
      ">> Epoch 13 finished \tANN training loss 0.303708\n",
      ">> Epoch 14 finished \tANN training loss 0.292219\n",
      ">> Epoch 15 finished \tANN training loss 0.283522\n",
      ">> Epoch 16 finished \tANN training loss 0.277875\n",
      ">> Epoch 17 finished \tANN training loss 0.267163\n",
      ">> Epoch 18 finished \tANN training loss 0.260130\n",
      ">> Epoch 19 finished \tANN training loss 0.255586\n",
      ">> Epoch 20 finished \tANN training loss 0.248993\n",
      ">> Epoch 21 finished \tANN training loss 0.237659\n",
      ">> Epoch 22 finished \tANN training loss 0.235448\n",
      ">> Epoch 23 finished \tANN training loss 0.231541\n",
      ">> Epoch 24 finished \tANN training loss 0.222743\n",
      ">> Epoch 25 finished \tANN training loss 0.216500\n",
      ">> Epoch 26 finished \tANN training loss 0.214079\n",
      ">> Epoch 27 finished \tANN training loss 0.211491\n",
      ">> Epoch 28 finished \tANN training loss 0.211819\n",
      ">> Epoch 29 finished \tANN training loss 0.199755\n",
      ">> Epoch 30 finished \tANN training loss 0.194282\n",
      ">> Epoch 31 finished \tANN training loss 0.192168\n",
      ">> Epoch 32 finished \tANN training loss 0.189339\n",
      ">> Epoch 33 finished \tANN training loss 0.185098\n",
      ">> Epoch 34 finished \tANN training loss 0.182081\n",
      ">> Epoch 35 finished \tANN training loss 0.179471\n",
      ">> Epoch 36 finished \tANN training loss 0.173248\n",
      ">> Epoch 37 finished \tANN training loss 0.170629\n",
      ">> Epoch 38 finished \tANN training loss 0.166584\n",
      ">> Epoch 39 finished \tANN training loss 0.166431\n",
      ">> Epoch 40 finished \tANN training loss 0.160197\n",
      ">> Epoch 41 finished \tANN training loss 0.156991\n",
      ">> Epoch 42 finished \tANN training loss 0.153219\n",
      ">> Epoch 43 finished \tANN training loss 0.151327\n",
      ">> Epoch 44 finished \tANN training loss 0.150356\n",
      ">> Epoch 45 finished \tANN training loss 0.148230\n",
      ">> Epoch 46 finished \tANN training loss 0.151358\n",
      ">> Epoch 47 finished \tANN training loss 0.144002\n",
      ">> Epoch 48 finished \tANN training loss 0.139987\n",
      ">> Epoch 49 finished \tANN training loss 0.139422\n",
      ">> Epoch 50 finished \tANN training loss 0.134528\n",
      ">> Epoch 51 finished \tANN training loss 0.134255\n",
      ">> Epoch 52 finished \tANN training loss 0.132808\n",
      ">> Epoch 53 finished \tANN training loss 0.129911\n",
      ">> Epoch 54 finished \tANN training loss 0.126849\n",
      ">> Epoch 55 finished \tANN training loss 0.123737\n",
      ">> Epoch 56 finished \tANN training loss 0.123060\n",
      ">> Epoch 57 finished \tANN training loss 0.120189\n",
      ">> Epoch 58 finished \tANN training loss 0.121035\n",
      ">> Epoch 59 finished \tANN training loss 0.116809\n",
      ">> Epoch 60 finished \tANN training loss 0.115650\n",
      ">> Epoch 61 finished \tANN training loss 0.111809\n",
      ">> Epoch 62 finished \tANN training loss 0.114089\n",
      ">> Epoch 63 finished \tANN training loss 0.111344\n",
      ">> Epoch 64 finished \tANN training loss 0.107773\n",
      ">> Epoch 65 finished \tANN training loss 0.105873\n",
      ">> Epoch 66 finished \tANN training loss 0.106155\n",
      ">> Epoch 67 finished \tANN training loss 0.102239\n",
      ">> Epoch 68 finished \tANN training loss 0.104585\n",
      ">> Epoch 69 finished \tANN training loss 0.100008\n",
      ">> Epoch 70 finished \tANN training loss 0.099806\n",
      ">> Epoch 71 finished \tANN training loss 0.096420\n",
      ">> Epoch 72 finished \tANN training loss 0.095706\n",
      ">> Epoch 73 finished \tANN training loss 0.093789\n",
      ">> Epoch 74 finished \tANN training loss 0.092127\n",
      ">> Epoch 75 finished \tANN training loss 0.089948\n",
      ">> Epoch 76 finished \tANN training loss 0.088883\n",
      ">> Epoch 77 finished \tANN training loss 0.089548\n",
      ">> Epoch 78 finished \tANN training loss 0.086699\n",
      ">> Epoch 79 finished \tANN training loss 0.087140\n",
      ">> Epoch 80 finished \tANN training loss 0.083655\n",
      ">> Epoch 81 finished \tANN training loss 0.083052\n",
      ">> Epoch 82 finished \tANN training loss 0.082934\n",
      ">> Epoch 83 finished \tANN training loss 0.080912\n",
      ">> Epoch 84 finished \tANN training loss 0.078495\n",
      ">> Epoch 85 finished \tANN training loss 0.078432\n",
      ">> Epoch 86 finished \tANN training loss 0.076849\n",
      ">> Epoch 87 finished \tANN training loss 0.076220\n",
      ">> Epoch 88 finished \tANN training loss 0.074069\n",
      ">> Epoch 89 finished \tANN training loss 0.074916\n",
      ">> Epoch 90 finished \tANN training loss 0.071443\n",
      ">> Epoch 91 finished \tANN training loss 0.072242\n",
      ">> Epoch 92 finished \tANN training loss 0.069687\n",
      ">> Epoch 93 finished \tANN training loss 0.068627\n",
      ">> Epoch 94 finished \tANN training loss 0.068944\n",
      ">> Epoch 95 finished \tANN training loss 0.067667\n",
      ">> Epoch 96 finished \tANN training loss 0.067966\n",
      ">> Epoch 97 finished \tANN training loss 0.064765\n",
      ">> Epoch 98 finished \tANN training loss 0.064252\n",
      ">> Epoch 99 finished \tANN training loss 0.064404\n",
      ">> Epoch 100 finished \tANN training loss 0.063088\n",
      ">> Epoch 101 finished \tANN training loss 0.061834\n",
      ">> Epoch 102 finished \tANN training loss 0.063005\n",
      ">> Epoch 103 finished \tANN training loss 0.059850\n",
      ">> Epoch 104 finished \tANN training loss 0.059700\n",
      ">> Epoch 105 finished \tANN training loss 0.059272\n",
      ">> Epoch 106 finished \tANN training loss 0.057944\n",
      ">> Epoch 107 finished \tANN training loss 0.059798\n",
      ">> Epoch 108 finished \tANN training loss 0.057077\n",
      ">> Epoch 109 finished \tANN training loss 0.056709\n",
      ">> Epoch 110 finished \tANN training loss 0.057682\n",
      ">> Epoch 111 finished \tANN training loss 0.055551\n",
      ">> Epoch 112 finished \tANN training loss 0.054197\n",
      ">> Epoch 113 finished \tANN training loss 0.052079\n",
      ">> Epoch 114 finished \tANN training loss 0.051852\n",
      ">> Epoch 115 finished \tANN training loss 0.050854\n",
      ">> Epoch 116 finished \tANN training loss 0.051101\n",
      ">> Epoch 117 finished \tANN training loss 0.051081\n",
      ">> Epoch 118 finished \tANN training loss 0.049335\n",
      ">> Epoch 119 finished \tANN training loss 0.048444\n",
      ">> Epoch 120 finished \tANN training loss 0.046677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 121 finished \tANN training loss 0.046563\n",
      ">> Epoch 122 finished \tANN training loss 0.046307\n",
      ">> Epoch 123 finished \tANN training loss 0.047078\n",
      ">> Epoch 124 finished \tANN training loss 0.046094\n",
      ">> Epoch 125 finished \tANN training loss 0.044045\n",
      ">> Epoch 126 finished \tANN training loss 0.043938\n",
      ">> Epoch 127 finished \tANN training loss 0.043707\n",
      ">> Epoch 128 finished \tANN training loss 0.043026\n",
      ">> Epoch 129 finished \tANN training loss 0.043500\n",
      ">> Epoch 130 finished \tANN training loss 0.041800\n",
      ">> Epoch 131 finished \tANN training loss 0.041400\n",
      ">> Epoch 132 finished \tANN training loss 0.040661\n",
      ">> Epoch 133 finished \tANN training loss 0.039805\n",
      ">> Epoch 134 finished \tANN training loss 0.040103\n",
      ">> Epoch 135 finished \tANN training loss 0.040058\n",
      ">> Epoch 136 finished \tANN training loss 0.039108\n",
      ">> Epoch 137 finished \tANN training loss 0.038925\n",
      ">> Epoch 138 finished \tANN training loss 0.037337\n",
      ">> Epoch 139 finished \tANN training loss 0.037386\n",
      ">> Epoch 140 finished \tANN training loss 0.037181\n",
      ">> Epoch 141 finished \tANN training loss 0.036612\n",
      ">> Epoch 142 finished \tANN training loss 0.036788\n",
      ">> Epoch 143 finished \tANN training loss 0.035576\n",
      ">> Epoch 144 finished \tANN training loss 0.035235\n",
      ">> Epoch 145 finished \tANN training loss 0.034494\n",
      ">> Epoch 146 finished \tANN training loss 0.034027\n",
      ">> Epoch 147 finished \tANN training loss 0.033946\n",
      ">> Epoch 148 finished \tANN training loss 0.033116\n",
      ">> Epoch 149 finished \tANN training loss 0.032932\n",
      ">> Epoch 150 finished \tANN training loss 0.032445\n",
      ">> Epoch 151 finished \tANN training loss 0.032538\n",
      ">> Epoch 152 finished \tANN training loss 0.031991\n",
      ">> Epoch 153 finished \tANN training loss 0.031763\n",
      ">> Epoch 154 finished \tANN training loss 0.030953\n",
      ">> Epoch 155 finished \tANN training loss 0.030711\n",
      ">> Epoch 156 finished \tANN training loss 0.030280\n",
      ">> Epoch 157 finished \tANN training loss 0.030527\n",
      ">> Epoch 158 finished \tANN training loss 0.029342\n",
      ">> Epoch 159 finished \tANN training loss 0.028982\n",
      ">> Epoch 160 finished \tANN training loss 0.028938\n",
      ">> Epoch 161 finished \tANN training loss 0.030103\n",
      ">> Epoch 162 finished \tANN training loss 0.028069\n",
      ">> Epoch 163 finished \tANN training loss 0.027824\n",
      ">> Epoch 164 finished \tANN training loss 0.028156\n",
      ">> Epoch 165 finished \tANN training loss 0.026654\n",
      ">> Epoch 166 finished \tANN training loss 0.027255\n",
      ">> Epoch 167 finished \tANN training loss 0.027186\n",
      ">> Epoch 168 finished \tANN training loss 0.026104\n",
      ">> Epoch 169 finished \tANN training loss 0.026762\n",
      ">> Epoch 170 finished \tANN training loss 0.026078\n",
      ">> Epoch 171 finished \tANN training loss 0.025248\n",
      ">> Epoch 172 finished \tANN training loss 0.025239\n",
      ">> Epoch 173 finished \tANN training loss 0.024748\n",
      ">> Epoch 174 finished \tANN training loss 0.024953\n",
      ">> Epoch 175 finished \tANN training loss 0.024957\n",
      ">> Epoch 176 finished \tANN training loss 0.024372\n",
      ">> Epoch 177 finished \tANN training loss 0.024070\n",
      ">> Epoch 178 finished \tANN training loss 0.023700\n",
      ">> Epoch 179 finished \tANN training loss 0.023419\n",
      ">> Epoch 180 finished \tANN training loss 0.023801\n",
      ">> Epoch 181 finished \tANN training loss 0.023324\n",
      ">> Epoch 182 finished \tANN training loss 0.022724\n",
      ">> Epoch 183 finished \tANN training loss 0.022894\n",
      ">> Epoch 184 finished \tANN training loss 0.023263\n",
      ">> Epoch 185 finished \tANN training loss 0.021969\n",
      ">> Epoch 186 finished \tANN training loss 0.021412\n",
      ">> Epoch 187 finished \tANN training loss 0.021252\n",
      ">> Epoch 188 finished \tANN training loss 0.021840\n",
      ">> Epoch 189 finished \tANN training loss 0.021442\n",
      ">> Epoch 190 finished \tANN training loss 0.021317\n",
      ">> Epoch 191 finished \tANN training loss 0.020864\n",
      ">> Epoch 192 finished \tANN training loss 0.021455\n",
      ">> Epoch 193 finished \tANN training loss 0.020897\n",
      ">> Epoch 194 finished \tANN training loss 0.020465\n",
      ">> Epoch 195 finished \tANN training loss 0.020478\n",
      ">> Epoch 196 finished \tANN training loss 0.020300\n",
      ">> Epoch 197 finished \tANN training loss 0.019588\n",
      ">> Epoch 198 finished \tANN training loss 0.020095\n",
      ">> Epoch 199 finished \tANN training loss 0.020331\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.885000\n"
     ]
    }
   ],
   "source": [
    "# Sigmoidal activation, dropout=0.3\n",
    "acc4 = deep_belief_net(hidden_layers_structure=[300], \n",
    "                       learning_rate_rbm=0.05,\n",
    "                       learning_rate=0.1,\n",
    "                       n_epochs_rbm=40,\n",
    "                       n_iter_backprop=200,\n",
    "                       batch_size=32,\n",
    "                       activation_function='sigmoid',\n",
    "                       dropout_p=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.885\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(acc4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 44.140255\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 34.516369\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 29.879543\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 26.920717\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 24.939825\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 23.316221\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 21.916279\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 20.817162\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 19.809341\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 18.751474\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 17.958969\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 17.174906\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 16.434946\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 15.872507\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 15.349248\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 14.827047\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 14.338945\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 13.959961\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 13.526467\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 13.282947\n",
      ">> Epoch 21 finished \tRBM Reconstruction error 12.865698\n",
      ">> Epoch 22 finished \tRBM Reconstruction error 12.565104\n",
      ">> Epoch 23 finished \tRBM Reconstruction error 12.277325\n",
      ">> Epoch 24 finished \tRBM Reconstruction error 12.016171\n",
      ">> Epoch 25 finished \tRBM Reconstruction error 11.813180\n",
      ">> Epoch 26 finished \tRBM Reconstruction error 11.481167\n",
      ">> Epoch 27 finished \tRBM Reconstruction error 11.220469\n",
      ">> Epoch 28 finished \tRBM Reconstruction error 11.012957\n",
      ">> Epoch 29 finished \tRBM Reconstruction error 10.753363\n",
      ">> Epoch 30 finished \tRBM Reconstruction error 10.548894\n",
      ">> Epoch 31 finished \tRBM Reconstruction error 10.340069\n",
      ">> Epoch 32 finished \tRBM Reconstruction error 10.136040\n",
      ">> Epoch 33 finished \tRBM Reconstruction error 10.000112\n",
      ">> Epoch 34 finished \tRBM Reconstruction error 9.816780\n",
      ">> Epoch 35 finished \tRBM Reconstruction error 9.689515\n",
      ">> Epoch 36 finished \tRBM Reconstruction error 9.468905\n",
      ">> Epoch 37 finished \tRBM Reconstruction error 9.430795\n",
      ">> Epoch 38 finished \tRBM Reconstruction error 9.228283\n",
      ">> Epoch 39 finished \tRBM Reconstruction error 9.073233\n",
      ">> Epoch 40 finished \tRBM Reconstruction error 8.909946\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 1.066938\n",
      ">> Epoch 1 finished \tANN training loss 0.734275\n",
      ">> Epoch 2 finished \tANN training loss 0.594924\n",
      ">> Epoch 3 finished \tANN training loss 0.523809\n",
      ">> Epoch 4 finished \tANN training loss 0.476906\n",
      ">> Epoch 5 finished \tANN training loss 0.449035\n",
      ">> Epoch 6 finished \tANN training loss 0.419840\n",
      ">> Epoch 7 finished \tANN training loss 0.397110\n",
      ">> Epoch 8 finished \tANN training loss 0.380618\n",
      ">> Epoch 9 finished \tANN training loss 0.361953\n",
      ">> Epoch 10 finished \tANN training loss 0.352017\n",
      ">> Epoch 11 finished \tANN training loss 0.337664\n",
      ">> Epoch 12 finished \tANN training loss 0.323106\n",
      ">> Epoch 13 finished \tANN training loss 0.322950\n",
      ">> Epoch 14 finished \tANN training loss 0.301515\n",
      ">> Epoch 15 finished \tANN training loss 0.291547\n",
      ">> Epoch 16 finished \tANN training loss 0.287913\n",
      ">> Epoch 17 finished \tANN training loss 0.275095\n",
      ">> Epoch 18 finished \tANN training loss 0.270438\n",
      ">> Epoch 19 finished \tANN training loss 0.262995\n",
      ">> Epoch 20 finished \tANN training loss 0.257501\n",
      ">> Epoch 21 finished \tANN training loss 0.254009\n",
      ">> Epoch 22 finished \tANN training loss 0.243380\n",
      ">> Epoch 23 finished \tANN training loss 0.239698\n",
      ">> Epoch 24 finished \tANN training loss 0.231883\n",
      ">> Epoch 25 finished \tANN training loss 0.230240\n",
      ">> Epoch 26 finished \tANN training loss 0.226360\n",
      ">> Epoch 27 finished \tANN training loss 0.231455\n",
      ">> Epoch 28 finished \tANN training loss 0.217701\n",
      ">> Epoch 29 finished \tANN training loss 0.215385\n",
      ">> Epoch 30 finished \tANN training loss 0.207716\n",
      ">> Epoch 31 finished \tANN training loss 0.211143\n",
      ">> Epoch 32 finished \tANN training loss 0.200320\n",
      ">> Epoch 33 finished \tANN training loss 0.210898\n",
      ">> Epoch 34 finished \tANN training loss 0.194683\n",
      ">> Epoch 35 finished \tANN training loss 0.192158\n",
      ">> Epoch 36 finished \tANN training loss 0.191934\n",
      ">> Epoch 37 finished \tANN training loss 0.188256\n",
      ">> Epoch 38 finished \tANN training loss 0.183503\n",
      ">> Epoch 39 finished \tANN training loss 0.183189\n",
      ">> Epoch 40 finished \tANN training loss 0.179966\n",
      ">> Epoch 41 finished \tANN training loss 0.181487\n",
      ">> Epoch 42 finished \tANN training loss 0.172944\n",
      ">> Epoch 43 finished \tANN training loss 0.171273\n",
      ">> Epoch 44 finished \tANN training loss 0.165213\n",
      ">> Epoch 45 finished \tANN training loss 0.166067\n",
      ">> Epoch 46 finished \tANN training loss 0.163789\n",
      ">> Epoch 47 finished \tANN training loss 0.167600\n",
      ">> Epoch 48 finished \tANN training loss 0.155891\n",
      ">> Epoch 49 finished \tANN training loss 0.154835\n",
      ">> Epoch 50 finished \tANN training loss 0.151390\n",
      ">> Epoch 51 finished \tANN training loss 0.151304\n",
      ">> Epoch 52 finished \tANN training loss 0.148279\n",
      ">> Epoch 53 finished \tANN training loss 0.147794\n",
      ">> Epoch 54 finished \tANN training loss 0.147427\n",
      ">> Epoch 55 finished \tANN training loss 0.140367\n",
      ">> Epoch 56 finished \tANN training loss 0.138864\n",
      ">> Epoch 57 finished \tANN training loss 0.141895\n",
      ">> Epoch 58 finished \tANN training loss 0.136450\n",
      ">> Epoch 59 finished \tANN training loss 0.135003\n",
      ">> Epoch 60 finished \tANN training loss 0.131884\n",
      ">> Epoch 61 finished \tANN training loss 0.134179\n",
      ">> Epoch 62 finished \tANN training loss 0.131031\n",
      ">> Epoch 63 finished \tANN training loss 0.127614\n",
      ">> Epoch 64 finished \tANN training loss 0.123993\n",
      ">> Epoch 65 finished \tANN training loss 0.121772\n",
      ">> Epoch 66 finished \tANN training loss 0.120674\n",
      ">> Epoch 67 finished \tANN training loss 0.121848\n",
      ">> Epoch 68 finished \tANN training loss 0.119808\n",
      ">> Epoch 69 finished \tANN training loss 0.117407\n",
      ">> Epoch 70 finished \tANN training loss 0.117347\n",
      ">> Epoch 71 finished \tANN training loss 0.118752\n",
      ">> Epoch 72 finished \tANN training loss 0.117753\n",
      ">> Epoch 73 finished \tANN training loss 0.110410\n",
      ">> Epoch 74 finished \tANN training loss 0.109087\n",
      ">> Epoch 75 finished \tANN training loss 0.110105\n",
      ">> Epoch 76 finished \tANN training loss 0.109425\n",
      ">> Epoch 77 finished \tANN training loss 0.107489\n",
      ">> Epoch 78 finished \tANN training loss 0.105924\n",
      ">> Epoch 79 finished \tANN training loss 0.102476\n",
      ">> Epoch 80 finished \tANN training loss 0.104578\n",
      ">> Epoch 81 finished \tANN training loss 0.101349\n",
      ">> Epoch 82 finished \tANN training loss 0.098437\n",
      ">> Epoch 83 finished \tANN training loss 0.098407\n",
      ">> Epoch 84 finished \tANN training loss 0.097846\n",
      ">> Epoch 85 finished \tANN training loss 0.093516\n",
      ">> Epoch 86 finished \tANN training loss 0.095406\n",
      ">> Epoch 87 finished \tANN training loss 0.092529\n",
      ">> Epoch 88 finished \tANN training loss 0.093897\n",
      ">> Epoch 89 finished \tANN training loss 0.091094\n",
      ">> Epoch 90 finished \tANN training loss 0.090580\n",
      ">> Epoch 91 finished \tANN training loss 0.090806\n",
      ">> Epoch 92 finished \tANN training loss 0.088154\n",
      ">> Epoch 93 finished \tANN training loss 0.085090\n",
      ">> Epoch 94 finished \tANN training loss 0.085001\n",
      ">> Epoch 95 finished \tANN training loss 0.084441\n",
      ">> Epoch 96 finished \tANN training loss 0.085808\n",
      ">> Epoch 97 finished \tANN training loss 0.082942\n",
      ">> Epoch 98 finished \tANN training loss 0.080774\n",
      ">> Epoch 99 finished \tANN training loss 0.079185\n",
      ">> Epoch 100 finished \tANN training loss 0.078713\n",
      ">> Epoch 101 finished \tANN training loss 0.076861\n",
      ">> Epoch 102 finished \tANN training loss 0.079224\n",
      ">> Epoch 103 finished \tANN training loss 0.078161\n",
      ">> Epoch 104 finished \tANN training loss 0.075585\n",
      ">> Epoch 105 finished \tANN training loss 0.072891\n",
      ">> Epoch 106 finished \tANN training loss 0.073512\n",
      ">> Epoch 107 finished \tANN training loss 0.072596\n",
      ">> Epoch 108 finished \tANN training loss 0.071073\n",
      ">> Epoch 109 finished \tANN training loss 0.071214\n",
      ">> Epoch 110 finished \tANN training loss 0.068718\n",
      ">> Epoch 111 finished \tANN training loss 0.069827\n",
      ">> Epoch 112 finished \tANN training loss 0.069690\n",
      ">> Epoch 113 finished \tANN training loss 0.066436\n",
      ">> Epoch 114 finished \tANN training loss 0.065389\n",
      ">> Epoch 115 finished \tANN training loss 0.064521\n",
      ">> Epoch 116 finished \tANN training loss 0.065385\n",
      ">> Epoch 117 finished \tANN training loss 0.063349\n",
      ">> Epoch 118 finished \tANN training loss 0.064051\n",
      ">> Epoch 119 finished \tANN training loss 0.062765\n",
      ">> Epoch 120 finished \tANN training loss 0.062937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 121 finished \tANN training loss 0.063619\n",
      ">> Epoch 122 finished \tANN training loss 0.060987\n",
      ">> Epoch 123 finished \tANN training loss 0.060811\n",
      ">> Epoch 124 finished \tANN training loss 0.059828\n",
      ">> Epoch 125 finished \tANN training loss 0.060110\n",
      ">> Epoch 126 finished \tANN training loss 0.058511\n",
      ">> Epoch 127 finished \tANN training loss 0.058385\n",
      ">> Epoch 128 finished \tANN training loss 0.058820\n",
      ">> Epoch 129 finished \tANN training loss 0.056253\n",
      ">> Epoch 130 finished \tANN training loss 0.055482\n",
      ">> Epoch 131 finished \tANN training loss 0.055394\n",
      ">> Epoch 132 finished \tANN training loss 0.054382\n",
      ">> Epoch 133 finished \tANN training loss 0.054062\n",
      ">> Epoch 134 finished \tANN training loss 0.054221\n",
      ">> Epoch 135 finished \tANN training loss 0.054995\n",
      ">> Epoch 136 finished \tANN training loss 0.052053\n",
      ">> Epoch 137 finished \tANN training loss 0.052666\n",
      ">> Epoch 138 finished \tANN training loss 0.051470\n",
      ">> Epoch 139 finished \tANN training loss 0.051639\n",
      ">> Epoch 140 finished \tANN training loss 0.050752\n",
      ">> Epoch 141 finished \tANN training loss 0.049019\n",
      ">> Epoch 142 finished \tANN training loss 0.049914\n",
      ">> Epoch 143 finished \tANN training loss 0.049576\n",
      ">> Epoch 144 finished \tANN training loss 0.050435\n",
      ">> Epoch 145 finished \tANN training loss 0.047030\n",
      ">> Epoch 146 finished \tANN training loss 0.050140\n",
      ">> Epoch 147 finished \tANN training loss 0.047038\n",
      ">> Epoch 148 finished \tANN training loss 0.046597\n",
      ">> Epoch 149 finished \tANN training loss 0.044643\n",
      ">> Epoch 150 finished \tANN training loss 0.043996\n",
      ">> Epoch 151 finished \tANN training loss 0.044778\n",
      ">> Epoch 152 finished \tANN training loss 0.043207\n",
      ">> Epoch 153 finished \tANN training loss 0.043595\n",
      ">> Epoch 154 finished \tANN training loss 0.042821\n",
      ">> Epoch 155 finished \tANN training loss 0.041673\n",
      ">> Epoch 156 finished \tANN training loss 0.043117\n",
      ">> Epoch 157 finished \tANN training loss 0.041032\n",
      ">> Epoch 158 finished \tANN training loss 0.040862\n",
      ">> Epoch 159 finished \tANN training loss 0.039584\n",
      ">> Epoch 160 finished \tANN training loss 0.039544\n",
      ">> Epoch 161 finished \tANN training loss 0.039196\n",
      ">> Epoch 162 finished \tANN training loss 0.038407\n",
      ">> Epoch 163 finished \tANN training loss 0.038817\n",
      ">> Epoch 164 finished \tANN training loss 0.037738\n",
      ">> Epoch 165 finished \tANN training loss 0.036932\n",
      ">> Epoch 166 finished \tANN training loss 0.036816\n",
      ">> Epoch 167 finished \tANN training loss 0.037366\n",
      ">> Epoch 168 finished \tANN training loss 0.037214\n",
      ">> Epoch 169 finished \tANN training loss 0.036398\n",
      ">> Epoch 170 finished \tANN training loss 0.036167\n",
      ">> Epoch 171 finished \tANN training loss 0.034256\n",
      ">> Epoch 172 finished \tANN training loss 0.035277\n",
      ">> Epoch 173 finished \tANN training loss 0.034715\n",
      ">> Epoch 174 finished \tANN training loss 0.034012\n",
      ">> Epoch 175 finished \tANN training loss 0.032673\n",
      ">> Epoch 176 finished \tANN training loss 0.032604\n",
      ">> Epoch 177 finished \tANN training loss 0.032946\n",
      ">> Epoch 178 finished \tANN training loss 0.032165\n",
      ">> Epoch 179 finished \tANN training loss 0.032634\n",
      ">> Epoch 180 finished \tANN training loss 0.032136\n",
      ">> Epoch 181 finished \tANN training loss 0.032435\n",
      ">> Epoch 182 finished \tANN training loss 0.033255\n",
      ">> Epoch 183 finished \tANN training loss 0.031021\n",
      ">> Epoch 184 finished \tANN training loss 0.031597\n",
      ">> Epoch 185 finished \tANN training loss 0.029515\n",
      ">> Epoch 186 finished \tANN training loss 0.030117\n",
      ">> Epoch 187 finished \tANN training loss 0.029094\n",
      ">> Epoch 188 finished \tANN training loss 0.029761\n",
      ">> Epoch 189 finished \tANN training loss 0.029402\n",
      ">> Epoch 190 finished \tANN training loss 0.028540\n",
      ">> Epoch 191 finished \tANN training loss 0.028611\n",
      ">> Epoch 192 finished \tANN training loss 0.028487\n",
      ">> Epoch 193 finished \tANN training loss 0.027111\n",
      ">> Epoch 194 finished \tANN training loss 0.027326\n",
      ">> Epoch 195 finished \tANN training loss 0.027370\n",
      ">> Epoch 196 finished \tANN training loss 0.027969\n",
      ">> Epoch 197 finished \tANN training loss 0.027641\n",
      ">> Epoch 198 finished \tANN training loss 0.027018\n",
      ">> Epoch 199 finished \tANN training loss 0.025903\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.885000\n"
     ]
    }
   ],
   "source": [
    "# Sigmoidal activation, dropout=0.4\n",
    "acc5 = deep_belief_net(hidden_layers_structure=[300], \n",
    "                       learning_rate_rbm=0.05,\n",
    "                       learning_rate=0.1,\n",
    "                       n_epochs_rbm=40,\n",
    "                       n_iter_backprop=200,\n",
    "                       batch_size=32,\n",
    "                       activation_function='sigmoid',\n",
    "                       dropout_p=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.885\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(acc5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 44.573238\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 34.630795\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 30.121592\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 26.837561\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 24.935493\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 23.162682\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 21.974854\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 20.764902\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 19.719866\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 18.765295\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 17.900713\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 17.103657\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 16.434870\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 15.841487\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 15.312390\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 14.815372\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 14.355344\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 13.964416\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 13.502627\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 13.164868\n",
      ">> Epoch 21 finished \tRBM Reconstruction error 12.808501\n",
      ">> Epoch 22 finished \tRBM Reconstruction error 12.508628\n",
      ">> Epoch 23 finished \tRBM Reconstruction error 12.175906\n",
      ">> Epoch 24 finished \tRBM Reconstruction error 11.939978\n",
      ">> Epoch 25 finished \tRBM Reconstruction error 11.676118\n",
      ">> Epoch 26 finished \tRBM Reconstruction error 11.405881\n",
      ">> Epoch 27 finished \tRBM Reconstruction error 11.143344\n",
      ">> Epoch 28 finished \tRBM Reconstruction error 10.943728\n",
      ">> Epoch 29 finished \tRBM Reconstruction error 10.718337\n",
      ">> Epoch 30 finished \tRBM Reconstruction error 10.472020\n",
      ">> Epoch 31 finished \tRBM Reconstruction error 10.317543\n",
      ">> Epoch 32 finished \tRBM Reconstruction error 10.093082\n",
      ">> Epoch 33 finished \tRBM Reconstruction error 9.895876\n",
      ">> Epoch 34 finished \tRBM Reconstruction error 9.735769\n",
      ">> Epoch 35 finished \tRBM Reconstruction error 9.620356\n",
      ">> Epoch 36 finished \tRBM Reconstruction error 9.389066\n",
      ">> Epoch 37 finished \tRBM Reconstruction error 9.231999\n",
      ">> Epoch 38 finished \tRBM Reconstruction error 9.104783\n",
      ">> Epoch 39 finished \tRBM Reconstruction error 8.953965\n",
      ">> Epoch 40 finished \tRBM Reconstruction error 8.879975\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 1.029506\n",
      ">> Epoch 1 finished \tANN training loss 0.783091\n",
      ">> Epoch 2 finished \tANN training loss 0.615450\n",
      ">> Epoch 3 finished \tANN training loss 0.539779\n",
      ">> Epoch 4 finished \tANN training loss 0.491151\n",
      ">> Epoch 5 finished \tANN training loss 0.470389\n",
      ">> Epoch 6 finished \tANN training loss 0.425696\n",
      ">> Epoch 7 finished \tANN training loss 0.432870\n",
      ">> Epoch 8 finished \tANN training loss 0.388121\n",
      ">> Epoch 9 finished \tANN training loss 0.374311\n",
      ">> Epoch 10 finished \tANN training loss 0.370608\n",
      ">> Epoch 11 finished \tANN training loss 0.344941\n",
      ">> Epoch 12 finished \tANN training loss 0.332405\n",
      ">> Epoch 13 finished \tANN training loss 0.323167\n",
      ">> Epoch 14 finished \tANN training loss 0.316941\n",
      ">> Epoch 15 finished \tANN training loss 0.308172\n",
      ">> Epoch 16 finished \tANN training loss 0.298531\n",
      ">> Epoch 17 finished \tANN training loss 0.304628\n",
      ">> Epoch 18 finished \tANN training loss 0.291222\n",
      ">> Epoch 19 finished \tANN training loss 0.286756\n",
      ">> Epoch 20 finished \tANN training loss 0.275031\n",
      ">> Epoch 21 finished \tANN training loss 0.270081\n",
      ">> Epoch 22 finished \tANN training loss 0.268243\n",
      ">> Epoch 23 finished \tANN training loss 0.255230\n",
      ">> Epoch 24 finished \tANN training loss 0.257759\n",
      ">> Epoch 25 finished \tANN training loss 0.254832\n",
      ">> Epoch 26 finished \tANN training loss 0.248638\n",
      ">> Epoch 27 finished \tANN training loss 0.243590\n",
      ">> Epoch 28 finished \tANN training loss 0.244656\n",
      ">> Epoch 29 finished \tANN training loss 0.243002\n",
      ">> Epoch 30 finished \tANN training loss 0.234312\n",
      ">> Epoch 31 finished \tANN training loss 0.228486\n",
      ">> Epoch 32 finished \tANN training loss 0.226636\n",
      ">> Epoch 33 finished \tANN training loss 0.229403\n",
      ">> Epoch 34 finished \tANN training loss 0.223033\n",
      ">> Epoch 35 finished \tANN training loss 0.215411\n",
      ">> Epoch 36 finished \tANN training loss 0.212733\n",
      ">> Epoch 37 finished \tANN training loss 0.215245\n",
      ">> Epoch 38 finished \tANN training loss 0.215365\n",
      ">> Epoch 39 finished \tANN training loss 0.204600\n",
      ">> Epoch 40 finished \tANN training loss 0.202094\n",
      ">> Epoch 41 finished \tANN training loss 0.204231\n",
      ">> Epoch 42 finished \tANN training loss 0.199522\n",
      ">> Epoch 43 finished \tANN training loss 0.194332\n",
      ">> Epoch 44 finished \tANN training loss 0.196914\n",
      ">> Epoch 45 finished \tANN training loss 0.191636\n",
      ">> Epoch 46 finished \tANN training loss 0.184668\n",
      ">> Epoch 47 finished \tANN training loss 0.188735\n",
      ">> Epoch 48 finished \tANN training loss 0.187260\n",
      ">> Epoch 49 finished \tANN training loss 0.179908\n",
      ">> Epoch 50 finished \tANN training loss 0.178960\n",
      ">> Epoch 51 finished \tANN training loss 0.180915\n",
      ">> Epoch 52 finished \tANN training loss 0.175768\n",
      ">> Epoch 53 finished \tANN training loss 0.177390\n",
      ">> Epoch 54 finished \tANN training loss 0.173746\n",
      ">> Epoch 55 finished \tANN training loss 0.174097\n",
      ">> Epoch 56 finished \tANN training loss 0.169286\n",
      ">> Epoch 57 finished \tANN training loss 0.168682\n",
      ">> Epoch 58 finished \tANN training loss 0.165092\n",
      ">> Epoch 59 finished \tANN training loss 0.161985\n",
      ">> Epoch 60 finished \tANN training loss 0.158393\n",
      ">> Epoch 61 finished \tANN training loss 0.157357\n",
      ">> Epoch 62 finished \tANN training loss 0.158232\n",
      ">> Epoch 63 finished \tANN training loss 0.159759\n",
      ">> Epoch 64 finished \tANN training loss 0.155991\n",
      ">> Epoch 65 finished \tANN training loss 0.154985\n",
      ">> Epoch 66 finished \tANN training loss 0.159384\n",
      ">> Epoch 67 finished \tANN training loss 0.150831\n",
      ">> Epoch 68 finished \tANN training loss 0.150883\n",
      ">> Epoch 69 finished \tANN training loss 0.158309\n",
      ">> Epoch 70 finished \tANN training loss 0.144489\n",
      ">> Epoch 71 finished \tANN training loss 0.147804\n",
      ">> Epoch 72 finished \tANN training loss 0.143351\n",
      ">> Epoch 73 finished \tANN training loss 0.141749\n",
      ">> Epoch 74 finished \tANN training loss 0.145374\n",
      ">> Epoch 75 finished \tANN training loss 0.142773\n",
      ">> Epoch 76 finished \tANN training loss 0.136577\n",
      ">> Epoch 77 finished \tANN training loss 0.133758\n",
      ">> Epoch 78 finished \tANN training loss 0.132530\n",
      ">> Epoch 79 finished \tANN training loss 0.135015\n",
      ">> Epoch 80 finished \tANN training loss 0.137384\n",
      ">> Epoch 81 finished \tANN training loss 0.129984\n",
      ">> Epoch 82 finished \tANN training loss 0.130589\n",
      ">> Epoch 83 finished \tANN training loss 0.134831\n",
      ">> Epoch 84 finished \tANN training loss 0.123814\n",
      ">> Epoch 85 finished \tANN training loss 0.127403\n",
      ">> Epoch 86 finished \tANN training loss 0.122149\n",
      ">> Epoch 87 finished \tANN training loss 0.120554\n",
      ">> Epoch 88 finished \tANN training loss 0.120919\n",
      ">> Epoch 89 finished \tANN training loss 0.128998\n",
      ">> Epoch 90 finished \tANN training loss 0.119175\n",
      ">> Epoch 91 finished \tANN training loss 0.117204\n",
      ">> Epoch 92 finished \tANN training loss 0.117279\n",
      ">> Epoch 93 finished \tANN training loss 0.117866\n",
      ">> Epoch 94 finished \tANN training loss 0.115391\n",
      ">> Epoch 95 finished \tANN training loss 0.115274\n",
      ">> Epoch 96 finished \tANN training loss 0.113404\n",
      ">> Epoch 97 finished \tANN training loss 0.111995\n",
      ">> Epoch 98 finished \tANN training loss 0.112632\n",
      ">> Epoch 99 finished \tANN training loss 0.111424\n",
      ">> Epoch 100 finished \tANN training loss 0.111886\n",
      ">> Epoch 101 finished \tANN training loss 0.109582\n",
      ">> Epoch 102 finished \tANN training loss 0.108680\n",
      ">> Epoch 103 finished \tANN training loss 0.108157\n",
      ">> Epoch 104 finished \tANN training loss 0.105164\n",
      ">> Epoch 105 finished \tANN training loss 0.109304\n",
      ">> Epoch 106 finished \tANN training loss 0.102602\n",
      ">> Epoch 107 finished \tANN training loss 0.104751\n",
      ">> Epoch 108 finished \tANN training loss 0.100942\n",
      ">> Epoch 109 finished \tANN training loss 0.099841\n",
      ">> Epoch 110 finished \tANN training loss 0.099209\n",
      ">> Epoch 111 finished \tANN training loss 0.095703\n",
      ">> Epoch 112 finished \tANN training loss 0.094847\n",
      ">> Epoch 113 finished \tANN training loss 0.101631\n",
      ">> Epoch 114 finished \tANN training loss 0.096926\n",
      ">> Epoch 115 finished \tANN training loss 0.095116\n",
      ">> Epoch 116 finished \tANN training loss 0.097598\n",
      ">> Epoch 117 finished \tANN training loss 0.098958\n",
      ">> Epoch 118 finished \tANN training loss 0.093077\n",
      ">> Epoch 119 finished \tANN training loss 0.089837\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 120 finished \tANN training loss 0.091666\n",
      ">> Epoch 121 finished \tANN training loss 0.090795\n",
      ">> Epoch 122 finished \tANN training loss 0.090826\n",
      ">> Epoch 123 finished \tANN training loss 0.086482\n",
      ">> Epoch 124 finished \tANN training loss 0.085695\n",
      ">> Epoch 125 finished \tANN training loss 0.085151\n",
      ">> Epoch 126 finished \tANN training loss 0.088090\n",
      ">> Epoch 127 finished \tANN training loss 0.089811\n",
      ">> Epoch 128 finished \tANN training loss 0.084178\n",
      ">> Epoch 129 finished \tANN training loss 0.083127\n",
      ">> Epoch 130 finished \tANN training loss 0.082611\n",
      ">> Epoch 131 finished \tANN training loss 0.084550\n",
      ">> Epoch 132 finished \tANN training loss 0.083672\n",
      ">> Epoch 133 finished \tANN training loss 0.083040\n",
      ">> Epoch 134 finished \tANN training loss 0.080540\n",
      ">> Epoch 135 finished \tANN training loss 0.078377\n",
      ">> Epoch 136 finished \tANN training loss 0.078733\n",
      ">> Epoch 137 finished \tANN training loss 0.078687\n",
      ">> Epoch 138 finished \tANN training loss 0.077426\n",
      ">> Epoch 139 finished \tANN training loss 0.077130\n",
      ">> Epoch 140 finished \tANN training loss 0.079029\n",
      ">> Epoch 141 finished \tANN training loss 0.073585\n",
      ">> Epoch 142 finished \tANN training loss 0.074813\n",
      ">> Epoch 143 finished \tANN training loss 0.073263\n",
      ">> Epoch 144 finished \tANN training loss 0.072243\n",
      ">> Epoch 145 finished \tANN training loss 0.072398\n",
      ">> Epoch 146 finished \tANN training loss 0.070220\n",
      ">> Epoch 147 finished \tANN training loss 0.069503\n",
      ">> Epoch 148 finished \tANN training loss 0.069849\n",
      ">> Epoch 149 finished \tANN training loss 0.069325\n",
      ">> Epoch 150 finished \tANN training loss 0.067089\n",
      ">> Epoch 151 finished \tANN training loss 0.065187\n",
      ">> Epoch 152 finished \tANN training loss 0.065232\n",
      ">> Epoch 153 finished \tANN training loss 0.065788\n",
      ">> Epoch 154 finished \tANN training loss 0.065809\n",
      ">> Epoch 155 finished \tANN training loss 0.064530\n",
      ">> Epoch 156 finished \tANN training loss 0.062614\n",
      ">> Epoch 157 finished \tANN training loss 0.064008\n",
      ">> Epoch 158 finished \tANN training loss 0.063099\n",
      ">> Epoch 159 finished \tANN training loss 0.064316\n",
      ">> Epoch 160 finished \tANN training loss 0.065581\n",
      ">> Epoch 161 finished \tANN training loss 0.060609\n",
      ">> Epoch 162 finished \tANN training loss 0.059868\n",
      ">> Epoch 163 finished \tANN training loss 0.062034\n",
      ">> Epoch 164 finished \tANN training loss 0.059640\n",
      ">> Epoch 165 finished \tANN training loss 0.058968\n",
      ">> Epoch 166 finished \tANN training loss 0.057682\n",
      ">> Epoch 167 finished \tANN training loss 0.058363\n",
      ">> Epoch 168 finished \tANN training loss 0.057457\n",
      ">> Epoch 169 finished \tANN training loss 0.057689\n",
      ">> Epoch 170 finished \tANN training loss 0.058487\n",
      ">> Epoch 171 finished \tANN training loss 0.056778\n",
      ">> Epoch 172 finished \tANN training loss 0.054990\n",
      ">> Epoch 173 finished \tANN training loss 0.053820\n",
      ">> Epoch 174 finished \tANN training loss 0.055533\n",
      ">> Epoch 175 finished \tANN training loss 0.053863\n",
      ">> Epoch 176 finished \tANN training loss 0.052631\n",
      ">> Epoch 177 finished \tANN training loss 0.051800\n",
      ">> Epoch 178 finished \tANN training loss 0.051748\n",
      ">> Epoch 179 finished \tANN training loss 0.053723\n",
      ">> Epoch 180 finished \tANN training loss 0.050954\n",
      ">> Epoch 181 finished \tANN training loss 0.050201\n",
      ">> Epoch 182 finished \tANN training loss 0.055064\n",
      ">> Epoch 183 finished \tANN training loss 0.050334\n",
      ">> Epoch 184 finished \tANN training loss 0.049993\n",
      ">> Epoch 185 finished \tANN training loss 0.050272\n",
      ">> Epoch 186 finished \tANN training loss 0.049472\n",
      ">> Epoch 187 finished \tANN training loss 0.048316\n",
      ">> Epoch 188 finished \tANN training loss 0.050607\n",
      ">> Epoch 189 finished \tANN training loss 0.049316\n",
      ">> Epoch 190 finished \tANN training loss 0.050565\n",
      ">> Epoch 191 finished \tANN training loss 0.050284\n",
      ">> Epoch 192 finished \tANN training loss 0.049247\n",
      ">> Epoch 193 finished \tANN training loss 0.048176\n",
      ">> Epoch 194 finished \tANN training loss 0.048126\n",
      ">> Epoch 195 finished \tANN training loss 0.047812\n",
      ">> Epoch 196 finished \tANN training loss 0.046406\n",
      ">> Epoch 197 finished \tANN training loss 0.047042\n",
      ">> Epoch 198 finished \tANN training loss 0.048685\n",
      ">> Epoch 199 finished \tANN training loss 0.048325\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.875000\n"
     ]
    }
   ],
   "source": [
    "# Sigmoidal activation, dropout=0.5\n",
    "acc6 = deep_belief_net(hidden_layers_structure=[300], \n",
    "                       learning_rate_rbm=0.05,\n",
    "                       learning_rate=0.1,\n",
    "                       n_epochs_rbm=40,\n",
    "                       n_iter_backprop=200,\n",
    "                       batch_size=32,\n",
    "                       activation_function='sigmoid',\n",
    "                       dropout_p=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.875\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(acc6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 43.978573\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 34.663868\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 29.835709\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 26.734238\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 24.634024\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 23.120874\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 21.792395\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 20.564890\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 19.499239\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 18.631668\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 17.790821\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 17.104307\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 16.414818\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 15.772669\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 15.287756\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 14.800046\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 14.310347\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 13.852696\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 13.496503\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 13.173393\n",
      ">> Epoch 21 finished \tRBM Reconstruction error 12.813791\n",
      ">> Epoch 22 finished \tRBM Reconstruction error 12.492845\n",
      ">> Epoch 23 finished \tRBM Reconstruction error 12.198965\n",
      ">> Epoch 24 finished \tRBM Reconstruction error 11.911744\n",
      ">> Epoch 25 finished \tRBM Reconstruction error 11.605242\n",
      ">> Epoch 26 finished \tRBM Reconstruction error 11.385957\n",
      ">> Epoch 27 finished \tRBM Reconstruction error 11.180469\n",
      ">> Epoch 28 finished \tRBM Reconstruction error 10.864896\n",
      ">> Epoch 29 finished \tRBM Reconstruction error 10.740730\n",
      ">> Epoch 30 finished \tRBM Reconstruction error 10.482827\n",
      ">> Epoch 31 finished \tRBM Reconstruction error 10.279586\n",
      ">> Epoch 32 finished \tRBM Reconstruction error 10.076273\n",
      ">> Epoch 33 finished \tRBM Reconstruction error 9.907208\n",
      ">> Epoch 34 finished \tRBM Reconstruction error 9.734780\n",
      ">> Epoch 35 finished \tRBM Reconstruction error 9.549190\n",
      ">> Epoch 36 finished \tRBM Reconstruction error 9.444825\n",
      ">> Epoch 37 finished \tRBM Reconstruction error 9.257287\n",
      ">> Epoch 38 finished \tRBM Reconstruction error 9.102218\n",
      ">> Epoch 39 finished \tRBM Reconstruction error 8.930707\n",
      ">> Epoch 40 finished \tRBM Reconstruction error 8.805639\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 1.115489\n",
      ">> Epoch 1 finished \tANN training loss 0.791461\n",
      ">> Epoch 2 finished \tANN training loss 0.643817\n",
      ">> Epoch 3 finished \tANN training loss 0.561672\n",
      ">> Epoch 4 finished \tANN training loss 0.529912\n",
      ">> Epoch 5 finished \tANN training loss 0.486901\n",
      ">> Epoch 6 finished \tANN training loss 0.461026\n",
      ">> Epoch 7 finished \tANN training loss 0.429949\n",
      ">> Epoch 8 finished \tANN training loss 0.418390\n",
      ">> Epoch 9 finished \tANN training loss 0.404743\n",
      ">> Epoch 10 finished \tANN training loss 0.387626\n",
      ">> Epoch 11 finished \tANN training loss 0.369497\n",
      ">> Epoch 12 finished \tANN training loss 0.368574\n",
      ">> Epoch 13 finished \tANN training loss 0.358239\n",
      ">> Epoch 14 finished \tANN training loss 0.346524\n",
      ">> Epoch 15 finished \tANN training loss 0.339551\n",
      ">> Epoch 16 finished \tANN training loss 0.337265\n",
      ">> Epoch 17 finished \tANN training loss 0.328813\n",
      ">> Epoch 18 finished \tANN training loss 0.319953\n",
      ">> Epoch 19 finished \tANN training loss 0.320939\n",
      ">> Epoch 20 finished \tANN training loss 0.310229\n",
      ">> Epoch 21 finished \tANN training loss 0.310573\n",
      ">> Epoch 22 finished \tANN training loss 0.309830\n",
      ">> Epoch 23 finished \tANN training loss 0.298436\n",
      ">> Epoch 24 finished \tANN training loss 0.295336\n",
      ">> Epoch 25 finished \tANN training loss 0.288047\n",
      ">> Epoch 26 finished \tANN training loss 0.287187\n",
      ">> Epoch 27 finished \tANN training loss 0.281451\n",
      ">> Epoch 28 finished \tANN training loss 0.276167\n",
      ">> Epoch 29 finished \tANN training loss 0.273731\n",
      ">> Epoch 30 finished \tANN training loss 0.267410\n",
      ">> Epoch 31 finished \tANN training loss 0.271180\n",
      ">> Epoch 32 finished \tANN training loss 0.265336\n",
      ">> Epoch 33 finished \tANN training loss 0.284411\n",
      ">> Epoch 34 finished \tANN training loss 0.258249\n",
      ">> Epoch 35 finished \tANN training loss 0.260898\n",
      ">> Epoch 36 finished \tANN training loss 0.252059\n",
      ">> Epoch 37 finished \tANN training loss 0.253712\n",
      ">> Epoch 38 finished \tANN training loss 0.245933\n",
      ">> Epoch 39 finished \tANN training loss 0.250076\n",
      ">> Epoch 40 finished \tANN training loss 0.253628\n",
      ">> Epoch 41 finished \tANN training loss 0.248063\n",
      ">> Epoch 42 finished \tANN training loss 0.241777\n",
      ">> Epoch 43 finished \tANN training loss 0.242915\n",
      ">> Epoch 44 finished \tANN training loss 0.235039\n",
      ">> Epoch 45 finished \tANN training loss 0.234279\n",
      ">> Epoch 46 finished \tANN training loss 0.231037\n",
      ">> Epoch 47 finished \tANN training loss 0.227532\n",
      ">> Epoch 48 finished \tANN training loss 0.233283\n",
      ">> Epoch 49 finished \tANN training loss 0.236004\n",
      ">> Epoch 50 finished \tANN training loss 0.219128\n",
      ">> Epoch 51 finished \tANN training loss 0.220457\n",
      ">> Epoch 52 finished \tANN training loss 0.219733\n",
      ">> Epoch 53 finished \tANN training loss 0.217143\n",
      ">> Epoch 54 finished \tANN training loss 0.213681\n",
      ">> Epoch 55 finished \tANN training loss 0.217661\n",
      ">> Epoch 56 finished \tANN training loss 0.213855\n",
      ">> Epoch 57 finished \tANN training loss 0.216073\n",
      ">> Epoch 58 finished \tANN training loss 0.204925\n",
      ">> Epoch 59 finished \tANN training loss 0.214120\n",
      ">> Epoch 60 finished \tANN training loss 0.204864\n",
      ">> Epoch 61 finished \tANN training loss 0.205037\n",
      ">> Epoch 62 finished \tANN training loss 0.200658\n",
      ">> Epoch 63 finished \tANN training loss 0.201439\n",
      ">> Epoch 64 finished \tANN training loss 0.201797\n",
      ">> Epoch 65 finished \tANN training loss 0.197796\n",
      ">> Epoch 66 finished \tANN training loss 0.201273\n",
      ">> Epoch 67 finished \tANN training loss 0.199129\n",
      ">> Epoch 68 finished \tANN training loss 0.191820\n",
      ">> Epoch 69 finished \tANN training loss 0.185820\n",
      ">> Epoch 70 finished \tANN training loss 0.191642\n",
      ">> Epoch 71 finished \tANN training loss 0.193530\n",
      ">> Epoch 72 finished \tANN training loss 0.201490\n",
      ">> Epoch 73 finished \tANN training loss 0.183174\n",
      ">> Epoch 74 finished \tANN training loss 0.191453\n",
      ">> Epoch 75 finished \tANN training loss 0.184355\n",
      ">> Epoch 76 finished \tANN training loss 0.177283\n",
      ">> Epoch 77 finished \tANN training loss 0.181185\n",
      ">> Epoch 78 finished \tANN training loss 0.175510\n",
      ">> Epoch 79 finished \tANN training loss 0.177267\n",
      ">> Epoch 80 finished \tANN training loss 0.168348\n",
      ">> Epoch 81 finished \tANN training loss 0.170192\n",
      ">> Epoch 82 finished \tANN training loss 0.171001\n",
      ">> Epoch 83 finished \tANN training loss 0.173320\n",
      ">> Epoch 84 finished \tANN training loss 0.173404\n",
      ">> Epoch 85 finished \tANN training loss 0.171942\n",
      ">> Epoch 86 finished \tANN training loss 0.171879\n",
      ">> Epoch 87 finished \tANN training loss 0.170637\n",
      ">> Epoch 88 finished \tANN training loss 0.166060\n",
      ">> Epoch 89 finished \tANN training loss 0.168460\n",
      ">> Epoch 90 finished \tANN training loss 0.161801\n",
      ">> Epoch 91 finished \tANN training loss 0.165759\n",
      ">> Epoch 92 finished \tANN training loss 0.162604\n",
      ">> Epoch 93 finished \tANN training loss 0.160371\n",
      ">> Epoch 94 finished \tANN training loss 0.156196\n",
      ">> Epoch 95 finished \tANN training loss 0.161891\n",
      ">> Epoch 96 finished \tANN training loss 0.157538\n",
      ">> Epoch 97 finished \tANN training loss 0.155994\n",
      ">> Epoch 98 finished \tANN training loss 0.159310\n",
      ">> Epoch 99 finished \tANN training loss 0.150435\n",
      ">> Epoch 100 finished \tANN training loss 0.152916\n",
      ">> Epoch 101 finished \tANN training loss 0.153213\n",
      ">> Epoch 102 finished \tANN training loss 0.154478\n",
      ">> Epoch 103 finished \tANN training loss 0.153958\n",
      ">> Epoch 104 finished \tANN training loss 0.144763\n",
      ">> Epoch 105 finished \tANN training loss 0.148127\n",
      ">> Epoch 106 finished \tANN training loss 0.146419\n",
      ">> Epoch 107 finished \tANN training loss 0.143481\n",
      ">> Epoch 108 finished \tANN training loss 0.141018\n",
      ">> Epoch 109 finished \tANN training loss 0.146728\n",
      ">> Epoch 110 finished \tANN training loss 0.140763\n",
      ">> Epoch 111 finished \tANN training loss 0.137469\n",
      ">> Epoch 112 finished \tANN training loss 0.139575\n",
      ">> Epoch 113 finished \tANN training loss 0.139502\n",
      ">> Epoch 114 finished \tANN training loss 0.137920\n",
      ">> Epoch 115 finished \tANN training loss 0.136513\n",
      ">> Epoch 116 finished \tANN training loss 0.145315\n",
      ">> Epoch 117 finished \tANN training loss 0.137122\n",
      ">> Epoch 118 finished \tANN training loss 0.138107\n",
      ">> Epoch 119 finished \tANN training loss 0.137007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 120 finished \tANN training loss 0.134427\n",
      ">> Epoch 121 finished \tANN training loss 0.135621\n",
      ">> Epoch 122 finished \tANN training loss 0.131319\n",
      ">> Epoch 123 finished \tANN training loss 0.133787\n",
      ">> Epoch 124 finished \tANN training loss 0.131439\n",
      ">> Epoch 125 finished \tANN training loss 0.128671\n",
      ">> Epoch 126 finished \tANN training loss 0.134186\n",
      ">> Epoch 127 finished \tANN training loss 0.129938\n",
      ">> Epoch 128 finished \tANN training loss 0.127983\n",
      ">> Epoch 129 finished \tANN training loss 0.125656\n",
      ">> Epoch 130 finished \tANN training loss 0.126113\n",
      ">> Epoch 131 finished \tANN training loss 0.124972\n",
      ">> Epoch 132 finished \tANN training loss 0.123501\n",
      ">> Epoch 133 finished \tANN training loss 0.128233\n",
      ">> Epoch 134 finished \tANN training loss 0.122174\n",
      ">> Epoch 135 finished \tANN training loss 0.121157\n",
      ">> Epoch 136 finished \tANN training loss 0.123222\n",
      ">> Epoch 137 finished \tANN training loss 0.121419\n",
      ">> Epoch 138 finished \tANN training loss 0.123946\n",
      ">> Epoch 139 finished \tANN training loss 0.125110\n",
      ">> Epoch 140 finished \tANN training loss 0.118655\n",
      ">> Epoch 141 finished \tANN training loss 0.120244\n",
      ">> Epoch 142 finished \tANN training loss 0.117132\n",
      ">> Epoch 143 finished \tANN training loss 0.119045\n",
      ">> Epoch 144 finished \tANN training loss 0.122723\n",
      ">> Epoch 145 finished \tANN training loss 0.115598\n",
      ">> Epoch 146 finished \tANN training loss 0.118322\n",
      ">> Epoch 147 finished \tANN training loss 0.112165\n",
      ">> Epoch 148 finished \tANN training loss 0.111550\n",
      ">> Epoch 149 finished \tANN training loss 0.114756\n",
      ">> Epoch 150 finished \tANN training loss 0.111263\n",
      ">> Epoch 151 finished \tANN training loss 0.111981\n",
      ">> Epoch 152 finished \tANN training loss 0.112909\n",
      ">> Epoch 153 finished \tANN training loss 0.112969\n",
      ">> Epoch 154 finished \tANN training loss 0.109700\n",
      ">> Epoch 155 finished \tANN training loss 0.107204\n",
      ">> Epoch 156 finished \tANN training loss 0.107628\n",
      ">> Epoch 157 finished \tANN training loss 0.107137\n",
      ">> Epoch 158 finished \tANN training loss 0.108847\n",
      ">> Epoch 159 finished \tANN training loss 0.107382\n",
      ">> Epoch 160 finished \tANN training loss 0.108037\n",
      ">> Epoch 161 finished \tANN training loss 0.106086\n",
      ">> Epoch 162 finished \tANN training loss 0.106475\n",
      ">> Epoch 163 finished \tANN training loss 0.102585\n",
      ">> Epoch 164 finished \tANN training loss 0.104679\n",
      ">> Epoch 165 finished \tANN training loss 0.104645\n",
      ">> Epoch 166 finished \tANN training loss 0.106817\n",
      ">> Epoch 167 finished \tANN training loss 0.106777\n",
      ">> Epoch 168 finished \tANN training loss 0.103360\n",
      ">> Epoch 169 finished \tANN training loss 0.104491\n",
      ">> Epoch 170 finished \tANN training loss 0.104175\n",
      ">> Epoch 171 finished \tANN training loss 0.102827\n",
      ">> Epoch 172 finished \tANN training loss 0.099703\n",
      ">> Epoch 173 finished \tANN training loss 0.098482\n",
      ">> Epoch 174 finished \tANN training loss 0.098762\n",
      ">> Epoch 175 finished \tANN training loss 0.097532\n",
      ">> Epoch 176 finished \tANN training loss 0.098422\n",
      ">> Epoch 177 finished \tANN training loss 0.095419\n",
      ">> Epoch 178 finished \tANN training loss 0.094636\n",
      ">> Epoch 179 finished \tANN training loss 0.095054\n",
      ">> Epoch 180 finished \tANN training loss 0.095446\n",
      ">> Epoch 181 finished \tANN training loss 0.094544\n",
      ">> Epoch 182 finished \tANN training loss 0.093373\n",
      ">> Epoch 183 finished \tANN training loss 0.093241\n",
      ">> Epoch 184 finished \tANN training loss 0.092431\n",
      ">> Epoch 185 finished \tANN training loss 0.090068\n",
      ">> Epoch 186 finished \tANN training loss 0.092778\n",
      ">> Epoch 187 finished \tANN training loss 0.090008\n",
      ">> Epoch 188 finished \tANN training loss 0.091968\n",
      ">> Epoch 189 finished \tANN training loss 0.090693\n",
      ">> Epoch 190 finished \tANN training loss 0.092100\n",
      ">> Epoch 191 finished \tANN training loss 0.087732\n",
      ">> Epoch 192 finished \tANN training loss 0.093978\n",
      ">> Epoch 193 finished \tANN training loss 0.089193\n",
      ">> Epoch 194 finished \tANN training loss 0.088208\n",
      ">> Epoch 195 finished \tANN training loss 0.086890\n",
      ">> Epoch 196 finished \tANN training loss 0.086318\n",
      ">> Epoch 197 finished \tANN training loss 0.083534\n",
      ">> Epoch 198 finished \tANN training loss 0.087466\n",
      ">> Epoch 199 finished \tANN training loss 0.086072\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.880000\n"
     ]
    }
   ],
   "source": [
    "# Sigmoidal activation, dropout=0.6\n",
    "acc7 = deep_belief_net(hidden_layers_structure=[300], \n",
    "                       learning_rate_rbm=0.05,\n",
    "                       learning_rate=0.1,\n",
    "                       n_epochs_rbm=40,\n",
    "                       n_iter_backprop=200,\n",
    "                       batch_size=32,\n",
    "                       activation_function='sigmoid',\n",
    "                       dropout_p=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.88\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(acc7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 45.129498\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 35.027809\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 30.016743\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 27.039062\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 24.958534\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 23.295965\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 22.010513\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 20.783161\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 19.772827\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 18.897926\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 18.084539\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 17.287664\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 16.526663\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 15.959409\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 15.371964\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 14.881211\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 14.398421\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 13.998322\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 13.635403\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 13.256783\n",
      ">> Epoch 21 finished \tRBM Reconstruction error 12.907571\n",
      ">> Epoch 22 finished \tRBM Reconstruction error 12.537066\n",
      ">> Epoch 23 finished \tRBM Reconstruction error 12.248904\n",
      ">> Epoch 24 finished \tRBM Reconstruction error 11.962505\n",
      ">> Epoch 25 finished \tRBM Reconstruction error 11.719161\n",
      ">> Epoch 26 finished \tRBM Reconstruction error 11.460996\n",
      ">> Epoch 27 finished \tRBM Reconstruction error 11.195226\n",
      ">> Epoch 28 finished \tRBM Reconstruction error 10.997808\n",
      ">> Epoch 29 finished \tRBM Reconstruction error 10.800192\n",
      ">> Epoch 30 finished \tRBM Reconstruction error 10.568363\n",
      ">> Epoch 31 finished \tRBM Reconstruction error 10.428193\n",
      ">> Epoch 32 finished \tRBM Reconstruction error 10.189788\n",
      ">> Epoch 33 finished \tRBM Reconstruction error 10.028370\n",
      ">> Epoch 34 finished \tRBM Reconstruction error 9.871050\n",
      ">> Epoch 35 finished \tRBM Reconstruction error 9.679565\n",
      ">> Epoch 36 finished \tRBM Reconstruction error 9.528945\n",
      ">> Epoch 37 finished \tRBM Reconstruction error 9.363136\n",
      ">> Epoch 38 finished \tRBM Reconstruction error 9.186206\n",
      ">> Epoch 39 finished \tRBM Reconstruction error 9.027005\n",
      ">> Epoch 40 finished \tRBM Reconstruction error 8.889925\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 1.168265\n",
      ">> Epoch 1 finished \tANN training loss 0.817597\n",
      ">> Epoch 2 finished \tANN training loss 0.705631\n",
      ">> Epoch 3 finished \tANN training loss 0.611002\n",
      ">> Epoch 4 finished \tANN training loss 0.566429\n",
      ">> Epoch 5 finished \tANN training loss 0.555223\n",
      ">> Epoch 6 finished \tANN training loss 0.538432\n",
      ">> Epoch 7 finished \tANN training loss 0.491775\n",
      ">> Epoch 8 finished \tANN training loss 0.493121\n",
      ">> Epoch 9 finished \tANN training loss 0.455786\n",
      ">> Epoch 10 finished \tANN training loss 0.451377\n",
      ">> Epoch 11 finished \tANN training loss 0.443678\n",
      ">> Epoch 12 finished \tANN training loss 0.447752\n",
      ">> Epoch 13 finished \tANN training loss 0.409869\n",
      ">> Epoch 14 finished \tANN training loss 0.416259\n",
      ">> Epoch 15 finished \tANN training loss 0.403333\n",
      ">> Epoch 16 finished \tANN training loss 0.406020\n",
      ">> Epoch 17 finished \tANN training loss 0.402217\n",
      ">> Epoch 18 finished \tANN training loss 0.386973\n",
      ">> Epoch 19 finished \tANN training loss 0.392831\n",
      ">> Epoch 20 finished \tANN training loss 0.372542\n",
      ">> Epoch 21 finished \tANN training loss 0.367313\n",
      ">> Epoch 22 finished \tANN training loss 0.372490\n",
      ">> Epoch 23 finished \tANN training loss 0.365962\n",
      ">> Epoch 24 finished \tANN training loss 0.374955\n",
      ">> Epoch 25 finished \tANN training loss 0.366566\n",
      ">> Epoch 26 finished \tANN training loss 0.348136\n",
      ">> Epoch 27 finished \tANN training loss 0.357677\n",
      ">> Epoch 28 finished \tANN training loss 0.340200\n",
      ">> Epoch 29 finished \tANN training loss 0.355988\n",
      ">> Epoch 30 finished \tANN training loss 0.340412\n",
      ">> Epoch 31 finished \tANN training loss 0.341756\n",
      ">> Epoch 32 finished \tANN training loss 0.333226\n",
      ">> Epoch 33 finished \tANN training loss 0.340298\n",
      ">> Epoch 34 finished \tANN training loss 0.321034\n",
      ">> Epoch 35 finished \tANN training loss 0.333012\n",
      ">> Epoch 36 finished \tANN training loss 0.343355\n",
      ">> Epoch 37 finished \tANN training loss 0.329214\n",
      ">> Epoch 38 finished \tANN training loss 0.319972\n",
      ">> Epoch 39 finished \tANN training loss 0.316643\n",
      ">> Epoch 40 finished \tANN training loss 0.302144\n",
      ">> Epoch 41 finished \tANN training loss 0.309066\n",
      ">> Epoch 42 finished \tANN training loss 0.305790\n",
      ">> Epoch 43 finished \tANN training loss 0.309620\n",
      ">> Epoch 44 finished \tANN training loss 0.297511\n",
      ">> Epoch 45 finished \tANN training loss 0.298379\n",
      ">> Epoch 46 finished \tANN training loss 0.300215\n",
      ">> Epoch 47 finished \tANN training loss 0.302717\n",
      ">> Epoch 48 finished \tANN training loss 0.299086\n",
      ">> Epoch 49 finished \tANN training loss 0.293210\n",
      ">> Epoch 50 finished \tANN training loss 0.299337\n",
      ">> Epoch 51 finished \tANN training loss 0.291601\n",
      ">> Epoch 52 finished \tANN training loss 0.286936\n",
      ">> Epoch 53 finished \tANN training loss 0.284368\n",
      ">> Epoch 54 finished \tANN training loss 0.291358\n",
      ">> Epoch 55 finished \tANN training loss 0.280798\n",
      ">> Epoch 56 finished \tANN training loss 0.284360\n",
      ">> Epoch 57 finished \tANN training loss 0.287753\n",
      ">> Epoch 58 finished \tANN training loss 0.283120\n",
      ">> Epoch 59 finished \tANN training loss 0.277539\n",
      ">> Epoch 60 finished \tANN training loss 0.288471\n",
      ">> Epoch 61 finished \tANN training loss 0.278024\n",
      ">> Epoch 62 finished \tANN training loss 0.277743\n",
      ">> Epoch 63 finished \tANN training loss 0.277031\n",
      ">> Epoch 64 finished \tANN training loss 0.283207\n",
      ">> Epoch 65 finished \tANN training loss 0.272383\n",
      ">> Epoch 66 finished \tANN training loss 0.278617\n",
      ">> Epoch 67 finished \tANN training loss 0.268871\n",
      ">> Epoch 68 finished \tANN training loss 0.263037\n",
      ">> Epoch 69 finished \tANN training loss 0.271641\n",
      ">> Epoch 70 finished \tANN training loss 0.267790\n",
      ">> Epoch 71 finished \tANN training loss 0.266842\n",
      ">> Epoch 72 finished \tANN training loss 0.259844\n",
      ">> Epoch 73 finished \tANN training loss 0.261224\n",
      ">> Epoch 74 finished \tANN training loss 0.266322\n",
      ">> Epoch 75 finished \tANN training loss 0.255454\n",
      ">> Epoch 76 finished \tANN training loss 0.259321\n",
      ">> Epoch 77 finished \tANN training loss 0.251795\n",
      ">> Epoch 78 finished \tANN training loss 0.257515\n",
      ">> Epoch 79 finished \tANN training loss 0.251228\n",
      ">> Epoch 80 finished \tANN training loss 0.255615\n",
      ">> Epoch 81 finished \tANN training loss 0.252230\n",
      ">> Epoch 82 finished \tANN training loss 0.252102\n",
      ">> Epoch 83 finished \tANN training loss 0.244738\n",
      ">> Epoch 84 finished \tANN training loss 0.242152\n",
      ">> Epoch 85 finished \tANN training loss 0.246067\n",
      ">> Epoch 86 finished \tANN training loss 0.243291\n",
      ">> Epoch 87 finished \tANN training loss 0.248281\n",
      ">> Epoch 88 finished \tANN training loss 0.240058\n",
      ">> Epoch 89 finished \tANN training loss 0.239271\n",
      ">> Epoch 90 finished \tANN training loss 0.238568\n",
      ">> Epoch 91 finished \tANN training loss 0.237210\n",
      ">> Epoch 92 finished \tANN training loss 0.252273\n",
      ">> Epoch 93 finished \tANN training loss 0.246739\n",
      ">> Epoch 94 finished \tANN training loss 0.238642\n",
      ">> Epoch 95 finished \tANN training loss 0.239068\n",
      ">> Epoch 96 finished \tANN training loss 0.231104\n",
      ">> Epoch 97 finished \tANN training loss 0.234601\n",
      ">> Epoch 98 finished \tANN training loss 0.242512\n",
      ">> Epoch 99 finished \tANN training loss 0.228658\n",
      ">> Epoch 100 finished \tANN training loss 0.234066\n",
      ">> Epoch 101 finished \tANN training loss 0.236769\n",
      ">> Epoch 102 finished \tANN training loss 0.225873\n",
      ">> Epoch 103 finished \tANN training loss 0.228414\n",
      ">> Epoch 104 finished \tANN training loss 0.225910\n",
      ">> Epoch 105 finished \tANN training loss 0.225697\n",
      ">> Epoch 106 finished \tANN training loss 0.223208\n",
      ">> Epoch 107 finished \tANN training loss 0.221879\n",
      ">> Epoch 108 finished \tANN training loss 0.221763\n",
      ">> Epoch 109 finished \tANN training loss 0.222117\n",
      ">> Epoch 110 finished \tANN training loss 0.220784\n",
      ">> Epoch 111 finished \tANN training loss 0.227739\n",
      ">> Epoch 112 finished \tANN training loss 0.215243\n",
      ">> Epoch 113 finished \tANN training loss 0.226366\n",
      ">> Epoch 114 finished \tANN training loss 0.214206\n",
      ">> Epoch 115 finished \tANN training loss 0.220646\n",
      ">> Epoch 116 finished \tANN training loss 0.216523\n",
      ">> Epoch 117 finished \tANN training loss 0.212300\n",
      ">> Epoch 118 finished \tANN training loss 0.209034\n",
      ">> Epoch 119 finished \tANN training loss 0.213692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 120 finished \tANN training loss 0.213162\n",
      ">> Epoch 121 finished \tANN training loss 0.211234\n",
      ">> Epoch 122 finished \tANN training loss 0.215013\n",
      ">> Epoch 123 finished \tANN training loss 0.218264\n",
      ">> Epoch 124 finished \tANN training loss 0.209774\n",
      ">> Epoch 125 finished \tANN training loss 0.208416\n",
      ">> Epoch 126 finished \tANN training loss 0.204336\n",
      ">> Epoch 127 finished \tANN training loss 0.208802\n",
      ">> Epoch 128 finished \tANN training loss 0.202960\n",
      ">> Epoch 129 finished \tANN training loss 0.208405\n",
      ">> Epoch 130 finished \tANN training loss 0.203195\n",
      ">> Epoch 131 finished \tANN training loss 0.199603\n",
      ">> Epoch 132 finished \tANN training loss 0.204918\n",
      ">> Epoch 133 finished \tANN training loss 0.203853\n",
      ">> Epoch 134 finished \tANN training loss 0.197251\n",
      ">> Epoch 135 finished \tANN training loss 0.197964\n",
      ">> Epoch 136 finished \tANN training loss 0.201631\n",
      ">> Epoch 137 finished \tANN training loss 0.199562\n",
      ">> Epoch 138 finished \tANN training loss 0.195648\n",
      ">> Epoch 139 finished \tANN training loss 0.194599\n",
      ">> Epoch 140 finished \tANN training loss 0.194024\n",
      ">> Epoch 141 finished \tANN training loss 0.189902\n",
      ">> Epoch 142 finished \tANN training loss 0.190317\n",
      ">> Epoch 143 finished \tANN training loss 0.198279\n",
      ">> Epoch 144 finished \tANN training loss 0.191549\n",
      ">> Epoch 145 finished \tANN training loss 0.190931\n",
      ">> Epoch 146 finished \tANN training loss 0.190822\n",
      ">> Epoch 147 finished \tANN training loss 0.190905\n",
      ">> Epoch 148 finished \tANN training loss 0.190692\n",
      ">> Epoch 149 finished \tANN training loss 0.190779\n",
      ">> Epoch 150 finished \tANN training loss 0.190282\n",
      ">> Epoch 151 finished \tANN training loss 0.186290\n",
      ">> Epoch 152 finished \tANN training loss 0.183520\n",
      ">> Epoch 153 finished \tANN training loss 0.185414\n",
      ">> Epoch 154 finished \tANN training loss 0.182725\n",
      ">> Epoch 155 finished \tANN training loss 0.180402\n",
      ">> Epoch 156 finished \tANN training loss 0.182977\n",
      ">> Epoch 157 finished \tANN training loss 0.180846\n",
      ">> Epoch 158 finished \tANN training loss 0.179465\n",
      ">> Epoch 159 finished \tANN training loss 0.187546\n",
      ">> Epoch 160 finished \tANN training loss 0.183646\n",
      ">> Epoch 161 finished \tANN training loss 0.180903\n",
      ">> Epoch 162 finished \tANN training loss 0.187745\n",
      ">> Epoch 163 finished \tANN training loss 0.186265\n",
      ">> Epoch 164 finished \tANN training loss 0.184046\n",
      ">> Epoch 165 finished \tANN training loss 0.178230\n",
      ">> Epoch 166 finished \tANN training loss 0.184532\n",
      ">> Epoch 167 finished \tANN training loss 0.178035\n",
      ">> Epoch 168 finished \tANN training loss 0.177425\n",
      ">> Epoch 169 finished \tANN training loss 0.174099\n",
      ">> Epoch 170 finished \tANN training loss 0.177033\n",
      ">> Epoch 171 finished \tANN training loss 0.179848\n",
      ">> Epoch 172 finished \tANN training loss 0.176363\n",
      ">> Epoch 173 finished \tANN training loss 0.174290\n",
      ">> Epoch 174 finished \tANN training loss 0.172699\n",
      ">> Epoch 175 finished \tANN training loss 0.177080\n",
      ">> Epoch 176 finished \tANN training loss 0.176676\n",
      ">> Epoch 177 finished \tANN training loss 0.172992\n",
      ">> Epoch 178 finished \tANN training loss 0.174406\n",
      ">> Epoch 179 finished \tANN training loss 0.172644\n",
      ">> Epoch 180 finished \tANN training loss 0.171962\n",
      ">> Epoch 181 finished \tANN training loss 0.170383\n",
      ">> Epoch 182 finished \tANN training loss 0.169441\n",
      ">> Epoch 183 finished \tANN training loss 0.171784\n",
      ">> Epoch 184 finished \tANN training loss 0.172497\n",
      ">> Epoch 185 finished \tANN training loss 0.172056\n",
      ">> Epoch 186 finished \tANN training loss 0.166534\n",
      ">> Epoch 187 finished \tANN training loss 0.165108\n",
      ">> Epoch 188 finished \tANN training loss 0.173310\n",
      ">> Epoch 189 finished \tANN training loss 0.169985\n",
      ">> Epoch 190 finished \tANN training loss 0.166613\n",
      ">> Epoch 191 finished \tANN training loss 0.165692\n",
      ">> Epoch 192 finished \tANN training loss 0.165352\n",
      ">> Epoch 193 finished \tANN training loss 0.165666\n",
      ">> Epoch 194 finished \tANN training loss 0.165380\n",
      ">> Epoch 195 finished \tANN training loss 0.162680\n",
      ">> Epoch 196 finished \tANN training loss 0.162424\n",
      ">> Epoch 197 finished \tANN training loss 0.163724\n",
      ">> Epoch 198 finished \tANN training loss 0.159991\n",
      ">> Epoch 199 finished \tANN training loss 0.159692\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.850000\n"
     ]
    }
   ],
   "source": [
    "# Sigmoidal activation, dropout=0.7\n",
    "acc8 = deep_belief_net(hidden_layers_structure=[300], \n",
    "                       learning_rate_rbm=0.05,\n",
    "                       learning_rate=0.1,\n",
    "                       n_epochs_rbm=40,\n",
    "                       n_iter_backprop=200,\n",
    "                       batch_size=32,\n",
    "                       activation_function='sigmoid',\n",
    "                       dropout_p=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.85\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(acc8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 44.051876\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 35.091831\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 30.071026\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 26.948423\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 24.779713\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 23.062523\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 21.873995\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 20.669756\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 19.651505\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 18.680679\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 17.984848\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 17.155359\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 16.523981\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 15.846703\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 15.326969\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 14.814431\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 14.364479\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 13.926455\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 13.601260\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 13.149766\n",
      ">> Epoch 21 finished \tRBM Reconstruction error 12.794353\n",
      ">> Epoch 22 finished \tRBM Reconstruction error 12.522580\n",
      ">> Epoch 23 finished \tRBM Reconstruction error 12.163539\n",
      ">> Epoch 24 finished \tRBM Reconstruction error 11.921522\n",
      ">> Epoch 25 finished \tRBM Reconstruction error 11.649753\n",
      ">> Epoch 26 finished \tRBM Reconstruction error 11.368609\n",
      ">> Epoch 27 finished \tRBM Reconstruction error 11.125876\n",
      ">> Epoch 28 finished \tRBM Reconstruction error 10.889465\n",
      ">> Epoch 29 finished \tRBM Reconstruction error 10.717951\n",
      ">> Epoch 30 finished \tRBM Reconstruction error 10.532270\n",
      ">> Epoch 31 finished \tRBM Reconstruction error 10.301174\n",
      ">> Epoch 32 finished \tRBM Reconstruction error 10.070171\n",
      ">> Epoch 33 finished \tRBM Reconstruction error 9.890275\n",
      ">> Epoch 34 finished \tRBM Reconstruction error 9.736781\n",
      ">> Epoch 35 finished \tRBM Reconstruction error 9.579923\n",
      ">> Epoch 36 finished \tRBM Reconstruction error 9.400433\n",
      ">> Epoch 37 finished \tRBM Reconstruction error 9.264004\n",
      ">> Epoch 38 finished \tRBM Reconstruction error 9.137755\n",
      ">> Epoch 39 finished \tRBM Reconstruction error 8.970227\n",
      ">> Epoch 40 finished \tRBM Reconstruction error 8.831291\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 1.197311\n",
      ">> Epoch 1 finished \tANN training loss 0.879009\n",
      ">> Epoch 2 finished \tANN training loss 0.786728\n",
      ">> Epoch 3 finished \tANN training loss 0.746337\n",
      ">> Epoch 4 finished \tANN training loss 0.663383\n",
      ">> Epoch 5 finished \tANN training loss 0.650929\n",
      ">> Epoch 6 finished \tANN training loss 0.608713\n",
      ">> Epoch 7 finished \tANN training loss 0.616611\n",
      ">> Epoch 8 finished \tANN training loss 0.570545\n",
      ">> Epoch 9 finished \tANN training loss 0.548444\n",
      ">> Epoch 10 finished \tANN training loss 0.566834\n",
      ">> Epoch 11 finished \tANN training loss 0.543822\n",
      ">> Epoch 12 finished \tANN training loss 0.535848\n",
      ">> Epoch 13 finished \tANN training loss 0.523275\n",
      ">> Epoch 14 finished \tANN training loss 0.546479\n",
      ">> Epoch 15 finished \tANN training loss 0.548565\n",
      ">> Epoch 16 finished \tANN training loss 0.505017\n",
      ">> Epoch 17 finished \tANN training loss 0.504105\n",
      ">> Epoch 18 finished \tANN training loss 0.510965\n",
      ">> Epoch 19 finished \tANN training loss 0.484821\n",
      ">> Epoch 20 finished \tANN training loss 0.505967\n",
      ">> Epoch 21 finished \tANN training loss 0.497168\n",
      ">> Epoch 22 finished \tANN training loss 0.501077\n",
      ">> Epoch 23 finished \tANN training loss 0.491682\n",
      ">> Epoch 24 finished \tANN training loss 0.489566\n",
      ">> Epoch 25 finished \tANN training loss 0.495663\n",
      ">> Epoch 26 finished \tANN training loss 0.468524\n",
      ">> Epoch 27 finished \tANN training loss 0.489247\n",
      ">> Epoch 28 finished \tANN training loss 0.473931\n",
      ">> Epoch 29 finished \tANN training loss 0.486811\n",
      ">> Epoch 30 finished \tANN training loss 0.501228\n",
      ">> Epoch 31 finished \tANN training loss 0.481001\n",
      ">> Epoch 32 finished \tANN training loss 0.486516\n",
      ">> Epoch 33 finished \tANN training loss 0.479438\n",
      ">> Epoch 34 finished \tANN training loss 0.480938\n",
      ">> Epoch 35 finished \tANN training loss 0.459002\n",
      ">> Epoch 36 finished \tANN training loss 0.464196\n",
      ">> Epoch 37 finished \tANN training loss 0.453129\n",
      ">> Epoch 38 finished \tANN training loss 0.468088\n",
      ">> Epoch 39 finished \tANN training loss 0.448380\n",
      ">> Epoch 40 finished \tANN training loss 0.454965\n",
      ">> Epoch 41 finished \tANN training loss 0.434930\n",
      ">> Epoch 42 finished \tANN training loss 0.442347\n",
      ">> Epoch 43 finished \tANN training loss 0.457862\n",
      ">> Epoch 44 finished \tANN training loss 0.449341\n",
      ">> Epoch 45 finished \tANN training loss 0.435763\n",
      ">> Epoch 46 finished \tANN training loss 0.434142\n",
      ">> Epoch 47 finished \tANN training loss 0.434923\n",
      ">> Epoch 48 finished \tANN training loss 0.436819\n",
      ">> Epoch 49 finished \tANN training loss 0.434249\n",
      ">> Epoch 50 finished \tANN training loss 0.430929\n",
      ">> Epoch 51 finished \tANN training loss 0.420766\n",
      ">> Epoch 52 finished \tANN training loss 0.455648\n",
      ">> Epoch 53 finished \tANN training loss 0.430054\n",
      ">> Epoch 54 finished \tANN training loss 0.426654\n",
      ">> Epoch 55 finished \tANN training loss 0.426639\n",
      ">> Epoch 56 finished \tANN training loss 0.459979\n",
      ">> Epoch 57 finished \tANN training loss 0.436670\n",
      ">> Epoch 58 finished \tANN training loss 0.456835\n",
      ">> Epoch 59 finished \tANN training loss 0.419081\n",
      ">> Epoch 60 finished \tANN training loss 0.423072\n",
      ">> Epoch 61 finished \tANN training loss 0.425299\n",
      ">> Epoch 62 finished \tANN training loss 0.416078\n",
      ">> Epoch 63 finished \tANN training loss 0.426492\n",
      ">> Epoch 64 finished \tANN training loss 0.419595\n",
      ">> Epoch 65 finished \tANN training loss 0.410276\n",
      ">> Epoch 66 finished \tANN training loss 0.422648\n",
      ">> Epoch 67 finished \tANN training loss 0.433058\n",
      ">> Epoch 68 finished \tANN training loss 0.421340\n",
      ">> Epoch 69 finished \tANN training loss 0.422265\n",
      ">> Epoch 70 finished \tANN training loss 0.420131\n",
      ">> Epoch 71 finished \tANN training loss 0.420609\n",
      ">> Epoch 72 finished \tANN training loss 0.417104\n",
      ">> Epoch 73 finished \tANN training loss 0.418925\n",
      ">> Epoch 74 finished \tANN training loss 0.407185\n",
      ">> Epoch 75 finished \tANN training loss 0.406672\n",
      ">> Epoch 76 finished \tANN training loss 0.405924\n",
      ">> Epoch 77 finished \tANN training loss 0.412633\n",
      ">> Epoch 78 finished \tANN training loss 0.404423\n",
      ">> Epoch 79 finished \tANN training loss 0.402863\n",
      ">> Epoch 80 finished \tANN training loss 0.398761\n",
      ">> Epoch 81 finished \tANN training loss 0.393052\n",
      ">> Epoch 82 finished \tANN training loss 0.398918\n",
      ">> Epoch 83 finished \tANN training loss 0.398103\n",
      ">> Epoch 84 finished \tANN training loss 0.400057\n",
      ">> Epoch 85 finished \tANN training loss 0.396988\n",
      ">> Epoch 86 finished \tANN training loss 0.389918\n",
      ">> Epoch 87 finished \tANN training loss 0.393875\n",
      ">> Epoch 88 finished \tANN training loss 0.384677\n",
      ">> Epoch 89 finished \tANN training loss 0.396135\n",
      ">> Epoch 90 finished \tANN training loss 0.388414\n",
      ">> Epoch 91 finished \tANN training loss 0.386503\n",
      ">> Epoch 92 finished \tANN training loss 0.387332\n",
      ">> Epoch 93 finished \tANN training loss 0.388569\n",
      ">> Epoch 94 finished \tANN training loss 0.402984\n",
      ">> Epoch 95 finished \tANN training loss 0.393824\n",
      ">> Epoch 96 finished \tANN training loss 0.397785\n",
      ">> Epoch 97 finished \tANN training loss 0.390353\n",
      ">> Epoch 98 finished \tANN training loss 0.400741\n",
      ">> Epoch 99 finished \tANN training loss 0.399407\n",
      ">> Epoch 100 finished \tANN training loss 0.397126\n",
      ">> Epoch 101 finished \tANN training loss 0.388735\n",
      ">> Epoch 102 finished \tANN training loss 0.387563\n",
      ">> Epoch 103 finished \tANN training loss 0.385610\n",
      ">> Epoch 104 finished \tANN training loss 0.400191\n",
      ">> Epoch 105 finished \tANN training loss 0.396197\n",
      ">> Epoch 106 finished \tANN training loss 0.402334\n",
      ">> Epoch 107 finished \tANN training loss 0.414529\n",
      ">> Epoch 108 finished \tANN training loss 0.401707\n",
      ">> Epoch 109 finished \tANN training loss 0.385221\n",
      ">> Epoch 110 finished \tANN training loss 0.378387\n",
      ">> Epoch 111 finished \tANN training loss 0.373294\n",
      ">> Epoch 112 finished \tANN training loss 0.393576\n",
      ">> Epoch 113 finished \tANN training loss 0.374799\n",
      ">> Epoch 114 finished \tANN training loss 0.385533\n",
      ">> Epoch 115 finished \tANN training loss 0.382824\n",
      ">> Epoch 116 finished \tANN training loss 0.373538\n",
      ">> Epoch 117 finished \tANN training loss 0.384861\n",
      ">> Epoch 118 finished \tANN training loss 0.378914\n",
      ">> Epoch 119 finished \tANN training loss 0.376635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 120 finished \tANN training loss 0.380071\n",
      ">> Epoch 121 finished \tANN training loss 0.393818\n",
      ">> Epoch 122 finished \tANN training loss 0.374101\n",
      ">> Epoch 123 finished \tANN training loss 0.372263\n",
      ">> Epoch 124 finished \tANN training loss 0.375740\n",
      ">> Epoch 125 finished \tANN training loss 0.362361\n",
      ">> Epoch 126 finished \tANN training loss 0.373870\n",
      ">> Epoch 127 finished \tANN training loss 0.376361\n",
      ">> Epoch 128 finished \tANN training loss 0.370081\n",
      ">> Epoch 129 finished \tANN training loss 0.373316\n",
      ">> Epoch 130 finished \tANN training loss 0.371415\n",
      ">> Epoch 131 finished \tANN training loss 0.371890\n",
      ">> Epoch 132 finished \tANN training loss 0.373026\n",
      ">> Epoch 133 finished \tANN training loss 0.372642\n",
      ">> Epoch 134 finished \tANN training loss 0.364450\n",
      ">> Epoch 135 finished \tANN training loss 0.367608\n",
      ">> Epoch 136 finished \tANN training loss 0.374121\n",
      ">> Epoch 137 finished \tANN training loss 0.364372\n",
      ">> Epoch 138 finished \tANN training loss 0.366773\n",
      ">> Epoch 139 finished \tANN training loss 0.366469\n",
      ">> Epoch 140 finished \tANN training loss 0.365046\n",
      ">> Epoch 141 finished \tANN training loss 0.363372\n",
      ">> Epoch 142 finished \tANN training loss 0.364958\n",
      ">> Epoch 143 finished \tANN training loss 0.361661\n",
      ">> Epoch 144 finished \tANN training loss 0.365296\n",
      ">> Epoch 145 finished \tANN training loss 0.369483\n",
      ">> Epoch 146 finished \tANN training loss 0.373509\n",
      ">> Epoch 147 finished \tANN training loss 0.362654\n",
      ">> Epoch 148 finished \tANN training loss 0.363740\n",
      ">> Epoch 149 finished \tANN training loss 0.372904\n",
      ">> Epoch 150 finished \tANN training loss 0.362907\n",
      ">> Epoch 151 finished \tANN training loss 0.372450\n",
      ">> Epoch 152 finished \tANN training loss 0.359762\n",
      ">> Epoch 153 finished \tANN training loss 0.359082\n",
      ">> Epoch 154 finished \tANN training loss 0.360989\n",
      ">> Epoch 155 finished \tANN training loss 0.361960\n",
      ">> Epoch 156 finished \tANN training loss 0.374389\n",
      ">> Epoch 157 finished \tANN training loss 0.360220\n",
      ">> Epoch 158 finished \tANN training loss 0.358409\n",
      ">> Epoch 159 finished \tANN training loss 0.361251\n",
      ">> Epoch 160 finished \tANN training loss 0.356795\n",
      ">> Epoch 161 finished \tANN training loss 0.357533\n",
      ">> Epoch 162 finished \tANN training loss 0.351548\n",
      ">> Epoch 163 finished \tANN training loss 0.348298\n",
      ">> Epoch 164 finished \tANN training loss 0.351413\n",
      ">> Epoch 165 finished \tANN training loss 0.350194\n",
      ">> Epoch 166 finished \tANN training loss 0.342127\n",
      ">> Epoch 167 finished \tANN training loss 0.345565\n",
      ">> Epoch 168 finished \tANN training loss 0.342463\n",
      ">> Epoch 169 finished \tANN training loss 0.351872\n",
      ">> Epoch 170 finished \tANN training loss 0.354732\n",
      ">> Epoch 171 finished \tANN training loss 0.353189\n",
      ">> Epoch 172 finished \tANN training loss 0.347826\n",
      ">> Epoch 173 finished \tANN training loss 0.348754\n",
      ">> Epoch 174 finished \tANN training loss 0.338771\n",
      ">> Epoch 175 finished \tANN training loss 0.343471\n",
      ">> Epoch 176 finished \tANN training loss 0.362243\n",
      ">> Epoch 177 finished \tANN training loss 0.357183\n",
      ">> Epoch 178 finished \tANN training loss 0.344816\n",
      ">> Epoch 179 finished \tANN training loss 0.342733\n",
      ">> Epoch 180 finished \tANN training loss 0.345733\n",
      ">> Epoch 181 finished \tANN training loss 0.347818\n",
      ">> Epoch 182 finished \tANN training loss 0.339812\n",
      ">> Epoch 183 finished \tANN training loss 0.336914\n",
      ">> Epoch 184 finished \tANN training loss 0.339579\n",
      ">> Epoch 185 finished \tANN training loss 0.342231\n",
      ">> Epoch 186 finished \tANN training loss 0.343136\n",
      ">> Epoch 187 finished \tANN training loss 0.346076\n",
      ">> Epoch 188 finished \tANN training loss 0.345793\n",
      ">> Epoch 189 finished \tANN training loss 0.349371\n",
      ">> Epoch 190 finished \tANN training loss 0.348608\n",
      ">> Epoch 191 finished \tANN training loss 0.343179\n",
      ">> Epoch 192 finished \tANN training loss 0.351237\n",
      ">> Epoch 193 finished \tANN training loss 0.345751\n",
      ">> Epoch 194 finished \tANN training loss 0.348666\n",
      ">> Epoch 195 finished \tANN training loss 0.337579\n",
      ">> Epoch 196 finished \tANN training loss 0.340430\n",
      ">> Epoch 197 finished \tANN training loss 0.344151\n",
      ">> Epoch 198 finished \tANN training loss 0.335697\n",
      ">> Epoch 199 finished \tANN training loss 0.338536\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.845000\n"
     ]
    }
   ],
   "source": [
    "# Sigmoidal activation, dropout=0.8\n",
    "acc9 = deep_belief_net(hidden_layers_structure=[300], \n",
    "                       learning_rate_rbm=0.05,\n",
    "                       learning_rate=0.1,\n",
    "                       n_epochs_rbm=40,\n",
    "                       n_iter_backprop=200,\n",
    "                       batch_size=32,\n",
    "                       activation_function='sigmoid',\n",
    "                       dropout_p=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.845\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(acc9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 44.711769\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 34.974682\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 30.115257\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 26.901627\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 24.820967\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 23.321211\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 21.972878\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 20.911711\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 19.807693\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 18.874910\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 17.981621\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 17.216724\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 16.589233\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 15.975595\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 15.418973\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 14.935682\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 14.402187\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 14.054141\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 13.662756\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 13.378401\n",
      ">> Epoch 21 finished \tRBM Reconstruction error 12.954887\n",
      ">> Epoch 22 finished \tRBM Reconstruction error 12.631792\n",
      ">> Epoch 23 finished \tRBM Reconstruction error 12.329613\n",
      ">> Epoch 24 finished \tRBM Reconstruction error 12.060605\n",
      ">> Epoch 25 finished \tRBM Reconstruction error 11.800629\n",
      ">> Epoch 26 finished \tRBM Reconstruction error 11.531872\n",
      ">> Epoch 27 finished \tRBM Reconstruction error 11.342194\n",
      ">> Epoch 28 finished \tRBM Reconstruction error 11.117928\n",
      ">> Epoch 29 finished \tRBM Reconstruction error 10.846569\n",
      ">> Epoch 30 finished \tRBM Reconstruction error 10.632398\n",
      ">> Epoch 31 finished \tRBM Reconstruction error 10.447753\n",
      ">> Epoch 32 finished \tRBM Reconstruction error 10.231464\n",
      ">> Epoch 33 finished \tRBM Reconstruction error 10.074735\n",
      ">> Epoch 34 finished \tRBM Reconstruction error 9.834185\n",
      ">> Epoch 35 finished \tRBM Reconstruction error 9.672203\n",
      ">> Epoch 36 finished \tRBM Reconstruction error 9.502356\n",
      ">> Epoch 37 finished \tRBM Reconstruction error 9.373305\n",
      ">> Epoch 38 finished \tRBM Reconstruction error 9.212821\n",
      ">> Epoch 39 finished \tRBM Reconstruction error 9.033991\n",
      ">> Epoch 40 finished \tRBM Reconstruction error 8.924314\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 1.448642\n",
      ">> Epoch 1 finished \tANN training loss 1.237007\n",
      ">> Epoch 2 finished \tANN training loss 1.055958\n",
      ">> Epoch 3 finished \tANN training loss 1.114109\n",
      ">> Epoch 4 finished \tANN training loss 0.981095\n",
      ">> Epoch 5 finished \tANN training loss 0.995394\n",
      ">> Epoch 6 finished \tANN training loss 0.974523\n",
      ">> Epoch 7 finished \tANN training loss 0.943195\n",
      ">> Epoch 8 finished \tANN training loss 0.959481\n",
      ">> Epoch 9 finished \tANN training loss 0.893483\n",
      ">> Epoch 10 finished \tANN training loss 0.889213\n",
      ">> Epoch 11 finished \tANN training loss 0.963330\n",
      ">> Epoch 12 finished \tANN training loss 0.889548\n",
      ">> Epoch 13 finished \tANN training loss 0.898284\n",
      ">> Epoch 14 finished \tANN training loss 0.933416\n",
      ">> Epoch 15 finished \tANN training loss 0.924219\n",
      ">> Epoch 16 finished \tANN training loss 0.866303\n",
      ">> Epoch 17 finished \tANN training loss 0.862005\n",
      ">> Epoch 18 finished \tANN training loss 0.898758\n",
      ">> Epoch 19 finished \tANN training loss 0.881912\n",
      ">> Epoch 20 finished \tANN training loss 0.893247\n",
      ">> Epoch 21 finished \tANN training loss 0.887337\n",
      ">> Epoch 22 finished \tANN training loss 0.875583\n",
      ">> Epoch 23 finished \tANN training loss 0.892087\n",
      ">> Epoch 24 finished \tANN training loss 0.913884\n",
      ">> Epoch 25 finished \tANN training loss 0.928366\n",
      ">> Epoch 26 finished \tANN training loss 0.871171\n",
      ">> Epoch 27 finished \tANN training loss 0.884568\n",
      ">> Epoch 28 finished \tANN training loss 0.871815\n",
      ">> Epoch 29 finished \tANN training loss 0.893337\n",
      ">> Epoch 30 finished \tANN training loss 0.890160\n",
      ">> Epoch 31 finished \tANN training loss 0.866827\n",
      ">> Epoch 32 finished \tANN training loss 0.902140\n",
      ">> Epoch 33 finished \tANN training loss 0.921352\n",
      ">> Epoch 34 finished \tANN training loss 0.904768\n",
      ">> Epoch 35 finished \tANN training loss 0.892043\n",
      ">> Epoch 36 finished \tANN training loss 0.905209\n",
      ">> Epoch 37 finished \tANN training loss 0.916492\n",
      ">> Epoch 38 finished \tANN training loss 0.895462\n",
      ">> Epoch 39 finished \tANN training loss 0.901257\n",
      ">> Epoch 40 finished \tANN training loss 0.891997\n",
      ">> Epoch 41 finished \tANN training loss 0.914339\n",
      ">> Epoch 42 finished \tANN training loss 0.892619\n",
      ">> Epoch 43 finished \tANN training loss 0.904813\n",
      ">> Epoch 44 finished \tANN training loss 0.909045\n",
      ">> Epoch 45 finished \tANN training loss 0.915594\n",
      ">> Epoch 46 finished \tANN training loss 0.955341\n",
      ">> Epoch 47 finished \tANN training loss 0.923195\n",
      ">> Epoch 48 finished \tANN training loss 0.918469\n",
      ">> Epoch 49 finished \tANN training loss 0.940288\n",
      ">> Epoch 50 finished \tANN training loss 0.941440\n",
      ">> Epoch 51 finished \tANN training loss 0.943729\n",
      ">> Epoch 52 finished \tANN training loss 0.902763\n",
      ">> Epoch 53 finished \tANN training loss 0.910621\n",
      ">> Epoch 54 finished \tANN training loss 0.885924\n",
      ">> Epoch 55 finished \tANN training loss 0.906362\n",
      ">> Epoch 56 finished \tANN training loss 0.932334\n",
      ">> Epoch 57 finished \tANN training loss 0.930039\n",
      ">> Epoch 58 finished \tANN training loss 0.922613\n",
      ">> Epoch 59 finished \tANN training loss 0.921587\n",
      ">> Epoch 60 finished \tANN training loss 0.951778\n",
      ">> Epoch 61 finished \tANN training loss 0.963138\n",
      ">> Epoch 62 finished \tANN training loss 0.959891\n",
      ">> Epoch 63 finished \tANN training loss 0.970203\n",
      ">> Epoch 64 finished \tANN training loss 0.966613\n",
      ">> Epoch 65 finished \tANN training loss 0.950974\n",
      ">> Epoch 66 finished \tANN training loss 0.942440\n",
      ">> Epoch 67 finished \tANN training loss 0.977817\n",
      ">> Epoch 68 finished \tANN training loss 0.966397\n",
      ">> Epoch 69 finished \tANN training loss 0.951992\n",
      ">> Epoch 70 finished \tANN training loss 0.968305\n",
      ">> Epoch 71 finished \tANN training loss 0.980575\n",
      ">> Epoch 72 finished \tANN training loss 0.961645\n",
      ">> Epoch 73 finished \tANN training loss 0.983051\n",
      ">> Epoch 74 finished \tANN training loss 0.985086\n",
      ">> Epoch 75 finished \tANN training loss 0.999105\n",
      ">> Epoch 76 finished \tANN training loss 0.980587\n",
      ">> Epoch 77 finished \tANN training loss 0.970480\n",
      ">> Epoch 78 finished \tANN training loss 1.001633\n",
      ">> Epoch 79 finished \tANN training loss 1.004237\n",
      ">> Epoch 80 finished \tANN training loss 1.015956\n",
      ">> Epoch 81 finished \tANN training loss 1.008122\n",
      ">> Epoch 82 finished \tANN training loss 1.026527\n",
      ">> Epoch 83 finished \tANN training loss 0.992966\n",
      ">> Epoch 84 finished \tANN training loss 0.986935\n",
      ">> Epoch 85 finished \tANN training loss 1.003623\n",
      ">> Epoch 86 finished \tANN training loss 1.022724\n",
      ">> Epoch 87 finished \tANN training loss 1.020194\n",
      ">> Epoch 88 finished \tANN training loss 1.012425\n",
      ">> Epoch 89 finished \tANN training loss 1.033532\n",
      ">> Epoch 90 finished \tANN training loss 1.036311\n",
      ">> Epoch 91 finished \tANN training loss 1.040827\n",
      ">> Epoch 92 finished \tANN training loss 1.041897\n",
      ">> Epoch 93 finished \tANN training loss 1.014408\n",
      ">> Epoch 94 finished \tANN training loss 1.020862\n",
      ">> Epoch 95 finished \tANN training loss 1.015541\n",
      ">> Epoch 96 finished \tANN training loss 1.039117\n",
      ">> Epoch 97 finished \tANN training loss 1.056093\n",
      ">> Epoch 98 finished \tANN training loss 1.039339\n",
      ">> Epoch 99 finished \tANN training loss 1.040997\n",
      ">> Epoch 100 finished \tANN training loss 1.050683\n",
      ">> Epoch 101 finished \tANN training loss 1.041008\n",
      ">> Epoch 102 finished \tANN training loss 1.034413\n",
      ">> Epoch 103 finished \tANN training loss 1.044057\n",
      ">> Epoch 104 finished \tANN training loss 1.066663\n",
      ">> Epoch 105 finished \tANN training loss 1.070550\n",
      ">> Epoch 106 finished \tANN training loss 1.067222\n",
      ">> Epoch 107 finished \tANN training loss 1.044643\n",
      ">> Epoch 108 finished \tANN training loss 1.039049\n",
      ">> Epoch 109 finished \tANN training loss 1.045426\n",
      ">> Epoch 110 finished \tANN training loss 1.056045\n",
      ">> Epoch 111 finished \tANN training loss 1.055706\n",
      ">> Epoch 112 finished \tANN training loss 1.056678\n",
      ">> Epoch 113 finished \tANN training loss 1.051877\n",
      ">> Epoch 114 finished \tANN training loss 1.070614\n",
      ">> Epoch 115 finished \tANN training loss 1.053739\n",
      ">> Epoch 116 finished \tANN training loss 1.052616\n",
      ">> Epoch 117 finished \tANN training loss 1.046241\n",
      ">> Epoch 118 finished \tANN training loss 1.073852\n",
      ">> Epoch 119 finished \tANN training loss 1.051737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 120 finished \tANN training loss 1.066807\n",
      ">> Epoch 121 finished \tANN training loss 1.066254\n",
      ">> Epoch 122 finished \tANN training loss 1.076323\n",
      ">> Epoch 123 finished \tANN training loss 1.060406\n",
      ">> Epoch 124 finished \tANN training loss 1.047807\n",
      ">> Epoch 125 finished \tANN training loss 1.065490\n",
      ">> Epoch 126 finished \tANN training loss 1.073627\n",
      ">> Epoch 127 finished \tANN training loss 1.075184\n",
      ">> Epoch 128 finished \tANN training loss 1.068134\n",
      ">> Epoch 129 finished \tANN training loss 1.074774\n",
      ">> Epoch 130 finished \tANN training loss 1.084933\n",
      ">> Epoch 131 finished \tANN training loss 1.084066\n",
      ">> Epoch 132 finished \tANN training loss 1.066242\n",
      ">> Epoch 133 finished \tANN training loss 1.081202\n",
      ">> Epoch 134 finished \tANN training loss 1.089067\n",
      ">> Epoch 135 finished \tANN training loss 1.087328\n",
      ">> Epoch 136 finished \tANN training loss 1.104776\n",
      ">> Epoch 137 finished \tANN training loss 1.086491\n",
      ">> Epoch 138 finished \tANN training loss 1.086645\n",
      ">> Epoch 139 finished \tANN training loss 1.102701\n",
      ">> Epoch 140 finished \tANN training loss 1.107967\n",
      ">> Epoch 141 finished \tANN training loss 1.103331\n",
      ">> Epoch 142 finished \tANN training loss 1.101829\n",
      ">> Epoch 143 finished \tANN training loss 1.092513\n",
      ">> Epoch 144 finished \tANN training loss 1.098590\n",
      ">> Epoch 145 finished \tANN training loss 1.103724\n",
      ">> Epoch 146 finished \tANN training loss 1.109991\n",
      ">> Epoch 147 finished \tANN training loss 1.119724\n",
      ">> Epoch 148 finished \tANN training loss 1.103122\n",
      ">> Epoch 149 finished \tANN training loss 1.109717\n",
      ">> Epoch 150 finished \tANN training loss 1.106461\n",
      ">> Epoch 151 finished \tANN training loss 1.109076\n",
      ">> Epoch 152 finished \tANN training loss 1.108715\n",
      ">> Epoch 153 finished \tANN training loss 1.105100\n",
      ">> Epoch 154 finished \tANN training loss 1.093201\n",
      ">> Epoch 155 finished \tANN training loss 1.114325\n",
      ">> Epoch 156 finished \tANN training loss 1.114015\n",
      ">> Epoch 157 finished \tANN training loss 1.106276\n",
      ">> Epoch 158 finished \tANN training loss 1.123178\n",
      ">> Epoch 159 finished \tANN training loss 1.128970\n",
      ">> Epoch 160 finished \tANN training loss 1.129217\n",
      ">> Epoch 161 finished \tANN training loss 1.114643\n",
      ">> Epoch 162 finished \tANN training loss 1.118690\n",
      ">> Epoch 163 finished \tANN training loss 1.110184\n",
      ">> Epoch 164 finished \tANN training loss 1.139600\n",
      ">> Epoch 165 finished \tANN training loss 1.136358\n",
      ">> Epoch 166 finished \tANN training loss 1.123079\n",
      ">> Epoch 167 finished \tANN training loss 1.092713\n",
      ">> Epoch 168 finished \tANN training loss 1.105672\n",
      ">> Epoch 169 finished \tANN training loss 1.087426\n",
      ">> Epoch 170 finished \tANN training loss 1.096069\n",
      ">> Epoch 171 finished \tANN training loss 1.108930\n",
      ">> Epoch 172 finished \tANN training loss 1.101587\n",
      ">> Epoch 173 finished \tANN training loss 1.090164\n",
      ">> Epoch 174 finished \tANN training loss 1.095603\n",
      ">> Epoch 175 finished \tANN training loss 1.112462\n",
      ">> Epoch 176 finished \tANN training loss 1.113933\n",
      ">> Epoch 177 finished \tANN training loss 1.114493\n",
      ">> Epoch 178 finished \tANN training loss 1.114586\n",
      ">> Epoch 179 finished \tANN training loss 1.111509\n",
      ">> Epoch 180 finished \tANN training loss 1.124164\n",
      ">> Epoch 181 finished \tANN training loss 1.106056\n",
      ">> Epoch 182 finished \tANN training loss 1.116321\n",
      ">> Epoch 183 finished \tANN training loss 1.100466\n",
      ">> Epoch 184 finished \tANN training loss 1.103373\n",
      ">> Epoch 185 finished \tANN training loss 1.124081\n",
      ">> Epoch 186 finished \tANN training loss 1.106868\n",
      ">> Epoch 187 finished \tANN training loss 1.107670\n",
      ">> Epoch 188 finished \tANN training loss 1.120240\n",
      ">> Epoch 189 finished \tANN training loss 1.129315\n",
      ">> Epoch 190 finished \tANN training loss 1.127424\n",
      ">> Epoch 191 finished \tANN training loss 1.127233\n",
      ">> Epoch 192 finished \tANN training loss 1.121810\n",
      ">> Epoch 193 finished \tANN training loss 1.115196\n",
      ">> Epoch 194 finished \tANN training loss 1.121475\n",
      ">> Epoch 195 finished \tANN training loss 1.122354\n",
      ">> Epoch 196 finished \tANN training loss 1.115823\n",
      ">> Epoch 197 finished \tANN training loss 1.119198\n",
      ">> Epoch 198 finished \tANN training loss 1.111917\n",
      ">> Epoch 199 finished \tANN training loss 1.113940\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.700000\n"
     ]
    }
   ],
   "source": [
    "# Sigmoidal activation, dropout=0.9\n",
    "acc10 = deep_belief_net(hidden_layers_structure=[300], \n",
    "                       learning_rate_rbm=0.05,\n",
    "                       learning_rate=0.1,\n",
    "                       n_epochs_rbm=40,\n",
    "                       n_iter_backprop=200,\n",
    "                       batch_size=32,\n",
    "                       activation_function='sigmoid',\n",
    "                       dropout_p=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.7\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(acc10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 44.149258\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 34.662308\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 29.567949\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 26.718691\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 24.708843\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 23.210997\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 21.998230\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 20.819212\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 19.719660\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 18.906761\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 17.922014\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 17.175596\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 16.521608\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 15.899694\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 15.352788\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 14.864987\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 14.368469\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 13.940740\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 13.611406\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 13.196278\n",
      ">> Epoch 21 finished \tRBM Reconstruction error 12.901297\n",
      ">> Epoch 22 finished \tRBM Reconstruction error 12.537432\n",
      ">> Epoch 23 finished \tRBM Reconstruction error 12.228384\n",
      ">> Epoch 24 finished \tRBM Reconstruction error 11.930333\n",
      ">> Epoch 25 finished \tRBM Reconstruction error 11.678274\n",
      ">> Epoch 26 finished \tRBM Reconstruction error 11.430597\n",
      ">> Epoch 27 finished \tRBM Reconstruction error 11.203878\n",
      ">> Epoch 28 finished \tRBM Reconstruction error 10.977073\n",
      ">> Epoch 29 finished \tRBM Reconstruction error 10.768891\n",
      ">> Epoch 30 finished \tRBM Reconstruction error 10.515144\n",
      ">> Epoch 31 finished \tRBM Reconstruction error 10.356886\n",
      ">> Epoch 32 finished \tRBM Reconstruction error 10.145551\n",
      ">> Epoch 33 finished \tRBM Reconstruction error 9.912625\n",
      ">> Epoch 34 finished \tRBM Reconstruction error 9.777631\n",
      ">> Epoch 35 finished \tRBM Reconstruction error 9.614018\n",
      ">> Epoch 36 finished \tRBM Reconstruction error 9.486042\n",
      ">> Epoch 37 finished \tRBM Reconstruction error 9.297201\n",
      ">> Epoch 38 finished \tRBM Reconstruction error 9.130317\n",
      ">> Epoch 39 finished \tRBM Reconstruction error 8.998882\n",
      ">> Epoch 40 finished \tRBM Reconstruction error 8.870162\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss nan\n",
      ">> Epoch 1 finished \tANN training loss nan\n",
      ">> Epoch 2 finished \tANN training loss nan\n",
      ">> Epoch 3 finished \tANN training loss nan\n",
      ">> Epoch 4 finished \tANN training loss nan\n",
      ">> Epoch 5 finished \tANN training loss nan\n",
      ">> Epoch 6 finished \tANN training loss nan\n",
      ">> Epoch 7 finished \tANN training loss nan\n",
      ">> Epoch 8 finished \tANN training loss nan\n",
      ">> Epoch 9 finished \tANN training loss nan\n",
      ">> Epoch 10 finished \tANN training loss nan\n",
      ">> Epoch 11 finished \tANN training loss nan\n",
      ">> Epoch 12 finished \tANN training loss nan\n",
      ">> Epoch 13 finished \tANN training loss nan\n",
      ">> Epoch 14 finished \tANN training loss nan\n",
      ">> Epoch 15 finished \tANN training loss nan\n",
      ">> Epoch 16 finished \tANN training loss nan\n",
      ">> Epoch 17 finished \tANN training loss nan\n",
      ">> Epoch 18 finished \tANN training loss nan\n",
      ">> Epoch 19 finished \tANN training loss nan\n",
      ">> Epoch 20 finished \tANN training loss nan\n",
      ">> Epoch 21 finished \tANN training loss nan\n",
      ">> Epoch 22 finished \tANN training loss nan\n",
      ">> Epoch 23 finished \tANN training loss nan\n",
      ">> Epoch 24 finished \tANN training loss nan\n",
      ">> Epoch 25 finished \tANN training loss nan\n",
      ">> Epoch 26 finished \tANN training loss nan\n",
      ">> Epoch 27 finished \tANN training loss nan\n",
      ">> Epoch 28 finished \tANN training loss nan\n",
      ">> Epoch 29 finished \tANN training loss nan\n",
      ">> Epoch 30 finished \tANN training loss nan\n",
      ">> Epoch 31 finished \tANN training loss nan\n",
      ">> Epoch 32 finished \tANN training loss nan\n",
      ">> Epoch 33 finished \tANN training loss nan\n",
      ">> Epoch 34 finished \tANN training loss nan\n",
      ">> Epoch 35 finished \tANN training loss nan\n",
      ">> Epoch 36 finished \tANN training loss nan\n",
      ">> Epoch 37 finished \tANN training loss nan\n",
      ">> Epoch 38 finished \tANN training loss nan\n",
      ">> Epoch 39 finished \tANN training loss nan\n",
      ">> Epoch 40 finished \tANN training loss nan\n",
      ">> Epoch 41 finished \tANN training loss nan\n",
      ">> Epoch 42 finished \tANN training loss nan\n",
      ">> Epoch 43 finished \tANN training loss nan\n",
      ">> Epoch 44 finished \tANN training loss nan\n",
      ">> Epoch 45 finished \tANN training loss nan\n",
      ">> Epoch 46 finished \tANN training loss nan\n",
      ">> Epoch 47 finished \tANN training loss nan\n",
      ">> Epoch 48 finished \tANN training loss nan\n",
      ">> Epoch 49 finished \tANN training loss nan\n",
      ">> Epoch 50 finished \tANN training loss nan\n",
      ">> Epoch 51 finished \tANN training loss nan\n",
      ">> Epoch 52 finished \tANN training loss nan\n",
      ">> Epoch 53 finished \tANN training loss nan\n",
      ">> Epoch 54 finished \tANN training loss nan\n",
      ">> Epoch 55 finished \tANN training loss nan\n",
      ">> Epoch 56 finished \tANN training loss nan\n",
      ">> Epoch 57 finished \tANN training loss nan\n",
      ">> Epoch 58 finished \tANN training loss nan\n",
      ">> Epoch 59 finished \tANN training loss nan\n",
      ">> Epoch 60 finished \tANN training loss nan\n",
      ">> Epoch 61 finished \tANN training loss nan\n",
      ">> Epoch 62 finished \tANN training loss nan\n",
      ">> Epoch 63 finished \tANN training loss nan\n",
      ">> Epoch 64 finished \tANN training loss nan\n",
      ">> Epoch 65 finished \tANN training loss nan\n",
      ">> Epoch 66 finished \tANN training loss nan\n",
      ">> Epoch 67 finished \tANN training loss nan\n",
      ">> Epoch 68 finished \tANN training loss nan\n",
      ">> Epoch 69 finished \tANN training loss nan\n",
      ">> Epoch 70 finished \tANN training loss nan\n",
      ">> Epoch 71 finished \tANN training loss nan\n",
      ">> Epoch 72 finished \tANN training loss nan\n",
      ">> Epoch 73 finished \tANN training loss nan\n",
      ">> Epoch 74 finished \tANN training loss nan\n",
      ">> Epoch 75 finished \tANN training loss nan\n",
      ">> Epoch 76 finished \tANN training loss nan\n",
      ">> Epoch 77 finished \tANN training loss nan\n",
      ">> Epoch 78 finished \tANN training loss nan\n",
      ">> Epoch 79 finished \tANN training loss nan\n",
      ">> Epoch 80 finished \tANN training loss nan\n",
      ">> Epoch 81 finished \tANN training loss nan\n",
      ">> Epoch 82 finished \tANN training loss nan\n",
      ">> Epoch 83 finished \tANN training loss nan\n",
      ">> Epoch 84 finished \tANN training loss nan\n",
      ">> Epoch 85 finished \tANN training loss nan\n",
      ">> Epoch 86 finished \tANN training loss nan\n",
      ">> Epoch 87 finished \tANN training loss nan\n",
      ">> Epoch 88 finished \tANN training loss nan\n",
      ">> Epoch 89 finished \tANN training loss nan\n",
      ">> Epoch 90 finished \tANN training loss nan\n",
      ">> Epoch 91 finished \tANN training loss nan\n",
      ">> Epoch 92 finished \tANN training loss nan\n",
      ">> Epoch 93 finished \tANN training loss nan\n",
      ">> Epoch 94 finished \tANN training loss nan\n",
      ">> Epoch 95 finished \tANN training loss nan\n",
      ">> Epoch 96 finished \tANN training loss nan\n",
      ">> Epoch 97 finished \tANN training loss nan\n",
      ">> Epoch 98 finished \tANN training loss nan\n",
      ">> Epoch 99 finished \tANN training loss nan\n",
      ">> Epoch 100 finished \tANN training loss nan\n",
      ">> Epoch 101 finished \tANN training loss nan\n",
      ">> Epoch 102 finished \tANN training loss nan\n",
      ">> Epoch 103 finished \tANN training loss nan\n",
      ">> Epoch 104 finished \tANN training loss nan\n",
      ">> Epoch 105 finished \tANN training loss nan\n",
      ">> Epoch 106 finished \tANN training loss nan\n",
      ">> Epoch 107 finished \tANN training loss nan\n",
      ">> Epoch 108 finished \tANN training loss nan\n",
      ">> Epoch 109 finished \tANN training loss nan\n",
      ">> Epoch 110 finished \tANN training loss nan\n",
      ">> Epoch 111 finished \tANN training loss nan\n",
      ">> Epoch 112 finished \tANN training loss nan\n",
      ">> Epoch 113 finished \tANN training loss nan\n",
      ">> Epoch 114 finished \tANN training loss nan\n",
      ">> Epoch 115 finished \tANN training loss nan\n",
      ">> Epoch 116 finished \tANN training loss nan\n",
      ">> Epoch 117 finished \tANN training loss nan\n",
      ">> Epoch 118 finished \tANN training loss nan\n",
      ">> Epoch 119 finished \tANN training loss nan\n",
      ">> Epoch 120 finished \tANN training loss nan\n",
      ">> Epoch 121 finished \tANN training loss nan\n",
      ">> Epoch 122 finished \tANN training loss nan\n",
      ">> Epoch 123 finished \tANN training loss nan\n",
      ">> Epoch 124 finished \tANN training loss nan\n",
      ">> Epoch 125 finished \tANN training loss nan\n",
      ">> Epoch 126 finished \tANN training loss nan\n",
      ">> Epoch 127 finished \tANN training loss nan\n",
      ">> Epoch 128 finished \tANN training loss nan\n",
      ">> Epoch 129 finished \tANN training loss nan\n",
      ">> Epoch 130 finished \tANN training loss nan\n",
      ">> Epoch 131 finished \tANN training loss nan\n",
      ">> Epoch 132 finished \tANN training loss nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 133 finished \tANN training loss nan\n",
      ">> Epoch 134 finished \tANN training loss nan\n",
      ">> Epoch 135 finished \tANN training loss nan\n",
      ">> Epoch 136 finished \tANN training loss nan\n",
      ">> Epoch 137 finished \tANN training loss nan\n",
      ">> Epoch 138 finished \tANN training loss nan\n",
      ">> Epoch 139 finished \tANN training loss nan\n",
      ">> Epoch 140 finished \tANN training loss nan\n",
      ">> Epoch 141 finished \tANN training loss nan\n",
      ">> Epoch 142 finished \tANN training loss nan\n",
      ">> Epoch 143 finished \tANN training loss nan\n",
      ">> Epoch 144 finished \tANN training loss nan\n",
      ">> Epoch 145 finished \tANN training loss nan\n",
      ">> Epoch 146 finished \tANN training loss nan\n",
      ">> Epoch 147 finished \tANN training loss nan\n",
      ">> Epoch 148 finished \tANN training loss nan\n",
      ">> Epoch 149 finished \tANN training loss nan\n",
      ">> Epoch 150 finished \tANN training loss nan\n",
      ">> Epoch 151 finished \tANN training loss nan\n",
      ">> Epoch 152 finished \tANN training loss nan\n",
      ">> Epoch 153 finished \tANN training loss nan\n",
      ">> Epoch 154 finished \tANN training loss nan\n",
      ">> Epoch 155 finished \tANN training loss nan\n",
      ">> Epoch 156 finished \tANN training loss nan\n",
      ">> Epoch 157 finished \tANN training loss nan\n",
      ">> Epoch 158 finished \tANN training loss nan\n",
      ">> Epoch 159 finished \tANN training loss nan\n",
      ">> Epoch 160 finished \tANN training loss nan\n",
      ">> Epoch 161 finished \tANN training loss nan\n",
      ">> Epoch 162 finished \tANN training loss nan\n",
      ">> Epoch 163 finished \tANN training loss nan\n",
      ">> Epoch 164 finished \tANN training loss nan\n",
      ">> Epoch 165 finished \tANN training loss nan\n",
      ">> Epoch 166 finished \tANN training loss nan\n",
      ">> Epoch 167 finished \tANN training loss nan\n",
      ">> Epoch 168 finished \tANN training loss nan\n",
      ">> Epoch 169 finished \tANN training loss nan\n",
      ">> Epoch 170 finished \tANN training loss nan\n",
      ">> Epoch 171 finished \tANN training loss nan\n",
      ">> Epoch 172 finished \tANN training loss nan\n",
      ">> Epoch 173 finished \tANN training loss nan\n",
      ">> Epoch 174 finished \tANN training loss nan\n",
      ">> Epoch 175 finished \tANN training loss nan\n",
      ">> Epoch 176 finished \tANN training loss nan\n",
      ">> Epoch 177 finished \tANN training loss nan\n",
      ">> Epoch 178 finished \tANN training loss nan\n",
      ">> Epoch 179 finished \tANN training loss nan\n",
      ">> Epoch 180 finished \tANN training loss nan\n",
      ">> Epoch 181 finished \tANN training loss nan\n",
      ">> Epoch 182 finished \tANN training loss nan\n",
      ">> Epoch 183 finished \tANN training loss nan\n",
      ">> Epoch 184 finished \tANN training loss nan\n",
      ">> Epoch 185 finished \tANN training loss nan\n",
      ">> Epoch 186 finished \tANN training loss nan\n",
      ">> Epoch 187 finished \tANN training loss nan\n",
      ">> Epoch 188 finished \tANN training loss nan\n",
      ">> Epoch 189 finished \tANN training loss nan\n",
      ">> Epoch 190 finished \tANN training loss nan\n",
      ">> Epoch 191 finished \tANN training loss nan\n",
      ">> Epoch 192 finished \tANN training loss nan\n",
      ">> Epoch 193 finished \tANN training loss nan\n",
      ">> Epoch 194 finished \tANN training loss nan\n",
      ">> Epoch 195 finished \tANN training loss nan\n",
      ">> Epoch 196 finished \tANN training loss nan\n",
      ">> Epoch 197 finished \tANN training loss nan\n",
      ">> Epoch 198 finished \tANN training loss nan\n",
      ">> Epoch 199 finished \tANN training loss nan\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.100000\n"
     ]
    }
   ],
   "source": [
    "# Sigmoidal activation, dropout=1\n",
    "acc11 = deep_belief_net(hidden_layers_structure=[300], \n",
    "                       learning_rate_rbm=0.05,\n",
    "                       learning_rate=0.1,\n",
    "                       n_epochs_rbm=40,\n",
    "                       n_iter_backprop=200,\n",
    "                       batch_size=32,\n",
    "                       activation_function='sigmoid',\n",
    "                       dropout_p=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.1\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(acc11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEeCAYAAABlggnIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAHtFJREFUeJzt3XmcHFW99/HPNwkkICFhiSBJYJBNICpgAAWugogQVvXK\nNbnoJQgi9zHAVVwiKiJXeVA2NxTiBgISI494I4TFiwaRRYmILAmRGEBiRJIQZF8Cv+ePc7osmp6Z\nnpmu6cnk+3695jVdVafrd6q7un5V59SiiMDMzAxgSLsrYGZmA4eTgpmZFZwUzMys4KRgZmYFJwUz\nMys4KZiZWcFJwVpGUoekkDSsTfH3lrSkHbFLdbha0pEDKe4A+F5OlvTddsS2nnNSGKQkPSDpGUlP\nSlop6SpJ49tdr4FC0kGSfiPpMUkPS/qOpJF9nW9ETIqIi1pRx/6KK2kvSTdL+oekRyXdJGnXJt8b\nkrYuDb8iMUfE6RFxTG/qZv3PSWFwOyQi1gNeA/wd+Eab61OZXuwFjwK+CGwGbA+MA85sdb0GOknr\nA1eS1o0NgbHAF4Dn2lkvax8nhTVARDwLXA7sUBsnabiksyT9RdLfJZ0vaZ08bW9JSySdJOkRSX+T\ndFTpvetIOlvSg3nv8je192ZH5Pkul/SZ0vtOlfQTSZdIekLSXZK2lfTpHOchSe8slT9K0oJcdrGk\nD5em1er4KUkPAz+oX25JJ0iaL2lcg8/kRxFxTUQ8HRErge8AezbzeUoakZdhRT7SuE3SJnnaXEnH\n5NdD8+e0XNL9kqaVm3Fy2S/mvfQnJf1c0kaSLpX0eJ5vRynuHnncP/L/PUrT6uOeleMuBg7qYnG2\nzZ/HZRHxYkQ8ExHXRcSdpXl/MH8PKyVdK2mLPP7Xucgfc/2PBK4GNsvDT0raLH/vl+T31Jqyjuxk\nHVlH0kU51gJJnywfeeTv+695nVgoad9mvjNrnpPCGkDSusD7gFtLo79M2iDsBGxN2kM8pTR9U9Le\n9FjgaOA8SRvkaWcBbwL2IO1dfhJ4qfTevYDtgH2BUyRtX5p2CHAxsAHwB+Ba0no4FjgNuKBU9hHg\nYGB94CjgXEm71NVxQ2AL4Ni6Zf4cMBV4W0Q008/wVuCeJsoBHEn6bMYDGwHHAc80KPchYBLpM94F\neFeDMpOBD5CWfyvgFlKC2xBYAHw+L8+GwFXA13PMc4CrJG3USdyDgZ2BicB7u1iWPwEv5g3xpNJ3\nTI77LuBk4D3AGOBG4DKAiHhrLvbGiFgvN19NApbm4fUiYmkncTtbRz4PdACvBfYD3l+qy3bANGDX\niBgJ7A880MWyWW9EhP8G4R/px/Ik8BiwClgKvD5PE/AUsFWp/FuA+/PrvUkbuWGl6Y8AbyZtwJ8h\nbQjqY3YAAYwrjfsdMDm/PhX4RWnaIbmOQ/PwyPz+0Z0s08+AE0t1fB4YUZq+N/BX0gbzN8CoJj+r\n/YCVwLZNlv8gcDPwhgbT5gLH5Ne/BD5cmvaOvHzDSmU/U5p+NnB13edzR379AeB3dbFuAaZ2Eve4\nUrl3luM2qPP2wIXAkryuzAY2ydOuBo4ulR0CPA1skYcD2LruO1hSN/9TgUuaXEcWA/uXph1Tmx9p\n5+WR/Dmu1e7f2GD985HC4PauiBgNDCftYd0gaVPSHt+6wO9z88djwDV5fM2KiFhVGn4aWA/YGBgB\n/LmLuA83eF/N30uvnwGWR8SLpWFq5fOe661KnZ+PAQfm+DXLIjWNlY0mHTX834j4Rxd1JMd4M/Aj\n4L0R8afuymcXk45wZkpaKukrktZqUG4z4KHS8EMNytR/HvXDtc9uM+DBuvc+SDrC6C5u/fteJiIW\nRMTUiBgHTMjv/2qevAXwtdJ68ihpp6JR3J7obB3p9DOLiEXAf5GSzCOSZkrarI/1sDpOCmuASG3F\nPwVeJB22LydtcHaMiNH5b1SkTunuLAeeJTV1VEbScOD/kZqqNsnJbQ5pg1TT6Ba/K0lNJz+Q1GUf\ngaSdSXvFH4yI65utW0S8EBFfiIgdSE1oBwP/0aDo30gd2DV9OftrKWkDXbY56cioUdzxdeWaEhH3\nko4aJuRRD5GOdkaX/taJiJs7m0WzsTrR5WcWqS9oL9JnEaRmUGshJ4U1gJLDSO34CyLiJVLH6rmS\nXp3LjJW0f3fzyu/9PnBO7kQcKukteSPeSmuTjnCWAaskTSI1g3QrIuYCRwBXSNq9URlJE0hHR8dH\nxM8bTD9V0txO3ruPpNdLGgo8DrxASrj1ZgEn5s92NPCpZurfiTnAtpL+XdIwSe8jnThwZSdxT5A0\nLvcRTO9sppJep3RCwbg8PB6Ywj/7n84HPi1pxzx9lKTDS7P4O6n9vzy8kaRRvVtMZuV4G0gaSzrC\nrdV1O0lvz+vas6Qdm0afu/WBk8Lg9nNJT5I2XF8CjoyIWmfqp4BFwK2SHgf+l9Tx14yPA3cBt5Ga\nE75Mi9eliHgCOIG0kVgJ/Dtpr77Z9/+C1Dk9W9KbGhQ5idRc9r3SmTLljubxwE2dzH5T0tlcj5M6\ng28ALmlQ7jvAdcCdpE71OaQ2+x5vyCJiBemI5CRgBalz/+CIWN5J3GuBPwK3Az/tYtZPALsDv5X0\nFCkZ3J3jEBFXkL7fmXk9uZvUmVxzKnBRbl76t3ykcRmwOI/rafPOaaS+jftJ6+Tl/PP02OHAGaSj\n1YeBV5M6wa2FlDtwzKxE0h3Avnlj3Kp5TgLOj4j6ZiDrhKT/JHVCv63ddVlT+EjBrIGI2KmvCSGf\nc39gbu4ZSzrd8orW1HBwkvQaSXtKGpJPQT0Jf2b9ykcKZhXJ14fcALyO1P59FemU2sfbWrEBLF8Y\ndxWwJel06pnApyPi+bZWbA3ipGBmZgU3H5mZWcFJwczMCm25v3pfbLzxxtHR0dHuapiZrVZ+//vf\nL4+IMd2VW+2SQkdHB/PmzWt3NczMViuSurzdSY2bj8zMrOCkYGZmBScFMzMrOCmYmVnBScHMzApO\nCmZmVnBSMDOzgpOCmZkVVruL11YnHdOvqmzeD5xxUGXzNrM1l48UzMys4KRgZmYFJwUzMys4KZiZ\nWcFJwczMCj77aJCp6ownn+1ktmbwkYKZmRXWqCMFXzfQev19ZOLv0Kxaa1RSMBvonPSs3ZwUzLrg\njbStadynYGZmBScFMzMrOCmYmVnBScHMzApOCmZmVvDZR2ZrOF8Fb2VOCmbWr5yEBjY3H5mZWcFJ\nwczMCk4KZmZWcFIwM7OCk4KZmRWcFMzMrFBpUpB0gKSFkhZJmt5g+uaSfiXpD5LulHRglfUxM7Ou\nVZYUJA0FzgMmATsAUyTtUFfss8CsiNgZmAx8q6r6mJlZ96o8UtgNWBQRiyPieWAmcFhdmQDWz69H\nAUsrrI+ZmXWjyqQwFnioNLwkjys7FXi/pCXAHOD4RjOSdKykeZLmLVu2rIq6mpkZ1SYFNRgXdcNT\ngAsjYhxwIHCxpFfUKSJmRMTEiJg4ZsyYCqpqZmZQbVJYAowvDY/jlc1DRwOzACLiFmAEsHGFdTIz\nsy5UmRRuA7aRtKWktUkdybPryvwF2BdA0vakpOD2ITOzNqksKUTEKmAacC2wgHSW0T2STpN0aC52\nEvAhSX8ELgOmRkR9E5OZmfWTSm+dHRFzSB3I5XGnlF7PB/assg5mZtY8X9FsZmYFJwUzMys4KZiZ\nWcFJwczMCk4KZmZWcFIwM7OCk4KZmRWcFMzMrOCkYGZmBScFMzMrOCmYmVnBScHMzApOCmZmVnBS\nMDOzgpOCmZkVnBTMzKzgpGBmZoVKn7xmZtZuHdOvqmS+D5xxUCXzbTcfKZiZWcFJwczMCk4KZmZW\ncFIwM7OCk4KZmRWcFMzMrOCkYGZmBScFMzMrOCmYmVnBScHMzApOCmZmVnBSMDOzgpOCmZkVnBTM\nzKzgpGBmZgUnBTMzKzgpmJlZwUnBzMwKlSYFSQdIWihpkaTpnZT5N0nzJd0j6UdV1sfMzLpW2TOa\nJQ0FzgP2A5YAt0maHRHzS2W2AT4N7BkRKyW9uqr6mJlZ96o8UtgNWBQRiyPieWAmcFhdmQ8B50XE\nSoCIeKTC+piZWTeqTApjgYdKw0vyuLJtgW0l3STpVkkHNJqRpGMlzZM0b9myZRVV18zMqkwKajAu\n6oaHAdsAewNTgO9KGv2KN0XMiIiJETFxzJgxLa+omZklVSaFJcD40vA4YGmDMv8TES9ExP3AQlKS\nMDOzNqgyKdwGbCNpS0lrA5OB2XVlfgbsAyBpY1Jz0uIK62RmZl2oLClExCpgGnAtsACYFRH3SDpN\n0qG52LXACknzgV8Bn4iIFVXVyczMulbZKakAETEHmFM37pTS6wA+lv/MzKzNuj1SkDRN0gb9URkz\nM2uvZpqPNiVdeDYrX6Hc6KwiMzMbBLpNChHxWdIZQd8DpgL3STpd0lYV183MzPpZUx3Nue3/4fy3\nCtgAuFzSVyqsm5mZ9bNuO5olnQAcCSwHvks6Q+gFSUOA+4BPVltFMzPrL82cfbQx8J6IeLA8MiJe\nknRwNdUyM7N2aKb5aA7waG1A0khJuwNExIKqKmZmZv2vmaTwbeDJ0vBTeZyZmQ0yzSQF5Y5mIDUb\nUfFFb2Zm1h7NJIXFkk6QtFb+OxHfn8jMbFBqJikcB+wB/JV0V9PdgWOrrJSZmbVHt81A+Wlok/uh\nLmZm1mbNXKcwAjga2BEYURsfER+ssF5mZtYGzTQfXUy6/9H+wA2kh+U8UWWlzMysPZpJCltHxOeA\npyLiIuAg4PXVVsvMzNqhmaTwQv7/mKQJwCigo7IamZlZ2zRzvcGM/DyFz5Iep7ke8LlKa2VmZm3R\nZVLIN717PCJWAr8GXtsvtTIzs7bosvkoX708rZ/qYmZmbdZMn8IvJH1c0nhJG9b+Kq+ZmZn1u2b6\nFGrXI3ykNC5wU5KZ2aDTzBXNW/ZHRczMrP2auaL5PxqNj4gftr46ZmbWTs00H+1aej0C2Be4HXBS\nMDMbZJppPjq+PCxpFOnWF2ZmNsg0c/ZRvaeBbVpdETMza79m+hR+TjrbCFIS2QGYVWWlzMysPZrp\nUzir9HoV8GBELKmoPmZm1kbNJIW/AH+LiGcBJK0jqSMiHqi0ZmZm1u+a6VP4CfBSafjFPM7MzAaZ\nZpLCsIh4vjaQX69dXZXMzKxdmkkKyyQdWhuQdBiwvLoqmZlZuzTTp3AccKmkb+bhJUDDq5zNzGz1\n1szFa38G3ixpPUAR4eczm5kNUt02H0k6XdLoiHgyIp6QtIGkL/ZH5czMrH8106cwKSIeqw3kp7Ad\nWF2VzMysXZpJCkMlDa8NSFoHGN5F+YKkAyQtlLRI0vQuyr1XUkia2Mx8zcysGs10NF8CXC/pB3n4\nKOCi7t4kaShwHrAfqXP6NkmzI2J+XbmRwAnAb3tScTMza71ujxQi4ivAF4HtSfc9ugbYool57wYs\niojF+dqGmcBhDcr9N/AV4NlmK21mZtVo9i6pD5Ouav5X0vMUFjTxnrHAQ6XhJXlcQdLOwPiIuLKr\nGUk6VtI8SfOWLVvWZJXNzKynOm0+krQtMBmYAqwAfkw6JXWfJuetBuOimCgNAc4FpnY3o4iYAcwA\nmDhxYnRT3MzMeqmrPoV7gRuBQyJiEYCkj/Zg3kuA8aXhccDS0vBIYAIwVxLApsBsSYdGxLwexDEz\nsxbpqvnoX0nNRr+S9B1J+9J4778ztwHbSNpS0tqko47ZtYkR8Y+I2DgiOiKiA7gVcEIwM2ujTpNC\nRFwREe8DXgfMBT4KbCLp25Le2d2MI2IVMA24ltQHMSsi7pF0WvleSmZmNnA0c5uLp4BLSfc/2hA4\nHJgOXNfEe+cAc+rGndJJ2b2bqK+ZmVWoR89ojohHI+KCiHh7VRUyM7P26VFSMDOzwc1JwczMCk4K\nZmZWcFIwM7OCk4KZmRWcFMzMrOCkYGZmBScFMzMrOCmYmVnBScHMzApOCmZmVnBSMDOzgpOCmZkV\nnBTMzKzgpGBmZgUnBTMzKzgpmJlZwUnBzMwKTgpmZlZwUjAzs4KTgpmZFZwUzMys4KRgZmYFJwUz\nMys4KZiZWcFJwczMCk4KZmZWcFIwM7OCk4KZmRWcFMzMrOCkYGZmBScFMzMrOCmYmVnBScHMzApO\nCmZmVqg0KUg6QNJCSYskTW8w/WOS5ku6U9L1kraosj5mZta1ypKCpKHAecAkYAdgiqQd6or9AZgY\nEW8ALge+UlV9zMyse1UeKewGLIqIxRHxPDATOKxcICJ+FRFP58FbgXEV1sfMzLpRZVIYCzxUGl6S\nx3XmaODqRhMkHStpnqR5y5Yta2EVzcysrMqkoAbjomFB6f3ARODMRtMjYkZETIyIiWPGjGlhFc3M\nrGxYhfNeAowvDY8DltYXkvQO4DPA2yLiuQrrY2Zm3ajySOE2YBtJW0paG5gMzC4XkLQzcAFwaEQ8\nUmFdzMysCZUlhYhYBUwDrgUWALMi4h5Jp0k6NBc7E1gP+ImkOyTN7mR2ZmbWD6psPiIi5gBz6sad\nUnr9jirjm5lZz/iKZjMzKzgpmJlZwUnBzMwKTgpmZlZwUjAzs4KTgpmZFZwUzMys4KRgZmYFJwUz\nMys4KZiZWcFJwczMCk4KZmZWcFIwM7OCk4KZmRWcFMzMrOCkYGZmBScFMzMrVPrkNTOzNU3H9Ksq\nm/cDZxxU2bxrfKRgZmYFJwUzMys4KZiZWcFJwczMCk4KZmZWcFIwM7OCk4KZmRWcFMzMrOCkYGZm\nBScFMzMrOCmYmVnBScHMzApOCmZmVnBSMDOzgpOCmZkVnBTMzKzgpGBmZgUnBTMzK1SaFCQdIGmh\npEWSpjeYPlzSj/P030rqqLI+ZmbWtcqSgqShwHnAJGAHYIqkHeqKHQ2sjIitgXOBL1dVHzMz616V\nRwq7AYsiYnFEPA/MBA6rK3MYcFF+fTmwryRVWCczM+uCIqKaGUvvBQ6IiGPy8AeA3SNiWqnM3bnM\nkjz851xmed28jgWOzYPbAQsrqfQrbQws77aU4zlee+K1I6bjrb7xtoiIMd0VGlZhBRrt8ddnoGbK\nEBEzgBmtqFRPSJoXERMdz/EGYrx2xHS81TteM6psPloCjC8NjwOWdlZG0jBgFPBohXUyM7MuVJkU\nbgO2kbSlpLWBycDsujKzgSPz6/cCv4yq2rPMzKxblTUfRcQqSdOAa4GhwPcj4h5JpwHzImI28D3g\nYkmLSEcIk6uqTy/1d5OV4zneQI/peKt3vG5V1tFsZmarH1/RbGZmBScFMzMrOCk0kM+EsgpIenUb\nYm4p6VX9Hbcd+vviT0mVb0MkjZU0uuo4/WkgX6TrpFAiaZiks4CzJb2jXXXo53gb9VOcV0k6E7ha\n0jmSDs7jK/1xSNoK+DPwgXwWXOUkvVbS7v0RK8fbTtIxAP1x9p6kHSSdneO9VGGcdfI68wvgQklH\n5fFVrzMbVzz/IeRrtPojqfbUgKtQu+QV7evAa4DfAZ+S9BFJw/sp/rqSvg0cK2ndfoj3qvzDniPp\nS5L2zeNbvk7kDfOPgRHAu4FF5CvU+2Ej9mrS9TG7AZtXGSh/h2cCVwDrVxkrxxsi6Rzgp8DIqpNe\naflmAe/vh8R3Mun3uCPpTMVKE19OQrXfxPTajmG+j1urYhxFuj7rC62aZ6s5KfzTSGAn4LiIuBQ4\nC9gWOLzqwJJGAWcDBwK7ABMqjrcVaUMyBDgK+BtwsiRVtOf3KPCxiDg+Iv5Cukhxbi3hVpSIanuT\nTwH/DawFHNHqOKV4mwFXAm+KiDdGxC+qilXyWmB8ROwYEefme4xVIieAa0jrzOHApVR7Q83hwKuA\n/8lJYBPgGkmvydOriD2ddNuJA4A7gB9KelVEvNiKmUtaj3S/ty8DB0naOiJeGmhHCwOqMu0UEY8D\nDwBT86ibgD8Ab5G0acXhnwO+DbweeAJ4a8WHsE8B34uIj0bEfOBq4K+8/Ar0XqtvAouIlRHxp7wn\n9lngI6R7WF0hqSP/MPrUJNAgZm1v8k3AZsBHgT0kvUfSnhU0QTxDuibn+lyfXSXtUVt3Kvrhr0c+\nIpG0n6SjJO1RQRyAh4CpEXFSRCwAdgZ2zbH7vGwNvr/nSDsT+0u6CfgMMBr4naRdWrHO1MUfAWwK\nfCsiHo2Ia0i/xTPz9D7HiogngRMi4mvAdcBpeXxlTXC94aTwclcAO0l6Tf4C7wKeJx3CtkyDH8Cz\nwMKcmH4KvAHYuVUrfYN4DwNzSqNGANuTfvh9itOoT6a2HBHxDDAnIsZFxIeBu4EL8rReNQl0EbN2\nyH83sCDfZHE06W68E/raBNEo8QFzgQ5J95FuBT+VtHe7ZV83Yp30NY0C7pT0MeDzwEbA5ZImVZBk\nl0bEYklr5VEXAnv19eiys+8vOwP4HGmHZaeIOIm0vpyV69Tr77CT3+Bw4H2SRksaS9oxPDTv0UeL\nEsNf8suvAltLemeuT8uaqPrKSeHlfgOsIB8tRMTvSXtD67Ri5l1tNPOeERFxI+mIZR/6uOfeTbwn\nS0U3BO7r44+s1iezKXV9MvkHNQQgIm4vve0K4P7ShqaVMWuH/BOB0yTdAdwP3Ags6E28HLOrjdgd\npA3J+RGxV0QcS9oj/Ab0biPWTbx7gG2At5P24s8CTgVOrCDJ1hL7C3nU88DDwPDebiy7+v5yrFWk\n5pxHgdr6egHwnKSRvYzZ1ed5MjCGdNR+HfBD4BLgQ7k+LevLyDtm3yMdARERL/b2d9BqTgolEfE3\n4GfAJEmHKz0J7llgVV/n3cRGU6XD8EtIe30TJB0vaedWxyuVgfQQpPl53BRJ2/diEWt9Mv/ZSZ/M\ny35QeZnOAOaXNjStjgnwI+B2YFpETAYuIzVJ9LhTtomN2HPA5RFxdults4EHO9nT72u85aRO35HA\nlnncDGBt9eKssibX0do6Mx84CHipDxvLZr6/u4E9gRMlvZu0vLdHxBOtXD5IR0OkPrbPA3tHxFzg\nEeDO0vtbQtKQiLgAWCbpa5K+QWqSa7+I8F/dH+lpcd8H7iVtTFoxz/WBm4GReXh/4GvA+2vb6Lry\n5wOPkfo1JlQZj7RH9FVS08rVwNa9XMYfAcfn1+uRjrjOAzYrldko1+N2YEoLPtduY9aVVx9i9fQ7\n3IV0ZHJ8FfFK5U4m7d2eANxAOmlhWFXLV/p/HfC+qr8/4F9Ie9Rz+7LONLF8Q+qWb1fg18D+fV1P\nO6nPunn+y0h9DS2P0Zs/Hyk0EBFXAx8mbYy/2aJ5dtWRvVnU1sbkHcAhpIS0c0TcXWG8tUlnO72N\ntJc7KSIW9WohX9kncyepE32jHGvHiFgBzIyIXSLisl7G6UnMCb3ZS2+kB5/p+pJOJzUPfCsivlFF\nvFLRc0lt/OOBGZE6g3t8dNvs8kVE5CORW4A/9jROnW7XGeDmiPhSROzdl3WmieWr9Y0MV7ru41Lg\nOxFxbW9jduP/kHaOxkXE1yuK0WNOCp2IiBd688PqRrcbMNJ3cktEjI2ISyqOt2Ok0xhPy8lnZh/j\n1ffJ3E66PmCEpEOB3SUNjYhb+hinJzFf9gCT2oatD5r5TB8HrsifaV8TXzPrzKqI+G1EfCJSM0yl\n8SStFRErIuLzEXFvH+N19/3tSmu3U818f8+STojYNiIubmHseudExH9F7k8cKJwU+lczPwBFxFP9\nFO/NuW3zZ60IFo37ZJ7Pfz+PiO9Hi8757kHMC1uc3JtNfLf1U7xWP7WrX+M1+f31ts+pke6WbzdJ\nwyL1L1QqBtipqIV2t1+taX/AHqR2xMOBDuCXwBvpQ1v3QIqXY7a8T2YgxRzs3+FgX2fasXyr05+f\np9AGkiaRVsg9gG9Gi/otBkq8HHMtUmtNq5vgBkTMwf4dDvZ1ph3Lt7pwUmiT/t5otmMjPdgN9u9w\nsK8zg335estJwczMCu5oNjOzgpOCmZkVnBTMzKzgpGBmZgUnBTMzKzgp2BpL0qmSPt6m2Ce3I65Z\nd5wUzEpadfO8Jjgp2IDkpGBrFEmfkbRQ0v+SHgmKpLmSTpd0A+m+/VtIul7Snfn/5rnchZLOl3Sj\npD9JOjiPHyHpB5LukvQHSfvk8VMlfbMU+0pJe0s6A1hH0h2SGt7ATlKHpHslXZTrcbmkdSv+eMyc\nFGzNIelNwGTSw0zeQ37GcDY6It4W6QE53wR+GBFvIN0+uXxb4w7SbcYPAs5XerbvRwAi4vXAFOCi\nPL6hiJgOPBMRO0XEEV1UeTvSrbDfADxOutWyWaWcFGxN8i+kW1o/Hen21rNL035cev0W0sNfAC4G\n9ipNmxURL0XEfcBi4HV5+sUAkW4l/SDpCWJ99VBE3JRfX1JXD7NKOCnYmqaz+7p0dbvy6OR1bbiz\nxzSu4uW/sU6PHpqI22jYrOWcFGxN8mvg3ZLWUXrw+yGdlLuZ1MwEcATpHvw1h0saImkr4LXAwjzf\nIwAkbQtsnsc/QHqgyxBJ40n37a95Qd0/qH1zSW/Jr6fU1cOsEv11poVZ20XE7ZJ+DNxBauK5sZOi\nJwDfl/QJ0vNzjypNW0h6DvImwHER8aykb5H6F+4iHR1MjYjnJN0E3A/cRXoA/e2l+cwA7pR0exf9\nCguAIyVdANxHeg6zWaV8l1SzJkm6ELgyIi7vh1gdOdaEqmOZlbn5yMzMCj5SMGsjSRsB1zeYtG9E\nrOjv+pg5KZiZWcHNR2ZmVnBSMDOzgpOCmZkVnBTMzKzgpGBmZoX/D+PEHOCvgYwLAAAAAElFTkSu\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1cd03d5cc18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create bar graph\n",
    "plt.clf()\n",
    "objects = ('0', '0.1', '0.2', '0.3', '0.4', '0.5', '0.6', '0.7', '0.8', '0.9', '1')\n",
    "y_pos = np.arange(len(objects))\n",
    "performance = [0.885, 0.92, 0.895, 0.885, 0.885, 0.875, 0.88, 0.85, 0.845, 0.7, 0.1]\n",
    "plt.bar(y_pos, performance, align='center', alpha=1)\n",
    "plt.xticks(y_pos, objects, rotation=30)\n",
    "plt.xlabel('dropout_p')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Benchmark 2, sigmoid Settings')\n",
    " \n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark 2 (ReLu)\n",
    "Same benchmark as benchmark 2 but with the ReLu function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 76.494263\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 49.433613\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 34.763378\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 53.639404\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 47.919071\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 52.792355\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 34.348198\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 37.879623\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 40.244701\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 24.620647\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 46.829220\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 39.373817\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 34.669823\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 33.453056\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 37.854984\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 41.508236\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 34.404789\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 37.564243\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 29.922712\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 45.012032\n",
      ">> Epoch 21 finished \tRBM Reconstruction error 45.408573\n",
      ">> Epoch 22 finished \tRBM Reconstruction error 28.551580\n",
      ">> Epoch 23 finished \tRBM Reconstruction error 37.426979\n",
      ">> Epoch 24 finished \tRBM Reconstruction error 40.499168\n",
      ">> Epoch 25 finished \tRBM Reconstruction error 35.235374\n",
      ">> Epoch 26 finished \tRBM Reconstruction error 35.480835\n",
      ">> Epoch 27 finished \tRBM Reconstruction error 41.282398\n",
      ">> Epoch 28 finished \tRBM Reconstruction error 42.309975\n",
      ">> Epoch 29 finished \tRBM Reconstruction error 40.058102\n",
      ">> Epoch 30 finished \tRBM Reconstruction error 31.168457\n",
      ">> Epoch 31 finished \tRBM Reconstruction error 43.066540\n",
      ">> Epoch 32 finished \tRBM Reconstruction error 37.291489\n",
      ">> Epoch 33 finished \tRBM Reconstruction error 31.662176\n",
      ">> Epoch 34 finished \tRBM Reconstruction error 35.579239\n",
      ">> Epoch 35 finished \tRBM Reconstruction error 46.759823\n",
      ">> Epoch 36 finished \tRBM Reconstruction error 45.324329\n",
      ">> Epoch 37 finished \tRBM Reconstruction error 42.645927\n",
      ">> Epoch 38 finished \tRBM Reconstruction error 45.038387\n",
      ">> Epoch 39 finished \tRBM Reconstruction error 44.802437\n",
      ">> Epoch 40 finished \tRBM Reconstruction error 40.336037\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.755771\n",
      ">> Epoch 1 finished \tANN training loss 0.483619\n",
      ">> Epoch 2 finished \tANN training loss 0.382748\n",
      ">> Epoch 3 finished \tANN training loss 0.327483\n",
      ">> Epoch 4 finished \tANN training loss 0.279391\n",
      ">> Epoch 5 finished \tANN training loss 0.246873\n",
      ">> Epoch 6 finished \tANN training loss 0.211682\n",
      ">> Epoch 7 finished \tANN training loss 0.200859\n",
      ">> Epoch 8 finished \tANN training loss 0.170853\n",
      ">> Epoch 9 finished \tANN training loss 0.153780\n",
      ">> Epoch 10 finished \tANN training loss 0.140875\n",
      ">> Epoch 11 finished \tANN training loss 0.123597\n",
      ">> Epoch 12 finished \tANN training loss 0.109557\n",
      ">> Epoch 13 finished \tANN training loss 0.102595\n",
      ">> Epoch 14 finished \tANN training loss 0.089996\n",
      ">> Epoch 15 finished \tANN training loss 0.084945\n",
      ">> Epoch 16 finished \tANN training loss 0.075162\n",
      ">> Epoch 17 finished \tANN training loss 0.069710\n",
      ">> Epoch 18 finished \tANN training loss 0.062900\n",
      ">> Epoch 19 finished \tANN training loss 0.059716\n",
      ">> Epoch 20 finished \tANN training loss 0.054521\n",
      ">> Epoch 21 finished \tANN training loss 0.050511\n",
      ">> Epoch 22 finished \tANN training loss 0.047649\n",
      ">> Epoch 23 finished \tANN training loss 0.044071\n",
      ">> Epoch 24 finished \tANN training loss 0.040684\n",
      ">> Epoch 25 finished \tANN training loss 0.038501\n",
      ">> Epoch 26 finished \tANN training loss 0.036993\n",
      ">> Epoch 27 finished \tANN training loss 0.034550\n",
      ">> Epoch 28 finished \tANN training loss 0.032743\n",
      ">> Epoch 29 finished \tANN training loss 0.031268\n",
      ">> Epoch 30 finished \tANN training loss 0.029284\n",
      ">> Epoch 31 finished \tANN training loss 0.027729\n",
      ">> Epoch 32 finished \tANN training loss 0.026954\n",
      ">> Epoch 33 finished \tANN training loss 0.025263\n",
      ">> Epoch 34 finished \tANN training loss 0.024667\n",
      ">> Epoch 35 finished \tANN training loss 0.023226\n",
      ">> Epoch 36 finished \tANN training loss 0.022346\n",
      ">> Epoch 37 finished \tANN training loss 0.021432\n",
      ">> Epoch 38 finished \tANN training loss 0.020543\n",
      ">> Epoch 39 finished \tANN training loss 0.019933\n",
      ">> Epoch 40 finished \tANN training loss 0.019033\n",
      ">> Epoch 41 finished \tANN training loss 0.018427\n",
      ">> Epoch 42 finished \tANN training loss 0.017753\n",
      ">> Epoch 43 finished \tANN training loss 0.017092\n",
      ">> Epoch 44 finished \tANN training loss 0.016526\n",
      ">> Epoch 45 finished \tANN training loss 0.015984\n",
      ">> Epoch 46 finished \tANN training loss 0.015479\n",
      ">> Epoch 47 finished \tANN training loss 0.015020\n",
      ">> Epoch 48 finished \tANN training loss 0.014671\n",
      ">> Epoch 49 finished \tANN training loss 0.014150\n",
      ">> Epoch 50 finished \tANN training loss 0.013766\n",
      ">> Epoch 51 finished \tANN training loss 0.013455\n",
      ">> Epoch 52 finished \tANN training loss 0.013073\n",
      ">> Epoch 53 finished \tANN training loss 0.012703\n",
      ">> Epoch 54 finished \tANN training loss 0.012307\n",
      ">> Epoch 55 finished \tANN training loss 0.012017\n",
      ">> Epoch 56 finished \tANN training loss 0.011727\n",
      ">> Epoch 57 finished \tANN training loss 0.011401\n",
      ">> Epoch 58 finished \tANN training loss 0.011191\n",
      ">> Epoch 59 finished \tANN training loss 0.010887\n",
      ">> Epoch 60 finished \tANN training loss 0.010613\n",
      ">> Epoch 61 finished \tANN training loss 0.010374\n",
      ">> Epoch 62 finished \tANN training loss 0.010134\n",
      ">> Epoch 63 finished \tANN training loss 0.009925\n",
      ">> Epoch 64 finished \tANN training loss 0.009697\n",
      ">> Epoch 65 finished \tANN training loss 0.009490\n",
      ">> Epoch 66 finished \tANN training loss 0.009292\n",
      ">> Epoch 67 finished \tANN training loss 0.009128\n",
      ">> Epoch 68 finished \tANN training loss 0.008924\n",
      ">> Epoch 69 finished \tANN training loss 0.008746\n",
      ">> Epoch 70 finished \tANN training loss 0.008568\n",
      ">> Epoch 71 finished \tANN training loss 0.008414\n",
      ">> Epoch 72 finished \tANN training loss 0.008248\n",
      ">> Epoch 73 finished \tANN training loss 0.008089\n",
      ">> Epoch 74 finished \tANN training loss 0.007953\n",
      ">> Epoch 75 finished \tANN training loss 0.007801\n",
      ">> Epoch 76 finished \tANN training loss 0.007668\n",
      ">> Epoch 77 finished \tANN training loss 0.007523\n",
      ">> Epoch 78 finished \tANN training loss 0.007399\n",
      ">> Epoch 79 finished \tANN training loss 0.007277\n",
      ">> Epoch 80 finished \tANN training loss 0.007143\n",
      ">> Epoch 81 finished \tANN training loss 0.007029\n",
      ">> Epoch 82 finished \tANN training loss 0.006910\n",
      ">> Epoch 83 finished \tANN training loss 0.006798\n",
      ">> Epoch 84 finished \tANN training loss 0.006692\n",
      ">> Epoch 85 finished \tANN training loss 0.006583\n",
      ">> Epoch 86 finished \tANN training loss 0.006489\n",
      ">> Epoch 87 finished \tANN training loss 0.006383\n",
      ">> Epoch 88 finished \tANN training loss 0.006290\n",
      ">> Epoch 89 finished \tANN training loss 0.006192\n",
      ">> Epoch 90 finished \tANN training loss 0.006115\n",
      ">> Epoch 91 finished \tANN training loss 0.006008\n",
      ">> Epoch 92 finished \tANN training loss 0.005921\n",
      ">> Epoch 93 finished \tANN training loss 0.005838\n",
      ">> Epoch 94 finished \tANN training loss 0.005755\n",
      ">> Epoch 95 finished \tANN training loss 0.005676\n",
      ">> Epoch 96 finished \tANN training loss 0.005597\n",
      ">> Epoch 97 finished \tANN training loss 0.005521\n",
      ">> Epoch 98 finished \tANN training loss 0.005446\n",
      ">> Epoch 99 finished \tANN training loss 0.005373\n",
      ">> Epoch 100 finished \tANN training loss 0.005301\n",
      ">> Epoch 101 finished \tANN training loss 0.005232\n",
      ">> Epoch 102 finished \tANN training loss 0.005167\n",
      ">> Epoch 103 finished \tANN training loss 0.005100\n",
      ">> Epoch 104 finished \tANN training loss 0.005035\n",
      ">> Epoch 105 finished \tANN training loss 0.004973\n",
      ">> Epoch 106 finished \tANN training loss 0.004911\n",
      ">> Epoch 107 finished \tANN training loss 0.004854\n",
      ">> Epoch 108 finished \tANN training loss 0.004794\n",
      ">> Epoch 109 finished \tANN training loss 0.004735\n",
      ">> Epoch 110 finished \tANN training loss 0.004678\n",
      ">> Epoch 111 finished \tANN training loss 0.004623\n",
      ">> Epoch 112 finished \tANN training loss 0.004570\n",
      ">> Epoch 113 finished \tANN training loss 0.004517\n",
      ">> Epoch 114 finished \tANN training loss 0.004465\n",
      ">> Epoch 115 finished \tANN training loss 0.004413\n",
      ">> Epoch 116 finished \tANN training loss 0.004364\n",
      ">> Epoch 117 finished \tANN training loss 0.004319\n",
      ">> Epoch 118 finished \tANN training loss 0.004269\n",
      ">> Epoch 119 finished \tANN training loss 0.004224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 120 finished \tANN training loss 0.004176\n",
      ">> Epoch 121 finished \tANN training loss 0.004132\n",
      ">> Epoch 122 finished \tANN training loss 0.004087\n",
      ">> Epoch 123 finished \tANN training loss 0.004045\n",
      ">> Epoch 124 finished \tANN training loss 0.004003\n",
      ">> Epoch 125 finished \tANN training loss 0.003962\n",
      ">> Epoch 126 finished \tANN training loss 0.003920\n",
      ">> Epoch 127 finished \tANN training loss 0.003880\n",
      ">> Epoch 128 finished \tANN training loss 0.003841\n",
      ">> Epoch 129 finished \tANN training loss 0.003803\n",
      ">> Epoch 130 finished \tANN training loss 0.003765\n",
      ">> Epoch 131 finished \tANN training loss 0.003728\n",
      ">> Epoch 132 finished \tANN training loss 0.003691\n",
      ">> Epoch 133 finished \tANN training loss 0.003656\n",
      ">> Epoch 134 finished \tANN training loss 0.003621\n",
      ">> Epoch 135 finished \tANN training loss 0.003587\n",
      ">> Epoch 136 finished \tANN training loss 0.003555\n",
      ">> Epoch 137 finished \tANN training loss 0.003520\n",
      ">> Epoch 138 finished \tANN training loss 0.003487\n",
      ">> Epoch 139 finished \tANN training loss 0.003455\n",
      ">> Epoch 140 finished \tANN training loss 0.003423\n",
      ">> Epoch 141 finished \tANN training loss 0.003391\n",
      ">> Epoch 142 finished \tANN training loss 0.003362\n",
      ">> Epoch 143 finished \tANN training loss 0.003331\n",
      ">> Epoch 144 finished \tANN training loss 0.003301\n",
      ">> Epoch 145 finished \tANN training loss 0.003272\n",
      ">> Epoch 146 finished \tANN training loss 0.003244\n",
      ">> Epoch 147 finished \tANN training loss 0.003216\n",
      ">> Epoch 148 finished \tANN training loss 0.003188\n",
      ">> Epoch 149 finished \tANN training loss 0.003161\n",
      ">> Epoch 150 finished \tANN training loss 0.003134\n",
      ">> Epoch 151 finished \tANN training loss 0.003108\n",
      ">> Epoch 152 finished \tANN training loss 0.003082\n",
      ">> Epoch 153 finished \tANN training loss 0.003056\n",
      ">> Epoch 154 finished \tANN training loss 0.003031\n",
      ">> Epoch 155 finished \tANN training loss 0.003006\n",
      ">> Epoch 156 finished \tANN training loss 0.002981\n",
      ">> Epoch 157 finished \tANN training loss 0.002957\n",
      ">> Epoch 158 finished \tANN training loss 0.002934\n",
      ">> Epoch 159 finished \tANN training loss 0.002910\n",
      ">> Epoch 160 finished \tANN training loss 0.002887\n",
      ">> Epoch 161 finished \tANN training loss 0.002865\n",
      ">> Epoch 162 finished \tANN training loss 0.002842\n",
      ">> Epoch 163 finished \tANN training loss 0.002820\n",
      ">> Epoch 164 finished \tANN training loss 0.002798\n",
      ">> Epoch 165 finished \tANN training loss 0.002777\n",
      ">> Epoch 166 finished \tANN training loss 0.002756\n",
      ">> Epoch 167 finished \tANN training loss 0.002735\n",
      ">> Epoch 168 finished \tANN training loss 0.002715\n",
      ">> Epoch 169 finished \tANN training loss 0.002695\n",
      ">> Epoch 170 finished \tANN training loss 0.002674\n",
      ">> Epoch 171 finished \tANN training loss 0.002655\n",
      ">> Epoch 172 finished \tANN training loss 0.002635\n",
      ">> Epoch 173 finished \tANN training loss 0.002616\n",
      ">> Epoch 174 finished \tANN training loss 0.002597\n",
      ">> Epoch 175 finished \tANN training loss 0.002578\n",
      ">> Epoch 176 finished \tANN training loss 0.002560\n",
      ">> Epoch 177 finished \tANN training loss 0.002542\n",
      ">> Epoch 178 finished \tANN training loss 0.002524\n",
      ">> Epoch 179 finished \tANN training loss 0.002506\n",
      ">> Epoch 180 finished \tANN training loss 0.002489\n",
      ">> Epoch 181 finished \tANN training loss 0.002472\n",
      ">> Epoch 182 finished \tANN training loss 0.002455\n",
      ">> Epoch 183 finished \tANN training loss 0.002438\n",
      ">> Epoch 184 finished \tANN training loss 0.002421\n",
      ">> Epoch 185 finished \tANN training loss 0.002405\n",
      ">> Epoch 186 finished \tANN training loss 0.002389\n",
      ">> Epoch 187 finished \tANN training loss 0.002373\n",
      ">> Epoch 188 finished \tANN training loss 0.002357\n",
      ">> Epoch 189 finished \tANN training loss 0.002341\n",
      ">> Epoch 190 finished \tANN training loss 0.002326\n",
      ">> Epoch 191 finished \tANN training loss 0.002311\n",
      ">> Epoch 192 finished \tANN training loss 0.002296\n",
      ">> Epoch 193 finished \tANN training loss 0.002281\n",
      ">> Epoch 194 finished \tANN training loss 0.002266\n",
      ">> Epoch 195 finished \tANN training loss 0.002252\n",
      ">> Epoch 196 finished \tANN training loss 0.002237\n",
      ">> Epoch 197 finished \tANN training loss 0.002223\n",
      ">> Epoch 198 finished \tANN training loss 0.002209\n",
      ">> Epoch 199 finished \tANN training loss 0.002195\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.880000\n"
     ]
    }
   ],
   "source": [
    "# ReLu, dropout = 0\n",
    "acc0 = deep_belief_net(hidden_layers_structure=[300], \n",
    "                       learning_rate_rbm=0.05,\n",
    "                       learning_rate=0.1,\n",
    "                       n_epochs_rbm=40,\n",
    "                       n_iter_backprop=200,\n",
    "                       batch_size=32,\n",
    "                       activation_function='relu',\n",
    "                       dropout_p=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.88\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(acc0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 72.055847\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 60.757393\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 61.084770\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 52.822067\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 45.157761\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 43.368752\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 55.778893\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 45.162216\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 37.964745\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 33.017380\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 54.482384\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 52.822216\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 46.630253\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 47.379044\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 43.645729\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 39.365711\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 47.167969\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 48.273788\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 45.013912\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 53.182144\n",
      ">> Epoch 21 finished \tRBM Reconstruction error 50.217506\n",
      ">> Epoch 22 finished \tRBM Reconstruction error 54.040089\n",
      ">> Epoch 23 finished \tRBM Reconstruction error 50.279682\n",
      ">> Epoch 24 finished \tRBM Reconstruction error 44.892334\n",
      ">> Epoch 25 finished \tRBM Reconstruction error 60.720314\n",
      ">> Epoch 26 finished \tRBM Reconstruction error 47.933243\n",
      ">> Epoch 27 finished \tRBM Reconstruction error 45.958370\n",
      ">> Epoch 28 finished \tRBM Reconstruction error 49.027050\n",
      ">> Epoch 29 finished \tRBM Reconstruction error 51.609917\n",
      ">> Epoch 30 finished \tRBM Reconstruction error 57.335293\n",
      ">> Epoch 31 finished \tRBM Reconstruction error 53.782314\n",
      ">> Epoch 32 finished \tRBM Reconstruction error 60.037567\n",
      ">> Epoch 33 finished \tRBM Reconstruction error 41.730480\n",
      ">> Epoch 34 finished \tRBM Reconstruction error 49.835506\n",
      ">> Epoch 35 finished \tRBM Reconstruction error 45.948681\n",
      ">> Epoch 36 finished \tRBM Reconstruction error 45.618214\n",
      ">> Epoch 37 finished \tRBM Reconstruction error 58.829765\n",
      ">> Epoch 38 finished \tRBM Reconstruction error 44.395443\n",
      ">> Epoch 39 finished \tRBM Reconstruction error 41.659542\n",
      ">> Epoch 40 finished \tRBM Reconstruction error 56.914200\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.719136\n",
      ">> Epoch 1 finished \tANN training loss 0.504289\n",
      ">> Epoch 2 finished \tANN training loss 0.404829\n",
      ">> Epoch 3 finished \tANN training loss 0.336654\n",
      ">> Epoch 4 finished \tANN training loss 0.293528\n",
      ">> Epoch 5 finished \tANN training loss 0.264411\n",
      ">> Epoch 6 finished \tANN training loss 0.225134\n",
      ">> Epoch 7 finished \tANN training loss 0.204277\n",
      ">> Epoch 8 finished \tANN training loss 0.206538\n",
      ">> Epoch 9 finished \tANN training loss 0.167138\n",
      ">> Epoch 10 finished \tANN training loss 0.155270\n",
      ">> Epoch 11 finished \tANN training loss 0.138381\n",
      ">> Epoch 12 finished \tANN training loss 0.125381\n",
      ">> Epoch 13 finished \tANN training loss 0.110737\n",
      ">> Epoch 14 finished \tANN training loss 0.101545\n",
      ">> Epoch 15 finished \tANN training loss 0.095920\n",
      ">> Epoch 16 finished \tANN training loss 0.087776\n",
      ">> Epoch 17 finished \tANN training loss 0.083908\n",
      ">> Epoch 18 finished \tANN training loss 0.074305\n",
      ">> Epoch 19 finished \tANN training loss 0.067529\n",
      ">> Epoch 20 finished \tANN training loss 0.062484\n",
      ">> Epoch 21 finished \tANN training loss 0.058037\n",
      ">> Epoch 22 finished \tANN training loss 0.054194\n",
      ">> Epoch 23 finished \tANN training loss 0.050317\n",
      ">> Epoch 24 finished \tANN training loss 0.047677\n",
      ">> Epoch 25 finished \tANN training loss 0.045548\n",
      ">> Epoch 26 finished \tANN training loss 0.040314\n",
      ">> Epoch 27 finished \tANN training loss 0.039659\n",
      ">> Epoch 28 finished \tANN training loss 0.036556\n",
      ">> Epoch 29 finished \tANN training loss 0.035293\n",
      ">> Epoch 30 finished \tANN training loss 0.032459\n",
      ">> Epoch 31 finished \tANN training loss 0.030028\n",
      ">> Epoch 32 finished \tANN training loss 0.028420\n",
      ">> Epoch 33 finished \tANN training loss 0.026577\n",
      ">> Epoch 34 finished \tANN training loss 0.026243\n",
      ">> Epoch 35 finished \tANN training loss 0.024595\n",
      ">> Epoch 36 finished \tANN training loss 0.022506\n",
      ">> Epoch 37 finished \tANN training loss 0.021923\n",
      ">> Epoch 38 finished \tANN training loss 0.020706\n",
      ">> Epoch 39 finished \tANN training loss 0.019700\n",
      ">> Epoch 40 finished \tANN training loss 0.018735\n",
      ">> Epoch 41 finished \tANN training loss 0.018380\n",
      ">> Epoch 42 finished \tANN training loss 0.018464\n",
      ">> Epoch 43 finished \tANN training loss 0.016393\n",
      ">> Epoch 44 finished \tANN training loss 0.016940\n",
      ">> Epoch 45 finished \tANN training loss 0.015171\n",
      ">> Epoch 46 finished \tANN training loss 0.014689\n",
      ">> Epoch 47 finished \tANN training loss 0.013990\n",
      ">> Epoch 48 finished \tANN training loss 0.013253\n",
      ">> Epoch 49 finished \tANN training loss 0.013086\n",
      ">> Epoch 50 finished \tANN training loss 0.013376\n",
      ">> Epoch 51 finished \tANN training loss 0.012374\n",
      ">> Epoch 52 finished \tANN training loss 0.011646\n",
      ">> Epoch 53 finished \tANN training loss 0.011284\n",
      ">> Epoch 54 finished \tANN training loss 0.010702\n",
      ">> Epoch 55 finished \tANN training loss 0.010475\n",
      ">> Epoch 56 finished \tANN training loss 0.010147\n",
      ">> Epoch 57 finished \tANN training loss 0.009713\n",
      ">> Epoch 58 finished \tANN training loss 0.009365\n",
      ">> Epoch 59 finished \tANN training loss 0.009080\n",
      ">> Epoch 60 finished \tANN training loss 0.008744\n",
      ">> Epoch 61 finished \tANN training loss 0.008477\n",
      ">> Epoch 62 finished \tANN training loss 0.008562\n",
      ">> Epoch 63 finished \tANN training loss 0.008399\n",
      ">> Epoch 64 finished \tANN training loss 0.007853\n",
      ">> Epoch 65 finished \tANN training loss 0.007480\n",
      ">> Epoch 66 finished \tANN training loss 0.007428\n",
      ">> Epoch 67 finished \tANN training loss 0.007179\n",
      ">> Epoch 68 finished \tANN training loss 0.007125\n",
      ">> Epoch 69 finished \tANN training loss 0.006811\n",
      ">> Epoch 70 finished \tANN training loss 0.006664\n",
      ">> Epoch 71 finished \tANN training loss 0.006558\n",
      ">> Epoch 72 finished \tANN training loss 0.006270\n",
      ">> Epoch 73 finished \tANN training loss 0.006157\n",
      ">> Epoch 74 finished \tANN training loss 0.006110\n",
      ">> Epoch 75 finished \tANN training loss 0.006064\n",
      ">> Epoch 76 finished \tANN training loss 0.005861\n",
      ">> Epoch 77 finished \tANN training loss 0.005647\n",
      ">> Epoch 78 finished \tANN training loss 0.005401\n",
      ">> Epoch 79 finished \tANN training loss 0.005372\n",
      ">> Epoch 80 finished \tANN training loss 0.005188\n",
      ">> Epoch 81 finished \tANN training loss 0.005044\n",
      ">> Epoch 82 finished \tANN training loss 0.005081\n",
      ">> Epoch 83 finished \tANN training loss 0.005009\n",
      ">> Epoch 84 finished \tANN training loss 0.004811\n",
      ">> Epoch 85 finished \tANN training loss 0.004680\n",
      ">> Epoch 86 finished \tANN training loss 0.004622\n",
      ">> Epoch 87 finished \tANN training loss 0.004423\n",
      ">> Epoch 88 finished \tANN training loss 0.004543\n",
      ">> Epoch 89 finished \tANN training loss 0.004276\n",
      ">> Epoch 90 finished \tANN training loss 0.004191\n",
      ">> Epoch 91 finished \tANN training loss 0.004067\n",
      ">> Epoch 92 finished \tANN training loss 0.004063\n",
      ">> Epoch 93 finished \tANN training loss 0.003892\n",
      ">> Epoch 94 finished \tANN training loss 0.003982\n",
      ">> Epoch 95 finished \tANN training loss 0.003900\n",
      ">> Epoch 96 finished \tANN training loss 0.003828\n",
      ">> Epoch 97 finished \tANN training loss 0.003719\n",
      ">> Epoch 98 finished \tANN training loss 0.003543\n",
      ">> Epoch 99 finished \tANN training loss 0.003537\n",
      ">> Epoch 100 finished \tANN training loss 0.003511\n",
      ">> Epoch 101 finished \tANN training loss 0.003412\n",
      ">> Epoch 102 finished \tANN training loss 0.003446\n",
      ">> Epoch 103 finished \tANN training loss 0.003311\n",
      ">> Epoch 104 finished \tANN training loss 0.003231\n",
      ">> Epoch 105 finished \tANN training loss 0.003341\n",
      ">> Epoch 106 finished \tANN training loss 0.003307\n",
      ">> Epoch 107 finished \tANN training loss 0.003057\n",
      ">> Epoch 108 finished \tANN training loss 0.002992\n",
      ">> Epoch 109 finished \tANN training loss 0.002903\n",
      ">> Epoch 110 finished \tANN training loss 0.002841\n",
      ">> Epoch 111 finished \tANN training loss 0.002836\n",
      ">> Epoch 112 finished \tANN training loss 0.002826\n",
      ">> Epoch 113 finished \tANN training loss 0.002757\n",
      ">> Epoch 114 finished \tANN training loss 0.002888\n",
      ">> Epoch 115 finished \tANN training loss 0.002703\n",
      ">> Epoch 116 finished \tANN training loss 0.002736\n",
      ">> Epoch 117 finished \tANN training loss 0.002767\n",
      ">> Epoch 118 finished \tANN training loss 0.002571\n",
      ">> Epoch 119 finished \tANN training loss 0.002694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 120 finished \tANN training loss 0.002488\n",
      ">> Epoch 121 finished \tANN training loss 0.002435\n",
      ">> Epoch 122 finished \tANN training loss 0.002469\n",
      ">> Epoch 123 finished \tANN training loss 0.002422\n",
      ">> Epoch 124 finished \tANN training loss 0.002348\n",
      ">> Epoch 125 finished \tANN training loss 0.002443\n",
      ">> Epoch 126 finished \tANN training loss 0.002441\n",
      ">> Epoch 127 finished \tANN training loss 0.002294\n",
      ">> Epoch 128 finished \tANN training loss 0.002246\n",
      ">> Epoch 129 finished \tANN training loss 0.002297\n",
      ">> Epoch 130 finished \tANN training loss 0.002211\n",
      ">> Epoch 131 finished \tANN training loss 0.002310\n",
      ">> Epoch 132 finished \tANN training loss 0.002305\n",
      ">> Epoch 133 finished \tANN training loss 0.002118\n",
      ">> Epoch 134 finished \tANN training loss 0.002141\n",
      ">> Epoch 135 finished \tANN training loss 0.002060\n",
      ">> Epoch 136 finished \tANN training loss 0.002042\n",
      ">> Epoch 137 finished \tANN training loss 0.002031\n",
      ">> Epoch 138 finished \tANN training loss 0.002056\n",
      ">> Epoch 139 finished \tANN training loss 0.001979\n",
      ">> Epoch 140 finished \tANN training loss 0.002029\n",
      ">> Epoch 141 finished \tANN training loss 0.001950\n",
      ">> Epoch 142 finished \tANN training loss 0.001955\n",
      ">> Epoch 143 finished \tANN training loss 0.001966\n",
      ">> Epoch 144 finished \tANN training loss 0.001910\n",
      ">> Epoch 145 finished \tANN training loss 0.001879\n",
      ">> Epoch 146 finished \tANN training loss 0.001856\n",
      ">> Epoch 147 finished \tANN training loss 0.001843\n",
      ">> Epoch 148 finished \tANN training loss 0.001780\n",
      ">> Epoch 149 finished \tANN training loss 0.001761\n",
      ">> Epoch 150 finished \tANN training loss 0.001729\n",
      ">> Epoch 151 finished \tANN training loss 0.001707\n",
      ">> Epoch 152 finished \tANN training loss 0.001705\n",
      ">> Epoch 153 finished \tANN training loss 0.001686\n",
      ">> Epoch 154 finished \tANN training loss 0.001666\n",
      ">> Epoch 155 finished \tANN training loss 0.001641\n",
      ">> Epoch 156 finished \tANN training loss 0.001602\n",
      ">> Epoch 157 finished \tANN training loss 0.001607\n",
      ">> Epoch 158 finished \tANN training loss 0.001651\n",
      ">> Epoch 159 finished \tANN training loss 0.001547\n",
      ">> Epoch 160 finished \tANN training loss 0.001616\n",
      ">> Epoch 161 finished \tANN training loss 0.001520\n",
      ">> Epoch 162 finished \tANN training loss 0.001496\n",
      ">> Epoch 163 finished \tANN training loss 0.001513\n",
      ">> Epoch 164 finished \tANN training loss 0.001478\n",
      ">> Epoch 165 finished \tANN training loss 0.001435\n",
      ">> Epoch 166 finished \tANN training loss 0.001418\n",
      ">> Epoch 167 finished \tANN training loss 0.001439\n",
      ">> Epoch 168 finished \tANN training loss 0.001394\n",
      ">> Epoch 169 finished \tANN training loss 0.001379\n",
      ">> Epoch 170 finished \tANN training loss 0.001406\n",
      ">> Epoch 171 finished \tANN training loss 0.001360\n",
      ">> Epoch 172 finished \tANN training loss 0.001347\n",
      ">> Epoch 173 finished \tANN training loss 0.001317\n",
      ">> Epoch 174 finished \tANN training loss 0.001292\n",
      ">> Epoch 175 finished \tANN training loss 0.001286\n",
      ">> Epoch 176 finished \tANN training loss 0.001330\n",
      ">> Epoch 177 finished \tANN training loss 0.001292\n",
      ">> Epoch 178 finished \tANN training loss 0.001255\n",
      ">> Epoch 179 finished \tANN training loss 0.001262\n",
      ">> Epoch 180 finished \tANN training loss 0.001244\n",
      ">> Epoch 181 finished \tANN training loss 0.001239\n",
      ">> Epoch 182 finished \tANN training loss 0.001227\n",
      ">> Epoch 183 finished \tANN training loss 0.001194\n",
      ">> Epoch 184 finished \tANN training loss 0.001229\n",
      ">> Epoch 185 finished \tANN training loss 0.001202\n",
      ">> Epoch 186 finished \tANN training loss 0.001213\n",
      ">> Epoch 187 finished \tANN training loss 0.001196\n",
      ">> Epoch 188 finished \tANN training loss 0.001206\n",
      ">> Epoch 189 finished \tANN training loss 0.001159\n",
      ">> Epoch 190 finished \tANN training loss 0.001127\n",
      ">> Epoch 191 finished \tANN training loss 0.001106\n",
      ">> Epoch 192 finished \tANN training loss 0.001119\n",
      ">> Epoch 193 finished \tANN training loss 0.001094\n",
      ">> Epoch 194 finished \tANN training loss 0.001082\n",
      ">> Epoch 195 finished \tANN training loss 0.001071\n",
      ">> Epoch 196 finished \tANN training loss 0.001051\n",
      ">> Epoch 197 finished \tANN training loss 0.001081\n",
      ">> Epoch 198 finished \tANN training loss 0.001072\n",
      ">> Epoch 199 finished \tANN training loss 0.001053\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.890000\n"
     ]
    }
   ],
   "source": [
    "# ReLu, dropout = 0.1\n",
    "acc1 = deep_belief_net(hidden_layers_structure=[300], \n",
    "                       learning_rate_rbm=0.05,\n",
    "                       learning_rate=0.1,\n",
    "                       n_epochs_rbm=40,\n",
    "                       n_iter_backprop=200,\n",
    "                       batch_size=32,\n",
    "                       activation_function='relu',\n",
    "                       dropout_p=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.89\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(acc1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 56.206200\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 77.201317\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 43.497169\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 35.043930\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 29.859175\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 36.687565\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 43.480713\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 46.617226\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 41.150391\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 35.490501\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 34.342411\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 29.118097\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 33.308773\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 37.300919\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 44.223289\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 43.610748\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 42.425957\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 33.686352\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 32.535210\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 40.499744\n",
      ">> Epoch 21 finished \tRBM Reconstruction error 30.235912\n",
      ">> Epoch 22 finished \tRBM Reconstruction error 39.815880\n",
      ">> Epoch 23 finished \tRBM Reconstruction error 46.056564\n",
      ">> Epoch 24 finished \tRBM Reconstruction error 43.469513\n",
      ">> Epoch 25 finished \tRBM Reconstruction error 37.808216\n",
      ">> Epoch 26 finished \tRBM Reconstruction error 38.587616\n",
      ">> Epoch 27 finished \tRBM Reconstruction error 39.078217\n",
      ">> Epoch 28 finished \tRBM Reconstruction error 31.427929\n",
      ">> Epoch 29 finished \tRBM Reconstruction error 38.274857\n",
      ">> Epoch 30 finished \tRBM Reconstruction error 37.169632\n",
      ">> Epoch 31 finished \tRBM Reconstruction error 36.369694\n",
      ">> Epoch 32 finished \tRBM Reconstruction error 44.888748\n",
      ">> Epoch 33 finished \tRBM Reconstruction error 60.992508\n",
      ">> Epoch 34 finished \tRBM Reconstruction error 43.566807\n",
      ">> Epoch 35 finished \tRBM Reconstruction error 71.165504\n",
      ">> Epoch 36 finished \tRBM Reconstruction error 48.004337\n",
      ">> Epoch 37 finished \tRBM Reconstruction error 56.236366\n",
      ">> Epoch 38 finished \tRBM Reconstruction error 37.914242\n",
      ">> Epoch 39 finished \tRBM Reconstruction error 40.765881\n",
      ">> Epoch 40 finished \tRBM Reconstruction error 58.849442\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.763057\n",
      ">> Epoch 1 finished \tANN training loss 0.530294\n",
      ">> Epoch 2 finished \tANN training loss 0.407203\n",
      ">> Epoch 3 finished \tANN training loss 0.366598\n",
      ">> Epoch 4 finished \tANN training loss 0.309556\n",
      ">> Epoch 5 finished \tANN training loss 0.273724\n",
      ">> Epoch 6 finished \tANN training loss 0.249534\n",
      ">> Epoch 7 finished \tANN training loss 0.225815\n",
      ">> Epoch 8 finished \tANN training loss 0.228244\n",
      ">> Epoch 9 finished \tANN training loss 0.196719\n",
      ">> Epoch 10 finished \tANN training loss 0.180711\n",
      ">> Epoch 11 finished \tANN training loss 0.159670\n",
      ">> Epoch 12 finished \tANN training loss 0.151425\n",
      ">> Epoch 13 finished \tANN training loss 0.145652\n",
      ">> Epoch 14 finished \tANN training loss 0.130921\n",
      ">> Epoch 15 finished \tANN training loss 0.117465\n",
      ">> Epoch 16 finished \tANN training loss 0.108762\n",
      ">> Epoch 17 finished \tANN training loss 0.103295\n",
      ">> Epoch 18 finished \tANN training loss 0.101955\n",
      ">> Epoch 19 finished \tANN training loss 0.095027\n",
      ">> Epoch 20 finished \tANN training loss 0.085982\n",
      ">> Epoch 21 finished \tANN training loss 0.082087\n",
      ">> Epoch 22 finished \tANN training loss 0.075599\n",
      ">> Epoch 23 finished \tANN training loss 0.069679\n",
      ">> Epoch 24 finished \tANN training loss 0.063957\n",
      ">> Epoch 25 finished \tANN training loss 0.063379\n",
      ">> Epoch 26 finished \tANN training loss 0.057712\n",
      ">> Epoch 27 finished \tANN training loss 0.057132\n",
      ">> Epoch 28 finished \tANN training loss 0.051099\n",
      ">> Epoch 29 finished \tANN training loss 0.050962\n",
      ">> Epoch 30 finished \tANN training loss 0.045647\n",
      ">> Epoch 31 finished \tANN training loss 0.041795\n",
      ">> Epoch 32 finished \tANN training loss 0.040142\n",
      ">> Epoch 33 finished \tANN training loss 0.037400\n",
      ">> Epoch 34 finished \tANN training loss 0.037238\n",
      ">> Epoch 35 finished \tANN training loss 0.033529\n",
      ">> Epoch 36 finished \tANN training loss 0.032800\n",
      ">> Epoch 37 finished \tANN training loss 0.030926\n",
      ">> Epoch 38 finished \tANN training loss 0.029241\n",
      ">> Epoch 39 finished \tANN training loss 0.031015\n",
      ">> Epoch 40 finished \tANN training loss 0.028163\n",
      ">> Epoch 41 finished \tANN training loss 0.027726\n",
      ">> Epoch 42 finished \tANN training loss 0.024614\n",
      ">> Epoch 43 finished \tANN training loss 0.022441\n",
      ">> Epoch 44 finished \tANN training loss 0.020821\n",
      ">> Epoch 45 finished \tANN training loss 0.020281\n",
      ">> Epoch 46 finished \tANN training loss 0.020302\n",
      ">> Epoch 47 finished \tANN training loss 0.019095\n",
      ">> Epoch 48 finished \tANN training loss 0.018188\n",
      ">> Epoch 49 finished \tANN training loss 0.017712\n",
      ">> Epoch 50 finished \tANN training loss 0.018247\n",
      ">> Epoch 51 finished \tANN training loss 0.015728\n",
      ">> Epoch 52 finished \tANN training loss 0.015964\n",
      ">> Epoch 53 finished \tANN training loss 0.015110\n",
      ">> Epoch 54 finished \tANN training loss 0.015308\n",
      ">> Epoch 55 finished \tANN training loss 0.014228\n",
      ">> Epoch 56 finished \tANN training loss 0.013407\n",
      ">> Epoch 57 finished \tANN training loss 0.012899\n",
      ">> Epoch 58 finished \tANN training loss 0.011997\n",
      ">> Epoch 59 finished \tANN training loss 0.012016\n",
      ">> Epoch 60 finished \tANN training loss 0.011425\n",
      ">> Epoch 61 finished \tANN training loss 0.011710\n",
      ">> Epoch 62 finished \tANN training loss 0.011182\n",
      ">> Epoch 63 finished \tANN training loss 0.010530\n",
      ">> Epoch 64 finished \tANN training loss 0.010343\n",
      ">> Epoch 65 finished \tANN training loss 0.010221\n",
      ">> Epoch 66 finished \tANN training loss 0.009676\n",
      ">> Epoch 67 finished \tANN training loss 0.009703\n",
      ">> Epoch 68 finished \tANN training loss 0.009371\n",
      ">> Epoch 69 finished \tANN training loss 0.008724\n",
      ">> Epoch 70 finished \tANN training loss 0.008507\n",
      ">> Epoch 71 finished \tANN training loss 0.008132\n",
      ">> Epoch 72 finished \tANN training loss 0.007937\n",
      ">> Epoch 73 finished \tANN training loss 0.007824\n",
      ">> Epoch 74 finished \tANN training loss 0.007243\n",
      ">> Epoch 75 finished \tANN training loss 0.007016\n",
      ">> Epoch 76 finished \tANN training loss 0.007076\n",
      ">> Epoch 77 finished \tANN training loss 0.007010\n",
      ">> Epoch 78 finished \tANN training loss 0.007044\n",
      ">> Epoch 79 finished \tANN training loss 0.006529\n",
      ">> Epoch 80 finished \tANN training loss 0.006264\n",
      ">> Epoch 81 finished \tANN training loss 0.006160\n",
      ">> Epoch 82 finished \tANN training loss 0.005895\n",
      ">> Epoch 83 finished \tANN training loss 0.005891\n",
      ">> Epoch 84 finished \tANN training loss 0.006012\n",
      ">> Epoch 85 finished \tANN training loss 0.005802\n",
      ">> Epoch 86 finished \tANN training loss 0.005600\n",
      ">> Epoch 87 finished \tANN training loss 0.005332\n",
      ">> Epoch 88 finished \tANN training loss 0.005320\n",
      ">> Epoch 89 finished \tANN training loss 0.005220\n",
      ">> Epoch 90 finished \tANN training loss 0.004750\n",
      ">> Epoch 91 finished \tANN training loss 0.004751\n",
      ">> Epoch 92 finished \tANN training loss 0.004809\n",
      ">> Epoch 93 finished \tANN training loss 0.004848\n",
      ">> Epoch 94 finished \tANN training loss 0.004493\n",
      ">> Epoch 95 finished \tANN training loss 0.004508\n",
      ">> Epoch 96 finished \tANN training loss 0.004324\n",
      ">> Epoch 97 finished \tANN training loss 0.003982\n",
      ">> Epoch 98 finished \tANN training loss 0.004481\n",
      ">> Epoch 99 finished \tANN training loss 0.003984\n",
      ">> Epoch 100 finished \tANN training loss 0.003924\n",
      ">> Epoch 101 finished \tANN training loss 0.003975\n",
      ">> Epoch 102 finished \tANN training loss 0.003715\n",
      ">> Epoch 103 finished \tANN training loss 0.003609\n",
      ">> Epoch 104 finished \tANN training loss 0.004448\n",
      ">> Epoch 105 finished \tANN training loss 0.003661\n",
      ">> Epoch 106 finished \tANN training loss 0.003542\n",
      ">> Epoch 107 finished \tANN training loss 0.003499\n",
      ">> Epoch 108 finished \tANN training loss 0.003301\n",
      ">> Epoch 109 finished \tANN training loss 0.003398\n",
      ">> Epoch 110 finished \tANN training loss 0.003172\n",
      ">> Epoch 111 finished \tANN training loss 0.003518\n",
      ">> Epoch 112 finished \tANN training loss 0.003141\n",
      ">> Epoch 113 finished \tANN training loss 0.003205\n",
      ">> Epoch 114 finished \tANN training loss 0.003429\n",
      ">> Epoch 115 finished \tANN training loss 0.003150\n",
      ">> Epoch 116 finished \tANN training loss 0.002912\n",
      ">> Epoch 117 finished \tANN training loss 0.002873\n",
      ">> Epoch 118 finished \tANN training loss 0.002985\n",
      ">> Epoch 119 finished \tANN training loss 0.002825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 120 finished \tANN training loss 0.002813\n",
      ">> Epoch 121 finished \tANN training loss 0.002841\n",
      ">> Epoch 122 finished \tANN training loss 0.002842\n",
      ">> Epoch 123 finished \tANN training loss 0.002755\n",
      ">> Epoch 124 finished \tANN training loss 0.002744\n",
      ">> Epoch 125 finished \tANN training loss 0.002752\n",
      ">> Epoch 126 finished \tANN training loss 0.002589\n",
      ">> Epoch 127 finished \tANN training loss 0.002507\n",
      ">> Epoch 128 finished \tANN training loss 0.002486\n",
      ">> Epoch 129 finished \tANN training loss 0.002604\n",
      ">> Epoch 130 finished \tANN training loss 0.002511\n",
      ">> Epoch 131 finished \tANN training loss 0.002479\n",
      ">> Epoch 132 finished \tANN training loss 0.002458\n",
      ">> Epoch 133 finished \tANN training loss 0.002214\n",
      ">> Epoch 134 finished \tANN training loss 0.002235\n",
      ">> Epoch 135 finished \tANN training loss 0.002137\n",
      ">> Epoch 136 finished \tANN training loss 0.002161\n",
      ">> Epoch 137 finished \tANN training loss 0.002176\n",
      ">> Epoch 138 finished \tANN training loss 0.002038\n",
      ">> Epoch 139 finished \tANN training loss 0.002051\n",
      ">> Epoch 140 finished \tANN training loss 0.002209\n",
      ">> Epoch 141 finished \tANN training loss 0.002014\n",
      ">> Epoch 142 finished \tANN training loss 0.002032\n",
      ">> Epoch 143 finished \tANN training loss 0.001981\n",
      ">> Epoch 144 finished \tANN training loss 0.001948\n",
      ">> Epoch 145 finished \tANN training loss 0.001881\n",
      ">> Epoch 146 finished \tANN training loss 0.001842\n",
      ">> Epoch 147 finished \tANN training loss 0.001859\n",
      ">> Epoch 148 finished \tANN training loss 0.001873\n",
      ">> Epoch 149 finished \tANN training loss 0.001858\n",
      ">> Epoch 150 finished \tANN training loss 0.001693\n",
      ">> Epoch 151 finished \tANN training loss 0.001726\n",
      ">> Epoch 152 finished \tANN training loss 0.001710\n",
      ">> Epoch 153 finished \tANN training loss 0.001604\n",
      ">> Epoch 154 finished \tANN training loss 0.001633\n",
      ">> Epoch 155 finished \tANN training loss 0.001661\n",
      ">> Epoch 156 finished \tANN training loss 0.001658\n",
      ">> Epoch 157 finished \tANN training loss 0.001620\n",
      ">> Epoch 158 finished \tANN training loss 0.001585\n",
      ">> Epoch 159 finished \tANN training loss 0.001647\n",
      ">> Epoch 160 finished \tANN training loss 0.001519\n",
      ">> Epoch 161 finished \tANN training loss 0.001536\n",
      ">> Epoch 162 finished \tANN training loss 0.001548\n",
      ">> Epoch 163 finished \tANN training loss 0.001427\n",
      ">> Epoch 164 finished \tANN training loss 0.001456\n",
      ">> Epoch 165 finished \tANN training loss 0.001384\n",
      ">> Epoch 166 finished \tANN training loss 0.001317\n",
      ">> Epoch 167 finished \tANN training loss 0.001319\n",
      ">> Epoch 168 finished \tANN training loss 0.001314\n",
      ">> Epoch 169 finished \tANN training loss 0.001441\n",
      ">> Epoch 170 finished \tANN training loss 0.001251\n",
      ">> Epoch 171 finished \tANN training loss 0.001252\n",
      ">> Epoch 172 finished \tANN training loss 0.001346\n",
      ">> Epoch 173 finished \tANN training loss 0.001220\n",
      ">> Epoch 174 finished \tANN training loss 0.001165\n",
      ">> Epoch 175 finished \tANN training loss 0.001212\n",
      ">> Epoch 176 finished \tANN training loss 0.001158\n",
      ">> Epoch 177 finished \tANN training loss 0.001117\n",
      ">> Epoch 178 finished \tANN training loss 0.001086\n",
      ">> Epoch 179 finished \tANN training loss 0.001166\n",
      ">> Epoch 180 finished \tANN training loss 0.001109\n",
      ">> Epoch 181 finished \tANN training loss 0.001073\n",
      ">> Epoch 182 finished \tANN training loss 0.001125\n",
      ">> Epoch 183 finished \tANN training loss 0.001089\n",
      ">> Epoch 184 finished \tANN training loss 0.001099\n",
      ">> Epoch 185 finished \tANN training loss 0.001049\n",
      ">> Epoch 186 finished \tANN training loss 0.001056\n",
      ">> Epoch 187 finished \tANN training loss 0.001073\n",
      ">> Epoch 188 finished \tANN training loss 0.001067\n",
      ">> Epoch 189 finished \tANN training loss 0.001031\n",
      ">> Epoch 190 finished \tANN training loss 0.000993\n",
      ">> Epoch 191 finished \tANN training loss 0.000976\n",
      ">> Epoch 192 finished \tANN training loss 0.001036\n",
      ">> Epoch 193 finished \tANN training loss 0.001026\n",
      ">> Epoch 194 finished \tANN training loss 0.000949\n",
      ">> Epoch 195 finished \tANN training loss 0.000948\n",
      ">> Epoch 196 finished \tANN training loss 0.000929\n",
      ">> Epoch 197 finished \tANN training loss 0.000967\n",
      ">> Epoch 198 finished \tANN training loss 0.000988\n",
      ">> Epoch 199 finished \tANN training loss 0.001036\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.900000\n"
     ]
    }
   ],
   "source": [
    "# ReLu, dropout = 0.3\n",
    "acc3 = deep_belief_net(hidden_layers_structure=[300], \n",
    "                       learning_rate_rbm=0.05,\n",
    "                       learning_rate=0.1,\n",
    "                       n_epochs_rbm=40,\n",
    "                       n_iter_backprop=200,\n",
    "                       batch_size=32,\n",
    "                       activation_function='relu',\n",
    "                       dropout_p=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.9\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(acc3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 65.088890\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 31.600693\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 38.023712\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 38.858429\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 33.014267\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 27.287451\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 27.105581\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 36.422344\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 59.886730\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 32.112198\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 26.038902\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 29.801992\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 28.663811\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 26.208193\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 25.640636\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 25.160589\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 33.832520\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 35.933903\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 38.471088\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 40.283703\n",
      ">> Epoch 21 finished \tRBM Reconstruction error 34.686333\n",
      ">> Epoch 22 finished \tRBM Reconstruction error 36.088261\n",
      ">> Epoch 23 finished \tRBM Reconstruction error 33.995361\n",
      ">> Epoch 24 finished \tRBM Reconstruction error 32.413864\n",
      ">> Epoch 25 finished \tRBM Reconstruction error 35.722523\n",
      ">> Epoch 26 finished \tRBM Reconstruction error 27.134438\n",
      ">> Epoch 27 finished \tRBM Reconstruction error 35.185036\n",
      ">> Epoch 28 finished \tRBM Reconstruction error 20.068762\n",
      ">> Epoch 29 finished \tRBM Reconstruction error 29.205156\n",
      ">> Epoch 30 finished \tRBM Reconstruction error 27.306763\n",
      ">> Epoch 31 finished \tRBM Reconstruction error 38.133926\n",
      ">> Epoch 32 finished \tRBM Reconstruction error 29.115192\n",
      ">> Epoch 33 finished \tRBM Reconstruction error 31.015118\n",
      ">> Epoch 34 finished \tRBM Reconstruction error 30.365646\n",
      ">> Epoch 35 finished \tRBM Reconstruction error 27.149424\n",
      ">> Epoch 36 finished \tRBM Reconstruction error 34.585625\n",
      ">> Epoch 37 finished \tRBM Reconstruction error 39.828106\n",
      ">> Epoch 38 finished \tRBM Reconstruction error 36.040863\n",
      ">> Epoch 39 finished \tRBM Reconstruction error 51.653358\n",
      ">> Epoch 40 finished \tRBM Reconstruction error 43.784103\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.760663\n",
      ">> Epoch 1 finished \tANN training loss 0.540571\n",
      ">> Epoch 2 finished \tANN training loss 0.433031\n",
      ">> Epoch 3 finished \tANN training loss 0.367661\n",
      ">> Epoch 4 finished \tANN training loss 0.334500\n",
      ">> Epoch 5 finished \tANN training loss 0.305008\n",
      ">> Epoch 6 finished \tANN training loss 0.263462\n",
      ">> Epoch 7 finished \tANN training loss 0.247169\n",
      ">> Epoch 8 finished \tANN training loss 0.230201\n",
      ">> Epoch 9 finished \tANN training loss 0.207002\n",
      ">> Epoch 10 finished \tANN training loss 0.187797\n",
      ">> Epoch 11 finished \tANN training loss 0.187918\n",
      ">> Epoch 12 finished \tANN training loss 0.167484\n",
      ">> Epoch 13 finished \tANN training loss 0.161223\n",
      ">> Epoch 14 finished \tANN training loss 0.140987\n",
      ">> Epoch 15 finished \tANN training loss 0.131901\n",
      ">> Epoch 16 finished \tANN training loss 0.132539\n",
      ">> Epoch 17 finished \tANN training loss 0.113445\n",
      ">> Epoch 18 finished \tANN training loss 0.107376\n",
      ">> Epoch 19 finished \tANN training loss 0.099010\n",
      ">> Epoch 20 finished \tANN training loss 0.093148\n",
      ">> Epoch 21 finished \tANN training loss 0.087651\n",
      ">> Epoch 22 finished \tANN training loss 0.088220\n",
      ">> Epoch 23 finished \tANN training loss 0.076922\n",
      ">> Epoch 24 finished \tANN training loss 0.070750\n",
      ">> Epoch 25 finished \tANN training loss 0.067853\n",
      ">> Epoch 26 finished \tANN training loss 0.061786\n",
      ">> Epoch 27 finished \tANN training loss 0.060651\n",
      ">> Epoch 28 finished \tANN training loss 0.059891\n",
      ">> Epoch 29 finished \tANN training loss 0.054117\n",
      ">> Epoch 30 finished \tANN training loss 0.049212\n",
      ">> Epoch 31 finished \tANN training loss 0.047731\n",
      ">> Epoch 32 finished \tANN training loss 0.047263\n",
      ">> Epoch 33 finished \tANN training loss 0.045344\n",
      ">> Epoch 34 finished \tANN training loss 0.039465\n",
      ">> Epoch 35 finished \tANN training loss 0.040156\n",
      ">> Epoch 36 finished \tANN training loss 0.038256\n",
      ">> Epoch 37 finished \tANN training loss 0.033516\n",
      ">> Epoch 38 finished \tANN training loss 0.032147\n",
      ">> Epoch 39 finished \tANN training loss 0.030175\n",
      ">> Epoch 40 finished \tANN training loss 0.028922\n",
      ">> Epoch 41 finished \tANN training loss 0.028049\n",
      ">> Epoch 42 finished \tANN training loss 0.028505\n",
      ">> Epoch 43 finished \tANN training loss 0.025962\n",
      ">> Epoch 44 finished \tANN training loss 0.027882\n",
      ">> Epoch 45 finished \tANN training loss 0.023889\n",
      ">> Epoch 46 finished \tANN training loss 0.024069\n",
      ">> Epoch 47 finished \tANN training loss 0.022757\n",
      ">> Epoch 48 finished \tANN training loss 0.020988\n",
      ">> Epoch 49 finished \tANN training loss 0.019761\n",
      ">> Epoch 50 finished \tANN training loss 0.019384\n",
      ">> Epoch 51 finished \tANN training loss 0.018481\n",
      ">> Epoch 52 finished \tANN training loss 0.018197\n",
      ">> Epoch 53 finished \tANN training loss 0.017184\n",
      ">> Epoch 54 finished \tANN training loss 0.016559\n",
      ">> Epoch 55 finished \tANN training loss 0.015837\n",
      ">> Epoch 56 finished \tANN training loss 0.015603\n",
      ">> Epoch 57 finished \tANN training loss 0.014552\n",
      ">> Epoch 58 finished \tANN training loss 0.014755\n",
      ">> Epoch 59 finished \tANN training loss 0.014393\n",
      ">> Epoch 60 finished \tANN training loss 0.013081\n",
      ">> Epoch 61 finished \tANN training loss 0.013090\n",
      ">> Epoch 62 finished \tANN training loss 0.012672\n",
      ">> Epoch 63 finished \tANN training loss 0.012080\n",
      ">> Epoch 64 finished \tANN training loss 0.011608\n",
      ">> Epoch 65 finished \tANN training loss 0.011293\n",
      ">> Epoch 66 finished \tANN training loss 0.011283\n",
      ">> Epoch 67 finished \tANN training loss 0.010149\n",
      ">> Epoch 68 finished \tANN training loss 0.010224\n",
      ">> Epoch 69 finished \tANN training loss 0.009526\n",
      ">> Epoch 70 finished \tANN training loss 0.009065\n",
      ">> Epoch 71 finished \tANN training loss 0.008932\n",
      ">> Epoch 72 finished \tANN training loss 0.008509\n",
      ">> Epoch 73 finished \tANN training loss 0.008653\n",
      ">> Epoch 74 finished \tANN training loss 0.008121\n",
      ">> Epoch 75 finished \tANN training loss 0.008296\n",
      ">> Epoch 76 finished \tANN training loss 0.009232\n",
      ">> Epoch 77 finished \tANN training loss 0.008287\n",
      ">> Epoch 78 finished \tANN training loss 0.007904\n",
      ">> Epoch 79 finished \tANN training loss 0.007892\n",
      ">> Epoch 80 finished \tANN training loss 0.007847\n",
      ">> Epoch 81 finished \tANN training loss 0.007044\n",
      ">> Epoch 82 finished \tANN training loss 0.007070\n",
      ">> Epoch 83 finished \tANN training loss 0.007527\n",
      ">> Epoch 84 finished \tANN training loss 0.006631\n",
      ">> Epoch 85 finished \tANN training loss 0.006455\n",
      ">> Epoch 86 finished \tANN training loss 0.006417\n",
      ">> Epoch 87 finished \tANN training loss 0.006191\n",
      ">> Epoch 88 finished \tANN training loss 0.006217\n",
      ">> Epoch 89 finished \tANN training loss 0.005586\n",
      ">> Epoch 90 finished \tANN training loss 0.005784\n",
      ">> Epoch 91 finished \tANN training loss 0.006124\n",
      ">> Epoch 92 finished \tANN training loss 0.005275\n",
      ">> Epoch 93 finished \tANN training loss 0.005089\n",
      ">> Epoch 94 finished \tANN training loss 0.005158\n",
      ">> Epoch 95 finished \tANN training loss 0.005294\n",
      ">> Epoch 96 finished \tANN training loss 0.005072\n",
      ">> Epoch 97 finished \tANN training loss 0.004727\n",
      ">> Epoch 98 finished \tANN training loss 0.004769\n",
      ">> Epoch 99 finished \tANN training loss 0.004815\n",
      ">> Epoch 100 finished \tANN training loss 0.004321\n",
      ">> Epoch 101 finished \tANN training loss 0.004229\n",
      ">> Epoch 102 finished \tANN training loss 0.004163\n",
      ">> Epoch 103 finished \tANN training loss 0.003947\n",
      ">> Epoch 104 finished \tANN training loss 0.004345\n",
      ">> Epoch 105 finished \tANN training loss 0.004266\n",
      ">> Epoch 106 finished \tANN training loss 0.004294\n",
      ">> Epoch 107 finished \tANN training loss 0.004411\n",
      ">> Epoch 108 finished \tANN training loss 0.004287\n",
      ">> Epoch 109 finished \tANN training loss 0.004515\n",
      ">> Epoch 110 finished \tANN training loss 0.003764\n",
      ">> Epoch 111 finished \tANN training loss 0.003756\n",
      ">> Epoch 112 finished \tANN training loss 0.003706\n",
      ">> Epoch 113 finished \tANN training loss 0.003733\n",
      ">> Epoch 114 finished \tANN training loss 0.003600\n",
      ">> Epoch 115 finished \tANN training loss 0.003618\n",
      ">> Epoch 116 finished \tANN training loss 0.003612\n",
      ">> Epoch 117 finished \tANN training loss 0.003341\n",
      ">> Epoch 118 finished \tANN training loss 0.003087\n",
      ">> Epoch 119 finished \tANN training loss 0.002835\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 120 finished \tANN training loss 0.002890\n",
      ">> Epoch 121 finished \tANN training loss 0.002846\n",
      ">> Epoch 122 finished \tANN training loss 0.002828\n",
      ">> Epoch 123 finished \tANN training loss 0.002786\n",
      ">> Epoch 124 finished \tANN training loss 0.002700\n",
      ">> Epoch 125 finished \tANN training loss 0.002664\n",
      ">> Epoch 126 finished \tANN training loss 0.002936\n",
      ">> Epoch 127 finished \tANN training loss 0.002664\n",
      ">> Epoch 128 finished \tANN training loss 0.002792\n",
      ">> Epoch 129 finished \tANN training loss 0.002532\n",
      ">> Epoch 130 finished \tANN training loss 0.002754\n",
      ">> Epoch 131 finished \tANN training loss 0.002500\n",
      ">> Epoch 132 finished \tANN training loss 0.002630\n",
      ">> Epoch 133 finished \tANN training loss 0.002571\n",
      ">> Epoch 134 finished \tANN training loss 0.002294\n",
      ">> Epoch 135 finished \tANN training loss 0.002241\n",
      ">> Epoch 136 finished \tANN training loss 0.002210\n",
      ">> Epoch 137 finished \tANN training loss 0.002151\n",
      ">> Epoch 138 finished \tANN training loss 0.002121\n",
      ">> Epoch 139 finished \tANN training loss 0.002192\n",
      ">> Epoch 140 finished \tANN training loss 0.002248\n",
      ">> Epoch 141 finished \tANN training loss 0.002020\n",
      ">> Epoch 142 finished \tANN training loss 0.002027\n",
      ">> Epoch 143 finished \tANN training loss 0.002152\n",
      ">> Epoch 144 finished \tANN training loss 0.002011\n",
      ">> Epoch 145 finished \tANN training loss 0.002446\n",
      ">> Epoch 146 finished \tANN training loss 0.002088\n",
      ">> Epoch 147 finished \tANN training loss 0.002051\n",
      ">> Epoch 148 finished \tANN training loss 0.002013\n",
      ">> Epoch 149 finished \tANN training loss 0.002015\n",
      ">> Epoch 150 finished \tANN training loss 0.001877\n",
      ">> Epoch 151 finished \tANN training loss 0.002002\n",
      ">> Epoch 152 finished \tANN training loss 0.001985\n",
      ">> Epoch 153 finished \tANN training loss 0.002217\n",
      ">> Epoch 154 finished \tANN training loss 0.001890\n",
      ">> Epoch 155 finished \tANN training loss 0.001975\n",
      ">> Epoch 156 finished \tANN training loss 0.001878\n",
      ">> Epoch 157 finished \tANN training loss 0.001709\n",
      ">> Epoch 158 finished \tANN training loss 0.001701\n",
      ">> Epoch 159 finished \tANN training loss 0.001623\n",
      ">> Epoch 160 finished \tANN training loss 0.001566\n",
      ">> Epoch 161 finished \tANN training loss 0.001873\n",
      ">> Epoch 162 finished \tANN training loss 0.001759\n",
      ">> Epoch 163 finished \tANN training loss 0.001612\n",
      ">> Epoch 164 finished \tANN training loss 0.001554\n",
      ">> Epoch 165 finished \tANN training loss 0.001533\n",
      ">> Epoch 166 finished \tANN training loss 0.001443\n",
      ">> Epoch 167 finished \tANN training loss 0.001404\n",
      ">> Epoch 168 finished \tANN training loss 0.001473\n",
      ">> Epoch 169 finished \tANN training loss 0.001419\n",
      ">> Epoch 170 finished \tANN training loss 0.001429\n",
      ">> Epoch 171 finished \tANN training loss 0.001300\n",
      ">> Epoch 172 finished \tANN training loss 0.001283\n",
      ">> Epoch 173 finished \tANN training loss 0.001290\n",
      ">> Epoch 174 finished \tANN training loss 0.001271\n",
      ">> Epoch 175 finished \tANN training loss 0.001236\n",
      ">> Epoch 176 finished \tANN training loss 0.001290\n",
      ">> Epoch 177 finished \tANN training loss 0.001356\n",
      ">> Epoch 178 finished \tANN training loss 0.001264\n",
      ">> Epoch 179 finished \tANN training loss 0.001360\n",
      ">> Epoch 180 finished \tANN training loss 0.001280\n",
      ">> Epoch 181 finished \tANN training loss 0.001267\n",
      ">> Epoch 182 finished \tANN training loss 0.001251\n",
      ">> Epoch 183 finished \tANN training loss 0.001207\n",
      ">> Epoch 184 finished \tANN training loss 0.001175\n",
      ">> Epoch 185 finished \tANN training loss 0.001172\n",
      ">> Epoch 186 finished \tANN training loss 0.001127\n",
      ">> Epoch 187 finished \tANN training loss 0.001076\n",
      ">> Epoch 188 finished \tANN training loss 0.001185\n",
      ">> Epoch 189 finished \tANN training loss 0.001050\n",
      ">> Epoch 190 finished \tANN training loss 0.001120\n",
      ">> Epoch 191 finished \tANN training loss 0.001193\n",
      ">> Epoch 192 finished \tANN training loss 0.001123\n",
      ">> Epoch 193 finished \tANN training loss 0.001136\n",
      ">> Epoch 194 finished \tANN training loss 0.001106\n",
      ">> Epoch 195 finished \tANN training loss 0.001060\n",
      ">> Epoch 196 finished \tANN training loss 0.001148\n",
      ">> Epoch 197 finished \tANN training loss 0.000971\n",
      ">> Epoch 198 finished \tANN training loss 0.000978\n",
      ">> Epoch 199 finished \tANN training loss 0.000997\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.890000\n"
     ]
    }
   ],
   "source": [
    "# ReLu, dropout = 0.4\n",
    "acc4 = deep_belief_net(hidden_layers_structure=[300], \n",
    "                       learning_rate_rbm=0.05,\n",
    "                       learning_rate=0.1,\n",
    "                       n_epochs_rbm=40,\n",
    "                       n_iter_backprop=200,\n",
    "                       batch_size=32,\n",
    "                       activation_function='relu',\n",
    "                       dropout_p=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.89\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(acc4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 61.709766\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 54.472504\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 75.873703\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 41.272430\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 43.754398\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 42.647362\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 40.159855\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 60.702248\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 37.104389\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 36.450516\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 50.309250\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 32.404320\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 40.761646\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 39.678955\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 43.482391\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 48.358349\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 41.619835\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 28.050406\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 44.146065\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 64.320587\n",
      ">> Epoch 21 finished \tRBM Reconstruction error 34.633278\n",
      ">> Epoch 22 finished \tRBM Reconstruction error 50.839394\n",
      ">> Epoch 23 finished \tRBM Reconstruction error 50.193310\n",
      ">> Epoch 24 finished \tRBM Reconstruction error 50.998871\n",
      ">> Epoch 25 finished \tRBM Reconstruction error 46.640747\n",
      ">> Epoch 26 finished \tRBM Reconstruction error 43.082211\n",
      ">> Epoch 27 finished \tRBM Reconstruction error 48.230694\n",
      ">> Epoch 28 finished \tRBM Reconstruction error 66.244759\n",
      ">> Epoch 29 finished \tRBM Reconstruction error 50.090733\n",
      ">> Epoch 30 finished \tRBM Reconstruction error 43.973370\n",
      ">> Epoch 31 finished \tRBM Reconstruction error 42.613564\n",
      ">> Epoch 32 finished \tRBM Reconstruction error 47.859264\n",
      ">> Epoch 33 finished \tRBM Reconstruction error 52.287685\n",
      ">> Epoch 34 finished \tRBM Reconstruction error 51.919575\n",
      ">> Epoch 35 finished \tRBM Reconstruction error 54.550453\n",
      ">> Epoch 36 finished \tRBM Reconstruction error 50.095306\n",
      ">> Epoch 37 finished \tRBM Reconstruction error 53.549805\n",
      ">> Epoch 38 finished \tRBM Reconstruction error 46.231861\n",
      ">> Epoch 39 finished \tRBM Reconstruction error 56.310070\n",
      ">> Epoch 40 finished \tRBM Reconstruction error 48.051514\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.836896\n",
      ">> Epoch 1 finished \tANN training loss 0.605394\n",
      ">> Epoch 2 finished \tANN training loss 0.482195\n",
      ">> Epoch 3 finished \tANN training loss 0.447327\n",
      ">> Epoch 4 finished \tANN training loss 0.407878\n",
      ">> Epoch 5 finished \tANN training loss 0.364694\n",
      ">> Epoch 6 finished \tANN training loss 0.331751\n",
      ">> Epoch 7 finished \tANN training loss 0.307161\n",
      ">> Epoch 8 finished \tANN training loss 0.286104\n",
      ">> Epoch 9 finished \tANN training loss 0.264597\n",
      ">> Epoch 10 finished \tANN training loss 0.254600\n",
      ">> Epoch 11 finished \tANN training loss 0.236196\n",
      ">> Epoch 12 finished \tANN training loss 0.222835\n",
      ">> Epoch 13 finished \tANN training loss 0.217063\n",
      ">> Epoch 14 finished \tANN training loss 0.194284\n",
      ">> Epoch 15 finished \tANN training loss 0.184488\n",
      ">> Epoch 16 finished \tANN training loss 0.180249\n",
      ">> Epoch 17 finished \tANN training loss 0.166425\n",
      ">> Epoch 18 finished \tANN training loss 0.163207\n",
      ">> Epoch 19 finished \tANN training loss 0.154860\n",
      ">> Epoch 20 finished \tANN training loss 0.144113\n",
      ">> Epoch 21 finished \tANN training loss 0.137016\n",
      ">> Epoch 22 finished \tANN training loss 0.130890\n",
      ">> Epoch 23 finished \tANN training loss 0.125865\n",
      ">> Epoch 24 finished \tANN training loss 0.125159\n",
      ">> Epoch 25 finished \tANN training loss 0.116664\n",
      ">> Epoch 26 finished \tANN training loss 0.114315\n",
      ">> Epoch 27 finished \tANN training loss 0.108297\n",
      ">> Epoch 28 finished \tANN training loss 0.106967\n",
      ">> Epoch 29 finished \tANN training loss 0.096678\n",
      ">> Epoch 30 finished \tANN training loss 0.089675\n",
      ">> Epoch 31 finished \tANN training loss 0.088421\n",
      ">> Epoch 32 finished \tANN training loss 0.082038\n",
      ">> Epoch 33 finished \tANN training loss 0.075160\n",
      ">> Epoch 34 finished \tANN training loss 0.075184\n",
      ">> Epoch 35 finished \tANN training loss 0.072693\n",
      ">> Epoch 36 finished \tANN training loss 0.073774\n",
      ">> Epoch 37 finished \tANN training loss 0.064967\n",
      ">> Epoch 38 finished \tANN training loss 0.060920\n",
      ">> Epoch 39 finished \tANN training loss 0.067424\n",
      ">> Epoch 40 finished \tANN training loss 0.059324\n",
      ">> Epoch 41 finished \tANN training loss 0.056265\n",
      ">> Epoch 42 finished \tANN training loss 0.051049\n",
      ">> Epoch 43 finished \tANN training loss 0.051929\n",
      ">> Epoch 44 finished \tANN training loss 0.053568\n",
      ">> Epoch 45 finished \tANN training loss 0.049829\n",
      ">> Epoch 46 finished \tANN training loss 0.048672\n",
      ">> Epoch 47 finished \tANN training loss 0.044799\n",
      ">> Epoch 48 finished \tANN training loss 0.043688\n",
      ">> Epoch 49 finished \tANN training loss 0.043264\n",
      ">> Epoch 50 finished \tANN training loss 0.039973\n",
      ">> Epoch 51 finished \tANN training loss 0.041697\n",
      ">> Epoch 52 finished \tANN training loss 0.039316\n",
      ">> Epoch 53 finished \tANN training loss 0.037126\n",
      ">> Epoch 54 finished \tANN training loss 0.037368\n",
      ">> Epoch 55 finished \tANN training loss 0.037381\n",
      ">> Epoch 56 finished \tANN training loss 0.036043\n",
      ">> Epoch 57 finished \tANN training loss 0.034141\n",
      ">> Epoch 58 finished \tANN training loss 0.033120\n",
      ">> Epoch 59 finished \tANN training loss 0.034545\n",
      ">> Epoch 60 finished \tANN training loss 0.031147\n",
      ">> Epoch 61 finished \tANN training loss 0.030991\n",
      ">> Epoch 62 finished \tANN training loss 0.030868\n",
      ">> Epoch 63 finished \tANN training loss 0.030967\n",
      ">> Epoch 64 finished \tANN training loss 0.029502\n",
      ">> Epoch 65 finished \tANN training loss 0.028446\n",
      ">> Epoch 66 finished \tANN training loss 0.027263\n",
      ">> Epoch 67 finished \tANN training loss 0.027058\n",
      ">> Epoch 68 finished \tANN training loss 0.026121\n",
      ">> Epoch 69 finished \tANN training loss 0.026443\n",
      ">> Epoch 70 finished \tANN training loss 0.022847\n",
      ">> Epoch 71 finished \tANN training loss 0.023266\n",
      ">> Epoch 72 finished \tANN training loss 0.021971\n",
      ">> Epoch 73 finished \tANN training loss 0.021610\n",
      ">> Epoch 74 finished \tANN training loss 0.021076\n",
      ">> Epoch 75 finished \tANN training loss 0.020608\n",
      ">> Epoch 76 finished \tANN training loss 0.019689\n",
      ">> Epoch 77 finished \tANN training loss 0.020859\n",
      ">> Epoch 78 finished \tANN training loss 0.019367\n",
      ">> Epoch 79 finished \tANN training loss 0.017330\n",
      ">> Epoch 80 finished \tANN training loss 0.015969\n",
      ">> Epoch 81 finished \tANN training loss 0.016560\n",
      ">> Epoch 82 finished \tANN training loss 0.016135\n",
      ">> Epoch 83 finished \tANN training loss 0.015515\n",
      ">> Epoch 84 finished \tANN training loss 0.018853\n",
      ">> Epoch 85 finished \tANN training loss 0.015582\n",
      ">> Epoch 86 finished \tANN training loss 0.015680\n",
      ">> Epoch 87 finished \tANN training loss 0.015350\n",
      ">> Epoch 88 finished \tANN training loss 0.014875\n",
      ">> Epoch 89 finished \tANN training loss 0.013924\n",
      ">> Epoch 90 finished \tANN training loss 0.014588\n",
      ">> Epoch 91 finished \tANN training loss 0.014222\n",
      ">> Epoch 92 finished \tANN training loss 0.012982\n",
      ">> Epoch 93 finished \tANN training loss 0.012732\n",
      ">> Epoch 94 finished \tANN training loss 0.012364\n",
      ">> Epoch 95 finished \tANN training loss 0.013103\n",
      ">> Epoch 96 finished \tANN training loss 0.012489\n",
      ">> Epoch 97 finished \tANN training loss 0.012275\n",
      ">> Epoch 98 finished \tANN training loss 0.011450\n",
      ">> Epoch 99 finished \tANN training loss 0.011205\n",
      ">> Epoch 100 finished \tANN training loss 0.010934\n",
      ">> Epoch 101 finished \tANN training loss 0.011645\n",
      ">> Epoch 102 finished \tANN training loss 0.010772\n",
      ">> Epoch 103 finished \tANN training loss 0.011830\n",
      ">> Epoch 104 finished \tANN training loss 0.011680\n",
      ">> Epoch 105 finished \tANN training loss 0.010858\n",
      ">> Epoch 106 finished \tANN training loss 0.009570\n",
      ">> Epoch 107 finished \tANN training loss 0.009713\n",
      ">> Epoch 108 finished \tANN training loss 0.009374\n",
      ">> Epoch 109 finished \tANN training loss 0.009717\n",
      ">> Epoch 110 finished \tANN training loss 0.009514\n",
      ">> Epoch 111 finished \tANN training loss 0.009393\n",
      ">> Epoch 112 finished \tANN training loss 0.009223\n",
      ">> Epoch 113 finished \tANN training loss 0.008878\n",
      ">> Epoch 114 finished \tANN training loss 0.009303\n",
      ">> Epoch 115 finished \tANN training loss 0.009527\n",
      ">> Epoch 116 finished \tANN training loss 0.009009\n",
      ">> Epoch 117 finished \tANN training loss 0.008854\n",
      ">> Epoch 118 finished \tANN training loss 0.009319\n",
      ">> Epoch 119 finished \tANN training loss 0.008620\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 120 finished \tANN training loss 0.008130\n",
      ">> Epoch 121 finished \tANN training loss 0.008190\n",
      ">> Epoch 122 finished \tANN training loss 0.007763\n",
      ">> Epoch 123 finished \tANN training loss 0.007867\n",
      ">> Epoch 124 finished \tANN training loss 0.009135\n",
      ">> Epoch 125 finished \tANN training loss 0.007060\n",
      ">> Epoch 126 finished \tANN training loss 0.007608\n",
      ">> Epoch 127 finished \tANN training loss 0.007474\n",
      ">> Epoch 128 finished \tANN training loss 0.007104\n",
      ">> Epoch 129 finished \tANN training loss 0.007016\n",
      ">> Epoch 130 finished \tANN training loss 0.007401\n",
      ">> Epoch 131 finished \tANN training loss 0.006690\n",
      ">> Epoch 132 finished \tANN training loss 0.007211\n",
      ">> Epoch 133 finished \tANN training loss 0.006723\n",
      ">> Epoch 134 finished \tANN training loss 0.006278\n",
      ">> Epoch 135 finished \tANN training loss 0.006473\n",
      ">> Epoch 136 finished \tANN training loss 0.006607\n",
      ">> Epoch 137 finished \tANN training loss 0.006358\n",
      ">> Epoch 138 finished \tANN training loss 0.006463\n",
      ">> Epoch 139 finished \tANN training loss 0.006845\n",
      ">> Epoch 140 finished \tANN training loss 0.007025\n",
      ">> Epoch 141 finished \tANN training loss 0.006666\n",
      ">> Epoch 142 finished \tANN training loss 0.006112\n",
      ">> Epoch 143 finished \tANN training loss 0.005562\n",
      ">> Epoch 144 finished \tANN training loss 0.006110\n",
      ">> Epoch 145 finished \tANN training loss 0.007540\n",
      ">> Epoch 146 finished \tANN training loss 0.005666\n",
      ">> Epoch 147 finished \tANN training loss 0.006056\n",
      ">> Epoch 148 finished \tANN training loss 0.005590\n",
      ">> Epoch 149 finished \tANN training loss 0.005782\n",
      ">> Epoch 150 finished \tANN training loss 0.005757\n",
      ">> Epoch 151 finished \tANN training loss 0.005601\n",
      ">> Epoch 152 finished \tANN training loss 0.005293\n",
      ">> Epoch 153 finished \tANN training loss 0.005452\n",
      ">> Epoch 154 finished \tANN training loss 0.004550\n",
      ">> Epoch 155 finished \tANN training loss 0.004464\n",
      ">> Epoch 156 finished \tANN training loss 0.004388\n",
      ">> Epoch 157 finished \tANN training loss 0.005063\n",
      ">> Epoch 158 finished \tANN training loss 0.004621\n",
      ">> Epoch 159 finished \tANN training loss 0.004457\n",
      ">> Epoch 160 finished \tANN training loss 0.004434\n",
      ">> Epoch 161 finished \tANN training loss 0.004422\n",
      ">> Epoch 162 finished \tANN training loss 0.004695\n",
      ">> Epoch 163 finished \tANN training loss 0.004668\n",
      ">> Epoch 164 finished \tANN training loss 0.004531\n",
      ">> Epoch 165 finished \tANN training loss 0.004790\n",
      ">> Epoch 166 finished \tANN training loss 0.004258\n",
      ">> Epoch 167 finished \tANN training loss 0.004111\n",
      ">> Epoch 168 finished \tANN training loss 0.004585\n",
      ">> Epoch 169 finished \tANN training loss 0.003985\n",
      ">> Epoch 170 finished \tANN training loss 0.004056\n",
      ">> Epoch 171 finished \tANN training loss 0.004231\n",
      ">> Epoch 172 finished \tANN training loss 0.004339\n",
      ">> Epoch 173 finished \tANN training loss 0.005517\n",
      ">> Epoch 174 finished \tANN training loss 0.004709\n",
      ">> Epoch 175 finished \tANN training loss 0.004159\n",
      ">> Epoch 176 finished \tANN training loss 0.004013\n",
      ">> Epoch 177 finished \tANN training loss 0.003818\n",
      ">> Epoch 178 finished \tANN training loss 0.003683\n",
      ">> Epoch 179 finished \tANN training loss 0.003377\n",
      ">> Epoch 180 finished \tANN training loss 0.003048\n",
      ">> Epoch 181 finished \tANN training loss 0.003115\n",
      ">> Epoch 182 finished \tANN training loss 0.003113\n",
      ">> Epoch 183 finished \tANN training loss 0.003275\n",
      ">> Epoch 184 finished \tANN training loss 0.003483\n",
      ">> Epoch 185 finished \tANN training loss 0.003450\n",
      ">> Epoch 186 finished \tANN training loss 0.003731\n",
      ">> Epoch 187 finished \tANN training loss 0.003489\n",
      ">> Epoch 188 finished \tANN training loss 0.003478\n",
      ">> Epoch 189 finished \tANN training loss 0.003374\n",
      ">> Epoch 190 finished \tANN training loss 0.003038\n",
      ">> Epoch 191 finished \tANN training loss 0.003253\n",
      ">> Epoch 192 finished \tANN training loss 0.002931\n",
      ">> Epoch 193 finished \tANN training loss 0.002856\n",
      ">> Epoch 194 finished \tANN training loss 0.002921\n",
      ">> Epoch 195 finished \tANN training loss 0.002922\n",
      ">> Epoch 196 finished \tANN training loss 0.003118\n",
      ">> Epoch 197 finished \tANN training loss 0.003332\n",
      ">> Epoch 198 finished \tANN training loss 0.003246\n",
      ">> Epoch 199 finished \tANN training loss 0.002912\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.890000\n"
     ]
    }
   ],
   "source": [
    "# ReLu, dropout = 0.5\n",
    "acc5 = deep_belief_net(hidden_layers_structure=[300], \n",
    "                       learning_rate_rbm=0.05,\n",
    "                       learning_rate=0.1,\n",
    "                       n_epochs_rbm=40,\n",
    "                       n_iter_backprop=200,\n",
    "                       batch_size=32,\n",
    "                       activation_function='relu',\n",
    "                       dropout_p=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.89\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(acc5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 61.378994\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 38.144863\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 48.835243\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 43.290958\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 44.144161\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 42.889774\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 33.065033\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 24.430702\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 32.138363\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 45.784576\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 32.016129\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 25.012192\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 32.610817\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 54.927158\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 47.476357\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 28.648687\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 27.258995\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 28.335403\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 43.332287\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 34.647114\n",
      ">> Epoch 21 finished \tRBM Reconstruction error 33.540207\n",
      ">> Epoch 22 finished \tRBM Reconstruction error 43.665752\n",
      ">> Epoch 23 finished \tRBM Reconstruction error 35.126572\n",
      ">> Epoch 24 finished \tRBM Reconstruction error 33.687454\n",
      ">> Epoch 25 finished \tRBM Reconstruction error 26.624083\n",
      ">> Epoch 26 finished \tRBM Reconstruction error 35.344112\n",
      ">> Epoch 27 finished \tRBM Reconstruction error 35.424717\n",
      ">> Epoch 28 finished \tRBM Reconstruction error 23.503057\n",
      ">> Epoch 29 finished \tRBM Reconstruction error 28.569208\n",
      ">> Epoch 30 finished \tRBM Reconstruction error 29.112249\n",
      ">> Epoch 31 finished \tRBM Reconstruction error 38.162247\n",
      ">> Epoch 32 finished \tRBM Reconstruction error 39.954365\n",
      ">> Epoch 33 finished \tRBM Reconstruction error 34.830547\n",
      ">> Epoch 34 finished \tRBM Reconstruction error 18.477585\n",
      ">> Epoch 35 finished \tRBM Reconstruction error 26.755713\n",
      ">> Epoch 36 finished \tRBM Reconstruction error 37.530590\n",
      ">> Epoch 37 finished \tRBM Reconstruction error 39.838272\n",
      ">> Epoch 38 finished \tRBM Reconstruction error 48.593719\n",
      ">> Epoch 39 finished \tRBM Reconstruction error 33.874222\n",
      ">> Epoch 40 finished \tRBM Reconstruction error 39.812855\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.859841\n",
      ">> Epoch 1 finished \tANN training loss 0.614823\n",
      ">> Epoch 2 finished \tANN training loss 0.536251\n",
      ">> Epoch 3 finished \tANN training loss 0.453935\n",
      ">> Epoch 4 finished \tANN training loss 0.411489\n",
      ">> Epoch 5 finished \tANN training loss 0.381506\n",
      ">> Epoch 6 finished \tANN training loss 0.354274\n",
      ">> Epoch 7 finished \tANN training loss 0.341939\n",
      ">> Epoch 8 finished \tANN training loss 0.326547\n",
      ">> Epoch 9 finished \tANN training loss 0.309168\n",
      ">> Epoch 10 finished \tANN training loss 0.271252\n",
      ">> Epoch 11 finished \tANN training loss 0.266615\n",
      ">> Epoch 12 finished \tANN training loss 0.263812\n",
      ">> Epoch 13 finished \tANN training loss 0.231092\n",
      ">> Epoch 14 finished \tANN training loss 0.236166\n",
      ">> Epoch 15 finished \tANN training loss 0.221801\n",
      ">> Epoch 16 finished \tANN training loss 0.206456\n",
      ">> Epoch 17 finished \tANN training loss 0.193446\n",
      ">> Epoch 18 finished \tANN training loss 0.186529\n",
      ">> Epoch 19 finished \tANN training loss 0.178988\n",
      ">> Epoch 20 finished \tANN training loss 0.170951\n",
      ">> Epoch 21 finished \tANN training loss 0.167949\n",
      ">> Epoch 22 finished \tANN training loss 0.162326\n",
      ">> Epoch 23 finished \tANN training loss 0.150751\n",
      ">> Epoch 24 finished \tANN training loss 0.149804\n",
      ">> Epoch 25 finished \tANN training loss 0.142745\n",
      ">> Epoch 26 finished \tANN training loss 0.138540\n",
      ">> Epoch 27 finished \tANN training loss 0.133972\n",
      ">> Epoch 28 finished \tANN training loss 0.130696\n",
      ">> Epoch 29 finished \tANN training loss 0.123060\n",
      ">> Epoch 30 finished \tANN training loss 0.120239\n",
      ">> Epoch 31 finished \tANN training loss 0.117836\n",
      ">> Epoch 32 finished \tANN training loss 0.114693\n",
      ">> Epoch 33 finished \tANN training loss 0.110741\n",
      ">> Epoch 34 finished \tANN training loss 0.100533\n",
      ">> Epoch 35 finished \tANN training loss 0.093474\n",
      ">> Epoch 36 finished \tANN training loss 0.095851\n",
      ">> Epoch 37 finished \tANN training loss 0.091107\n",
      ">> Epoch 38 finished \tANN training loss 0.089250\n",
      ">> Epoch 39 finished \tANN training loss 0.086267\n",
      ">> Epoch 40 finished \tANN training loss 0.084422\n",
      ">> Epoch 41 finished \tANN training loss 0.081175\n",
      ">> Epoch 42 finished \tANN training loss 0.080481\n",
      ">> Epoch 43 finished \tANN training loss 0.079708\n",
      ">> Epoch 44 finished \tANN training loss 0.078181\n",
      ">> Epoch 45 finished \tANN training loss 0.070690\n",
      ">> Epoch 46 finished \tANN training loss 0.068087\n",
      ">> Epoch 47 finished \tANN training loss 0.068190\n",
      ">> Epoch 48 finished \tANN training loss 0.067907\n",
      ">> Epoch 49 finished \tANN training loss 0.062348\n",
      ">> Epoch 50 finished \tANN training loss 0.060985\n",
      ">> Epoch 51 finished \tANN training loss 0.059478\n",
      ">> Epoch 52 finished \tANN training loss 0.061557\n",
      ">> Epoch 53 finished \tANN training loss 0.062623\n",
      ">> Epoch 54 finished \tANN training loss 0.056796\n",
      ">> Epoch 55 finished \tANN training loss 0.064966\n",
      ">> Epoch 56 finished \tANN training loss 0.058383\n",
      ">> Epoch 57 finished \tANN training loss 0.058235\n",
      ">> Epoch 58 finished \tANN training loss 0.052088\n",
      ">> Epoch 59 finished \tANN training loss 0.052234\n",
      ">> Epoch 60 finished \tANN training loss 0.050665\n",
      ">> Epoch 61 finished \tANN training loss 0.048629\n",
      ">> Epoch 62 finished \tANN training loss 0.045685\n",
      ">> Epoch 63 finished \tANN training loss 0.046149\n",
      ">> Epoch 64 finished \tANN training loss 0.045293\n",
      ">> Epoch 65 finished \tANN training loss 0.046871\n",
      ">> Epoch 66 finished \tANN training loss 0.047676\n",
      ">> Epoch 67 finished \tANN training loss 0.048357\n",
      ">> Epoch 68 finished \tANN training loss 0.040732\n",
      ">> Epoch 69 finished \tANN training loss 0.041063\n",
      ">> Epoch 70 finished \tANN training loss 0.040346\n",
      ">> Epoch 71 finished \tANN training loss 0.039275\n",
      ">> Epoch 72 finished \tANN training loss 0.037873\n",
      ">> Epoch 73 finished \tANN training loss 0.037714\n",
      ">> Epoch 74 finished \tANN training loss 0.036731\n",
      ">> Epoch 75 finished \tANN training loss 0.036229\n",
      ">> Epoch 76 finished \tANN training loss 0.035779\n",
      ">> Epoch 77 finished \tANN training loss 0.037705\n",
      ">> Epoch 78 finished \tANN training loss 0.034248\n",
      ">> Epoch 79 finished \tANN training loss 0.031439\n",
      ">> Epoch 80 finished \tANN training loss 0.033669\n",
      ">> Epoch 81 finished \tANN training loss 0.030233\n",
      ">> Epoch 82 finished \tANN training loss 0.029235\n",
      ">> Epoch 83 finished \tANN training loss 0.031905\n",
      ">> Epoch 84 finished \tANN training loss 0.032913\n",
      ">> Epoch 85 finished \tANN training loss 0.031066\n",
      ">> Epoch 86 finished \tANN training loss 0.030548\n",
      ">> Epoch 87 finished \tANN training loss 0.028794\n",
      ">> Epoch 88 finished \tANN training loss 0.026432\n",
      ">> Epoch 89 finished \tANN training loss 0.028076\n",
      ">> Epoch 90 finished \tANN training loss 0.025435\n",
      ">> Epoch 91 finished \tANN training loss 0.025525\n",
      ">> Epoch 92 finished \tANN training loss 0.024881\n",
      ">> Epoch 93 finished \tANN training loss 0.025738\n",
      ">> Epoch 94 finished \tANN training loss 0.023477\n",
      ">> Epoch 95 finished \tANN training loss 0.029293\n",
      ">> Epoch 96 finished \tANN training loss 0.025131\n",
      ">> Epoch 97 finished \tANN training loss 0.023704\n",
      ">> Epoch 98 finished \tANN training loss 0.021918\n",
      ">> Epoch 99 finished \tANN training loss 0.020476\n",
      ">> Epoch 100 finished \tANN training loss 0.020894\n",
      ">> Epoch 101 finished \tANN training loss 0.022340\n",
      ">> Epoch 102 finished \tANN training loss 0.020515\n",
      ">> Epoch 103 finished \tANN training loss 0.019186\n",
      ">> Epoch 104 finished \tANN training loss 0.019892\n",
      ">> Epoch 105 finished \tANN training loss 0.019944\n",
      ">> Epoch 106 finished \tANN training loss 0.018603\n",
      ">> Epoch 107 finished \tANN training loss 0.018664\n",
      ">> Epoch 108 finished \tANN training loss 0.018768\n",
      ">> Epoch 109 finished \tANN training loss 0.020847\n",
      ">> Epoch 110 finished \tANN training loss 0.018714\n",
      ">> Epoch 111 finished \tANN training loss 0.017809\n",
      ">> Epoch 112 finished \tANN training loss 0.017677\n",
      ">> Epoch 113 finished \tANN training loss 0.016760\n",
      ">> Epoch 114 finished \tANN training loss 0.019247\n",
      ">> Epoch 115 finished \tANN training loss 0.017183\n",
      ">> Epoch 116 finished \tANN training loss 0.016561\n",
      ">> Epoch 117 finished \tANN training loss 0.016859\n",
      ">> Epoch 118 finished \tANN training loss 0.016207\n",
      ">> Epoch 119 finished \tANN training loss 0.016616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 120 finished \tANN training loss 0.016659\n",
      ">> Epoch 121 finished \tANN training loss 0.015730\n",
      ">> Epoch 122 finished \tANN training loss 0.016864\n",
      ">> Epoch 123 finished \tANN training loss 0.016000\n",
      ">> Epoch 124 finished \tANN training loss 0.016408\n",
      ">> Epoch 125 finished \tANN training loss 0.015784\n",
      ">> Epoch 126 finished \tANN training loss 0.015176\n",
      ">> Epoch 127 finished \tANN training loss 0.015397\n",
      ">> Epoch 128 finished \tANN training loss 0.015785\n",
      ">> Epoch 129 finished \tANN training loss 0.015116\n",
      ">> Epoch 130 finished \tANN training loss 0.014889\n",
      ">> Epoch 131 finished \tANN training loss 0.015282\n",
      ">> Epoch 132 finished \tANN training loss 0.014992\n",
      ">> Epoch 133 finished \tANN training loss 0.015322\n",
      ">> Epoch 134 finished \tANN training loss 0.013337\n",
      ">> Epoch 135 finished \tANN training loss 0.012993\n",
      ">> Epoch 136 finished \tANN training loss 0.012806\n",
      ">> Epoch 137 finished \tANN training loss 0.013074\n",
      ">> Epoch 138 finished \tANN training loss 0.012969\n",
      ">> Epoch 139 finished \tANN training loss 0.013080\n",
      ">> Epoch 140 finished \tANN training loss 0.013136\n",
      ">> Epoch 141 finished \tANN training loss 0.011857\n",
      ">> Epoch 142 finished \tANN training loss 0.012001\n",
      ">> Epoch 143 finished \tANN training loss 0.012229\n",
      ">> Epoch 144 finished \tANN training loss 0.012342\n",
      ">> Epoch 145 finished \tANN training loss 0.011292\n",
      ">> Epoch 146 finished \tANN training loss 0.011222\n",
      ">> Epoch 147 finished \tANN training loss 0.011230\n",
      ">> Epoch 148 finished \tANN training loss 0.010981\n",
      ">> Epoch 149 finished \tANN training loss 0.010827\n",
      ">> Epoch 150 finished \tANN training loss 0.012160\n",
      ">> Epoch 151 finished \tANN training loss 0.011082\n",
      ">> Epoch 152 finished \tANN training loss 0.010370\n",
      ">> Epoch 153 finished \tANN training loss 0.011008\n",
      ">> Epoch 154 finished \tANN training loss 0.011031\n",
      ">> Epoch 155 finished \tANN training loss 0.010472\n",
      ">> Epoch 156 finished \tANN training loss 0.011178\n",
      ">> Epoch 157 finished \tANN training loss 0.010868\n",
      ">> Epoch 158 finished \tANN training loss 0.011205\n",
      ">> Epoch 159 finished \tANN training loss 0.013078\n",
      ">> Epoch 160 finished \tANN training loss 0.012192\n",
      ">> Epoch 161 finished \tANN training loss 0.012053\n",
      ">> Epoch 162 finished \tANN training loss 0.013559\n",
      ">> Epoch 163 finished \tANN training loss 0.013747\n",
      ">> Epoch 164 finished \tANN training loss 0.012188\n",
      ">> Epoch 165 finished \tANN training loss 0.010757\n",
      ">> Epoch 166 finished \tANN training loss 0.010679\n",
      ">> Epoch 167 finished \tANN training loss 0.011204\n",
      ">> Epoch 168 finished \tANN training loss 0.010283\n",
      ">> Epoch 169 finished \tANN training loss 0.010288\n",
      ">> Epoch 170 finished \tANN training loss 0.008808\n",
      ">> Epoch 171 finished \tANN training loss 0.010152\n",
      ">> Epoch 172 finished \tANN training loss 0.008897\n",
      ">> Epoch 173 finished \tANN training loss 0.009171\n",
      ">> Epoch 174 finished \tANN training loss 0.010024\n",
      ">> Epoch 175 finished \tANN training loss 0.008512\n",
      ">> Epoch 176 finished \tANN training loss 0.008596\n",
      ">> Epoch 177 finished \tANN training loss 0.008314\n",
      ">> Epoch 178 finished \tANN training loss 0.008407\n",
      ">> Epoch 179 finished \tANN training loss 0.008145\n",
      ">> Epoch 180 finished \tANN training loss 0.008254\n",
      ">> Epoch 181 finished \tANN training loss 0.008751\n",
      ">> Epoch 182 finished \tANN training loss 0.009264\n",
      ">> Epoch 183 finished \tANN training loss 0.008374\n",
      ">> Epoch 184 finished \tANN training loss 0.007908\n",
      ">> Epoch 185 finished \tANN training loss 0.008612\n",
      ">> Epoch 186 finished \tANN training loss 0.008368\n",
      ">> Epoch 187 finished \tANN training loss 0.008049\n",
      ">> Epoch 188 finished \tANN training loss 0.008544\n",
      ">> Epoch 189 finished \tANN training loss 0.008372\n",
      ">> Epoch 190 finished \tANN training loss 0.008228\n",
      ">> Epoch 191 finished \tANN training loss 0.007931\n",
      ">> Epoch 192 finished \tANN training loss 0.007178\n",
      ">> Epoch 193 finished \tANN training loss 0.007475\n",
      ">> Epoch 194 finished \tANN training loss 0.007936\n",
      ">> Epoch 195 finished \tANN training loss 0.007389\n",
      ">> Epoch 196 finished \tANN training loss 0.007859\n",
      ">> Epoch 197 finished \tANN training loss 0.007093\n",
      ">> Epoch 198 finished \tANN training loss 0.007480\n",
      ">> Epoch 199 finished \tANN training loss 0.007590\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.870000\n"
     ]
    }
   ],
   "source": [
    "# ReLu, dropout = 0.6\n",
    "acc6 = deep_belief_net(hidden_layers_structure=[300], \n",
    "                       learning_rate_rbm=0.05,\n",
    "                       learning_rate=0.1,\n",
    "                       n_epochs_rbm=40,\n",
    "                       n_iter_backprop=200,\n",
    "                       batch_size=32,\n",
    "                       activation_function='relu',\n",
    "                       dropout_p=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.87\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(acc6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 56.047256\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 75.743523\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 30.459770\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 47.690372\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 45.758965\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 34.811928\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 34.652184\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 34.783401\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 29.111069\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 31.374155\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 57.646584\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 22.685879\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 38.631672\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 44.106903\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 46.644821\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 33.263466\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 32.393909\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 36.074516\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 37.753258\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 27.625925\n",
      ">> Epoch 21 finished \tRBM Reconstruction error 24.151003\n",
      ">> Epoch 22 finished \tRBM Reconstruction error 46.662666\n",
      ">> Epoch 23 finished \tRBM Reconstruction error 37.606102\n",
      ">> Epoch 24 finished \tRBM Reconstruction error 37.305729\n",
      ">> Epoch 25 finished \tRBM Reconstruction error 30.290499\n",
      ">> Epoch 26 finished \tRBM Reconstruction error 29.300077\n",
      ">> Epoch 27 finished \tRBM Reconstruction error 25.525688\n",
      ">> Epoch 28 finished \tRBM Reconstruction error 38.064743\n",
      ">> Epoch 29 finished \tRBM Reconstruction error 29.105440\n",
      ">> Epoch 30 finished \tRBM Reconstruction error 44.112823\n",
      ">> Epoch 31 finished \tRBM Reconstruction error 30.791983\n",
      ">> Epoch 32 finished \tRBM Reconstruction error 43.724434\n",
      ">> Epoch 33 finished \tRBM Reconstruction error 26.208227\n",
      ">> Epoch 34 finished \tRBM Reconstruction error 34.571922\n",
      ">> Epoch 35 finished \tRBM Reconstruction error 36.998856\n",
      ">> Epoch 36 finished \tRBM Reconstruction error 31.790464\n",
      ">> Epoch 37 finished \tRBM Reconstruction error 23.368891\n",
      ">> Epoch 38 finished \tRBM Reconstruction error 27.893360\n",
      ">> Epoch 39 finished \tRBM Reconstruction error 44.985336\n",
      ">> Epoch 40 finished \tRBM Reconstruction error 54.031116\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 0.973052\n",
      ">> Epoch 1 finished \tANN training loss 0.703370\n",
      ">> Epoch 2 finished \tANN training loss 0.585994\n",
      ">> Epoch 3 finished \tANN training loss 0.559072\n",
      ">> Epoch 4 finished \tANN training loss 0.513076\n",
      ">> Epoch 5 finished \tANN training loss 0.465241\n",
      ">> Epoch 6 finished \tANN training loss 0.460248\n",
      ">> Epoch 7 finished \tANN training loss 0.427421\n",
      ">> Epoch 8 finished \tANN training loss 0.436733\n",
      ">> Epoch 9 finished \tANN training loss 0.408710\n",
      ">> Epoch 10 finished \tANN training loss 0.400848\n",
      ">> Epoch 11 finished \tANN training loss 0.384625\n",
      ">> Epoch 12 finished \tANN training loss 0.369857\n",
      ">> Epoch 13 finished \tANN training loss 0.344506\n",
      ">> Epoch 14 finished \tANN training loss 0.333850\n",
      ">> Epoch 15 finished \tANN training loss 0.316969\n",
      ">> Epoch 16 finished \tANN training loss 0.317043\n",
      ">> Epoch 17 finished \tANN training loss 0.304677\n",
      ">> Epoch 18 finished \tANN training loss 0.287227\n",
      ">> Epoch 19 finished \tANN training loss 0.296242\n",
      ">> Epoch 20 finished \tANN training loss 0.281737\n",
      ">> Epoch 21 finished \tANN training loss 0.299196\n",
      ">> Epoch 22 finished \tANN training loss 0.280769\n",
      ">> Epoch 23 finished \tANN training loss 0.270248\n",
      ">> Epoch 24 finished \tANN training loss 0.258146\n",
      ">> Epoch 25 finished \tANN training loss 0.268486\n",
      ">> Epoch 26 finished \tANN training loss 0.255776\n",
      ">> Epoch 27 finished \tANN training loss 0.244936\n",
      ">> Epoch 28 finished \tANN training loss 0.252485\n",
      ">> Epoch 29 finished \tANN training loss 0.220985\n",
      ">> Epoch 30 finished \tANN training loss 0.229774\n",
      ">> Epoch 31 finished \tANN training loss 0.211965\n",
      ">> Epoch 32 finished \tANN training loss 0.218526\n",
      ">> Epoch 33 finished \tANN training loss 0.218491\n",
      ">> Epoch 34 finished \tANN training loss 0.205666\n",
      ">> Epoch 35 finished \tANN training loss 0.194410\n",
      ">> Epoch 36 finished \tANN training loss 0.194929\n",
      ">> Epoch 37 finished \tANN training loss 0.187216\n",
      ">> Epoch 38 finished \tANN training loss 0.181864\n",
      ">> Epoch 39 finished \tANN training loss 0.186409\n",
      ">> Epoch 40 finished \tANN training loss 0.183393\n",
      ">> Epoch 41 finished \tANN training loss 0.193113\n",
      ">> Epoch 42 finished \tANN training loss 0.183741\n",
      ">> Epoch 43 finished \tANN training loss 0.174380\n",
      ">> Epoch 44 finished \tANN training loss 0.174699\n",
      ">> Epoch 45 finished \tANN training loss 0.177344\n",
      ">> Epoch 46 finished \tANN training loss 0.177890\n",
      ">> Epoch 47 finished \tANN training loss 0.180121\n",
      ">> Epoch 48 finished \tANN training loss 0.178715\n",
      ">> Epoch 49 finished \tANN training loss 0.171130\n",
      ">> Epoch 50 finished \tANN training loss 0.171832\n",
      ">> Epoch 51 finished \tANN training loss 0.168926\n",
      ">> Epoch 52 finished \tANN training loss 0.171720\n",
      ">> Epoch 53 finished \tANN training loss 0.168353\n",
      ">> Epoch 54 finished \tANN training loss 0.156128\n",
      ">> Epoch 55 finished \tANN training loss 0.155456\n",
      ">> Epoch 56 finished \tANN training loss 0.150065\n",
      ">> Epoch 57 finished \tANN training loss 0.150066\n",
      ">> Epoch 58 finished \tANN training loss 0.147124\n",
      ">> Epoch 59 finished \tANN training loss 0.150208\n",
      ">> Epoch 60 finished \tANN training loss 0.144634\n",
      ">> Epoch 61 finished \tANN training loss 0.147727\n",
      ">> Epoch 62 finished \tANN training loss 0.146366\n",
      ">> Epoch 63 finished \tANN training loss 0.148834\n",
      ">> Epoch 64 finished \tANN training loss 0.156989\n",
      ">> Epoch 65 finished \tANN training loss 0.150146\n",
      ">> Epoch 66 finished \tANN training loss 0.139876\n",
      ">> Epoch 67 finished \tANN training loss 0.139270\n",
      ">> Epoch 68 finished \tANN training loss 0.145022\n",
      ">> Epoch 69 finished \tANN training loss 0.144172\n",
      ">> Epoch 70 finished \tANN training loss 0.133995\n",
      ">> Epoch 71 finished \tANN training loss 0.132417\n",
      ">> Epoch 72 finished \tANN training loss 0.128212\n",
      ">> Epoch 73 finished \tANN training loss 0.127191\n",
      ">> Epoch 74 finished \tANN training loss 0.133897\n",
      ">> Epoch 75 finished \tANN training loss 0.130620\n",
      ">> Epoch 76 finished \tANN training loss 0.120538\n",
      ">> Epoch 77 finished \tANN training loss 0.121108\n",
      ">> Epoch 78 finished \tANN training loss 0.119742\n",
      ">> Epoch 79 finished \tANN training loss 0.121870\n",
      ">> Epoch 80 finished \tANN training loss 0.126240\n",
      ">> Epoch 81 finished \tANN training loss 0.119546\n",
      ">> Epoch 82 finished \tANN training loss 0.123664\n",
      ">> Epoch 83 finished \tANN training loss 0.115060\n",
      ">> Epoch 84 finished \tANN training loss 0.119965\n",
      ">> Epoch 85 finished \tANN training loss 0.120976\n",
      ">> Epoch 86 finished \tANN training loss 0.110317\n",
      ">> Epoch 87 finished \tANN training loss 0.105291\n",
      ">> Epoch 88 finished \tANN training loss 0.107320\n",
      ">> Epoch 89 finished \tANN training loss 0.112926\n",
      ">> Epoch 90 finished \tANN training loss 0.114774\n",
      ">> Epoch 91 finished \tANN training loss 0.109561\n",
      ">> Epoch 92 finished \tANN training loss 0.101217\n",
      ">> Epoch 93 finished \tANN training loss 0.096898\n",
      ">> Epoch 94 finished \tANN training loss 0.099306\n",
      ">> Epoch 95 finished \tANN training loss 0.103072\n",
      ">> Epoch 96 finished \tANN training loss 0.113116\n",
      ">> Epoch 97 finished \tANN training loss 0.107537\n",
      ">> Epoch 98 finished \tANN training loss 0.101779\n",
      ">> Epoch 99 finished \tANN training loss 0.106275\n",
      ">> Epoch 100 finished \tANN training loss 0.100516\n",
      ">> Epoch 101 finished \tANN training loss 0.103054\n",
      ">> Epoch 102 finished \tANN training loss 0.105381\n",
      ">> Epoch 103 finished \tANN training loss 0.102134\n",
      ">> Epoch 104 finished \tANN training loss 0.104943\n",
      ">> Epoch 105 finished \tANN training loss 0.096621\n",
      ">> Epoch 106 finished \tANN training loss 0.091742\n",
      ">> Epoch 107 finished \tANN training loss 0.092124\n",
      ">> Epoch 108 finished \tANN training loss 0.090472\n",
      ">> Epoch 109 finished \tANN training loss 0.094556\n",
      ">> Epoch 110 finished \tANN training loss 0.093748\n",
      ">> Epoch 111 finished \tANN training loss 0.099566\n",
      ">> Epoch 112 finished \tANN training loss 0.093945\n",
      ">> Epoch 113 finished \tANN training loss 0.092885\n",
      ">> Epoch 114 finished \tANN training loss 0.092413\n",
      ">> Epoch 115 finished \tANN training loss 0.098421\n",
      ">> Epoch 116 finished \tANN training loss 0.100564\n",
      ">> Epoch 117 finished \tANN training loss 0.095143\n",
      ">> Epoch 118 finished \tANN training loss 0.088131\n",
      ">> Epoch 119 finished \tANN training loss 0.090358\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 120 finished \tANN training loss 0.094863\n",
      ">> Epoch 121 finished \tANN training loss 0.096233\n",
      ">> Epoch 122 finished \tANN training loss 0.086690\n",
      ">> Epoch 123 finished \tANN training loss 0.087597\n",
      ">> Epoch 124 finished \tANN training loss 0.087954\n",
      ">> Epoch 125 finished \tANN training loss 0.086892\n",
      ">> Epoch 126 finished \tANN training loss 0.088518\n",
      ">> Epoch 127 finished \tANN training loss 0.087626\n",
      ">> Epoch 128 finished \tANN training loss 0.088492\n",
      ">> Epoch 129 finished \tANN training loss 0.087782\n",
      ">> Epoch 130 finished \tANN training loss 0.090687\n",
      ">> Epoch 131 finished \tANN training loss 0.087098\n",
      ">> Epoch 132 finished \tANN training loss 0.083938\n",
      ">> Epoch 133 finished \tANN training loss 0.083891\n",
      ">> Epoch 134 finished \tANN training loss 0.085967\n",
      ">> Epoch 135 finished \tANN training loss 0.088994\n",
      ">> Epoch 136 finished \tANN training loss 0.081334\n",
      ">> Epoch 137 finished \tANN training loss 0.080027\n",
      ">> Epoch 138 finished \tANN training loss 0.083530\n",
      ">> Epoch 139 finished \tANN training loss 0.074376\n",
      ">> Epoch 140 finished \tANN training loss 0.069983\n",
      ">> Epoch 141 finished \tANN training loss 0.071416\n",
      ">> Epoch 142 finished \tANN training loss 0.073563\n",
      ">> Epoch 143 finished \tANN training loss 0.072102\n",
      ">> Epoch 144 finished \tANN training loss 0.075368\n",
      ">> Epoch 145 finished \tANN training loss 0.070104\n",
      ">> Epoch 146 finished \tANN training loss 0.074181\n",
      ">> Epoch 147 finished \tANN training loss 0.071471\n",
      ">> Epoch 148 finished \tANN training loss 0.072015\n",
      ">> Epoch 149 finished \tANN training loss 0.073198\n",
      ">> Epoch 150 finished \tANN training loss 0.069105\n",
      ">> Epoch 151 finished \tANN training loss 0.077705\n",
      ">> Epoch 152 finished \tANN training loss 0.073047\n",
      ">> Epoch 153 finished \tANN training loss 0.071821\n",
      ">> Epoch 154 finished \tANN training loss 0.070960\n",
      ">> Epoch 155 finished \tANN training loss 0.071478\n",
      ">> Epoch 156 finished \tANN training loss 0.074110\n",
      ">> Epoch 157 finished \tANN training loss 0.071285\n",
      ">> Epoch 158 finished \tANN training loss 0.074812\n",
      ">> Epoch 159 finished \tANN training loss 0.076291\n",
      ">> Epoch 160 finished \tANN training loss 0.071774\n",
      ">> Epoch 161 finished \tANN training loss 0.069770\n",
      ">> Epoch 162 finished \tANN training loss 0.071379\n",
      ">> Epoch 163 finished \tANN training loss 0.066810\n",
      ">> Epoch 164 finished \tANN training loss 0.072542\n",
      ">> Epoch 165 finished \tANN training loss 0.074412\n",
      ">> Epoch 166 finished \tANN training loss 0.065361\n",
      ">> Epoch 167 finished \tANN training loss 0.072267\n",
      ">> Epoch 168 finished \tANN training loss 0.074628\n",
      ">> Epoch 169 finished \tANN training loss 0.076956\n",
      ">> Epoch 170 finished \tANN training loss 0.077255\n",
      ">> Epoch 171 finished \tANN training loss 0.076564\n",
      ">> Epoch 172 finished \tANN training loss 0.074154\n",
      ">> Epoch 173 finished \tANN training loss 0.075528\n",
      ">> Epoch 174 finished \tANN training loss 0.070980\n",
      ">> Epoch 175 finished \tANN training loss 0.070886\n",
      ">> Epoch 176 finished \tANN training loss 0.076924\n",
      ">> Epoch 177 finished \tANN training loss 0.075772\n",
      ">> Epoch 178 finished \tANN training loss 0.073212\n",
      ">> Epoch 179 finished \tANN training loss 0.072196\n",
      ">> Epoch 180 finished \tANN training loss 0.067445\n",
      ">> Epoch 181 finished \tANN training loss 0.061758\n",
      ">> Epoch 182 finished \tANN training loss 0.063756\n",
      ">> Epoch 183 finished \tANN training loss 0.064796\n",
      ">> Epoch 184 finished \tANN training loss 0.064416\n",
      ">> Epoch 185 finished \tANN training loss 0.064928\n",
      ">> Epoch 186 finished \tANN training loss 0.062131\n",
      ">> Epoch 187 finished \tANN training loss 0.059296\n",
      ">> Epoch 188 finished \tANN training loss 0.061075\n",
      ">> Epoch 189 finished \tANN training loss 0.063031\n",
      ">> Epoch 190 finished \tANN training loss 0.058655\n",
      ">> Epoch 191 finished \tANN training loss 0.056797\n",
      ">> Epoch 192 finished \tANN training loss 0.056967\n",
      ">> Epoch 193 finished \tANN training loss 0.054322\n",
      ">> Epoch 194 finished \tANN training loss 0.056132\n",
      ">> Epoch 195 finished \tANN training loss 0.062337\n",
      ">> Epoch 196 finished \tANN training loss 0.061040\n",
      ">> Epoch 197 finished \tANN training loss 0.055425\n",
      ">> Epoch 198 finished \tANN training loss 0.058720\n",
      ">> Epoch 199 finished \tANN training loss 0.066024\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.900000\n"
     ]
    }
   ],
   "source": [
    "# ReLu, dropout = 0.7\n",
    "acc7 = deep_belief_net(hidden_layers_structure=[300], \n",
    "                       learning_rate_rbm=0.05,\n",
    "                       learning_rate=0.1,\n",
    "                       n_epochs_rbm=40,\n",
    "                       n_iter_backprop=200,\n",
    "                       batch_size=32,\n",
    "                       activation_function='relu',\n",
    "                       dropout_p=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.9\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(acc7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 60.473331\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 56.021935\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 71.656853\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 50.487885\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 50.820499\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 42.040997\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 34.458988\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 42.691341\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 52.118073\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 46.811501\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 32.541492\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 34.893253\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 52.058681\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 38.734688\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 37.273239\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 34.313351\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 48.837479\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 39.533573\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 30.994461\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 36.145123\n",
      ">> Epoch 21 finished \tRBM Reconstruction error 26.398838\n",
      ">> Epoch 22 finished \tRBM Reconstruction error 29.877769\n",
      ">> Epoch 23 finished \tRBM Reconstruction error 32.888607\n",
      ">> Epoch 24 finished \tRBM Reconstruction error 38.913994\n",
      ">> Epoch 25 finished \tRBM Reconstruction error 34.648663\n",
      ">> Epoch 26 finished \tRBM Reconstruction error 46.099884\n",
      ">> Epoch 27 finished \tRBM Reconstruction error 42.561821\n",
      ">> Epoch 28 finished \tRBM Reconstruction error 41.133087\n",
      ">> Epoch 29 finished \tRBM Reconstruction error 35.349255\n",
      ">> Epoch 30 finished \tRBM Reconstruction error 29.608042\n",
      ">> Epoch 31 finished \tRBM Reconstruction error 34.103809\n",
      ">> Epoch 32 finished \tRBM Reconstruction error 45.162472\n",
      ">> Epoch 33 finished \tRBM Reconstruction error 43.641148\n",
      ">> Epoch 34 finished \tRBM Reconstruction error 30.042070\n",
      ">> Epoch 35 finished \tRBM Reconstruction error 38.409088\n",
      ">> Epoch 36 finished \tRBM Reconstruction error 37.312607\n",
      ">> Epoch 37 finished \tRBM Reconstruction error 34.084312\n",
      ">> Epoch 38 finished \tRBM Reconstruction error 37.813393\n",
      ">> Epoch 39 finished \tRBM Reconstruction error 40.761871\n",
      ">> Epoch 40 finished \tRBM Reconstruction error 42.557953\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 1.132074\n",
      ">> Epoch 1 finished \tANN training loss 0.935824\n",
      ">> Epoch 2 finished \tANN training loss 0.899670\n",
      ">> Epoch 3 finished \tANN training loss 0.857884\n",
      ">> Epoch 4 finished \tANN training loss 0.895157\n",
      ">> Epoch 5 finished \tANN training loss 0.831615\n",
      ">> Epoch 6 finished \tANN training loss 0.847662\n",
      ">> Epoch 7 finished \tANN training loss 0.814241\n",
      ">> Epoch 8 finished \tANN training loss 0.768015\n",
      ">> Epoch 9 finished \tANN training loss 0.741344\n",
      ">> Epoch 10 finished \tANN training loss 0.741145\n",
      ">> Epoch 11 finished \tANN training loss 0.737562\n",
      ">> Epoch 12 finished \tANN training loss 0.760413\n",
      ">> Epoch 13 finished \tANN training loss 0.710643\n",
      ">> Epoch 14 finished \tANN training loss 0.712115\n",
      ">> Epoch 15 finished \tANN training loss 0.715964\n",
      ">> Epoch 16 finished \tANN training loss 0.677715\n",
      ">> Epoch 17 finished \tANN training loss 0.694968\n",
      ">> Epoch 18 finished \tANN training loss 0.690122\n",
      ">> Epoch 19 finished \tANN training loss 0.689990\n",
      ">> Epoch 20 finished \tANN training loss 0.696561\n",
      ">> Epoch 21 finished \tANN training loss 0.633091\n",
      ">> Epoch 22 finished \tANN training loss 0.647647\n",
      ">> Epoch 23 finished \tANN training loss 0.658715\n",
      ">> Epoch 24 finished \tANN training loss 0.643404\n",
      ">> Epoch 25 finished \tANN training loss 0.628812\n",
      ">> Epoch 26 finished \tANN training loss 0.640817\n",
      ">> Epoch 27 finished \tANN training loss 0.598085\n",
      ">> Epoch 28 finished \tANN training loss 0.618935\n",
      ">> Epoch 29 finished \tANN training loss 0.581256\n",
      ">> Epoch 30 finished \tANN training loss 0.595062\n",
      ">> Epoch 31 finished \tANN training loss 0.577188\n",
      ">> Epoch 32 finished \tANN training loss 0.616411\n",
      ">> Epoch 33 finished \tANN training loss 0.599234\n",
      ">> Epoch 34 finished \tANN training loss 0.615360\n",
      ">> Epoch 35 finished \tANN training loss 0.620599\n",
      ">> Epoch 36 finished \tANN training loss 0.588794\n",
      ">> Epoch 37 finished \tANN training loss 0.624106\n",
      ">> Epoch 38 finished \tANN training loss 0.577390\n",
      ">> Epoch 39 finished \tANN training loss 0.577865\n",
      ">> Epoch 40 finished \tANN training loss 0.581162\n",
      ">> Epoch 41 finished \tANN training loss 0.558854\n",
      ">> Epoch 42 finished \tANN training loss 0.594308\n",
      ">> Epoch 43 finished \tANN training loss 0.574351\n",
      ">> Epoch 44 finished \tANN training loss 0.550056\n",
      ">> Epoch 45 finished \tANN training loss 0.558080\n",
      ">> Epoch 46 finished \tANN training loss 0.576704\n",
      ">> Epoch 47 finished \tANN training loss 0.586663\n",
      ">> Epoch 48 finished \tANN training loss 0.565888\n",
      ">> Epoch 49 finished \tANN training loss 0.551348\n",
      ">> Epoch 50 finished \tANN training loss 0.565030\n",
      ">> Epoch 51 finished \tANN training loss 0.591374\n",
      ">> Epoch 52 finished \tANN training loss 0.581353\n",
      ">> Epoch 53 finished \tANN training loss 0.562089\n",
      ">> Epoch 54 finished \tANN training loss 0.563112\n",
      ">> Epoch 55 finished \tANN training loss 0.562353\n",
      ">> Epoch 56 finished \tANN training loss 0.554455\n",
      ">> Epoch 57 finished \tANN training loss 0.568357\n",
      ">> Epoch 58 finished \tANN training loss 0.552972\n",
      ">> Epoch 59 finished \tANN training loss 0.555093\n",
      ">> Epoch 60 finished \tANN training loss 0.544792\n",
      ">> Epoch 61 finished \tANN training loss 0.542625\n",
      ">> Epoch 62 finished \tANN training loss 0.556875\n",
      ">> Epoch 63 finished \tANN training loss 0.530907\n",
      ">> Epoch 64 finished \tANN training loss 0.555902\n",
      ">> Epoch 65 finished \tANN training loss 0.578795\n",
      ">> Epoch 66 finished \tANN training loss 0.553687\n",
      ">> Epoch 67 finished \tANN training loss 0.533920\n",
      ">> Epoch 68 finished \tANN training loss 0.527071\n",
      ">> Epoch 69 finished \tANN training loss 0.514335\n",
      ">> Epoch 70 finished \tANN training loss 0.507415\n",
      ">> Epoch 71 finished \tANN training loss 0.510460\n",
      ">> Epoch 72 finished \tANN training loss 0.480948\n",
      ">> Epoch 73 finished \tANN training loss 0.501444\n",
      ">> Epoch 74 finished \tANN training loss 0.510695\n",
      ">> Epoch 75 finished \tANN training loss 0.509253\n",
      ">> Epoch 76 finished \tANN training loss 0.496056\n",
      ">> Epoch 77 finished \tANN training loss 0.496375\n",
      ">> Epoch 78 finished \tANN training loss 0.485085\n",
      ">> Epoch 79 finished \tANN training loss 0.495151\n",
      ">> Epoch 80 finished \tANN training loss 0.506187\n",
      ">> Epoch 81 finished \tANN training loss 0.491050\n",
      ">> Epoch 82 finished \tANN training loss 0.517487\n",
      ">> Epoch 83 finished \tANN training loss 0.527828\n",
      ">> Epoch 84 finished \tANN training loss 0.516435\n",
      ">> Epoch 85 finished \tANN training loss 0.501296\n",
      ">> Epoch 86 finished \tANN training loss 0.507035\n",
      ">> Epoch 87 finished \tANN training loss 0.526031\n",
      ">> Epoch 88 finished \tANN training loss 0.506426\n",
      ">> Epoch 89 finished \tANN training loss 0.505084\n",
      ">> Epoch 90 finished \tANN training loss 0.524261\n",
      ">> Epoch 91 finished \tANN training loss 0.517440\n",
      ">> Epoch 92 finished \tANN training loss 0.523115\n",
      ">> Epoch 93 finished \tANN training loss 0.477661\n",
      ">> Epoch 94 finished \tANN training loss 0.471512\n",
      ">> Epoch 95 finished \tANN training loss 0.488449\n",
      ">> Epoch 96 finished \tANN training loss 0.478747\n",
      ">> Epoch 97 finished \tANN training loss 0.460625\n",
      ">> Epoch 98 finished \tANN training loss 0.455149\n",
      ">> Epoch 99 finished \tANN training loss 0.454151\n",
      ">> Epoch 100 finished \tANN training loss 0.484615\n",
      ">> Epoch 101 finished \tANN training loss 0.509662\n",
      ">> Epoch 102 finished \tANN training loss 0.500093\n",
      ">> Epoch 103 finished \tANN training loss 0.497978\n",
      ">> Epoch 104 finished \tANN training loss 0.479377\n",
      ">> Epoch 105 finished \tANN training loss 0.479578\n",
      ">> Epoch 106 finished \tANN training loss 0.484659\n",
      ">> Epoch 107 finished \tANN training loss 0.465586\n",
      ">> Epoch 108 finished \tANN training loss 0.455251\n",
      ">> Epoch 109 finished \tANN training loss 0.470125\n",
      ">> Epoch 110 finished \tANN training loss 0.462641\n",
      ">> Epoch 111 finished \tANN training loss 0.468803\n",
      ">> Epoch 112 finished \tANN training loss 0.480760\n",
      ">> Epoch 113 finished \tANN training loss 0.474909\n",
      ">> Epoch 114 finished \tANN training loss 0.476098\n",
      ">> Epoch 115 finished \tANN training loss 0.458587\n",
      ">> Epoch 116 finished \tANN training loss 0.453551\n",
      ">> Epoch 117 finished \tANN training loss 0.472606\n",
      ">> Epoch 118 finished \tANN training loss 0.463366\n",
      ">> Epoch 119 finished \tANN training loss 0.491521\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 120 finished \tANN training loss 0.478163\n",
      ">> Epoch 121 finished \tANN training loss 0.479955\n",
      ">> Epoch 122 finished \tANN training loss 0.478597\n",
      ">> Epoch 123 finished \tANN training loss 0.482490\n",
      ">> Epoch 124 finished \tANN training loss 0.475040\n",
      ">> Epoch 125 finished \tANN training loss 0.466021\n",
      ">> Epoch 126 finished \tANN training loss 0.467682\n",
      ">> Epoch 127 finished \tANN training loss 0.478625\n",
      ">> Epoch 128 finished \tANN training loss 0.492616\n",
      ">> Epoch 129 finished \tANN training loss 0.473480\n",
      ">> Epoch 130 finished \tANN training loss 0.461541\n",
      ">> Epoch 131 finished \tANN training loss 0.465311\n",
      ">> Epoch 132 finished \tANN training loss 0.465637\n",
      ">> Epoch 133 finished \tANN training loss 0.470708\n",
      ">> Epoch 134 finished \tANN training loss 0.483775\n",
      ">> Epoch 135 finished \tANN training loss 0.486034\n",
      ">> Epoch 136 finished \tANN training loss 0.463178\n",
      ">> Epoch 137 finished \tANN training loss 0.468607\n",
      ">> Epoch 138 finished \tANN training loss 0.473074\n",
      ">> Epoch 139 finished \tANN training loss 0.463344\n",
      ">> Epoch 140 finished \tANN training loss 0.446094\n",
      ">> Epoch 141 finished \tANN training loss 0.429035\n",
      ">> Epoch 142 finished \tANN training loss 0.420760\n",
      ">> Epoch 143 finished \tANN training loss 0.424564\n",
      ">> Epoch 144 finished \tANN training loss 0.444490\n",
      ">> Epoch 145 finished \tANN training loss 0.435584\n",
      ">> Epoch 146 finished \tANN training loss 0.419039\n",
      ">> Epoch 147 finished \tANN training loss 0.451174\n",
      ">> Epoch 148 finished \tANN training loss 0.446619\n",
      ">> Epoch 149 finished \tANN training loss 0.444245\n",
      ">> Epoch 150 finished \tANN training loss 0.426693\n",
      ">> Epoch 151 finished \tANN training loss 0.439684\n",
      ">> Epoch 152 finished \tANN training loss 0.456615\n",
      ">> Epoch 153 finished \tANN training loss 0.435980\n",
      ">> Epoch 154 finished \tANN training loss 0.446294\n",
      ">> Epoch 155 finished \tANN training loss 0.458092\n",
      ">> Epoch 156 finished \tANN training loss 0.477440\n",
      ">> Epoch 157 finished \tANN training loss 0.461287\n",
      ">> Epoch 158 finished \tANN training loss 0.446175\n",
      ">> Epoch 159 finished \tANN training loss 0.466359\n",
      ">> Epoch 160 finished \tANN training loss 0.448909\n",
      ">> Epoch 161 finished \tANN training loss 0.469638\n",
      ">> Epoch 162 finished \tANN training loss 0.488412\n",
      ">> Epoch 163 finished \tANN training loss 0.514167\n",
      ">> Epoch 164 finished \tANN training loss 0.468001\n",
      ">> Epoch 165 finished \tANN training loss 0.464470\n",
      ">> Epoch 166 finished \tANN training loss 0.445501\n",
      ">> Epoch 167 finished \tANN training loss 0.447396\n",
      ">> Epoch 168 finished \tANN training loss 0.438374\n",
      ">> Epoch 169 finished \tANN training loss 0.440141\n",
      ">> Epoch 170 finished \tANN training loss 0.445051\n",
      ">> Epoch 171 finished \tANN training loss 0.432966\n",
      ">> Epoch 172 finished \tANN training loss 0.420078\n",
      ">> Epoch 173 finished \tANN training loss 0.453872\n",
      ">> Epoch 174 finished \tANN training loss 0.451927\n",
      ">> Epoch 175 finished \tANN training loss 0.435021\n",
      ">> Epoch 176 finished \tANN training loss 0.448320\n",
      ">> Epoch 177 finished \tANN training loss 0.449271\n",
      ">> Epoch 178 finished \tANN training loss 0.448843\n",
      ">> Epoch 179 finished \tANN training loss 0.430400\n",
      ">> Epoch 180 finished \tANN training loss 0.426055\n",
      ">> Epoch 181 finished \tANN training loss 0.413947\n",
      ">> Epoch 182 finished \tANN training loss 0.432023\n",
      ">> Epoch 183 finished \tANN training loss 0.422865\n",
      ">> Epoch 184 finished \tANN training loss 0.416759\n",
      ">> Epoch 185 finished \tANN training loss 0.431288\n",
      ">> Epoch 186 finished \tANN training loss 0.421466\n",
      ">> Epoch 187 finished \tANN training loss 0.439843\n",
      ">> Epoch 188 finished \tANN training loss 0.452089\n",
      ">> Epoch 189 finished \tANN training loss 0.447998\n",
      ">> Epoch 190 finished \tANN training loss 0.428183\n",
      ">> Epoch 191 finished \tANN training loss 0.431471\n",
      ">> Epoch 192 finished \tANN training loss 0.415347\n",
      ">> Epoch 193 finished \tANN training loss 0.437009\n",
      ">> Epoch 194 finished \tANN training loss 0.427055\n",
      ">> Epoch 195 finished \tANN training loss 0.428547\n",
      ">> Epoch 196 finished \tANN training loss 0.420437\n",
      ">> Epoch 197 finished \tANN training loss 0.409065\n",
      ">> Epoch 198 finished \tANN training loss 0.418048\n",
      ">> Epoch 199 finished \tANN training loss 0.421983\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.800000\n"
     ]
    }
   ],
   "source": [
    "# ReLu, dropout = 0.8\n",
    "acc8 = deep_belief_net(hidden_layers_structure=[300], \n",
    "                       learning_rate_rbm=0.05,\n",
    "                       learning_rate=0.1,\n",
    "                       n_epochs_rbm=40,\n",
    "                       n_iter_backprop=200,\n",
    "                       batch_size=32,\n",
    "                       activation_function='relu',\n",
    "                       dropout_p=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.8\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(acc8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 51.625134\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 76.770470\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 72.536652\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 62.944092\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 53.268398\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 58.766701\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 48.798634\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 60.624760\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 56.839169\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 43.366913\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 43.427475\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 27.791582\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 40.248569\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 34.714977\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 57.294037\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 51.249142\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 49.431450\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 57.282455\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 39.906178\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 64.982338\n",
      ">> Epoch 21 finished \tRBM Reconstruction error 46.795116\n",
      ">> Epoch 22 finished \tRBM Reconstruction error 46.363087\n",
      ">> Epoch 23 finished \tRBM Reconstruction error 48.862373\n",
      ">> Epoch 24 finished \tRBM Reconstruction error 44.838661\n",
      ">> Epoch 25 finished \tRBM Reconstruction error 36.712193\n",
      ">> Epoch 26 finished \tRBM Reconstruction error 50.306717\n",
      ">> Epoch 27 finished \tRBM Reconstruction error 41.899246\n",
      ">> Epoch 28 finished \tRBM Reconstruction error 49.440899\n",
      ">> Epoch 29 finished \tRBM Reconstruction error 54.250488\n",
      ">> Epoch 30 finished \tRBM Reconstruction error 50.092716\n",
      ">> Epoch 31 finished \tRBM Reconstruction error 50.622646\n",
      ">> Epoch 32 finished \tRBM Reconstruction error 47.596855\n",
      ">> Epoch 33 finished \tRBM Reconstruction error 43.325298\n",
      ">> Epoch 34 finished \tRBM Reconstruction error 51.524601\n",
      ">> Epoch 35 finished \tRBM Reconstruction error 48.152855\n",
      ">> Epoch 36 finished \tRBM Reconstruction error 43.356514\n",
      ">> Epoch 37 finished \tRBM Reconstruction error 59.101551\n",
      ">> Epoch 38 finished \tRBM Reconstruction error 49.825939\n",
      ">> Epoch 39 finished \tRBM Reconstruction error 47.288212\n",
      ">> Epoch 40 finished \tRBM Reconstruction error 56.597286\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss 1.732169\n",
      ">> Epoch 1 finished \tANN training loss 1.941332\n",
      ">> Epoch 2 finished \tANN training loss 2.087359\n",
      ">> Epoch 3 finished \tANN training loss 2.121989\n",
      ">> Epoch 4 finished \tANN training loss 2.151807\n",
      ">> Epoch 5 finished \tANN training loss 2.160697\n",
      ">> Epoch 6 finished \tANN training loss 2.152679\n",
      ">> Epoch 7 finished \tANN training loss 2.130618\n",
      ">> Epoch 8 finished \tANN training loss 2.174693\n",
      ">> Epoch 9 finished \tANN training loss 2.188563\n",
      ">> Epoch 10 finished \tANN training loss 2.129323\n",
      ">> Epoch 11 finished \tANN training loss 2.088039\n",
      ">> Epoch 12 finished \tANN training loss 2.109590\n",
      ">> Epoch 13 finished \tANN training loss 2.092322\n",
      ">> Epoch 14 finished \tANN training loss 2.128825\n",
      ">> Epoch 15 finished \tANN training loss 2.093973\n",
      ">> Epoch 16 finished \tANN training loss 2.103493\n",
      ">> Epoch 17 finished \tANN training loss 2.120016\n",
      ">> Epoch 18 finished \tANN training loss 2.146897\n",
      ">> Epoch 19 finished \tANN training loss 2.146122\n",
      ">> Epoch 20 finished \tANN training loss 2.125919\n",
      ">> Epoch 21 finished \tANN training loss 2.137984\n",
      ">> Epoch 22 finished \tANN training loss 2.155367\n",
      ">> Epoch 23 finished \tANN training loss 2.158597\n",
      ">> Epoch 24 finished \tANN training loss 2.141308\n",
      ">> Epoch 25 finished \tANN training loss 2.129461\n",
      ">> Epoch 26 finished \tANN training loss 2.172035\n",
      ">> Epoch 27 finished \tANN training loss 2.153636\n",
      ">> Epoch 28 finished \tANN training loss 2.130220\n",
      ">> Epoch 29 finished \tANN training loss 2.167326\n",
      ">> Epoch 30 finished \tANN training loss 2.147037\n",
      ">> Epoch 31 finished \tANN training loss 2.139156\n",
      ">> Epoch 32 finished \tANN training loss 2.146676\n",
      ">> Epoch 33 finished \tANN training loss 2.145547\n",
      ">> Epoch 34 finished \tANN training loss 2.145848\n",
      ">> Epoch 35 finished \tANN training loss 2.131047\n",
      ">> Epoch 36 finished \tANN training loss 2.145546\n",
      ">> Epoch 37 finished \tANN training loss 2.163685\n",
      ">> Epoch 38 finished \tANN training loss 2.174070\n",
      ">> Epoch 39 finished \tANN training loss 2.167590\n",
      ">> Epoch 40 finished \tANN training loss 2.170692\n",
      ">> Epoch 41 finished \tANN training loss 2.172338\n",
      ">> Epoch 42 finished \tANN training loss 2.153179\n",
      ">> Epoch 43 finished \tANN training loss 2.179023\n",
      ">> Epoch 44 finished \tANN training loss 2.170428\n",
      ">> Epoch 45 finished \tANN training loss 2.174390\n",
      ">> Epoch 46 finished \tANN training loss 2.150687\n",
      ">> Epoch 47 finished \tANN training loss 2.157766\n",
      ">> Epoch 48 finished \tANN training loss 2.184215\n",
      ">> Epoch 49 finished \tANN training loss 2.156837\n",
      ">> Epoch 50 finished \tANN training loss 2.159907\n",
      ">> Epoch 51 finished \tANN training loss 2.176643\n",
      ">> Epoch 52 finished \tANN training loss 2.176802\n",
      ">> Epoch 53 finished \tANN training loss 2.169082\n",
      ">> Epoch 54 finished \tANN training loss 2.149548\n",
      ">> Epoch 55 finished \tANN training loss 2.154392\n",
      ">> Epoch 56 finished \tANN training loss 2.144527\n",
      ">> Epoch 57 finished \tANN training loss 2.161858\n",
      ">> Epoch 58 finished \tANN training loss 2.186012\n",
      ">> Epoch 59 finished \tANN training loss 2.160159\n",
      ">> Epoch 60 finished \tANN training loss 2.174149\n",
      ">> Epoch 61 finished \tANN training loss 2.182539\n",
      ">> Epoch 62 finished \tANN training loss 2.190384\n",
      ">> Epoch 63 finished \tANN training loss 2.168181\n",
      ">> Epoch 64 finished \tANN training loss 2.164005\n",
      ">> Epoch 65 finished \tANN training loss 2.146474\n",
      ">> Epoch 66 finished \tANN training loss 2.160167\n",
      ">> Epoch 67 finished \tANN training loss 2.154632\n",
      ">> Epoch 68 finished \tANN training loss 2.152047\n",
      ">> Epoch 69 finished \tANN training loss 2.183812\n",
      ">> Epoch 70 finished \tANN training loss 2.173420\n",
      ">> Epoch 71 finished \tANN training loss 2.178834\n",
      ">> Epoch 72 finished \tANN training loss 2.175931\n",
      ">> Epoch 73 finished \tANN training loss 2.168635\n",
      ">> Epoch 74 finished \tANN training loss 2.152881\n",
      ">> Epoch 75 finished \tANN training loss 2.153654\n",
      ">> Epoch 76 finished \tANN training loss 2.174276\n",
      ">> Epoch 77 finished \tANN training loss 2.187140\n",
      ">> Epoch 78 finished \tANN training loss 2.150635\n",
      ">> Epoch 79 finished \tANN training loss 2.151319\n",
      ">> Epoch 80 finished \tANN training loss 2.151626\n",
      ">> Epoch 81 finished \tANN training loss 2.125837\n",
      ">> Epoch 82 finished \tANN training loss 2.165482\n",
      ">> Epoch 83 finished \tANN training loss 2.158957\n",
      ">> Epoch 84 finished \tANN training loss 2.166259\n",
      ">> Epoch 85 finished \tANN training loss 2.153850\n",
      ">> Epoch 86 finished \tANN training loss 2.149735\n",
      ">> Epoch 87 finished \tANN training loss 2.171729\n",
      ">> Epoch 88 finished \tANN training loss 2.180561\n",
      ">> Epoch 89 finished \tANN training loss 2.210935\n",
      ">> Epoch 90 finished \tANN training loss 2.178512\n",
      ">> Epoch 91 finished \tANN training loss 2.150921\n",
      ">> Epoch 92 finished \tANN training loss 2.173715\n",
      ">> Epoch 93 finished \tANN training loss 2.194286\n",
      ">> Epoch 94 finished \tANN training loss 2.173527\n",
      ">> Epoch 95 finished \tANN training loss 2.138774\n",
      ">> Epoch 96 finished \tANN training loss 2.161061\n",
      ">> Epoch 97 finished \tANN training loss 2.164414\n",
      ">> Epoch 98 finished \tANN training loss 2.169857\n",
      ">> Epoch 99 finished \tANN training loss 2.157480\n",
      ">> Epoch 100 finished \tANN training loss 2.153630\n",
      ">> Epoch 101 finished \tANN training loss 2.149297\n",
      ">> Epoch 102 finished \tANN training loss 2.146027\n",
      ">> Epoch 103 finished \tANN training loss 2.134158\n",
      ">> Epoch 104 finished \tANN training loss 2.185303\n",
      ">> Epoch 105 finished \tANN training loss 2.192255\n",
      ">> Epoch 106 finished \tANN training loss 2.167178\n",
      ">> Epoch 107 finished \tANN training loss 2.155923\n",
      ">> Epoch 108 finished \tANN training loss 2.179603\n",
      ">> Epoch 109 finished \tANN training loss 2.165329\n",
      ">> Epoch 110 finished \tANN training loss 2.187927\n",
      ">> Epoch 111 finished \tANN training loss 2.167096\n",
      ">> Epoch 112 finished \tANN training loss 2.179219\n",
      ">> Epoch 113 finished \tANN training loss 2.178646\n",
      ">> Epoch 114 finished \tANN training loss 2.175715\n",
      ">> Epoch 115 finished \tANN training loss 2.194725\n",
      ">> Epoch 116 finished \tANN training loss 2.175158\n",
      ">> Epoch 117 finished \tANN training loss 2.184798\n",
      ">> Epoch 118 finished \tANN training loss 2.188413\n",
      ">> Epoch 119 finished \tANN training loss 2.178125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 120 finished \tANN training loss 2.176945\n",
      ">> Epoch 121 finished \tANN training loss 2.183486\n",
      ">> Epoch 122 finished \tANN training loss 2.163584\n",
      ">> Epoch 123 finished \tANN training loss 2.176296\n",
      ">> Epoch 124 finished \tANN training loss 2.159366\n",
      ">> Epoch 125 finished \tANN training loss 2.138997\n",
      ">> Epoch 126 finished \tANN training loss 2.149578\n",
      ">> Epoch 127 finished \tANN training loss 2.182500\n",
      ">> Epoch 128 finished \tANN training loss 2.168003\n",
      ">> Epoch 129 finished \tANN training loss 2.183071\n",
      ">> Epoch 130 finished \tANN training loss 2.190913\n",
      ">> Epoch 131 finished \tANN training loss 2.170638\n",
      ">> Epoch 132 finished \tANN training loss 2.212713\n",
      ">> Epoch 133 finished \tANN training loss 2.186089\n",
      ">> Epoch 134 finished \tANN training loss 2.195692\n",
      ">> Epoch 135 finished \tANN training loss 2.166410\n",
      ">> Epoch 136 finished \tANN training loss 2.148310\n",
      ">> Epoch 137 finished \tANN training loss 2.162990\n",
      ">> Epoch 138 finished \tANN training loss 2.150542\n",
      ">> Epoch 139 finished \tANN training loss 2.176941\n",
      ">> Epoch 140 finished \tANN training loss 2.186961\n",
      ">> Epoch 141 finished \tANN training loss 2.192263\n",
      ">> Epoch 142 finished \tANN training loss 2.182734\n",
      ">> Epoch 143 finished \tANN training loss 2.180749\n",
      ">> Epoch 144 finished \tANN training loss 2.152255\n",
      ">> Epoch 145 finished \tANN training loss 2.113683\n",
      ">> Epoch 146 finished \tANN training loss 2.173158\n",
      ">> Epoch 147 finished \tANN training loss 2.154315\n",
      ">> Epoch 148 finished \tANN training loss 2.165677\n",
      ">> Epoch 149 finished \tANN training loss 2.166407\n",
      ">> Epoch 150 finished \tANN training loss 2.152326\n",
      ">> Epoch 151 finished \tANN training loss 2.150802\n",
      ">> Epoch 152 finished \tANN training loss 2.173071\n",
      ">> Epoch 153 finished \tANN training loss 2.181078\n",
      ">> Epoch 154 finished \tANN training loss 2.169631\n",
      ">> Epoch 155 finished \tANN training loss 2.163605\n",
      ">> Epoch 156 finished \tANN training loss 2.163581\n",
      ">> Epoch 157 finished \tANN training loss 2.185646\n",
      ">> Epoch 158 finished \tANN training loss 2.182969\n",
      ">> Epoch 159 finished \tANN training loss 2.151333\n",
      ">> Epoch 160 finished \tANN training loss 2.183060\n",
      ">> Epoch 161 finished \tANN training loss 2.153569\n",
      ">> Epoch 162 finished \tANN training loss 2.129071\n",
      ">> Epoch 163 finished \tANN training loss 2.138309\n",
      ">> Epoch 164 finished \tANN training loss 2.142051\n",
      ">> Epoch 165 finished \tANN training loss 2.161184\n",
      ">> Epoch 166 finished \tANN training loss 2.123452\n",
      ">> Epoch 167 finished \tANN training loss 2.144324\n",
      ">> Epoch 168 finished \tANN training loss 2.156217\n",
      ">> Epoch 169 finished \tANN training loss 2.151315\n",
      ">> Epoch 170 finished \tANN training loss 2.180839\n",
      ">> Epoch 171 finished \tANN training loss 2.178761\n",
      ">> Epoch 172 finished \tANN training loss 2.160014\n",
      ">> Epoch 173 finished \tANN training loss 2.143563\n",
      ">> Epoch 174 finished \tANN training loss 2.170612\n",
      ">> Epoch 175 finished \tANN training loss 2.185767\n",
      ">> Epoch 176 finished \tANN training loss 2.161528\n",
      ">> Epoch 177 finished \tANN training loss 2.180478\n",
      ">> Epoch 178 finished \tANN training loss 2.163069\n",
      ">> Epoch 179 finished \tANN training loss 2.175171\n",
      ">> Epoch 180 finished \tANN training loss 2.147868\n",
      ">> Epoch 181 finished \tANN training loss 2.165702\n",
      ">> Epoch 182 finished \tANN training loss 2.159899\n",
      ">> Epoch 183 finished \tANN training loss 2.166686\n",
      ">> Epoch 184 finished \tANN training loss 2.157096\n",
      ">> Epoch 185 finished \tANN training loss 2.145808\n",
      ">> Epoch 186 finished \tANN training loss 2.167522\n",
      ">> Epoch 187 finished \tANN training loss 2.187545\n",
      ">> Epoch 188 finished \tANN training loss 2.160564\n",
      ">> Epoch 189 finished \tANN training loss 2.168859\n",
      ">> Epoch 190 finished \tANN training loss 2.152675\n",
      ">> Epoch 191 finished \tANN training loss 2.160844\n",
      ">> Epoch 192 finished \tANN training loss 2.137596\n",
      ">> Epoch 193 finished \tANN training loss 2.140012\n",
      ">> Epoch 194 finished \tANN training loss 2.136550\n",
      ">> Epoch 195 finished \tANN training loss 2.130581\n",
      ">> Epoch 196 finished \tANN training loss 2.121011\n",
      ">> Epoch 197 finished \tANN training loss 2.138720\n",
      ">> Epoch 198 finished \tANN training loss 2.155964\n",
      ">> Epoch 199 finished \tANN training loss 2.149277\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.155000\n"
     ]
    }
   ],
   "source": [
    "# ReLu, dropout = 0.9\n",
    "acc9 = deep_belief_net(hidden_layers_structure=[300], \n",
    "                       learning_rate_rbm=0.05,\n",
    "                       learning_rate=0.1,\n",
    "                       n_epochs_rbm=40,\n",
    "                       n_iter_backprop=200,\n",
    "                       batch_size=32,\n",
    "                       activation_function='relu',\n",
    "                       dropout_p=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.155\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(acc9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[START] Pre-training step:\n",
      ">> Epoch 1 finished \tRBM Reconstruction error 79.109840\n",
      ">> Epoch 2 finished \tRBM Reconstruction error 63.176834\n",
      ">> Epoch 3 finished \tRBM Reconstruction error 53.967606\n",
      ">> Epoch 4 finished \tRBM Reconstruction error 32.274902\n",
      ">> Epoch 5 finished \tRBM Reconstruction error 32.726784\n",
      ">> Epoch 6 finished \tRBM Reconstruction error 37.979504\n",
      ">> Epoch 7 finished \tRBM Reconstruction error 31.923939\n",
      ">> Epoch 8 finished \tRBM Reconstruction error 36.889969\n",
      ">> Epoch 9 finished \tRBM Reconstruction error 26.216021\n",
      ">> Epoch 10 finished \tRBM Reconstruction error 25.015684\n",
      ">> Epoch 11 finished \tRBM Reconstruction error 28.614719\n",
      ">> Epoch 12 finished \tRBM Reconstruction error 24.293184\n",
      ">> Epoch 13 finished \tRBM Reconstruction error 26.673061\n",
      ">> Epoch 14 finished \tRBM Reconstruction error 25.868410\n",
      ">> Epoch 15 finished \tRBM Reconstruction error 37.665257\n",
      ">> Epoch 16 finished \tRBM Reconstruction error 37.511024\n",
      ">> Epoch 17 finished \tRBM Reconstruction error 43.258770\n",
      ">> Epoch 18 finished \tRBM Reconstruction error 30.991518\n",
      ">> Epoch 19 finished \tRBM Reconstruction error 42.074825\n",
      ">> Epoch 20 finished \tRBM Reconstruction error 21.609245\n",
      ">> Epoch 21 finished \tRBM Reconstruction error 28.272455\n",
      ">> Epoch 22 finished \tRBM Reconstruction error 28.798027\n",
      ">> Epoch 23 finished \tRBM Reconstruction error 27.145079\n",
      ">> Epoch 24 finished \tRBM Reconstruction error 27.364443\n",
      ">> Epoch 25 finished \tRBM Reconstruction error 24.284081\n",
      ">> Epoch 26 finished \tRBM Reconstruction error 50.532295\n",
      ">> Epoch 27 finished \tRBM Reconstruction error 22.311724\n",
      ">> Epoch 28 finished \tRBM Reconstruction error 25.176216\n",
      ">> Epoch 29 finished \tRBM Reconstruction error 26.835533\n",
      ">> Epoch 30 finished \tRBM Reconstruction error 23.717609\n",
      ">> Epoch 31 finished \tRBM Reconstruction error 28.740952\n",
      ">> Epoch 32 finished \tRBM Reconstruction error 32.097599\n",
      ">> Epoch 33 finished \tRBM Reconstruction error 38.287491\n",
      ">> Epoch 34 finished \tRBM Reconstruction error 33.548656\n",
      ">> Epoch 35 finished \tRBM Reconstruction error 38.731396\n",
      ">> Epoch 36 finished \tRBM Reconstruction error 25.642601\n",
      ">> Epoch 37 finished \tRBM Reconstruction error 40.658470\n",
      ">> Epoch 38 finished \tRBM Reconstruction error 22.271059\n",
      ">> Epoch 39 finished \tRBM Reconstruction error 39.835522\n",
      ">> Epoch 40 finished \tRBM Reconstruction error 36.841530\n",
      "[END] Pre-training step\n",
      "[START] Fine tuning step:\n",
      ">> Epoch 0 finished \tANN training loss nan\n",
      ">> Epoch 1 finished \tANN training loss nan\n",
      ">> Epoch 2 finished \tANN training loss nan\n",
      ">> Epoch 3 finished \tANN training loss nan\n",
      ">> Epoch 4 finished \tANN training loss nan\n",
      ">> Epoch 5 finished \tANN training loss nan\n",
      ">> Epoch 6 finished \tANN training loss nan\n",
      ">> Epoch 7 finished \tANN training loss nan\n",
      ">> Epoch 8 finished \tANN training loss nan\n",
      ">> Epoch 9 finished \tANN training loss nan\n",
      ">> Epoch 10 finished \tANN training loss nan\n",
      ">> Epoch 11 finished \tANN training loss nan\n",
      ">> Epoch 12 finished \tANN training loss nan\n",
      ">> Epoch 13 finished \tANN training loss nan\n",
      ">> Epoch 14 finished \tANN training loss nan\n",
      ">> Epoch 15 finished \tANN training loss nan\n",
      ">> Epoch 16 finished \tANN training loss nan\n",
      ">> Epoch 17 finished \tANN training loss nan\n",
      ">> Epoch 18 finished \tANN training loss nan\n",
      ">> Epoch 19 finished \tANN training loss nan\n",
      ">> Epoch 20 finished \tANN training loss nan\n",
      ">> Epoch 21 finished \tANN training loss nan\n",
      ">> Epoch 22 finished \tANN training loss nan\n",
      ">> Epoch 23 finished \tANN training loss nan\n",
      ">> Epoch 24 finished \tANN training loss nan\n",
      ">> Epoch 25 finished \tANN training loss nan\n",
      ">> Epoch 26 finished \tANN training loss nan\n",
      ">> Epoch 27 finished \tANN training loss nan\n",
      ">> Epoch 28 finished \tANN training loss nan\n",
      ">> Epoch 29 finished \tANN training loss nan\n",
      ">> Epoch 30 finished \tANN training loss nan\n",
      ">> Epoch 31 finished \tANN training loss nan\n",
      ">> Epoch 32 finished \tANN training loss nan\n",
      ">> Epoch 33 finished \tANN training loss nan\n",
      ">> Epoch 34 finished \tANN training loss nan\n",
      ">> Epoch 35 finished \tANN training loss nan\n",
      ">> Epoch 36 finished \tANN training loss nan\n",
      ">> Epoch 37 finished \tANN training loss nan\n",
      ">> Epoch 38 finished \tANN training loss nan\n",
      ">> Epoch 39 finished \tANN training loss nan\n",
      ">> Epoch 40 finished \tANN training loss nan\n",
      ">> Epoch 41 finished \tANN training loss nan\n",
      ">> Epoch 42 finished \tANN training loss nan\n",
      ">> Epoch 43 finished \tANN training loss nan\n",
      ">> Epoch 44 finished \tANN training loss nan\n",
      ">> Epoch 45 finished \tANN training loss nan\n",
      ">> Epoch 46 finished \tANN training loss nan\n",
      ">> Epoch 47 finished \tANN training loss nan\n",
      ">> Epoch 48 finished \tANN training loss nan\n",
      ">> Epoch 49 finished \tANN training loss nan\n",
      ">> Epoch 50 finished \tANN training loss nan\n",
      ">> Epoch 51 finished \tANN training loss nan\n",
      ">> Epoch 52 finished \tANN training loss nan\n",
      ">> Epoch 53 finished \tANN training loss nan\n",
      ">> Epoch 54 finished \tANN training loss nan\n",
      ">> Epoch 55 finished \tANN training loss nan\n",
      ">> Epoch 56 finished \tANN training loss nan\n",
      ">> Epoch 57 finished \tANN training loss nan\n",
      ">> Epoch 58 finished \tANN training loss nan\n",
      ">> Epoch 59 finished \tANN training loss nan\n",
      ">> Epoch 60 finished \tANN training loss nan\n",
      ">> Epoch 61 finished \tANN training loss nan\n",
      ">> Epoch 62 finished \tANN training loss nan\n",
      ">> Epoch 63 finished \tANN training loss nan\n",
      ">> Epoch 64 finished \tANN training loss nan\n",
      ">> Epoch 65 finished \tANN training loss nan\n",
      ">> Epoch 66 finished \tANN training loss nan\n",
      ">> Epoch 67 finished \tANN training loss nan\n",
      ">> Epoch 68 finished \tANN training loss nan\n",
      ">> Epoch 69 finished \tANN training loss nan\n",
      ">> Epoch 70 finished \tANN training loss nan\n",
      ">> Epoch 71 finished \tANN training loss nan\n",
      ">> Epoch 72 finished \tANN training loss nan\n",
      ">> Epoch 73 finished \tANN training loss nan\n",
      ">> Epoch 74 finished \tANN training loss nan\n",
      ">> Epoch 75 finished \tANN training loss nan\n",
      ">> Epoch 76 finished \tANN training loss nan\n",
      ">> Epoch 77 finished \tANN training loss nan\n",
      ">> Epoch 78 finished \tANN training loss nan\n",
      ">> Epoch 79 finished \tANN training loss nan\n",
      ">> Epoch 80 finished \tANN training loss nan\n",
      ">> Epoch 81 finished \tANN training loss nan\n",
      ">> Epoch 82 finished \tANN training loss nan\n",
      ">> Epoch 83 finished \tANN training loss nan\n",
      ">> Epoch 84 finished \tANN training loss nan\n",
      ">> Epoch 85 finished \tANN training loss nan\n",
      ">> Epoch 86 finished \tANN training loss nan\n",
      ">> Epoch 87 finished \tANN training loss nan\n",
      ">> Epoch 88 finished \tANN training loss nan\n",
      ">> Epoch 89 finished \tANN training loss nan\n",
      ">> Epoch 90 finished \tANN training loss nan\n",
      ">> Epoch 91 finished \tANN training loss nan\n",
      ">> Epoch 92 finished \tANN training loss nan\n",
      ">> Epoch 93 finished \tANN training loss nan\n",
      ">> Epoch 94 finished \tANN training loss nan\n",
      ">> Epoch 95 finished \tANN training loss nan\n",
      ">> Epoch 96 finished \tANN training loss nan\n",
      ">> Epoch 97 finished \tANN training loss nan\n",
      ">> Epoch 98 finished \tANN training loss nan\n",
      ">> Epoch 99 finished \tANN training loss nan\n",
      ">> Epoch 100 finished \tANN training loss nan\n",
      ">> Epoch 101 finished \tANN training loss nan\n",
      ">> Epoch 102 finished \tANN training loss nan\n",
      ">> Epoch 103 finished \tANN training loss nan\n",
      ">> Epoch 104 finished \tANN training loss nan\n",
      ">> Epoch 105 finished \tANN training loss nan\n",
      ">> Epoch 106 finished \tANN training loss nan\n",
      ">> Epoch 107 finished \tANN training loss nan\n",
      ">> Epoch 108 finished \tANN training loss nan\n",
      ">> Epoch 109 finished \tANN training loss nan\n",
      ">> Epoch 110 finished \tANN training loss nan\n",
      ">> Epoch 111 finished \tANN training loss nan\n",
      ">> Epoch 112 finished \tANN training loss nan\n",
      ">> Epoch 113 finished \tANN training loss nan\n",
      ">> Epoch 114 finished \tANN training loss nan\n",
      ">> Epoch 115 finished \tANN training loss nan\n",
      ">> Epoch 116 finished \tANN training loss nan\n",
      ">> Epoch 117 finished \tANN training loss nan\n",
      ">> Epoch 118 finished \tANN training loss nan\n",
      ">> Epoch 119 finished \tANN training loss nan\n",
      ">> Epoch 120 finished \tANN training loss nan\n",
      ">> Epoch 121 finished \tANN training loss nan\n",
      ">> Epoch 122 finished \tANN training loss nan\n",
      ">> Epoch 123 finished \tANN training loss nan\n",
      ">> Epoch 124 finished \tANN training loss nan\n",
      ">> Epoch 125 finished \tANN training loss nan\n",
      ">> Epoch 126 finished \tANN training loss nan\n",
      ">> Epoch 127 finished \tANN training loss nan\n",
      ">> Epoch 128 finished \tANN training loss nan\n",
      ">> Epoch 129 finished \tANN training loss nan\n",
      ">> Epoch 130 finished \tANN training loss nan\n",
      ">> Epoch 131 finished \tANN training loss nan\n",
      ">> Epoch 132 finished \tANN training loss nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Epoch 133 finished \tANN training loss nan\n",
      ">> Epoch 134 finished \tANN training loss nan\n",
      ">> Epoch 135 finished \tANN training loss nan\n",
      ">> Epoch 136 finished \tANN training loss nan\n",
      ">> Epoch 137 finished \tANN training loss nan\n",
      ">> Epoch 138 finished \tANN training loss nan\n",
      ">> Epoch 139 finished \tANN training loss nan\n",
      ">> Epoch 140 finished \tANN training loss nan\n",
      ">> Epoch 141 finished \tANN training loss nan\n",
      ">> Epoch 142 finished \tANN training loss nan\n",
      ">> Epoch 143 finished \tANN training loss nan\n",
      ">> Epoch 144 finished \tANN training loss nan\n",
      ">> Epoch 145 finished \tANN training loss nan\n",
      ">> Epoch 146 finished \tANN training loss nan\n",
      ">> Epoch 147 finished \tANN training loss nan\n",
      ">> Epoch 148 finished \tANN training loss nan\n",
      ">> Epoch 149 finished \tANN training loss nan\n",
      ">> Epoch 150 finished \tANN training loss nan\n",
      ">> Epoch 151 finished \tANN training loss nan\n",
      ">> Epoch 152 finished \tANN training loss nan\n",
      ">> Epoch 153 finished \tANN training loss nan\n",
      ">> Epoch 154 finished \tANN training loss nan\n",
      ">> Epoch 155 finished \tANN training loss nan\n",
      ">> Epoch 156 finished \tANN training loss nan\n",
      ">> Epoch 157 finished \tANN training loss nan\n",
      ">> Epoch 158 finished \tANN training loss nan\n",
      ">> Epoch 159 finished \tANN training loss nan\n",
      ">> Epoch 160 finished \tANN training loss nan\n",
      ">> Epoch 161 finished \tANN training loss nan\n",
      ">> Epoch 162 finished \tANN training loss nan\n",
      ">> Epoch 163 finished \tANN training loss nan\n",
      ">> Epoch 164 finished \tANN training loss nan\n",
      ">> Epoch 165 finished \tANN training loss nan\n",
      ">> Epoch 166 finished \tANN training loss nan\n",
      ">> Epoch 167 finished \tANN training loss nan\n",
      ">> Epoch 168 finished \tANN training loss nan\n",
      ">> Epoch 169 finished \tANN training loss nan\n",
      ">> Epoch 170 finished \tANN training loss nan\n",
      ">> Epoch 171 finished \tANN training loss nan\n",
      ">> Epoch 172 finished \tANN training loss nan\n",
      ">> Epoch 173 finished \tANN training loss nan\n",
      ">> Epoch 174 finished \tANN training loss nan\n",
      ">> Epoch 175 finished \tANN training loss nan\n",
      ">> Epoch 176 finished \tANN training loss nan\n",
      ">> Epoch 177 finished \tANN training loss nan\n",
      ">> Epoch 178 finished \tANN training loss nan\n",
      ">> Epoch 179 finished \tANN training loss nan\n",
      ">> Epoch 180 finished \tANN training loss nan\n",
      ">> Epoch 181 finished \tANN training loss nan\n",
      ">> Epoch 182 finished \tANN training loss nan\n",
      ">> Epoch 183 finished \tANN training loss nan\n",
      ">> Epoch 184 finished \tANN training loss nan\n",
      ">> Epoch 185 finished \tANN training loss nan\n",
      ">> Epoch 186 finished \tANN training loss nan\n",
      ">> Epoch 187 finished \tANN training loss nan\n",
      ">> Epoch 188 finished \tANN training loss nan\n",
      ">> Epoch 189 finished \tANN training loss nan\n",
      ">> Epoch 190 finished \tANN training loss nan\n",
      ">> Epoch 191 finished \tANN training loss nan\n",
      ">> Epoch 192 finished \tANN training loss nan\n",
      ">> Epoch 193 finished \tANN training loss nan\n",
      ">> Epoch 194 finished \tANN training loss nan\n",
      ">> Epoch 195 finished \tANN training loss nan\n",
      ">> Epoch 196 finished \tANN training loss nan\n",
      ">> Epoch 197 finished \tANN training loss nan\n",
      ">> Epoch 198 finished \tANN training loss nan\n",
      ">> Epoch 199 finished \tANN training loss nan\n",
      "[END] Fine tuning step\n",
      "Done.\n",
      "Accuracy: 0.100000\n"
     ]
    }
   ],
   "source": [
    "# ReLu, dropout = 1\n",
    "acc10 = deep_belief_net(hidden_layers_structure=[300], \n",
    "                       learning_rate_rbm=0.05,\n",
    "                       learning_rate=0.1,\n",
    "                       n_epochs_rbm=40,\n",
    "                       n_iter_backprop=200,\n",
    "                       batch_size=32,\n",
    "                       activation_function='relu',\n",
    "                       dropout_p=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.1\n"
     ]
    }
   ],
   "source": [
    "print('ACCURACY: ' + str(acc10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEeCAYAAABlggnIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1cd02425cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create bar graph\n",
    "plt.clf()\n",
    "objects = ('0', '0.1', '0.2', '0.3', '0.4', '0.5', '0.6', '0.7', '0.8', '0.9', '1')\n",
    "y_pos = np.arange(len(objects))\n",
    "performance = [0.88, 0.89, 0.92, 0.90, 0.89, 0.89, 0.87, 0.90, 0.80, 0.155, 0.1]\n",
    "plt.bar(y_pos, performance, align='center', alpha=1)\n",
    "plt.xticks(y_pos, objects, rotation=30)\n",
    "plt.xlabel('dropout_p')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Benchmark 2, relu Settings')\n",
    " \n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import SVM modules\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Linear SVC Performance\n",
    "def linSVC(C):\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
    "    clf = LinearSVC(C=C)\n",
    "    clf.fit(X_train, Y_train)\n",
    "    Y_pred = clf.predict(X_test)\n",
    "    correct = 0\n",
    "    for i in range(0, len(Y_test)):\n",
    "        if Y_pred[i] == Y_test[i]:\n",
    "            correct += 1\n",
    "    return float(correct / len(Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.86\n"
     ]
    }
   ],
   "source": [
    "linSVC_acc1 = linSVC(C=0.1)\n",
    "print('ACCURACY: ' + str(linSVC_acc1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.845\n"
     ]
    }
   ],
   "source": [
    "linSVC_acc2 = linSVC(C=1.)\n",
    "print('ACCURACY: ' + str(linSVC_acc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.84\n"
     ]
    }
   ],
   "source": [
    "linSVC_acc3 = linSVC(C=10)\n",
    "print('ACCURACY: ' + str(linSVC_acc3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.84\n"
     ]
    }
   ],
   "source": [
    "linSVC_acc4 = linSVC(C=100)\n",
    "print('ACCURACY: ' + str(linSVC_acc4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.665\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree Performance\n",
    "def decision_tree():\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
    "    clf = DecisionTreeClassifier()\n",
    "    clf.fit(X_train, Y_train)\n",
    "    Y_pred = clf.predict(X_test)\n",
    "    correct = 0\n",
    "    for i in range(0, len(Y_test)):\n",
    "        if Y_pred[i] == Y_test[i]:\n",
    "            correct += 1\n",
    "    return float(correct / len(Y_test))\n",
    "\n",
    "decision_tree_acc = decision_tree()\n",
    "print('ACCURACY: ' + str(decision_tree_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.395\n"
     ]
    }
   ],
   "source": [
    "# Boosting Performance\n",
    "def boosting():\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
    "    clf = AdaBoostClassifier()\n",
    "    clf.fit(X_train, Y_train)\n",
    "    Y_pred = clf.predict(X_test)\n",
    "    correct = 0\n",
    "    for i in range(0, len(Y_test)):\n",
    "        if Y_pred[i] == Y_test[i]:\n",
    "            correct += 1\n",
    "    return float(correct / len(Y_test))\n",
    "\n",
    "boosting_acc = boosting()\n",
    "print('ACCURACY: ' + str(boosting_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.795\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Performance\n",
    "def random_forest():\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
    "    clf = RandomForestClassifier()\n",
    "    clf.fit(X_train, Y_train)\n",
    "    Y_pred = clf.predict(X_test)\n",
    "    correct = 0\n",
    "    for i in range(0, len(Y_test)):\n",
    "        if Y_pred[i] == Y_test[i]:\n",
    "            correct += 1\n",
    "    return float(correct / len(Y_test))\n",
    "\n",
    "random_forest_acc = random_forest()\n",
    "print('ACCURACY: ' + str(random_forest_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SVC, 3rd degree rbf\n",
    "def svc_classifier(C):\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
    "    clf = SVC(C=C, max_iter=-1)\n",
    "    clf.fit(X_train, Y_train)\n",
    "    Y_pred = clf.predict(X_test)\n",
    "    correct = 0\n",
    "    for i in range(0, len(Y_test)):\n",
    "        if Y_pred[i] == Y_test[i]:\n",
    "            correct += 1\n",
    "    return float(correct / len(Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.115\n"
     ]
    }
   ],
   "source": [
    "svc1_acc = svc_classifier(0.1)\n",
    "print('ACCURACY: ' + str(svc1_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.805\n"
     ]
    }
   ],
   "source": [
    "svc2_acc = svc_classifier(1.0)\n",
    "print('ACCURACY: ' + str(svc2_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.885\n"
     ]
    }
   ],
   "source": [
    "svc3_acc = svc_classifier(10.)\n",
    "print('ACCURACY: ' + str(svc3_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCURACY: 0.895\n"
     ]
    }
   ],
   "source": [
    "svc4_acc = svc_classifier(100.)\n",
    "print('ACCURACY: ' + str(svc4_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAFdCAYAAADsTnEKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1cd008afdd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create bar graph\n",
    "plt.clf()\n",
    "objects = ('ReLu (0.2)', 'Sigmoid (0.1)', \n",
    "           'LinearSVC (0.1)', 'LinearSVC (1)', 'LinearSVC (10)', 'LinearSVC (100)', \n",
    "           'DecisionTree', 'AdaBoost', 'RandomForest', \n",
    "           'SVC (0.1)', 'SVC (1)', 'SVC (10)', 'SVC (100)')\n",
    "\n",
    "y_pos = np.arange(len(objects))\n",
    "performance = [0.92, 0.92, \n",
    "               0.86, 0.845, 0.84, 0.84,\n",
    "               0.665, 0.395, 0.795,\n",
    "               0.115, 0.805, 0.885, 0.895]\n",
    "plt.bar(y_pos, performance, align='center', alpha=1)\n",
    "plt.xticks(y_pos, objects, rotation=90)\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Accuracy, Performance')\n",
    "plt.title('Model Comparisons')\n",
    " \n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

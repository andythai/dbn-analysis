<!DOCTYPE html>
<html>
<head><meta charset="utf-8" />
<title>DBN</title><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.1.10/require.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<style type="text/css">
    /*!
*
* Twitter Bootstrap
*
*/
/*!
 * Bootstrap v3.3.7 (http://getbootstrap.com)
 * Copyright 2011-2016 Twitter, Inc.
 * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE)
 */
/*! normalize.css v3.0.3 | MIT License | github.com/necolas/normalize.css */
html {
  font-family: sans-serif;
  -ms-text-size-adjust: 100%;
  -webkit-text-size-adjust: 100%;
}
body {
  margin: 0;
}
article,
aside,
details,
figcaption,
figure,
footer,
header,
hgroup,
main,
menu,
nav,
section,
summary {
  display: block;
}
audio,
canvas,
progress,
video {
  display: inline-block;
  vertical-align: baseline;
}
audio:not([controls]) {
  display: none;
  height: 0;
}
[hidden],
template {
  display: none;
}
a {
  background-color: transparent;
}
a:active,
a:hover {
  outline: 0;
}
abbr[title] {
  border-bottom: 1px dotted;
}
b,
strong {
  font-weight: bold;
}
dfn {
  font-style: italic;
}
h1 {
  font-size: 2em;
  margin: 0.67em 0;
}
mark {
  background: #ff0;
  color: #000;
}
small {
  font-size: 80%;
}
sub,
sup {
  font-size: 75%;
  line-height: 0;
  position: relative;
  vertical-align: baseline;
}
sup {
  top: -0.5em;
}
sub {
  bottom: -0.25em;
}
img {
  border: 0;
}
svg:not(:root) {
  overflow: hidden;
}
figure {
  margin: 1em 40px;
}
hr {
  box-sizing: content-box;
  height: 0;
}
pre {
  overflow: auto;
}
code,
kbd,
pre,
samp {
  font-family: monospace, monospace;
  font-size: 1em;
}
button,
input,
optgroup,
select,
textarea {
  color: inherit;
  font: inherit;
  margin: 0;
}
button {
  overflow: visible;
}
button,
select {
  text-transform: none;
}
button,
html input[type="button"],
input[type="reset"],
input[type="submit"] {
  -webkit-appearance: button;
  cursor: pointer;
}
button[disabled],
html input[disabled] {
  cursor: default;
}
button::-moz-focus-inner,
input::-moz-focus-inner {
  border: 0;
  padding: 0;
}
input {
  line-height: normal;
}
input[type="checkbox"],
input[type="radio"] {
  box-sizing: border-box;
  padding: 0;
}
input[type="number"]::-webkit-inner-spin-button,
input[type="number"]::-webkit-outer-spin-button {
  height: auto;
}
input[type="search"] {
  -webkit-appearance: textfield;
  box-sizing: content-box;
}
input[type="search"]::-webkit-search-cancel-button,
input[type="search"]::-webkit-search-decoration {
  -webkit-appearance: none;
}
fieldset {
  border: 1px solid #c0c0c0;
  margin: 0 2px;
  padding: 0.35em 0.625em 0.75em;
}
legend {
  border: 0;
  padding: 0;
}
textarea {
  overflow: auto;
}
optgroup {
  font-weight: bold;
}
table {
  border-collapse: collapse;
  border-spacing: 0;
}
td,
th {
  padding: 0;
}
/*! Source: https://github.com/h5bp/html5-boilerplate/blob/master/src/css/main.css */
@media print {
  *,
  *:before,
  *:after {
    background: transparent !important;
    color: #000 !important;
    box-shadow: none !important;
    text-shadow: none !important;
  }
  a,
  a:visited {
    text-decoration: underline;
  }
  a[href]:after {
    content: " (" attr(href) ")";
  }
  abbr[title]:after {
    content: " (" attr(title) ")";
  }
  a[href^="#"]:after,
  a[href^="javascript:"]:after {
    content: "";
  }
  pre,
  blockquote {
    border: 1px solid #999;
    page-break-inside: avoid;
  }
  thead {
    display: table-header-group;
  }
  tr,
  img {
    page-break-inside: avoid;
  }
  img {
    max-width: 100% !important;
  }
  p,
  h2,
  h3 {
    orphans: 3;
    widows: 3;
  }
  h2,
  h3 {
    page-break-after: avoid;
  }
  .navbar {
    display: none;
  }
  .btn > .caret,
  .dropup > .btn > .caret {
    border-top-color: #000 !important;
  }
  .label {
    border: 1px solid #000;
  }
  .table {
    border-collapse: collapse !important;
  }
  .table td,
  .table th {
    background-color: #fff !important;
  }
  .table-bordered th,
  .table-bordered td {
    border: 1px solid #ddd !important;
  }
}
@font-face {
  font-family: 'Glyphicons Halflings';
  src: url('../components/bootstrap/fonts/glyphicons-halflings-regular.eot');
  src: url('../components/bootstrap/fonts/glyphicons-halflings-regular.eot?#iefix') format('embedded-opentype'), url('../components/bootstrap/fonts/glyphicons-halflings-regular.woff2') format('woff2'), url('../components/bootstrap/fonts/glyphicons-halflings-regular.woff') format('woff'), url('../components/bootstrap/fonts/glyphicons-halflings-regular.ttf') format('truetype'), url('../components/bootstrap/fonts/glyphicons-halflings-regular.svg#glyphicons_halflingsregular') format('svg');
}
.glyphicon {
  position: relative;
  top: 1px;
  display: inline-block;
  font-family: 'Glyphicons Halflings';
  font-style: normal;
  font-weight: normal;
  line-height: 1;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}
.glyphicon-asterisk:before {
  content: "\002a";
}
.glyphicon-plus:before {
  content: "\002b";
}
.glyphicon-euro:before,
.glyphicon-eur:before {
  content: "\20ac";
}
.glyphicon-minus:before {
  content: "\2212";
}
.glyphicon-cloud:before {
  content: "\2601";
}
.glyphicon-envelope:before {
  content: "\2709";
}
.glyphicon-pencil:before {
  content: "\270f";
}
.glyphicon-glass:before {
  content: "\e001";
}
.glyphicon-music:before {
  content: "\e002";
}
.glyphicon-search:before {
  content: "\e003";
}
.glyphicon-heart:before {
  content: "\e005";
}
.glyphicon-star:before {
  content: "\e006";
}
.glyphicon-star-empty:before {
  content: "\e007";
}
.glyphicon-user:before {
  content: "\e008";
}
.glyphicon-film:before {
  content: "\e009";
}
.glyphicon-th-large:before {
  content: "\e010";
}
.glyphicon-th:before {
  content: "\e011";
}
.glyphicon-th-list:before {
  content: "\e012";
}
.glyphicon-ok:before {
  content: "\e013";
}
.glyphicon-remove:before {
  content: "\e014";
}
.glyphicon-zoom-in:before {
  content: "\e015";
}
.glyphicon-zoom-out:before {
  content: "\e016";
}
.glyphicon-off:before {
  content: "\e017";
}
.glyphicon-signal:before {
  content: "\e018";
}
.glyphicon-cog:before {
  content: "\e019";
}
.glyphicon-trash:before {
  content: "\e020";
}
.glyphicon-home:before {
  content: "\e021";
}
.glyphicon-file:before {
  content: "\e022";
}
.glyphicon-time:before {
  content: "\e023";
}
.glyphicon-road:before {
  content: "\e024";
}
.glyphicon-download-alt:before {
  content: "\e025";
}
.glyphicon-download:before {
  content: "\e026";
}
.glyphicon-upload:before {
  content: "\e027";
}
.glyphicon-inbox:before {
  content: "\e028";
}
.glyphicon-play-circle:before {
  content: "\e029";
}
.glyphicon-repeat:before {
  content: "\e030";
}
.glyphicon-refresh:before {
  content: "\e031";
}
.glyphicon-list-alt:before {
  content: "\e032";
}
.glyphicon-lock:before {
  content: "\e033";
}
.glyphicon-flag:before {
  content: "\e034";
}
.glyphicon-headphones:before {
  content: "\e035";
}
.glyphicon-volume-off:before {
  content: "\e036";
}
.glyphicon-volume-down:before {
  content: "\e037";
}
.glyphicon-volume-up:before {
  content: "\e038";
}
.glyphicon-qrcode:before {
  content: "\e039";
}
.glyphicon-barcode:before {
  content: "\e040";
}
.glyphicon-tag:before {
  content: "\e041";
}
.glyphicon-tags:before {
  content: "\e042";
}
.glyphicon-book:before {
  content: "\e043";
}
.glyphicon-bookmark:before {
  content: "\e044";
}
.glyphicon-print:before {
  content: "\e045";
}
.glyphicon-camera:before {
  content: "\e046";
}
.glyphicon-font:before {
  content: "\e047";
}
.glyphicon-bold:before {
  content: "\e048";
}
.glyphicon-italic:before {
  content: "\e049";
}
.glyphicon-text-height:before {
  content: "\e050";
}
.glyphicon-text-width:before {
  content: "\e051";
}
.glyphicon-align-left:before {
  content: "\e052";
}
.glyphicon-align-center:before {
  content: "\e053";
}
.glyphicon-align-right:before {
  content: "\e054";
}
.glyphicon-align-justify:before {
  content: "\e055";
}
.glyphicon-list:before {
  content: "\e056";
}
.glyphicon-indent-left:before {
  content: "\e057";
}
.glyphicon-indent-right:before {
  content: "\e058";
}
.glyphicon-facetime-video:before {
  content: "\e059";
}
.glyphicon-picture:before {
  content: "\e060";
}
.glyphicon-map-marker:before {
  content: "\e062";
}
.glyphicon-adjust:before {
  content: "\e063";
}
.glyphicon-tint:before {
  content: "\e064";
}
.glyphicon-edit:before {
  content: "\e065";
}
.glyphicon-share:before {
  content: "\e066";
}
.glyphicon-check:before {
  content: "\e067";
}
.glyphicon-move:before {
  content: "\e068";
}
.glyphicon-step-backward:before {
  content: "\e069";
}
.glyphicon-fast-backward:before {
  content: "\e070";
}
.glyphicon-backward:before {
  content: "\e071";
}
.glyphicon-play:before {
  content: "\e072";
}
.glyphicon-pause:before {
  content: "\e073";
}
.glyphicon-stop:before {
  content: "\e074";
}
.glyphicon-forward:before {
  content: "\e075";
}
.glyphicon-fast-forward:before {
  content: "\e076";
}
.glyphicon-step-forward:before {
  content: "\e077";
}
.glyphicon-eject:before {
  content: "\e078";
}
.glyphicon-chevron-left:before {
  content: "\e079";
}
.glyphicon-chevron-right:before {
  content: "\e080";
}
.glyphicon-plus-sign:before {
  content: "\e081";
}
.glyphicon-minus-sign:before {
  content: "\e082";
}
.glyphicon-remove-sign:before {
  content: "\e083";
}
.glyphicon-ok-sign:before {
  content: "\e084";
}
.glyphicon-question-sign:before {
  content: "\e085";
}
.glyphicon-info-sign:before {
  content: "\e086";
}
.glyphicon-screenshot:before {
  content: "\e087";
}
.glyphicon-remove-circle:before {
  content: "\e088";
}
.glyphicon-ok-circle:before {
  content: "\e089";
}
.glyphicon-ban-circle:before {
  content: "\e090";
}
.glyphicon-arrow-left:before {
  content: "\e091";
}
.glyphicon-arrow-right:before {
  content: "\e092";
}
.glyphicon-arrow-up:before {
  content: "\e093";
}
.glyphicon-arrow-down:before {
  content: "\e094";
}
.glyphicon-share-alt:before {
  content: "\e095";
}
.glyphicon-resize-full:before {
  content: "\e096";
}
.glyphicon-resize-small:before {
  content: "\e097";
}
.glyphicon-exclamation-sign:before {
  content: "\e101";
}
.glyphicon-gift:before {
  content: "\e102";
}
.glyphicon-leaf:before {
  content: "\e103";
}
.glyphicon-fire:before {
  content: "\e104";
}
.glyphicon-eye-open:before {
  content: "\e105";
}
.glyphicon-eye-close:before {
  content: "\e106";
}
.glyphicon-warning-sign:before {
  content: "\e107";
}
.glyphicon-plane:before {
  content: "\e108";
}
.glyphicon-calendar:before {
  content: "\e109";
}
.glyphicon-random:before {
  content: "\e110";
}
.glyphicon-comment:before {
  content: "\e111";
}
.glyphicon-magnet:before {
  content: "\e112";
}
.glyphicon-chevron-up:before {
  content: "\e113";
}
.glyphicon-chevron-down:before {
  content: "\e114";
}
.glyphicon-retweet:before {
  content: "\e115";
}
.glyphicon-shopping-cart:before {
  content: "\e116";
}
.glyphicon-folder-close:before {
  content: "\e117";
}
.glyphicon-folder-open:before {
  content: "\e118";
}
.glyphicon-resize-vertical:before {
  content: "\e119";
}
.glyphicon-resize-horizontal:before {
  content: "\e120";
}
.glyphicon-hdd:before {
  content: "\e121";
}
.glyphicon-bullhorn:before {
  content: "\e122";
}
.glyphicon-bell:before {
  content: "\e123";
}
.glyphicon-certificate:before {
  content: "\e124";
}
.glyphicon-thumbs-up:before {
  content: "\e125";
}
.glyphicon-thumbs-down:before {
  content: "\e126";
}
.glyphicon-hand-right:before {
  content: "\e127";
}
.glyphicon-hand-left:before {
  content: "\e128";
}
.glyphicon-hand-up:before {
  content: "\e129";
}
.glyphicon-hand-down:before {
  content: "\e130";
}
.glyphicon-circle-arrow-right:before {
  content: "\e131";
}
.glyphicon-circle-arrow-left:before {
  content: "\e132";
}
.glyphicon-circle-arrow-up:before {
  content: "\e133";
}
.glyphicon-circle-arrow-down:before {
  content: "\e134";
}
.glyphicon-globe:before {
  content: "\e135";
}
.glyphicon-wrench:before {
  content: "\e136";
}
.glyphicon-tasks:before {
  content: "\e137";
}
.glyphicon-filter:before {
  content: "\e138";
}
.glyphicon-briefcase:before {
  content: "\e139";
}
.glyphicon-fullscreen:before {
  content: "\e140";
}
.glyphicon-dashboard:before {
  content: "\e141";
}
.glyphicon-paperclip:before {
  content: "\e142";
}
.glyphicon-heart-empty:before {
  content: "\e143";
}
.glyphicon-link:before {
  content: "\e144";
}
.glyphicon-phone:before {
  content: "\e145";
}
.glyphicon-pushpin:before {
  content: "\e146";
}
.glyphicon-usd:before {
  content: "\e148";
}
.glyphicon-gbp:before {
  content: "\e149";
}
.glyphicon-sort:before {
  content: "\e150";
}
.glyphicon-sort-by-alphabet:before {
  content: "\e151";
}
.glyphicon-sort-by-alphabet-alt:before {
  content: "\e152";
}
.glyphicon-sort-by-order:before {
  content: "\e153";
}
.glyphicon-sort-by-order-alt:before {
  content: "\e154";
}
.glyphicon-sort-by-attributes:before {
  content: "\e155";
}
.glyphicon-sort-by-attributes-alt:before {
  content: "\e156";
}
.glyphicon-unchecked:before {
  content: "\e157";
}
.glyphicon-expand:before {
  content: "\e158";
}
.glyphicon-collapse-down:before {
  content: "\e159";
}
.glyphicon-collapse-up:before {
  content: "\e160";
}
.glyphicon-log-in:before {
  content: "\e161";
}
.glyphicon-flash:before {
  content: "\e162";
}
.glyphicon-log-out:before {
  content: "\e163";
}
.glyphicon-new-window:before {
  content: "\e164";
}
.glyphicon-record:before {
  content: "\e165";
}
.glyphicon-save:before {
  content: "\e166";
}
.glyphicon-open:before {
  content: "\e167";
}
.glyphicon-saved:before {
  content: "\e168";
}
.glyphicon-import:before {
  content: "\e169";
}
.glyphicon-export:before {
  content: "\e170";
}
.glyphicon-send:before {
  content: "\e171";
}
.glyphicon-floppy-disk:before {
  content: "\e172";
}
.glyphicon-floppy-saved:before {
  content: "\e173";
}
.glyphicon-floppy-remove:before {
  content: "\e174";
}
.glyphicon-floppy-save:before {
  content: "\e175";
}
.glyphicon-floppy-open:before {
  content: "\e176";
}
.glyphicon-credit-card:before {
  content: "\e177";
}
.glyphicon-transfer:before {
  content: "\e178";
}
.glyphicon-cutlery:before {
  content: "\e179";
}
.glyphicon-header:before {
  content: "\e180";
}
.glyphicon-compressed:before {
  content: "\e181";
}
.glyphicon-earphone:before {
  content: "\e182";
}
.glyphicon-phone-alt:before {
  content: "\e183";
}
.glyphicon-tower:before {
  content: "\e184";
}
.glyphicon-stats:before {
  content: "\e185";
}
.glyphicon-sd-video:before {
  content: "\e186";
}
.glyphicon-hd-video:before {
  content: "\e187";
}
.glyphicon-subtitles:before {
  content: "\e188";
}
.glyphicon-sound-stereo:before {
  content: "\e189";
}
.glyphicon-sound-dolby:before {
  content: "\e190";
}
.glyphicon-sound-5-1:before {
  content: "\e191";
}
.glyphicon-sound-6-1:before {
  content: "\e192";
}
.glyphicon-sound-7-1:before {
  content: "\e193";
}
.glyphicon-copyright-mark:before {
  content: "\e194";
}
.glyphicon-registration-mark:before {
  content: "\e195";
}
.glyphicon-cloud-download:before {
  content: "\e197";
}
.glyphicon-cloud-upload:before {
  content: "\e198";
}
.glyphicon-tree-conifer:before {
  content: "\e199";
}
.glyphicon-tree-deciduous:before {
  content: "\e200";
}
.glyphicon-cd:before {
  content: "\e201";
}
.glyphicon-save-file:before {
  content: "\e202";
}
.glyphicon-open-file:before {
  content: "\e203";
}
.glyphicon-level-up:before {
  content: "\e204";
}
.glyphicon-copy:before {
  content: "\e205";
}
.glyphicon-paste:before {
  content: "\e206";
}
.glyphicon-alert:before {
  content: "\e209";
}
.glyphicon-equalizer:before {
  content: "\e210";
}
.glyphicon-king:before {
  content: "\e211";
}
.glyphicon-queen:before {
  content: "\e212";
}
.glyphicon-pawn:before {
  content: "\e213";
}
.glyphicon-bishop:before {
  content: "\e214";
}
.glyphicon-knight:before {
  content: "\e215";
}
.glyphicon-baby-formula:before {
  content: "\e216";
}
.glyphicon-tent:before {
  content: "\26fa";
}
.glyphicon-blackboard:before {
  content: "\e218";
}
.glyphicon-bed:before {
  content: "\e219";
}
.glyphicon-apple:before {
  content: "\f8ff";
}
.glyphicon-erase:before {
  content: "\e221";
}
.glyphicon-hourglass:before {
  content: "\231b";
}
.glyphicon-lamp:before {
  content: "\e223";
}
.glyphicon-duplicate:before {
  content: "\e224";
}
.glyphicon-piggy-bank:before {
  content: "\e225";
}
.glyphicon-scissors:before {
  content: "\e226";
}
.glyphicon-bitcoin:before {
  content: "\e227";
}
.glyphicon-btc:before {
  content: "\e227";
}
.glyphicon-xbt:before {
  content: "\e227";
}
.glyphicon-yen:before {
  content: "\00a5";
}
.glyphicon-jpy:before {
  content: "\00a5";
}
.glyphicon-ruble:before {
  content: "\20bd";
}
.glyphicon-rub:before {
  content: "\20bd";
}
.glyphicon-scale:before {
  content: "\e230";
}
.glyphicon-ice-lolly:before {
  content: "\e231";
}
.glyphicon-ice-lolly-tasted:before {
  content: "\e232";
}
.glyphicon-education:before {
  content: "\e233";
}
.glyphicon-option-horizontal:before {
  content: "\e234";
}
.glyphicon-option-vertical:before {
  content: "\e235";
}
.glyphicon-menu-hamburger:before {
  content: "\e236";
}
.glyphicon-modal-window:before {
  content: "\e237";
}
.glyphicon-oil:before {
  content: "\e238";
}
.glyphicon-grain:before {
  content: "\e239";
}
.glyphicon-sunglasses:before {
  content: "\e240";
}
.glyphicon-text-size:before {
  content: "\e241";
}
.glyphicon-text-color:before {
  content: "\e242";
}
.glyphicon-text-background:before {
  content: "\e243";
}
.glyphicon-object-align-top:before {
  content: "\e244";
}
.glyphicon-object-align-bottom:before {
  content: "\e245";
}
.glyphicon-object-align-horizontal:before {
  content: "\e246";
}
.glyphicon-object-align-left:before {
  content: "\e247";
}
.glyphicon-object-align-vertical:before {
  content: "\e248";
}
.glyphicon-object-align-right:before {
  content: "\e249";
}
.glyphicon-triangle-right:before {
  content: "\e250";
}
.glyphicon-triangle-left:before {
  content: "\e251";
}
.glyphicon-triangle-bottom:before {
  content: "\e252";
}
.glyphicon-triangle-top:before {
  content: "\e253";
}
.glyphicon-console:before {
  content: "\e254";
}
.glyphicon-superscript:before {
  content: "\e255";
}
.glyphicon-subscript:before {
  content: "\e256";
}
.glyphicon-menu-left:before {
  content: "\e257";
}
.glyphicon-menu-right:before {
  content: "\e258";
}
.glyphicon-menu-down:before {
  content: "\e259";
}
.glyphicon-menu-up:before {
  content: "\e260";
}
* {
  -webkit-box-sizing: border-box;
  -moz-box-sizing: border-box;
  box-sizing: border-box;
}
*:before,
*:after {
  -webkit-box-sizing: border-box;
  -moz-box-sizing: border-box;
  box-sizing: border-box;
}
html {
  font-size: 10px;
  -webkit-tap-highlight-color: rgba(0, 0, 0, 0);
}
body {
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
  font-size: 13px;
  line-height: 1.42857143;
  color: #000;
  background-color: #fff;
}
input,
button,
select,
textarea {
  font-family: inherit;
  font-size: inherit;
  line-height: inherit;
}
a {
  color: #337ab7;
  text-decoration: none;
}
a:hover,
a:focus {
  color: #23527c;
  text-decoration: underline;
}
a:focus {
  outline: 5px auto -webkit-focus-ring-color;
  outline-offset: -2px;
}
figure {
  margin: 0;
}
img {
  vertical-align: middle;
}
.img-responsive,
.thumbnail > img,
.thumbnail a > img,
.carousel-inner > .item > img,
.carousel-inner > .item > a > img {
  display: block;
  max-width: 100%;
  height: auto;
}
.img-rounded {
  border-radius: 3px;
}
.img-thumbnail {
  padding: 4px;
  line-height: 1.42857143;
  background-color: #fff;
  border: 1px solid #ddd;
  border-radius: 2px;
  -webkit-transition: all 0.2s ease-in-out;
  -o-transition: all 0.2s ease-in-out;
  transition: all 0.2s ease-in-out;
  display: inline-block;
  max-width: 100%;
  height: auto;
}
.img-circle {
  border-radius: 50%;
}
hr {
  margin-top: 18px;
  margin-bottom: 18px;
  border: 0;
  border-top: 1px solid #eeeeee;
}
.sr-only {
  position: absolute;
  width: 1px;
  height: 1px;
  margin: -1px;
  padding: 0;
  overflow: hidden;
  clip: rect(0, 0, 0, 0);
  border: 0;
}
.sr-only-focusable:active,
.sr-only-focusable:focus {
  position: static;
  width: auto;
  height: auto;
  margin: 0;
  overflow: visible;
  clip: auto;
}
[role="button"] {
  cursor: pointer;
}
h1,
h2,
h3,
h4,
h5,
h6,
.h1,
.h2,
.h3,
.h4,
.h5,
.h6 {
  font-family: inherit;
  font-weight: 500;
  line-height: 1.1;
  color: inherit;
}
h1 small,
h2 small,
h3 small,
h4 small,
h5 small,
h6 small,
.h1 small,
.h2 small,
.h3 small,
.h4 small,
.h5 small,
.h6 small,
h1 .small,
h2 .small,
h3 .small,
h4 .small,
h5 .small,
h6 .small,
.h1 .small,
.h2 .small,
.h3 .small,
.h4 .small,
.h5 .small,
.h6 .small {
  font-weight: normal;
  line-height: 1;
  color: #777777;
}
h1,
.h1,
h2,
.h2,
h3,
.h3 {
  margin-top: 18px;
  margin-bottom: 9px;
}
h1 small,
.h1 small,
h2 small,
.h2 small,
h3 small,
.h3 small,
h1 .small,
.h1 .small,
h2 .small,
.h2 .small,
h3 .small,
.h3 .small {
  font-size: 65%;
}
h4,
.h4,
h5,
.h5,
h6,
.h6 {
  margin-top: 9px;
  margin-bottom: 9px;
}
h4 small,
.h4 small,
h5 small,
.h5 small,
h6 small,
.h6 small,
h4 .small,
.h4 .small,
h5 .small,
.h5 .small,
h6 .small,
.h6 .small {
  font-size: 75%;
}
h1,
.h1 {
  font-size: 33px;
}
h2,
.h2 {
  font-size: 27px;
}
h3,
.h3 {
  font-size: 23px;
}
h4,
.h4 {
  font-size: 17px;
}
h5,
.h5 {
  font-size: 13px;
}
h6,
.h6 {
  font-size: 12px;
}
p {
  margin: 0 0 9px;
}
.lead {
  margin-bottom: 18px;
  font-size: 14px;
  font-weight: 300;
  line-height: 1.4;
}
@media (min-width: 768px) {
  .lead {
    font-size: 19.5px;
  }
}
small,
.small {
  font-size: 92%;
}
mark,
.mark {
  background-color: #fcf8e3;
  padding: .2em;
}
.text-left {
  text-align: left;
}
.text-right {
  text-align: right;
}
.text-center {
  text-align: center;
}
.text-justify {
  text-align: justify;
}
.text-nowrap {
  white-space: nowrap;
}
.text-lowercase {
  text-transform: lowercase;
}
.text-uppercase {
  text-transform: uppercase;
}
.text-capitalize {
  text-transform: capitalize;
}
.text-muted {
  color: #777777;
}
.text-primary {
  color: #337ab7;
}
a.text-primary:hover,
a.text-primary:focus {
  color: #286090;
}
.text-success {
  color: #3c763d;
}
a.text-success:hover,
a.text-success:focus {
  color: #2b542c;
}
.text-info {
  color: #31708f;
}
a.text-info:hover,
a.text-info:focus {
  color: #245269;
}
.text-warning {
  color: #8a6d3b;
}
a.text-warning:hover,
a.text-warning:focus {
  color: #66512c;
}
.text-danger {
  color: #a94442;
}
a.text-danger:hover,
a.text-danger:focus {
  color: #843534;
}
.bg-primary {
  color: #fff;
  background-color: #337ab7;
}
a.bg-primary:hover,
a.bg-primary:focus {
  background-color: #286090;
}
.bg-success {
  background-color: #dff0d8;
}
a.bg-success:hover,
a.bg-success:focus {
  background-color: #c1e2b3;
}
.bg-info {
  background-color: #d9edf7;
}
a.bg-info:hover,
a.bg-info:focus {
  background-color: #afd9ee;
}
.bg-warning {
  background-color: #fcf8e3;
}
a.bg-warning:hover,
a.bg-warning:focus {
  background-color: #f7ecb5;
}
.bg-danger {
  background-color: #f2dede;
}
a.bg-danger:hover,
a.bg-danger:focus {
  background-color: #e4b9b9;
}
.page-header {
  padding-bottom: 8px;
  margin: 36px 0 18px;
  border-bottom: 1px solid #eeeeee;
}
ul,
ol {
  margin-top: 0;
  margin-bottom: 9px;
}
ul ul,
ol ul,
ul ol,
ol ol {
  margin-bottom: 0;
}
.list-unstyled {
  padding-left: 0;
  list-style: none;
}
.list-inline {
  padding-left: 0;
  list-style: none;
  margin-left: -5px;
}
.list-inline > li {
  display: inline-block;
  padding-left: 5px;
  padding-right: 5px;
}
dl {
  margin-top: 0;
  margin-bottom: 18px;
}
dt,
dd {
  line-height: 1.42857143;
}
dt {
  font-weight: bold;
}
dd {
  margin-left: 0;
}
@media (min-width: 541px) {
  .dl-horizontal dt {
    float: left;
    width: 160px;
    clear: left;
    text-align: right;
    overflow: hidden;
    text-overflow: ellipsis;
    white-space: nowrap;
  }
  .dl-horizontal dd {
    margin-left: 180px;
  }
}
abbr[title],
abbr[data-original-title] {
  cursor: help;
  border-bottom: 1px dotted #777777;
}
.initialism {
  font-size: 90%;
  text-transform: uppercase;
}
blockquote {
  padding: 9px 18px;
  margin: 0 0 18px;
  font-size: inherit;
  border-left: 5px solid #eeeeee;
}
blockquote p:last-child,
blockquote ul:last-child,
blockquote ol:last-child {
  margin-bottom: 0;
}
blockquote footer,
blockquote small,
blockquote .small {
  display: block;
  font-size: 80%;
  line-height: 1.42857143;
  color: #777777;
}
blockquote footer:before,
blockquote small:before,
blockquote .small:before {
  content: '\2014 \00A0';
}
.blockquote-reverse,
blockquote.pull-right {
  padding-right: 15px;
  padding-left: 0;
  border-right: 5px solid #eeeeee;
  border-left: 0;
  text-align: right;
}
.blockquote-reverse footer:before,
blockquote.pull-right footer:before,
.blockquote-reverse small:before,
blockquote.pull-right small:before,
.blockquote-reverse .small:before,
blockquote.pull-right .small:before {
  content: '';
}
.blockquote-reverse footer:after,
blockquote.pull-right footer:after,
.blockquote-reverse small:after,
blockquote.pull-right small:after,
.blockquote-reverse .small:after,
blockquote.pull-right .small:after {
  content: '\00A0 \2014';
}
address {
  margin-bottom: 18px;
  font-style: normal;
  line-height: 1.42857143;
}
code,
kbd,
pre,
samp {
  font-family: monospace;
}
code {
  padding: 2px 4px;
  font-size: 90%;
  color: #c7254e;
  background-color: #f9f2f4;
  border-radius: 2px;
}
kbd {
  padding: 2px 4px;
  font-size: 90%;
  color: #888;
  background-color: transparent;
  border-radius: 1px;
  box-shadow: inset 0 -1px 0 rgba(0, 0, 0, 0.25);
}
kbd kbd {
  padding: 0;
  font-size: 100%;
  font-weight: bold;
  box-shadow: none;
}
pre {
  display: block;
  padding: 8.5px;
  margin: 0 0 9px;
  font-size: 12px;
  line-height: 1.42857143;
  word-break: break-all;
  word-wrap: break-word;
  color: #333333;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 2px;
}
pre code {
  padding: 0;
  font-size: inherit;
  color: inherit;
  white-space: pre-wrap;
  background-color: transparent;
  border-radius: 0;
}
.pre-scrollable {
  max-height: 340px;
  overflow-y: scroll;
}
.container {
  margin-right: auto;
  margin-left: auto;
  padding-left: 0px;
  padding-right: 0px;
}
@media (min-width: 768px) {
  .container {
    width: 768px;
  }
}
@media (min-width: 992px) {
  .container {
    width: 940px;
  }
}
@media (min-width: 1200px) {
  .container {
    width: 1140px;
  }
}
.container-fluid {
  margin-right: auto;
  margin-left: auto;
  padding-left: 0px;
  padding-right: 0px;
}
.row {
  margin-left: 0px;
  margin-right: 0px;
}
.col-xs-1, .col-sm-1, .col-md-1, .col-lg-1, .col-xs-2, .col-sm-2, .col-md-2, .col-lg-2, .col-xs-3, .col-sm-3, .col-md-3, .col-lg-3, .col-xs-4, .col-sm-4, .col-md-4, .col-lg-4, .col-xs-5, .col-sm-5, .col-md-5, .col-lg-5, .col-xs-6, .col-sm-6, .col-md-6, .col-lg-6, .col-xs-7, .col-sm-7, .col-md-7, .col-lg-7, .col-xs-8, .col-sm-8, .col-md-8, .col-lg-8, .col-xs-9, .col-sm-9, .col-md-9, .col-lg-9, .col-xs-10, .col-sm-10, .col-md-10, .col-lg-10, .col-xs-11, .col-sm-11, .col-md-11, .col-lg-11, .col-xs-12, .col-sm-12, .col-md-12, .col-lg-12 {
  position: relative;
  min-height: 1px;
  padding-left: 0px;
  padding-right: 0px;
}
.col-xs-1, .col-xs-2, .col-xs-3, .col-xs-4, .col-xs-5, .col-xs-6, .col-xs-7, .col-xs-8, .col-xs-9, .col-xs-10, .col-xs-11, .col-xs-12 {
  float: left;
}
.col-xs-12 {
  width: 100%;
}
.col-xs-11 {
  width: 91.66666667%;
}
.col-xs-10 {
  width: 83.33333333%;
}
.col-xs-9 {
  width: 75%;
}
.col-xs-8 {
  width: 66.66666667%;
}
.col-xs-7 {
  width: 58.33333333%;
}
.col-xs-6 {
  width: 50%;
}
.col-xs-5 {
  width: 41.66666667%;
}
.col-xs-4 {
  width: 33.33333333%;
}
.col-xs-3 {
  width: 25%;
}
.col-xs-2 {
  width: 16.66666667%;
}
.col-xs-1 {
  width: 8.33333333%;
}
.col-xs-pull-12 {
  right: 100%;
}
.col-xs-pull-11 {
  right: 91.66666667%;
}
.col-xs-pull-10 {
  right: 83.33333333%;
}
.col-xs-pull-9 {
  right: 75%;
}
.col-xs-pull-8 {
  right: 66.66666667%;
}
.col-xs-pull-7 {
  right: 58.33333333%;
}
.col-xs-pull-6 {
  right: 50%;
}
.col-xs-pull-5 {
  right: 41.66666667%;
}
.col-xs-pull-4 {
  right: 33.33333333%;
}
.col-xs-pull-3 {
  right: 25%;
}
.col-xs-pull-2 {
  right: 16.66666667%;
}
.col-xs-pull-1 {
  right: 8.33333333%;
}
.col-xs-pull-0 {
  right: auto;
}
.col-xs-push-12 {
  left: 100%;
}
.col-xs-push-11 {
  left: 91.66666667%;
}
.col-xs-push-10 {
  left: 83.33333333%;
}
.col-xs-push-9 {
  left: 75%;
}
.col-xs-push-8 {
  left: 66.66666667%;
}
.col-xs-push-7 {
  left: 58.33333333%;
}
.col-xs-push-6 {
  left: 50%;
}
.col-xs-push-5 {
  left: 41.66666667%;
}
.col-xs-push-4 {
  left: 33.33333333%;
}
.col-xs-push-3 {
  left: 25%;
}
.col-xs-push-2 {
  left: 16.66666667%;
}
.col-xs-push-1 {
  left: 8.33333333%;
}
.col-xs-push-0 {
  left: auto;
}
.col-xs-offset-12 {
  margin-left: 100%;
}
.col-xs-offset-11 {
  margin-left: 91.66666667%;
}
.col-xs-offset-10 {
  margin-left: 83.33333333%;
}
.col-xs-offset-9 {
  margin-left: 75%;
}
.col-xs-offset-8 {
  margin-left: 66.66666667%;
}
.col-xs-offset-7 {
  margin-left: 58.33333333%;
}
.col-xs-offset-6 {
  margin-left: 50%;
}
.col-xs-offset-5 {
  margin-left: 41.66666667%;
}
.col-xs-offset-4 {
  margin-left: 33.33333333%;
}
.col-xs-offset-3 {
  margin-left: 25%;
}
.col-xs-offset-2 {
  margin-left: 16.66666667%;
}
.col-xs-offset-1 {
  margin-left: 8.33333333%;
}
.col-xs-offset-0 {
  margin-left: 0%;
}
@media (min-width: 768px) {
  .col-sm-1, .col-sm-2, .col-sm-3, .col-sm-4, .col-sm-5, .col-sm-6, .col-sm-7, .col-sm-8, .col-sm-9, .col-sm-10, .col-sm-11, .col-sm-12 {
    float: left;
  }
  .col-sm-12 {
    width: 100%;
  }
  .col-sm-11 {
    width: 91.66666667%;
  }
  .col-sm-10 {
    width: 83.33333333%;
  }
  .col-sm-9 {
    width: 75%;
  }
  .col-sm-8 {
    width: 66.66666667%;
  }
  .col-sm-7 {
    width: 58.33333333%;
  }
  .col-sm-6 {
    width: 50%;
  }
  .col-sm-5 {
    width: 41.66666667%;
  }
  .col-sm-4 {
    width: 33.33333333%;
  }
  .col-sm-3 {
    width: 25%;
  }
  .col-sm-2 {
    width: 16.66666667%;
  }
  .col-sm-1 {
    width: 8.33333333%;
  }
  .col-sm-pull-12 {
    right: 100%;
  }
  .col-sm-pull-11 {
    right: 91.66666667%;
  }
  .col-sm-pull-10 {
    right: 83.33333333%;
  }
  .col-sm-pull-9 {
    right: 75%;
  }
  .col-sm-pull-8 {
    right: 66.66666667%;
  }
  .col-sm-pull-7 {
    right: 58.33333333%;
  }
  .col-sm-pull-6 {
    right: 50%;
  }
  .col-sm-pull-5 {
    right: 41.66666667%;
  }
  .col-sm-pull-4 {
    right: 33.33333333%;
  }
  .col-sm-pull-3 {
    right: 25%;
  }
  .col-sm-pull-2 {
    right: 16.66666667%;
  }
  .col-sm-pull-1 {
    right: 8.33333333%;
  }
  .col-sm-pull-0 {
    right: auto;
  }
  .col-sm-push-12 {
    left: 100%;
  }
  .col-sm-push-11 {
    left: 91.66666667%;
  }
  .col-sm-push-10 {
    left: 83.33333333%;
  }
  .col-sm-push-9 {
    left: 75%;
  }
  .col-sm-push-8 {
    left: 66.66666667%;
  }
  .col-sm-push-7 {
    left: 58.33333333%;
  }
  .col-sm-push-6 {
    left: 50%;
  }
  .col-sm-push-5 {
    left: 41.66666667%;
  }
  .col-sm-push-4 {
    left: 33.33333333%;
  }
  .col-sm-push-3 {
    left: 25%;
  }
  .col-sm-push-2 {
    left: 16.66666667%;
  }
  .col-sm-push-1 {
    left: 8.33333333%;
  }
  .col-sm-push-0 {
    left: auto;
  }
  .col-sm-offset-12 {
    margin-left: 100%;
  }
  .col-sm-offset-11 {
    margin-left: 91.66666667%;
  }
  .col-sm-offset-10 {
    margin-left: 83.33333333%;
  }
  .col-sm-offset-9 {
    margin-left: 75%;
  }
  .col-sm-offset-8 {
    margin-left: 66.66666667%;
  }
  .col-sm-offset-7 {
    margin-left: 58.33333333%;
  }
  .col-sm-offset-6 {
    margin-left: 50%;
  }
  .col-sm-offset-5 {
    margin-left: 41.66666667%;
  }
  .col-sm-offset-4 {
    margin-left: 33.33333333%;
  }
  .col-sm-offset-3 {
    margin-left: 25%;
  }
  .col-sm-offset-2 {
    margin-left: 16.66666667%;
  }
  .col-sm-offset-1 {
    margin-left: 8.33333333%;
  }
  .col-sm-offset-0 {
    margin-left: 0%;
  }
}
@media (min-width: 992px) {
  .col-md-1, .col-md-2, .col-md-3, .col-md-4, .col-md-5, .col-md-6, .col-md-7, .col-md-8, .col-md-9, .col-md-10, .col-md-11, .col-md-12 {
    float: left;
  }
  .col-md-12 {
    width: 100%;
  }
  .col-md-11 {
    width: 91.66666667%;
  }
  .col-md-10 {
    width: 83.33333333%;
  }
  .col-md-9 {
    width: 75%;
  }
  .col-md-8 {
    width: 66.66666667%;
  }
  .col-md-7 {
    width: 58.33333333%;
  }
  .col-md-6 {
    width: 50%;
  }
  .col-md-5 {
    width: 41.66666667%;
  }
  .col-md-4 {
    width: 33.33333333%;
  }
  .col-md-3 {
    width: 25%;
  }
  .col-md-2 {
    width: 16.66666667%;
  }
  .col-md-1 {
    width: 8.33333333%;
  }
  .col-md-pull-12 {
    right: 100%;
  }
  .col-md-pull-11 {
    right: 91.66666667%;
  }
  .col-md-pull-10 {
    right: 83.33333333%;
  }
  .col-md-pull-9 {
    right: 75%;
  }
  .col-md-pull-8 {
    right: 66.66666667%;
  }
  .col-md-pull-7 {
    right: 58.33333333%;
  }
  .col-md-pull-6 {
    right: 50%;
  }
  .col-md-pull-5 {
    right: 41.66666667%;
  }
  .col-md-pull-4 {
    right: 33.33333333%;
  }
  .col-md-pull-3 {
    right: 25%;
  }
  .col-md-pull-2 {
    right: 16.66666667%;
  }
  .col-md-pull-1 {
    right: 8.33333333%;
  }
  .col-md-pull-0 {
    right: auto;
  }
  .col-md-push-12 {
    left: 100%;
  }
  .col-md-push-11 {
    left: 91.66666667%;
  }
  .col-md-push-10 {
    left: 83.33333333%;
  }
  .col-md-push-9 {
    left: 75%;
  }
  .col-md-push-8 {
    left: 66.66666667%;
  }
  .col-md-push-7 {
    left: 58.33333333%;
  }
  .col-md-push-6 {
    left: 50%;
  }
  .col-md-push-5 {
    left: 41.66666667%;
  }
  .col-md-push-4 {
    left: 33.33333333%;
  }
  .col-md-push-3 {
    left: 25%;
  }
  .col-md-push-2 {
    left: 16.66666667%;
  }
  .col-md-push-1 {
    left: 8.33333333%;
  }
  .col-md-push-0 {
    left: auto;
  }
  .col-md-offset-12 {
    margin-left: 100%;
  }
  .col-md-offset-11 {
    margin-left: 91.66666667%;
  }
  .col-md-offset-10 {
    margin-left: 83.33333333%;
  }
  .col-md-offset-9 {
    margin-left: 75%;
  }
  .col-md-offset-8 {
    margin-left: 66.66666667%;
  }
  .col-md-offset-7 {
    margin-left: 58.33333333%;
  }
  .col-md-offset-6 {
    margin-left: 50%;
  }
  .col-md-offset-5 {
    margin-left: 41.66666667%;
  }
  .col-md-offset-4 {
    margin-left: 33.33333333%;
  }
  .col-md-offset-3 {
    margin-left: 25%;
  }
  .col-md-offset-2 {
    margin-left: 16.66666667%;
  }
  .col-md-offset-1 {
    margin-left: 8.33333333%;
  }
  .col-md-offset-0 {
    margin-left: 0%;
  }
}
@media (min-width: 1200px) {
  .col-lg-1, .col-lg-2, .col-lg-3, .col-lg-4, .col-lg-5, .col-lg-6, .col-lg-7, .col-lg-8, .col-lg-9, .col-lg-10, .col-lg-11, .col-lg-12 {
    float: left;
  }
  .col-lg-12 {
    width: 100%;
  }
  .col-lg-11 {
    width: 91.66666667%;
  }
  .col-lg-10 {
    width: 83.33333333%;
  }
  .col-lg-9 {
    width: 75%;
  }
  .col-lg-8 {
    width: 66.66666667%;
  }
  .col-lg-7 {
    width: 58.33333333%;
  }
  .col-lg-6 {
    width: 50%;
  }
  .col-lg-5 {
    width: 41.66666667%;
  }
  .col-lg-4 {
    width: 33.33333333%;
  }
  .col-lg-3 {
    width: 25%;
  }
  .col-lg-2 {
    width: 16.66666667%;
  }
  .col-lg-1 {
    width: 8.33333333%;
  }
  .col-lg-pull-12 {
    right: 100%;
  }
  .col-lg-pull-11 {
    right: 91.66666667%;
  }
  .col-lg-pull-10 {
    right: 83.33333333%;
  }
  .col-lg-pull-9 {
    right: 75%;
  }
  .col-lg-pull-8 {
    right: 66.66666667%;
  }
  .col-lg-pull-7 {
    right: 58.33333333%;
  }
  .col-lg-pull-6 {
    right: 50%;
  }
  .col-lg-pull-5 {
    right: 41.66666667%;
  }
  .col-lg-pull-4 {
    right: 33.33333333%;
  }
  .col-lg-pull-3 {
    right: 25%;
  }
  .col-lg-pull-2 {
    right: 16.66666667%;
  }
  .col-lg-pull-1 {
    right: 8.33333333%;
  }
  .col-lg-pull-0 {
    right: auto;
  }
  .col-lg-push-12 {
    left: 100%;
  }
  .col-lg-push-11 {
    left: 91.66666667%;
  }
  .col-lg-push-10 {
    left: 83.33333333%;
  }
  .col-lg-push-9 {
    left: 75%;
  }
  .col-lg-push-8 {
    left: 66.66666667%;
  }
  .col-lg-push-7 {
    left: 58.33333333%;
  }
  .col-lg-push-6 {
    left: 50%;
  }
  .col-lg-push-5 {
    left: 41.66666667%;
  }
  .col-lg-push-4 {
    left: 33.33333333%;
  }
  .col-lg-push-3 {
    left: 25%;
  }
  .col-lg-push-2 {
    left: 16.66666667%;
  }
  .col-lg-push-1 {
    left: 8.33333333%;
  }
  .col-lg-push-0 {
    left: auto;
  }
  .col-lg-offset-12 {
    margin-left: 100%;
  }
  .col-lg-offset-11 {
    margin-left: 91.66666667%;
  }
  .col-lg-offset-10 {
    margin-left: 83.33333333%;
  }
  .col-lg-offset-9 {
    margin-left: 75%;
  }
  .col-lg-offset-8 {
    margin-left: 66.66666667%;
  }
  .col-lg-offset-7 {
    margin-left: 58.33333333%;
  }
  .col-lg-offset-6 {
    margin-left: 50%;
  }
  .col-lg-offset-5 {
    margin-left: 41.66666667%;
  }
  .col-lg-offset-4 {
    margin-left: 33.33333333%;
  }
  .col-lg-offset-3 {
    margin-left: 25%;
  }
  .col-lg-offset-2 {
    margin-left: 16.66666667%;
  }
  .col-lg-offset-1 {
    margin-left: 8.33333333%;
  }
  .col-lg-offset-0 {
    margin-left: 0%;
  }
}
table {
  background-color: transparent;
}
caption {
  padding-top: 8px;
  padding-bottom: 8px;
  color: #777777;
  text-align: left;
}
th {
  text-align: left;
}
.table {
  width: 100%;
  max-width: 100%;
  margin-bottom: 18px;
}
.table > thead > tr > th,
.table > tbody > tr > th,
.table > tfoot > tr > th,
.table > thead > tr > td,
.table > tbody > tr > td,
.table > tfoot > tr > td {
  padding: 8px;
  line-height: 1.42857143;
  vertical-align: top;
  border-top: 1px solid #ddd;
}
.table > thead > tr > th {
  vertical-align: bottom;
  border-bottom: 2px solid #ddd;
}
.table > caption + thead > tr:first-child > th,
.table > colgroup + thead > tr:first-child > th,
.table > thead:first-child > tr:first-child > th,
.table > caption + thead > tr:first-child > td,
.table > colgroup + thead > tr:first-child > td,
.table > thead:first-child > tr:first-child > td {
  border-top: 0;
}
.table > tbody + tbody {
  border-top: 2px solid #ddd;
}
.table .table {
  background-color: #fff;
}
.table-condensed > thead > tr > th,
.table-condensed > tbody > tr > th,
.table-condensed > tfoot > tr > th,
.table-condensed > thead > tr > td,
.table-condensed > tbody > tr > td,
.table-condensed > tfoot > tr > td {
  padding: 5px;
}
.table-bordered {
  border: 1px solid #ddd;
}
.table-bordered > thead > tr > th,
.table-bordered > tbody > tr > th,
.table-bordered > tfoot > tr > th,
.table-bordered > thead > tr > td,
.table-bordered > tbody > tr > td,
.table-bordered > tfoot > tr > td {
  border: 1px solid #ddd;
}
.table-bordered > thead > tr > th,
.table-bordered > thead > tr > td {
  border-bottom-width: 2px;
}
.table-striped > tbody > tr:nth-of-type(odd) {
  background-color: #f9f9f9;
}
.table-hover > tbody > tr:hover {
  background-color: #f5f5f5;
}
table col[class*="col-"] {
  position: static;
  float: none;
  display: table-column;
}
table td[class*="col-"],
table th[class*="col-"] {
  position: static;
  float: none;
  display: table-cell;
}
.table > thead > tr > td.active,
.table > tbody > tr > td.active,
.table > tfoot > tr > td.active,
.table > thead > tr > th.active,
.table > tbody > tr > th.active,
.table > tfoot > tr > th.active,
.table > thead > tr.active > td,
.table > tbody > tr.active > td,
.table > tfoot > tr.active > td,
.table > thead > tr.active > th,
.table > tbody > tr.active > th,
.table > tfoot > tr.active > th {
  background-color: #f5f5f5;
}
.table-hover > tbody > tr > td.active:hover,
.table-hover > tbody > tr > th.active:hover,
.table-hover > tbody > tr.active:hover > td,
.table-hover > tbody > tr:hover > .active,
.table-hover > tbody > tr.active:hover > th {
  background-color: #e8e8e8;
}
.table > thead > tr > td.success,
.table > tbody > tr > td.success,
.table > tfoot > tr > td.success,
.table > thead > tr > th.success,
.table > tbody > tr > th.success,
.table > tfoot > tr > th.success,
.table > thead > tr.success > td,
.table > tbody > tr.success > td,
.table > tfoot > tr.success > td,
.table > thead > tr.success > th,
.table > tbody > tr.success > th,
.table > tfoot > tr.success > th {
  background-color: #dff0d8;
}
.table-hover > tbody > tr > td.success:hover,
.table-hover > tbody > tr > th.success:hover,
.table-hover > tbody > tr.success:hover > td,
.table-hover > tbody > tr:hover > .success,
.table-hover > tbody > tr.success:hover > th {
  background-color: #d0e9c6;
}
.table > thead > tr > td.info,
.table > tbody > tr > td.info,
.table > tfoot > tr > td.info,
.table > thead > tr > th.info,
.table > tbody > tr > th.info,
.table > tfoot > tr > th.info,
.table > thead > tr.info > td,
.table > tbody > tr.info > td,
.table > tfoot > tr.info > td,
.table > thead > tr.info > th,
.table > tbody > tr.info > th,
.table > tfoot > tr.info > th {
  background-color: #d9edf7;
}
.table-hover > tbody > tr > td.info:hover,
.table-hover > tbody > tr > th.info:hover,
.table-hover > tbody > tr.info:hover > td,
.table-hover > tbody > tr:hover > .info,
.table-hover > tbody > tr.info:hover > th {
  background-color: #c4e3f3;
}
.table > thead > tr > td.warning,
.table > tbody > tr > td.warning,
.table > tfoot > tr > td.warning,
.table > thead > tr > th.warning,
.table > tbody > tr > th.warning,
.table > tfoot > tr > th.warning,
.table > thead > tr.warning > td,
.table > tbody > tr.warning > td,
.table > tfoot > tr.warning > td,
.table > thead > tr.warning > th,
.table > tbody > tr.warning > th,
.table > tfoot > tr.warning > th {
  background-color: #fcf8e3;
}
.table-hover > tbody > tr > td.warning:hover,
.table-hover > tbody > tr > th.warning:hover,
.table-hover > tbody > tr.warning:hover > td,
.table-hover > tbody > tr:hover > .warning,
.table-hover > tbody > tr.warning:hover > th {
  background-color: #faf2cc;
}
.table > thead > tr > td.danger,
.table > tbody > tr > td.danger,
.table > tfoot > tr > td.danger,
.table > thead > tr > th.danger,
.table > tbody > tr > th.danger,
.table > tfoot > tr > th.danger,
.table > thead > tr.danger > td,
.table > tbody > tr.danger > td,
.table > tfoot > tr.danger > td,
.table > thead > tr.danger > th,
.table > tbody > tr.danger > th,
.table > tfoot > tr.danger > th {
  background-color: #f2dede;
}
.table-hover > tbody > tr > td.danger:hover,
.table-hover > tbody > tr > th.danger:hover,
.table-hover > tbody > tr.danger:hover > td,
.table-hover > tbody > tr:hover > .danger,
.table-hover > tbody > tr.danger:hover > th {
  background-color: #ebcccc;
}
.table-responsive {
  overflow-x: auto;
  min-height: 0.01%;
}
@media screen and (max-width: 767px) {
  .table-responsive {
    width: 100%;
    margin-bottom: 13.5px;
    overflow-y: hidden;
    -ms-overflow-style: -ms-autohiding-scrollbar;
    border: 1px solid #ddd;
  }
  .table-responsive > .table {
    margin-bottom: 0;
  }
  .table-responsive > .table > thead > tr > th,
  .table-responsive > .table > tbody > tr > th,
  .table-responsive > .table > tfoot > tr > th,
  .table-responsive > .table > thead > tr > td,
  .table-responsive > .table > tbody > tr > td,
  .table-responsive > .table > tfoot > tr > td {
    white-space: nowrap;
  }
  .table-responsive > .table-bordered {
    border: 0;
  }
  .table-responsive > .table-bordered > thead > tr > th:first-child,
  .table-responsive > .table-bordered > tbody > tr > th:first-child,
  .table-responsive > .table-bordered > tfoot > tr > th:first-child,
  .table-responsive > .table-bordered > thead > tr > td:first-child,
  .table-responsive > .table-bordered > tbody > tr > td:first-child,
  .table-responsive > .table-bordered > tfoot > tr > td:first-child {
    border-left: 0;
  }
  .table-responsive > .table-bordered > thead > tr > th:last-child,
  .table-responsive > .table-bordered > tbody > tr > th:last-child,
  .table-responsive > .table-bordered > tfoot > tr > th:last-child,
  .table-responsive > .table-bordered > thead > tr > td:last-child,
  .table-responsive > .table-bordered > tbody > tr > td:last-child,
  .table-responsive > .table-bordered > tfoot > tr > td:last-child {
    border-right: 0;
  }
  .table-responsive > .table-bordered > tbody > tr:last-child > th,
  .table-responsive > .table-bordered > tfoot > tr:last-child > th,
  .table-responsive > .table-bordered > tbody > tr:last-child > td,
  .table-responsive > .table-bordered > tfoot > tr:last-child > td {
    border-bottom: 0;
  }
}
fieldset {
  padding: 0;
  margin: 0;
  border: 0;
  min-width: 0;
}
legend {
  display: block;
  width: 100%;
  padding: 0;
  margin-bottom: 18px;
  font-size: 19.5px;
  line-height: inherit;
  color: #333333;
  border: 0;
  border-bottom: 1px solid #e5e5e5;
}
label {
  display: inline-block;
  max-width: 100%;
  margin-bottom: 5px;
  font-weight: bold;
}
input[type="search"] {
  -webkit-box-sizing: border-box;
  -moz-box-sizing: border-box;
  box-sizing: border-box;
}
input[type="radio"],
input[type="checkbox"] {
  margin: 4px 0 0;
  margin-top: 1px \9;
  line-height: normal;
}
input[type="file"] {
  display: block;
}
input[type="range"] {
  display: block;
  width: 100%;
}
select[multiple],
select[size] {
  height: auto;
}
input[type="file"]:focus,
input[type="radio"]:focus,
input[type="checkbox"]:focus {
  outline: 5px auto -webkit-focus-ring-color;
  outline-offset: -2px;
}
output {
  display: block;
  padding-top: 7px;
  font-size: 13px;
  line-height: 1.42857143;
  color: #555555;
}
.form-control {
  display: block;
  width: 100%;
  height: 32px;
  padding: 6px 12px;
  font-size: 13px;
  line-height: 1.42857143;
  color: #555555;
  background-color: #fff;
  background-image: none;
  border: 1px solid #ccc;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  -webkit-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  -o-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
}
.form-control:focus {
  border-color: #66afe9;
  outline: 0;
  -webkit-box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
  box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
}
.form-control::-moz-placeholder {
  color: #999;
  opacity: 1;
}
.form-control:-ms-input-placeholder {
  color: #999;
}
.form-control::-webkit-input-placeholder {
  color: #999;
}
.form-control::-ms-expand {
  border: 0;
  background-color: transparent;
}
.form-control[disabled],
.form-control[readonly],
fieldset[disabled] .form-control {
  background-color: #eeeeee;
  opacity: 1;
}
.form-control[disabled],
fieldset[disabled] .form-control {
  cursor: not-allowed;
}
textarea.form-control {
  height: auto;
}
input[type="search"] {
  -webkit-appearance: none;
}
@media screen and (-webkit-min-device-pixel-ratio: 0) {
  input[type="date"].form-control,
  input[type="time"].form-control,
  input[type="datetime-local"].form-control,
  input[type="month"].form-control {
    line-height: 32px;
  }
  input[type="date"].input-sm,
  input[type="time"].input-sm,
  input[type="datetime-local"].input-sm,
  input[type="month"].input-sm,
  .input-group-sm input[type="date"],
  .input-group-sm input[type="time"],
  .input-group-sm input[type="datetime-local"],
  .input-group-sm input[type="month"] {
    line-height: 30px;
  }
  input[type="date"].input-lg,
  input[type="time"].input-lg,
  input[type="datetime-local"].input-lg,
  input[type="month"].input-lg,
  .input-group-lg input[type="date"],
  .input-group-lg input[type="time"],
  .input-group-lg input[type="datetime-local"],
  .input-group-lg input[type="month"] {
    line-height: 45px;
  }
}
.form-group {
  margin-bottom: 15px;
}
.radio,
.checkbox {
  position: relative;
  display: block;
  margin-top: 10px;
  margin-bottom: 10px;
}
.radio label,
.checkbox label {
  min-height: 18px;
  padding-left: 20px;
  margin-bottom: 0;
  font-weight: normal;
  cursor: pointer;
}
.radio input[type="radio"],
.radio-inline input[type="radio"],
.checkbox input[type="checkbox"],
.checkbox-inline input[type="checkbox"] {
  position: absolute;
  margin-left: -20px;
  margin-top: 4px \9;
}
.radio + .radio,
.checkbox + .checkbox {
  margin-top: -5px;
}
.radio-inline,
.checkbox-inline {
  position: relative;
  display: inline-block;
  padding-left: 20px;
  margin-bottom: 0;
  vertical-align: middle;
  font-weight: normal;
  cursor: pointer;
}
.radio-inline + .radio-inline,
.checkbox-inline + .checkbox-inline {
  margin-top: 0;
  margin-left: 10px;
}
input[type="radio"][disabled],
input[type="checkbox"][disabled],
input[type="radio"].disabled,
input[type="checkbox"].disabled,
fieldset[disabled] input[type="radio"],
fieldset[disabled] input[type="checkbox"] {
  cursor: not-allowed;
}
.radio-inline.disabled,
.checkbox-inline.disabled,
fieldset[disabled] .radio-inline,
fieldset[disabled] .checkbox-inline {
  cursor: not-allowed;
}
.radio.disabled label,
.checkbox.disabled label,
fieldset[disabled] .radio label,
fieldset[disabled] .checkbox label {
  cursor: not-allowed;
}
.form-control-static {
  padding-top: 7px;
  padding-bottom: 7px;
  margin-bottom: 0;
  min-height: 31px;
}
.form-control-static.input-lg,
.form-control-static.input-sm {
  padding-left: 0;
  padding-right: 0;
}
.input-sm {
  height: 30px;
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
select.input-sm {
  height: 30px;
  line-height: 30px;
}
textarea.input-sm,
select[multiple].input-sm {
  height: auto;
}
.form-group-sm .form-control {
  height: 30px;
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
.form-group-sm select.form-control {
  height: 30px;
  line-height: 30px;
}
.form-group-sm textarea.form-control,
.form-group-sm select[multiple].form-control {
  height: auto;
}
.form-group-sm .form-control-static {
  height: 30px;
  min-height: 30px;
  padding: 6px 10px;
  font-size: 12px;
  line-height: 1.5;
}
.input-lg {
  height: 45px;
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
  border-radius: 3px;
}
select.input-lg {
  height: 45px;
  line-height: 45px;
}
textarea.input-lg,
select[multiple].input-lg {
  height: auto;
}
.form-group-lg .form-control {
  height: 45px;
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
  border-radius: 3px;
}
.form-group-lg select.form-control {
  height: 45px;
  line-height: 45px;
}
.form-group-lg textarea.form-control,
.form-group-lg select[multiple].form-control {
  height: auto;
}
.form-group-lg .form-control-static {
  height: 45px;
  min-height: 35px;
  padding: 11px 16px;
  font-size: 17px;
  line-height: 1.3333333;
}
.has-feedback {
  position: relative;
}
.has-feedback .form-control {
  padding-right: 40px;
}
.form-control-feedback {
  position: absolute;
  top: 0;
  right: 0;
  z-index: 2;
  display: block;
  width: 32px;
  height: 32px;
  line-height: 32px;
  text-align: center;
  pointer-events: none;
}
.input-lg + .form-control-feedback,
.input-group-lg + .form-control-feedback,
.form-group-lg .form-control + .form-control-feedback {
  width: 45px;
  height: 45px;
  line-height: 45px;
}
.input-sm + .form-control-feedback,
.input-group-sm + .form-control-feedback,
.form-group-sm .form-control + .form-control-feedback {
  width: 30px;
  height: 30px;
  line-height: 30px;
}
.has-success .help-block,
.has-success .control-label,
.has-success .radio,
.has-success .checkbox,
.has-success .radio-inline,
.has-success .checkbox-inline,
.has-success.radio label,
.has-success.checkbox label,
.has-success.radio-inline label,
.has-success.checkbox-inline label {
  color: #3c763d;
}
.has-success .form-control {
  border-color: #3c763d;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
}
.has-success .form-control:focus {
  border-color: #2b542c;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #67b168;
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #67b168;
}
.has-success .input-group-addon {
  color: #3c763d;
  border-color: #3c763d;
  background-color: #dff0d8;
}
.has-success .form-control-feedback {
  color: #3c763d;
}
.has-warning .help-block,
.has-warning .control-label,
.has-warning .radio,
.has-warning .checkbox,
.has-warning .radio-inline,
.has-warning .checkbox-inline,
.has-warning.radio label,
.has-warning.checkbox label,
.has-warning.radio-inline label,
.has-warning.checkbox-inline label {
  color: #8a6d3b;
}
.has-warning .form-control {
  border-color: #8a6d3b;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
}
.has-warning .form-control:focus {
  border-color: #66512c;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #c0a16b;
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #c0a16b;
}
.has-warning .input-group-addon {
  color: #8a6d3b;
  border-color: #8a6d3b;
  background-color: #fcf8e3;
}
.has-warning .form-control-feedback {
  color: #8a6d3b;
}
.has-error .help-block,
.has-error .control-label,
.has-error .radio,
.has-error .checkbox,
.has-error .radio-inline,
.has-error .checkbox-inline,
.has-error.radio label,
.has-error.checkbox label,
.has-error.radio-inline label,
.has-error.checkbox-inline label {
  color: #a94442;
}
.has-error .form-control {
  border-color: #a94442;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
}
.has-error .form-control:focus {
  border-color: #843534;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #ce8483;
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #ce8483;
}
.has-error .input-group-addon {
  color: #a94442;
  border-color: #a94442;
  background-color: #f2dede;
}
.has-error .form-control-feedback {
  color: #a94442;
}
.has-feedback label ~ .form-control-feedback {
  top: 23px;
}
.has-feedback label.sr-only ~ .form-control-feedback {
  top: 0;
}
.help-block {
  display: block;
  margin-top: 5px;
  margin-bottom: 10px;
  color: #404040;
}
@media (min-width: 768px) {
  .form-inline .form-group {
    display: inline-block;
    margin-bottom: 0;
    vertical-align: middle;
  }
  .form-inline .form-control {
    display: inline-block;
    width: auto;
    vertical-align: middle;
  }
  .form-inline .form-control-static {
    display: inline-block;
  }
  .form-inline .input-group {
    display: inline-table;
    vertical-align: middle;
  }
  .form-inline .input-group .input-group-addon,
  .form-inline .input-group .input-group-btn,
  .form-inline .input-group .form-control {
    width: auto;
  }
  .form-inline .input-group > .form-control {
    width: 100%;
  }
  .form-inline .control-label {
    margin-bottom: 0;
    vertical-align: middle;
  }
  .form-inline .radio,
  .form-inline .checkbox {
    display: inline-block;
    margin-top: 0;
    margin-bottom: 0;
    vertical-align: middle;
  }
  .form-inline .radio label,
  .form-inline .checkbox label {
    padding-left: 0;
  }
  .form-inline .radio input[type="radio"],
  .form-inline .checkbox input[type="checkbox"] {
    position: relative;
    margin-left: 0;
  }
  .form-inline .has-feedback .form-control-feedback {
    top: 0;
  }
}
.form-horizontal .radio,
.form-horizontal .checkbox,
.form-horizontal .radio-inline,
.form-horizontal .checkbox-inline {
  margin-top: 0;
  margin-bottom: 0;
  padding-top: 7px;
}
.form-horizontal .radio,
.form-horizontal .checkbox {
  min-height: 25px;
}
.form-horizontal .form-group {
  margin-left: 0px;
  margin-right: 0px;
}
@media (min-width: 768px) {
  .form-horizontal .control-label {
    text-align: right;
    margin-bottom: 0;
    padding-top: 7px;
  }
}
.form-horizontal .has-feedback .form-control-feedback {
  right: 0px;
}
@media (min-width: 768px) {
  .form-horizontal .form-group-lg .control-label {
    padding-top: 11px;
    font-size: 17px;
  }
}
@media (min-width: 768px) {
  .form-horizontal .form-group-sm .control-label {
    padding-top: 6px;
    font-size: 12px;
  }
}
.btn {
  display: inline-block;
  margin-bottom: 0;
  font-weight: normal;
  text-align: center;
  vertical-align: middle;
  touch-action: manipulation;
  cursor: pointer;
  background-image: none;
  border: 1px solid transparent;
  white-space: nowrap;
  padding: 6px 12px;
  font-size: 13px;
  line-height: 1.42857143;
  border-radius: 2px;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}
.btn:focus,
.btn:active:focus,
.btn.active:focus,
.btn.focus,
.btn:active.focus,
.btn.active.focus {
  outline: 5px auto -webkit-focus-ring-color;
  outline-offset: -2px;
}
.btn:hover,
.btn:focus,
.btn.focus {
  color: #333;
  text-decoration: none;
}
.btn:active,
.btn.active {
  outline: 0;
  background-image: none;
  -webkit-box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
  box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
}
.btn.disabled,
.btn[disabled],
fieldset[disabled] .btn {
  cursor: not-allowed;
  opacity: 0.65;
  filter: alpha(opacity=65);
  -webkit-box-shadow: none;
  box-shadow: none;
}
a.btn.disabled,
fieldset[disabled] a.btn {
  pointer-events: none;
}
.btn-default {
  color: #333;
  background-color: #fff;
  border-color: #ccc;
}
.btn-default:focus,
.btn-default.focus {
  color: #333;
  background-color: #e6e6e6;
  border-color: #8c8c8c;
}
.btn-default:hover {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
.btn-default:active,
.btn-default.active,
.open > .dropdown-toggle.btn-default {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
.btn-default:active:hover,
.btn-default.active:hover,
.open > .dropdown-toggle.btn-default:hover,
.btn-default:active:focus,
.btn-default.active:focus,
.open > .dropdown-toggle.btn-default:focus,
.btn-default:active.focus,
.btn-default.active.focus,
.open > .dropdown-toggle.btn-default.focus {
  color: #333;
  background-color: #d4d4d4;
  border-color: #8c8c8c;
}
.btn-default:active,
.btn-default.active,
.open > .dropdown-toggle.btn-default {
  background-image: none;
}
.btn-default.disabled:hover,
.btn-default[disabled]:hover,
fieldset[disabled] .btn-default:hover,
.btn-default.disabled:focus,
.btn-default[disabled]:focus,
fieldset[disabled] .btn-default:focus,
.btn-default.disabled.focus,
.btn-default[disabled].focus,
fieldset[disabled] .btn-default.focus {
  background-color: #fff;
  border-color: #ccc;
}
.btn-default .badge {
  color: #fff;
  background-color: #333;
}
.btn-primary {
  color: #fff;
  background-color: #337ab7;
  border-color: #2e6da4;
}
.btn-primary:focus,
.btn-primary.focus {
  color: #fff;
  background-color: #286090;
  border-color: #122b40;
}
.btn-primary:hover {
  color: #fff;
  background-color: #286090;
  border-color: #204d74;
}
.btn-primary:active,
.btn-primary.active,
.open > .dropdown-toggle.btn-primary {
  color: #fff;
  background-color: #286090;
  border-color: #204d74;
}
.btn-primary:active:hover,
.btn-primary.active:hover,
.open > .dropdown-toggle.btn-primary:hover,
.btn-primary:active:focus,
.btn-primary.active:focus,
.open > .dropdown-toggle.btn-primary:focus,
.btn-primary:active.focus,
.btn-primary.active.focus,
.open > .dropdown-toggle.btn-primary.focus {
  color: #fff;
  background-color: #204d74;
  border-color: #122b40;
}
.btn-primary:active,
.btn-primary.active,
.open > .dropdown-toggle.btn-primary {
  background-image: none;
}
.btn-primary.disabled:hover,
.btn-primary[disabled]:hover,
fieldset[disabled] .btn-primary:hover,
.btn-primary.disabled:focus,
.btn-primary[disabled]:focus,
fieldset[disabled] .btn-primary:focus,
.btn-primary.disabled.focus,
.btn-primary[disabled].focus,
fieldset[disabled] .btn-primary.focus {
  background-color: #337ab7;
  border-color: #2e6da4;
}
.btn-primary .badge {
  color: #337ab7;
  background-color: #fff;
}
.btn-success {
  color: #fff;
  background-color: #5cb85c;
  border-color: #4cae4c;
}
.btn-success:focus,
.btn-success.focus {
  color: #fff;
  background-color: #449d44;
  border-color: #255625;
}
.btn-success:hover {
  color: #fff;
  background-color: #449d44;
  border-color: #398439;
}
.btn-success:active,
.btn-success.active,
.open > .dropdown-toggle.btn-success {
  color: #fff;
  background-color: #449d44;
  border-color: #398439;
}
.btn-success:active:hover,
.btn-success.active:hover,
.open > .dropdown-toggle.btn-success:hover,
.btn-success:active:focus,
.btn-success.active:focus,
.open > .dropdown-toggle.btn-success:focus,
.btn-success:active.focus,
.btn-success.active.focus,
.open > .dropdown-toggle.btn-success.focus {
  color: #fff;
  background-color: #398439;
  border-color: #255625;
}
.btn-success:active,
.btn-success.active,
.open > .dropdown-toggle.btn-success {
  background-image: none;
}
.btn-success.disabled:hover,
.btn-success[disabled]:hover,
fieldset[disabled] .btn-success:hover,
.btn-success.disabled:focus,
.btn-success[disabled]:focus,
fieldset[disabled] .btn-success:focus,
.btn-success.disabled.focus,
.btn-success[disabled].focus,
fieldset[disabled] .btn-success.focus {
  background-color: #5cb85c;
  border-color: #4cae4c;
}
.btn-success .badge {
  color: #5cb85c;
  background-color: #fff;
}
.btn-info {
  color: #fff;
  background-color: #5bc0de;
  border-color: #46b8da;
}
.btn-info:focus,
.btn-info.focus {
  color: #fff;
  background-color: #31b0d5;
  border-color: #1b6d85;
}
.btn-info:hover {
  color: #fff;
  background-color: #31b0d5;
  border-color: #269abc;
}
.btn-info:active,
.btn-info.active,
.open > .dropdown-toggle.btn-info {
  color: #fff;
  background-color: #31b0d5;
  border-color: #269abc;
}
.btn-info:active:hover,
.btn-info.active:hover,
.open > .dropdown-toggle.btn-info:hover,
.btn-info:active:focus,
.btn-info.active:focus,
.open > .dropdown-toggle.btn-info:focus,
.btn-info:active.focus,
.btn-info.active.focus,
.open > .dropdown-toggle.btn-info.focus {
  color: #fff;
  background-color: #269abc;
  border-color: #1b6d85;
}
.btn-info:active,
.btn-info.active,
.open > .dropdown-toggle.btn-info {
  background-image: none;
}
.btn-info.disabled:hover,
.btn-info[disabled]:hover,
fieldset[disabled] .btn-info:hover,
.btn-info.disabled:focus,
.btn-info[disabled]:focus,
fieldset[disabled] .btn-info:focus,
.btn-info.disabled.focus,
.btn-info[disabled].focus,
fieldset[disabled] .btn-info.focus {
  background-color: #5bc0de;
  border-color: #46b8da;
}
.btn-info .badge {
  color: #5bc0de;
  background-color: #fff;
}
.btn-warning {
  color: #fff;
  background-color: #f0ad4e;
  border-color: #eea236;
}
.btn-warning:focus,
.btn-warning.focus {
  color: #fff;
  background-color: #ec971f;
  border-color: #985f0d;
}
.btn-warning:hover {
  color: #fff;
  background-color: #ec971f;
  border-color: #d58512;
}
.btn-warning:active,
.btn-warning.active,
.open > .dropdown-toggle.btn-warning {
  color: #fff;
  background-color: #ec971f;
  border-color: #d58512;
}
.btn-warning:active:hover,
.btn-warning.active:hover,
.open > .dropdown-toggle.btn-warning:hover,
.btn-warning:active:focus,
.btn-warning.active:focus,
.open > .dropdown-toggle.btn-warning:focus,
.btn-warning:active.focus,
.btn-warning.active.focus,
.open > .dropdown-toggle.btn-warning.focus {
  color: #fff;
  background-color: #d58512;
  border-color: #985f0d;
}
.btn-warning:active,
.btn-warning.active,
.open > .dropdown-toggle.btn-warning {
  background-image: none;
}
.btn-warning.disabled:hover,
.btn-warning[disabled]:hover,
fieldset[disabled] .btn-warning:hover,
.btn-warning.disabled:focus,
.btn-warning[disabled]:focus,
fieldset[disabled] .btn-warning:focus,
.btn-warning.disabled.focus,
.btn-warning[disabled].focus,
fieldset[disabled] .btn-warning.focus {
  background-color: #f0ad4e;
  border-color: #eea236;
}
.btn-warning .badge {
  color: #f0ad4e;
  background-color: #fff;
}
.btn-danger {
  color: #fff;
  background-color: #d9534f;
  border-color: #d43f3a;
}
.btn-danger:focus,
.btn-danger.focus {
  color: #fff;
  background-color: #c9302c;
  border-color: #761c19;
}
.btn-danger:hover {
  color: #fff;
  background-color: #c9302c;
  border-color: #ac2925;
}
.btn-danger:active,
.btn-danger.active,
.open > .dropdown-toggle.btn-danger {
  color: #fff;
  background-color: #c9302c;
  border-color: #ac2925;
}
.btn-danger:active:hover,
.btn-danger.active:hover,
.open > .dropdown-toggle.btn-danger:hover,
.btn-danger:active:focus,
.btn-danger.active:focus,
.open > .dropdown-toggle.btn-danger:focus,
.btn-danger:active.focus,
.btn-danger.active.focus,
.open > .dropdown-toggle.btn-danger.focus {
  color: #fff;
  background-color: #ac2925;
  border-color: #761c19;
}
.btn-danger:active,
.btn-danger.active,
.open > .dropdown-toggle.btn-danger {
  background-image: none;
}
.btn-danger.disabled:hover,
.btn-danger[disabled]:hover,
fieldset[disabled] .btn-danger:hover,
.btn-danger.disabled:focus,
.btn-danger[disabled]:focus,
fieldset[disabled] .btn-danger:focus,
.btn-danger.disabled.focus,
.btn-danger[disabled].focus,
fieldset[disabled] .btn-danger.focus {
  background-color: #d9534f;
  border-color: #d43f3a;
}
.btn-danger .badge {
  color: #d9534f;
  background-color: #fff;
}
.btn-link {
  color: #337ab7;
  font-weight: normal;
  border-radius: 0;
}
.btn-link,
.btn-link:active,
.btn-link.active,
.btn-link[disabled],
fieldset[disabled] .btn-link {
  background-color: transparent;
  -webkit-box-shadow: none;
  box-shadow: none;
}
.btn-link,
.btn-link:hover,
.btn-link:focus,
.btn-link:active {
  border-color: transparent;
}
.btn-link:hover,
.btn-link:focus {
  color: #23527c;
  text-decoration: underline;
  background-color: transparent;
}
.btn-link[disabled]:hover,
fieldset[disabled] .btn-link:hover,
.btn-link[disabled]:focus,
fieldset[disabled] .btn-link:focus {
  color: #777777;
  text-decoration: none;
}
.btn-lg,
.btn-group-lg > .btn {
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
  border-radius: 3px;
}
.btn-sm,
.btn-group-sm > .btn {
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
.btn-xs,
.btn-group-xs > .btn {
  padding: 1px 5px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
.btn-block {
  display: block;
  width: 100%;
}
.btn-block + .btn-block {
  margin-top: 5px;
}
input[type="submit"].btn-block,
input[type="reset"].btn-block,
input[type="button"].btn-block {
  width: 100%;
}
.fade {
  opacity: 0;
  -webkit-transition: opacity 0.15s linear;
  -o-transition: opacity 0.15s linear;
  transition: opacity 0.15s linear;
}
.fade.in {
  opacity: 1;
}
.collapse {
  display: none;
}
.collapse.in {
  display: block;
}
tr.collapse.in {
  display: table-row;
}
tbody.collapse.in {
  display: table-row-group;
}
.collapsing {
  position: relative;
  height: 0;
  overflow: hidden;
  -webkit-transition-property: height, visibility;
  transition-property: height, visibility;
  -webkit-transition-duration: 0.35s;
  transition-duration: 0.35s;
  -webkit-transition-timing-function: ease;
  transition-timing-function: ease;
}
.caret {
  display: inline-block;
  width: 0;
  height: 0;
  margin-left: 2px;
  vertical-align: middle;
  border-top: 4px dashed;
  border-top: 4px solid \9;
  border-right: 4px solid transparent;
  border-left: 4px solid transparent;
}
.dropup,
.dropdown {
  position: relative;
}
.dropdown-toggle:focus {
  outline: 0;
}
.dropdown-menu {
  position: absolute;
  top: 100%;
  left: 0;
  z-index: 1000;
  display: none;
  float: left;
  min-width: 160px;
  padding: 5px 0;
  margin: 2px 0 0;
  list-style: none;
  font-size: 13px;
  text-align: left;
  background-color: #fff;
  border: 1px solid #ccc;
  border: 1px solid rgba(0, 0, 0, 0.15);
  border-radius: 2px;
  -webkit-box-shadow: 0 6px 12px rgba(0, 0, 0, 0.175);
  box-shadow: 0 6px 12px rgba(0, 0, 0, 0.175);
  background-clip: padding-box;
}
.dropdown-menu.pull-right {
  right: 0;
  left: auto;
}
.dropdown-menu .divider {
  height: 1px;
  margin: 8px 0;
  overflow: hidden;
  background-color: #e5e5e5;
}
.dropdown-menu > li > a {
  display: block;
  padding: 3px 20px;
  clear: both;
  font-weight: normal;
  line-height: 1.42857143;
  color: #333333;
  white-space: nowrap;
}
.dropdown-menu > li > a:hover,
.dropdown-menu > li > a:focus {
  text-decoration: none;
  color: #262626;
  background-color: #f5f5f5;
}
.dropdown-menu > .active > a,
.dropdown-menu > .active > a:hover,
.dropdown-menu > .active > a:focus {
  color: #fff;
  text-decoration: none;
  outline: 0;
  background-color: #337ab7;
}
.dropdown-menu > .disabled > a,
.dropdown-menu > .disabled > a:hover,
.dropdown-menu > .disabled > a:focus {
  color: #777777;
}
.dropdown-menu > .disabled > a:hover,
.dropdown-menu > .disabled > a:focus {
  text-decoration: none;
  background-color: transparent;
  background-image: none;
  filter: progid:DXImageTransform.Microsoft.gradient(enabled = false);
  cursor: not-allowed;
}
.open > .dropdown-menu {
  display: block;
}
.open > a {
  outline: 0;
}
.dropdown-menu-right {
  left: auto;
  right: 0;
}
.dropdown-menu-left {
  left: 0;
  right: auto;
}
.dropdown-header {
  display: block;
  padding: 3px 20px;
  font-size: 12px;
  line-height: 1.42857143;
  color: #777777;
  white-space: nowrap;
}
.dropdown-backdrop {
  position: fixed;
  left: 0;
  right: 0;
  bottom: 0;
  top: 0;
  z-index: 990;
}
.pull-right > .dropdown-menu {
  right: 0;
  left: auto;
}
.dropup .caret,
.navbar-fixed-bottom .dropdown .caret {
  border-top: 0;
  border-bottom: 4px dashed;
  border-bottom: 4px solid \9;
  content: "";
}
.dropup .dropdown-menu,
.navbar-fixed-bottom .dropdown .dropdown-menu {
  top: auto;
  bottom: 100%;
  margin-bottom: 2px;
}
@media (min-width: 541px) {
  .navbar-right .dropdown-menu {
    left: auto;
    right: 0;
  }
  .navbar-right .dropdown-menu-left {
    left: 0;
    right: auto;
  }
}
.btn-group,
.btn-group-vertical {
  position: relative;
  display: inline-block;
  vertical-align: middle;
}
.btn-group > .btn,
.btn-group-vertical > .btn {
  position: relative;
  float: left;
}
.btn-group > .btn:hover,
.btn-group-vertical > .btn:hover,
.btn-group > .btn:focus,
.btn-group-vertical > .btn:focus,
.btn-group > .btn:active,
.btn-group-vertical > .btn:active,
.btn-group > .btn.active,
.btn-group-vertical > .btn.active {
  z-index: 2;
}
.btn-group .btn + .btn,
.btn-group .btn + .btn-group,
.btn-group .btn-group + .btn,
.btn-group .btn-group + .btn-group {
  margin-left: -1px;
}
.btn-toolbar {
  margin-left: -5px;
}
.btn-toolbar .btn,
.btn-toolbar .btn-group,
.btn-toolbar .input-group {
  float: left;
}
.btn-toolbar > .btn,
.btn-toolbar > .btn-group,
.btn-toolbar > .input-group {
  margin-left: 5px;
}
.btn-group > .btn:not(:first-child):not(:last-child):not(.dropdown-toggle) {
  border-radius: 0;
}
.btn-group > .btn:first-child {
  margin-left: 0;
}
.btn-group > .btn:first-child:not(:last-child):not(.dropdown-toggle) {
  border-bottom-right-radius: 0;
  border-top-right-radius: 0;
}
.btn-group > .btn:last-child:not(:first-child),
.btn-group > .dropdown-toggle:not(:first-child) {
  border-bottom-left-radius: 0;
  border-top-left-radius: 0;
}
.btn-group > .btn-group {
  float: left;
}
.btn-group > .btn-group:not(:first-child):not(:last-child) > .btn {
  border-radius: 0;
}
.btn-group > .btn-group:first-child:not(:last-child) > .btn:last-child,
.btn-group > .btn-group:first-child:not(:last-child) > .dropdown-toggle {
  border-bottom-right-radius: 0;
  border-top-right-radius: 0;
}
.btn-group > .btn-group:last-child:not(:first-child) > .btn:first-child {
  border-bottom-left-radius: 0;
  border-top-left-radius: 0;
}
.btn-group .dropdown-toggle:active,
.btn-group.open .dropdown-toggle {
  outline: 0;
}
.btn-group > .btn + .dropdown-toggle {
  padding-left: 8px;
  padding-right: 8px;
}
.btn-group > .btn-lg + .dropdown-toggle {
  padding-left: 12px;
  padding-right: 12px;
}
.btn-group.open .dropdown-toggle {
  -webkit-box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
  box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
}
.btn-group.open .dropdown-toggle.btn-link {
  -webkit-box-shadow: none;
  box-shadow: none;
}
.btn .caret {
  margin-left: 0;
}
.btn-lg .caret {
  border-width: 5px 5px 0;
  border-bottom-width: 0;
}
.dropup .btn-lg .caret {
  border-width: 0 5px 5px;
}
.btn-group-vertical > .btn,
.btn-group-vertical > .btn-group,
.btn-group-vertical > .btn-group > .btn {
  display: block;
  float: none;
  width: 100%;
  max-width: 100%;
}
.btn-group-vertical > .btn-group > .btn {
  float: none;
}
.btn-group-vertical > .btn + .btn,
.btn-group-vertical > .btn + .btn-group,
.btn-group-vertical > .btn-group + .btn,
.btn-group-vertical > .btn-group + .btn-group {
  margin-top: -1px;
  margin-left: 0;
}
.btn-group-vertical > .btn:not(:first-child):not(:last-child) {
  border-radius: 0;
}
.btn-group-vertical > .btn:first-child:not(:last-child) {
  border-top-right-radius: 2px;
  border-top-left-radius: 2px;
  border-bottom-right-radius: 0;
  border-bottom-left-radius: 0;
}
.btn-group-vertical > .btn:last-child:not(:first-child) {
  border-top-right-radius: 0;
  border-top-left-radius: 0;
  border-bottom-right-radius: 2px;
  border-bottom-left-radius: 2px;
}
.btn-group-vertical > .btn-group:not(:first-child):not(:last-child) > .btn {
  border-radius: 0;
}
.btn-group-vertical > .btn-group:first-child:not(:last-child) > .btn:last-child,
.btn-group-vertical > .btn-group:first-child:not(:last-child) > .dropdown-toggle {
  border-bottom-right-radius: 0;
  border-bottom-left-radius: 0;
}
.btn-group-vertical > .btn-group:last-child:not(:first-child) > .btn:first-child {
  border-top-right-radius: 0;
  border-top-left-radius: 0;
}
.btn-group-justified {
  display: table;
  width: 100%;
  table-layout: fixed;
  border-collapse: separate;
}
.btn-group-justified > .btn,
.btn-group-justified > .btn-group {
  float: none;
  display: table-cell;
  width: 1%;
}
.btn-group-justified > .btn-group .btn {
  width: 100%;
}
.btn-group-justified > .btn-group .dropdown-menu {
  left: auto;
}
[data-toggle="buttons"] > .btn input[type="radio"],
[data-toggle="buttons"] > .btn-group > .btn input[type="radio"],
[data-toggle="buttons"] > .btn input[type="checkbox"],
[data-toggle="buttons"] > .btn-group > .btn input[type="checkbox"] {
  position: absolute;
  clip: rect(0, 0, 0, 0);
  pointer-events: none;
}
.input-group {
  position: relative;
  display: table;
  border-collapse: separate;
}
.input-group[class*="col-"] {
  float: none;
  padding-left: 0;
  padding-right: 0;
}
.input-group .form-control {
  position: relative;
  z-index: 2;
  float: left;
  width: 100%;
  margin-bottom: 0;
}
.input-group .form-control:focus {
  z-index: 3;
}
.input-group-lg > .form-control,
.input-group-lg > .input-group-addon,
.input-group-lg > .input-group-btn > .btn {
  height: 45px;
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
  border-radius: 3px;
}
select.input-group-lg > .form-control,
select.input-group-lg > .input-group-addon,
select.input-group-lg > .input-group-btn > .btn {
  height: 45px;
  line-height: 45px;
}
textarea.input-group-lg > .form-control,
textarea.input-group-lg > .input-group-addon,
textarea.input-group-lg > .input-group-btn > .btn,
select[multiple].input-group-lg > .form-control,
select[multiple].input-group-lg > .input-group-addon,
select[multiple].input-group-lg > .input-group-btn > .btn {
  height: auto;
}
.input-group-sm > .form-control,
.input-group-sm > .input-group-addon,
.input-group-sm > .input-group-btn > .btn {
  height: 30px;
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
select.input-group-sm > .form-control,
select.input-group-sm > .input-group-addon,
select.input-group-sm > .input-group-btn > .btn {
  height: 30px;
  line-height: 30px;
}
textarea.input-group-sm > .form-control,
textarea.input-group-sm > .input-group-addon,
textarea.input-group-sm > .input-group-btn > .btn,
select[multiple].input-group-sm > .form-control,
select[multiple].input-group-sm > .input-group-addon,
select[multiple].input-group-sm > .input-group-btn > .btn {
  height: auto;
}
.input-group-addon,
.input-group-btn,
.input-group .form-control {
  display: table-cell;
}
.input-group-addon:not(:first-child):not(:last-child),
.input-group-btn:not(:first-child):not(:last-child),
.input-group .form-control:not(:first-child):not(:last-child) {
  border-radius: 0;
}
.input-group-addon,
.input-group-btn {
  width: 1%;
  white-space: nowrap;
  vertical-align: middle;
}
.input-group-addon {
  padding: 6px 12px;
  font-size: 13px;
  font-weight: normal;
  line-height: 1;
  color: #555555;
  text-align: center;
  background-color: #eeeeee;
  border: 1px solid #ccc;
  border-radius: 2px;
}
.input-group-addon.input-sm {
  padding: 5px 10px;
  font-size: 12px;
  border-radius: 1px;
}
.input-group-addon.input-lg {
  padding: 10px 16px;
  font-size: 17px;
  border-radius: 3px;
}
.input-group-addon input[type="radio"],
.input-group-addon input[type="checkbox"] {
  margin-top: 0;
}
.input-group .form-control:first-child,
.input-group-addon:first-child,
.input-group-btn:first-child > .btn,
.input-group-btn:first-child > .btn-group > .btn,
.input-group-btn:first-child > .dropdown-toggle,
.input-group-btn:last-child > .btn:not(:last-child):not(.dropdown-toggle),
.input-group-btn:last-child > .btn-group:not(:last-child) > .btn {
  border-bottom-right-radius: 0;
  border-top-right-radius: 0;
}
.input-group-addon:first-child {
  border-right: 0;
}
.input-group .form-control:last-child,
.input-group-addon:last-child,
.input-group-btn:last-child > .btn,
.input-group-btn:last-child > .btn-group > .btn,
.input-group-btn:last-child > .dropdown-toggle,
.input-group-btn:first-child > .btn:not(:first-child),
.input-group-btn:first-child > .btn-group:not(:first-child) > .btn {
  border-bottom-left-radius: 0;
  border-top-left-radius: 0;
}
.input-group-addon:last-child {
  border-left: 0;
}
.input-group-btn {
  position: relative;
  font-size: 0;
  white-space: nowrap;
}
.input-group-btn > .btn {
  position: relative;
}
.input-group-btn > .btn + .btn {
  margin-left: -1px;
}
.input-group-btn > .btn:hover,
.input-group-btn > .btn:focus,
.input-group-btn > .btn:active {
  z-index: 2;
}
.input-group-btn:first-child > .btn,
.input-group-btn:first-child > .btn-group {
  margin-right: -1px;
}
.input-group-btn:last-child > .btn,
.input-group-btn:last-child > .btn-group {
  z-index: 2;
  margin-left: -1px;
}
.nav {
  margin-bottom: 0;
  padding-left: 0;
  list-style: none;
}
.nav > li {
  position: relative;
  display: block;
}
.nav > li > a {
  position: relative;
  display: block;
  padding: 10px 15px;
}
.nav > li > a:hover,
.nav > li > a:focus {
  text-decoration: none;
  background-color: #eeeeee;
}
.nav > li.disabled > a {
  color: #777777;
}
.nav > li.disabled > a:hover,
.nav > li.disabled > a:focus {
  color: #777777;
  text-decoration: none;
  background-color: transparent;
  cursor: not-allowed;
}
.nav .open > a,
.nav .open > a:hover,
.nav .open > a:focus {
  background-color: #eeeeee;
  border-color: #337ab7;
}
.nav .nav-divider {
  height: 1px;
  margin: 8px 0;
  overflow: hidden;
  background-color: #e5e5e5;
}
.nav > li > a > img {
  max-width: none;
}
.nav-tabs {
  border-bottom: 1px solid #ddd;
}
.nav-tabs > li {
  float: left;
  margin-bottom: -1px;
}
.nav-tabs > li > a {
  margin-right: 2px;
  line-height: 1.42857143;
  border: 1px solid transparent;
  border-radius: 2px 2px 0 0;
}
.nav-tabs > li > a:hover {
  border-color: #eeeeee #eeeeee #ddd;
}
.nav-tabs > li.active > a,
.nav-tabs > li.active > a:hover,
.nav-tabs > li.active > a:focus {
  color: #555555;
  background-color: #fff;
  border: 1px solid #ddd;
  border-bottom-color: transparent;
  cursor: default;
}
.nav-tabs.nav-justified {
  width: 100%;
  border-bottom: 0;
}
.nav-tabs.nav-justified > li {
  float: none;
}
.nav-tabs.nav-justified > li > a {
  text-align: center;
  margin-bottom: 5px;
}
.nav-tabs.nav-justified > .dropdown .dropdown-menu {
  top: auto;
  left: auto;
}
@media (min-width: 768px) {
  .nav-tabs.nav-justified > li {
    display: table-cell;
    width: 1%;
  }
  .nav-tabs.nav-justified > li > a {
    margin-bottom: 0;
  }
}
.nav-tabs.nav-justified > li > a {
  margin-right: 0;
  border-radius: 2px;
}
.nav-tabs.nav-justified > .active > a,
.nav-tabs.nav-justified > .active > a:hover,
.nav-tabs.nav-justified > .active > a:focus {
  border: 1px solid #ddd;
}
@media (min-width: 768px) {
  .nav-tabs.nav-justified > li > a {
    border-bottom: 1px solid #ddd;
    border-radius: 2px 2px 0 0;
  }
  .nav-tabs.nav-justified > .active > a,
  .nav-tabs.nav-justified > .active > a:hover,
  .nav-tabs.nav-justified > .active > a:focus {
    border-bottom-color: #fff;
  }
}
.nav-pills > li {
  float: left;
}
.nav-pills > li > a {
  border-radius: 2px;
}
.nav-pills > li + li {
  margin-left: 2px;
}
.nav-pills > li.active > a,
.nav-pills > li.active > a:hover,
.nav-pills > li.active > a:focus {
  color: #fff;
  background-color: #337ab7;
}
.nav-stacked > li {
  float: none;
}
.nav-stacked > li + li {
  margin-top: 2px;
  margin-left: 0;
}
.nav-justified {
  width: 100%;
}
.nav-justified > li {
  float: none;
}
.nav-justified > li > a {
  text-align: center;
  margin-bottom: 5px;
}
.nav-justified > .dropdown .dropdown-menu {
  top: auto;
  left: auto;
}
@media (min-width: 768px) {
  .nav-justified > li {
    display: table-cell;
    width: 1%;
  }
  .nav-justified > li > a {
    margin-bottom: 0;
  }
}
.nav-tabs-justified {
  border-bottom: 0;
}
.nav-tabs-justified > li > a {
  margin-right: 0;
  border-radius: 2px;
}
.nav-tabs-justified > .active > a,
.nav-tabs-justified > .active > a:hover,
.nav-tabs-justified > .active > a:focus {
  border: 1px solid #ddd;
}
@media (min-width: 768px) {
  .nav-tabs-justified > li > a {
    border-bottom: 1px solid #ddd;
    border-radius: 2px 2px 0 0;
  }
  .nav-tabs-justified > .active > a,
  .nav-tabs-justified > .active > a:hover,
  .nav-tabs-justified > .active > a:focus {
    border-bottom-color: #fff;
  }
}
.tab-content > .tab-pane {
  display: none;
}
.tab-content > .active {
  display: block;
}
.nav-tabs .dropdown-menu {
  margin-top: -1px;
  border-top-right-radius: 0;
  border-top-left-radius: 0;
}
.navbar {
  position: relative;
  min-height: 30px;
  margin-bottom: 18px;
  border: 1px solid transparent;
}
@media (min-width: 541px) {
  .navbar {
    border-radius: 2px;
  }
}
@media (min-width: 541px) {
  .navbar-header {
    float: left;
  }
}
.navbar-collapse {
  overflow-x: visible;
  padding-right: 0px;
  padding-left: 0px;
  border-top: 1px solid transparent;
  box-shadow: inset 0 1px 0 rgba(255, 255, 255, 0.1);
  -webkit-overflow-scrolling: touch;
}
.navbar-collapse.in {
  overflow-y: auto;
}
@media (min-width: 541px) {
  .navbar-collapse {
    width: auto;
    border-top: 0;
    box-shadow: none;
  }
  .navbar-collapse.collapse {
    display: block !important;
    height: auto !important;
    padding-bottom: 0;
    overflow: visible !important;
  }
  .navbar-collapse.in {
    overflow-y: visible;
  }
  .navbar-fixed-top .navbar-collapse,
  .navbar-static-top .navbar-collapse,
  .navbar-fixed-bottom .navbar-collapse {
    padding-left: 0;
    padding-right: 0;
  }
}
.navbar-fixed-top .navbar-collapse,
.navbar-fixed-bottom .navbar-collapse {
  max-height: 340px;
}
@media (max-device-width: 540px) and (orientation: landscape) {
  .navbar-fixed-top .navbar-collapse,
  .navbar-fixed-bottom .navbar-collapse {
    max-height: 200px;
  }
}
.container > .navbar-header,
.container-fluid > .navbar-header,
.container > .navbar-collapse,
.container-fluid > .navbar-collapse {
  margin-right: 0px;
  margin-left: 0px;
}
@media (min-width: 541px) {
  .container > .navbar-header,
  .container-fluid > .navbar-header,
  .container > .navbar-collapse,
  .container-fluid > .navbar-collapse {
    margin-right: 0;
    margin-left: 0;
  }
}
.navbar-static-top {
  z-index: 1000;
  border-width: 0 0 1px;
}
@media (min-width: 541px) {
  .navbar-static-top {
    border-radius: 0;
  }
}
.navbar-fixed-top,
.navbar-fixed-bottom {
  position: fixed;
  right: 0;
  left: 0;
  z-index: 1030;
}
@media (min-width: 541px) {
  .navbar-fixed-top,
  .navbar-fixed-bottom {
    border-radius: 0;
  }
}
.navbar-fixed-top {
  top: 0;
  border-width: 0 0 1px;
}
.navbar-fixed-bottom {
  bottom: 0;
  margin-bottom: 0;
  border-width: 1px 0 0;
}
.navbar-brand {
  float: left;
  padding: 6px 0px;
  font-size: 17px;
  line-height: 18px;
  height: 30px;
}
.navbar-brand:hover,
.navbar-brand:focus {
  text-decoration: none;
}
.navbar-brand > img {
  display: block;
}
@media (min-width: 541px) {
  .navbar > .container .navbar-brand,
  .navbar > .container-fluid .navbar-brand {
    margin-left: 0px;
  }
}
.navbar-toggle {
  position: relative;
  float: right;
  margin-right: 0px;
  padding: 9px 10px;
  margin-top: -2px;
  margin-bottom: -2px;
  background-color: transparent;
  background-image: none;
  border: 1px solid transparent;
  border-radius: 2px;
}
.navbar-toggle:focus {
  outline: 0;
}
.navbar-toggle .icon-bar {
  display: block;
  width: 22px;
  height: 2px;
  border-radius: 1px;
}
.navbar-toggle .icon-bar + .icon-bar {
  margin-top: 4px;
}
@media (min-width: 541px) {
  .navbar-toggle {
    display: none;
  }
}
.navbar-nav {
  margin: 3px 0px;
}
.navbar-nav > li > a {
  padding-top: 10px;
  padding-bottom: 10px;
  line-height: 18px;
}
@media (max-width: 540px) {
  .navbar-nav .open .dropdown-menu {
    position: static;
    float: none;
    width: auto;
    margin-top: 0;
    background-color: transparent;
    border: 0;
    box-shadow: none;
  }
  .navbar-nav .open .dropdown-menu > li > a,
  .navbar-nav .open .dropdown-menu .dropdown-header {
    padding: 5px 15px 5px 25px;
  }
  .navbar-nav .open .dropdown-menu > li > a {
    line-height: 18px;
  }
  .navbar-nav .open .dropdown-menu > li > a:hover,
  .navbar-nav .open .dropdown-menu > li > a:focus {
    background-image: none;
  }
}
@media (min-width: 541px) {
  .navbar-nav {
    float: left;
    margin: 0;
  }
  .navbar-nav > li {
    float: left;
  }
  .navbar-nav > li > a {
    padding-top: 6px;
    padding-bottom: 6px;
  }
}
.navbar-form {
  margin-left: 0px;
  margin-right: 0px;
  padding: 10px 0px;
  border-top: 1px solid transparent;
  border-bottom: 1px solid transparent;
  -webkit-box-shadow: inset 0 1px 0 rgba(255, 255, 255, 0.1), 0 1px 0 rgba(255, 255, 255, 0.1);
  box-shadow: inset 0 1px 0 rgba(255, 255, 255, 0.1), 0 1px 0 rgba(255, 255, 255, 0.1);
  margin-top: -1px;
  margin-bottom: -1px;
}
@media (min-width: 768px) {
  .navbar-form .form-group {
    display: inline-block;
    margin-bottom: 0;
    vertical-align: middle;
  }
  .navbar-form .form-control {
    display: inline-block;
    width: auto;
    vertical-align: middle;
  }
  .navbar-form .form-control-static {
    display: inline-block;
  }
  .navbar-form .input-group {
    display: inline-table;
    vertical-align: middle;
  }
  .navbar-form .input-group .input-group-addon,
  .navbar-form .input-group .input-group-btn,
  .navbar-form .input-group .form-control {
    width: auto;
  }
  .navbar-form .input-group > .form-control {
    width: 100%;
  }
  .navbar-form .control-label {
    margin-bottom: 0;
    vertical-align: middle;
  }
  .navbar-form .radio,
  .navbar-form .checkbox {
    display: inline-block;
    margin-top: 0;
    margin-bottom: 0;
    vertical-align: middle;
  }
  .navbar-form .radio label,
  .navbar-form .checkbox label {
    padding-left: 0;
  }
  .navbar-form .radio input[type="radio"],
  .navbar-form .checkbox input[type="checkbox"] {
    position: relative;
    margin-left: 0;
  }
  .navbar-form .has-feedback .form-control-feedback {
    top: 0;
  }
}
@media (max-width: 540px) {
  .navbar-form .form-group {
    margin-bottom: 5px;
  }
  .navbar-form .form-group:last-child {
    margin-bottom: 0;
  }
}
@media (min-width: 541px) {
  .navbar-form {
    width: auto;
    border: 0;
    margin-left: 0;
    margin-right: 0;
    padding-top: 0;
    padding-bottom: 0;
    -webkit-box-shadow: none;
    box-shadow: none;
  }
}
.navbar-nav > li > .dropdown-menu {
  margin-top: 0;
  border-top-right-radius: 0;
  border-top-left-radius: 0;
}
.navbar-fixed-bottom .navbar-nav > li > .dropdown-menu {
  margin-bottom: 0;
  border-top-right-radius: 2px;
  border-top-left-radius: 2px;
  border-bottom-right-radius: 0;
  border-bottom-left-radius: 0;
}
.navbar-btn {
  margin-top: -1px;
  margin-bottom: -1px;
}
.navbar-btn.btn-sm {
  margin-top: 0px;
  margin-bottom: 0px;
}
.navbar-btn.btn-xs {
  margin-top: 4px;
  margin-bottom: 4px;
}
.navbar-text {
  margin-top: 6px;
  margin-bottom: 6px;
}
@media (min-width: 541px) {
  .navbar-text {
    float: left;
    margin-left: 0px;
    margin-right: 0px;
  }
}
@media (min-width: 541px) {
  .navbar-left {
    float: left !important;
    float: left;
  }
  .navbar-right {
    float: right !important;
    float: right;
    margin-right: 0px;
  }
  .navbar-right ~ .navbar-right {
    margin-right: 0;
  }
}
.navbar-default {
  background-color: #f8f8f8;
  border-color: #e7e7e7;
}
.navbar-default .navbar-brand {
  color: #777;
}
.navbar-default .navbar-brand:hover,
.navbar-default .navbar-brand:focus {
  color: #5e5e5e;
  background-color: transparent;
}
.navbar-default .navbar-text {
  color: #777;
}
.navbar-default .navbar-nav > li > a {
  color: #777;
}
.navbar-default .navbar-nav > li > a:hover,
.navbar-default .navbar-nav > li > a:focus {
  color: #333;
  background-color: transparent;
}
.navbar-default .navbar-nav > .active > a,
.navbar-default .navbar-nav > .active > a:hover,
.navbar-default .navbar-nav > .active > a:focus {
  color: #555;
  background-color: #e7e7e7;
}
.navbar-default .navbar-nav > .disabled > a,
.navbar-default .navbar-nav > .disabled > a:hover,
.navbar-default .navbar-nav > .disabled > a:focus {
  color: #ccc;
  background-color: transparent;
}
.navbar-default .navbar-toggle {
  border-color: #ddd;
}
.navbar-default .navbar-toggle:hover,
.navbar-default .navbar-toggle:focus {
  background-color: #ddd;
}
.navbar-default .navbar-toggle .icon-bar {
  background-color: #888;
}
.navbar-default .navbar-collapse,
.navbar-default .navbar-form {
  border-color: #e7e7e7;
}
.navbar-default .navbar-nav > .open > a,
.navbar-default .navbar-nav > .open > a:hover,
.navbar-default .navbar-nav > .open > a:focus {
  background-color: #e7e7e7;
  color: #555;
}
@media (max-width: 540px) {
  .navbar-default .navbar-nav .open .dropdown-menu > li > a {
    color: #777;
  }
  .navbar-default .navbar-nav .open .dropdown-menu > li > a:hover,
  .navbar-default .navbar-nav .open .dropdown-menu > li > a:focus {
    color: #333;
    background-color: transparent;
  }
  .navbar-default .navbar-nav .open .dropdown-menu > .active > a,
  .navbar-default .navbar-nav .open .dropdown-menu > .active > a:hover,
  .navbar-default .navbar-nav .open .dropdown-menu > .active > a:focus {
    color: #555;
    background-color: #e7e7e7;
  }
  .navbar-default .navbar-nav .open .dropdown-menu > .disabled > a,
  .navbar-default .navbar-nav .open .dropdown-menu > .disabled > a:hover,
  .navbar-default .navbar-nav .open .dropdown-menu > .disabled > a:focus {
    color: #ccc;
    background-color: transparent;
  }
}
.navbar-default .navbar-link {
  color: #777;
}
.navbar-default .navbar-link:hover {
  color: #333;
}
.navbar-default .btn-link {
  color: #777;
}
.navbar-default .btn-link:hover,
.navbar-default .btn-link:focus {
  color: #333;
}
.navbar-default .btn-link[disabled]:hover,
fieldset[disabled] .navbar-default .btn-link:hover,
.navbar-default .btn-link[disabled]:focus,
fieldset[disabled] .navbar-default .btn-link:focus {
  color: #ccc;
}
.navbar-inverse {
  background-color: #222;
  border-color: #080808;
}
.navbar-inverse .navbar-brand {
  color: #9d9d9d;
}
.navbar-inverse .navbar-brand:hover,
.navbar-inverse .navbar-brand:focus {
  color: #fff;
  background-color: transparent;
}
.navbar-inverse .navbar-text {
  color: #9d9d9d;
}
.navbar-inverse .navbar-nav > li > a {
  color: #9d9d9d;
}
.navbar-inverse .navbar-nav > li > a:hover,
.navbar-inverse .navbar-nav > li > a:focus {
  color: #fff;
  background-color: transparent;
}
.navbar-inverse .navbar-nav > .active > a,
.navbar-inverse .navbar-nav > .active > a:hover,
.navbar-inverse .navbar-nav > .active > a:focus {
  color: #fff;
  background-color: #080808;
}
.navbar-inverse .navbar-nav > .disabled > a,
.navbar-inverse .navbar-nav > .disabled > a:hover,
.navbar-inverse .navbar-nav > .disabled > a:focus {
  color: #444;
  background-color: transparent;
}
.navbar-inverse .navbar-toggle {
  border-color: #333;
}
.navbar-inverse .navbar-toggle:hover,
.navbar-inverse .navbar-toggle:focus {
  background-color: #333;
}
.navbar-inverse .navbar-toggle .icon-bar {
  background-color: #fff;
}
.navbar-inverse .navbar-collapse,
.navbar-inverse .navbar-form {
  border-color: #101010;
}
.navbar-inverse .navbar-nav > .open > a,
.navbar-inverse .navbar-nav > .open > a:hover,
.navbar-inverse .navbar-nav > .open > a:focus {
  background-color: #080808;
  color: #fff;
}
@media (max-width: 540px) {
  .navbar-inverse .navbar-nav .open .dropdown-menu > .dropdown-header {
    border-color: #080808;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu .divider {
    background-color: #080808;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu > li > a {
    color: #9d9d9d;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu > li > a:hover,
  .navbar-inverse .navbar-nav .open .dropdown-menu > li > a:focus {
    color: #fff;
    background-color: transparent;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu > .active > a,
  .navbar-inverse .navbar-nav .open .dropdown-menu > .active > a:hover,
  .navbar-inverse .navbar-nav .open .dropdown-menu > .active > a:focus {
    color: #fff;
    background-color: #080808;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu > .disabled > a,
  .navbar-inverse .navbar-nav .open .dropdown-menu > .disabled > a:hover,
  .navbar-inverse .navbar-nav .open .dropdown-menu > .disabled > a:focus {
    color: #444;
    background-color: transparent;
  }
}
.navbar-inverse .navbar-link {
  color: #9d9d9d;
}
.navbar-inverse .navbar-link:hover {
  color: #fff;
}
.navbar-inverse .btn-link {
  color: #9d9d9d;
}
.navbar-inverse .btn-link:hover,
.navbar-inverse .btn-link:focus {
  color: #fff;
}
.navbar-inverse .btn-link[disabled]:hover,
fieldset[disabled] .navbar-inverse .btn-link:hover,
.navbar-inverse .btn-link[disabled]:focus,
fieldset[disabled] .navbar-inverse .btn-link:focus {
  color: #444;
}
.breadcrumb {
  padding: 8px 15px;
  margin-bottom: 18px;
  list-style: none;
  background-color: #f5f5f5;
  border-radius: 2px;
}
.breadcrumb > li {
  display: inline-block;
}
.breadcrumb > li + li:before {
  content: "/\00a0";
  padding: 0 5px;
  color: #5e5e5e;
}
.breadcrumb > .active {
  color: #777777;
}
.pagination {
  display: inline-block;
  padding-left: 0;
  margin: 18px 0;
  border-radius: 2px;
}
.pagination > li {
  display: inline;
}
.pagination > li > a,
.pagination > li > span {
  position: relative;
  float: left;
  padding: 6px 12px;
  line-height: 1.42857143;
  text-decoration: none;
  color: #337ab7;
  background-color: #fff;
  border: 1px solid #ddd;
  margin-left: -1px;
}
.pagination > li:first-child > a,
.pagination > li:first-child > span {
  margin-left: 0;
  border-bottom-left-radius: 2px;
  border-top-left-radius: 2px;
}
.pagination > li:last-child > a,
.pagination > li:last-child > span {
  border-bottom-right-radius: 2px;
  border-top-right-radius: 2px;
}
.pagination > li > a:hover,
.pagination > li > span:hover,
.pagination > li > a:focus,
.pagination > li > span:focus {
  z-index: 2;
  color: #23527c;
  background-color: #eeeeee;
  border-color: #ddd;
}
.pagination > .active > a,
.pagination > .active > span,
.pagination > .active > a:hover,
.pagination > .active > span:hover,
.pagination > .active > a:focus,
.pagination > .active > span:focus {
  z-index: 3;
  color: #fff;
  background-color: #337ab7;
  border-color: #337ab7;
  cursor: default;
}
.pagination > .disabled > span,
.pagination > .disabled > span:hover,
.pagination > .disabled > span:focus,
.pagination > .disabled > a,
.pagination > .disabled > a:hover,
.pagination > .disabled > a:focus {
  color: #777777;
  background-color: #fff;
  border-color: #ddd;
  cursor: not-allowed;
}
.pagination-lg > li > a,
.pagination-lg > li > span {
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
}
.pagination-lg > li:first-child > a,
.pagination-lg > li:first-child > span {
  border-bottom-left-radius: 3px;
  border-top-left-radius: 3px;
}
.pagination-lg > li:last-child > a,
.pagination-lg > li:last-child > span {
  border-bottom-right-radius: 3px;
  border-top-right-radius: 3px;
}
.pagination-sm > li > a,
.pagination-sm > li > span {
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
}
.pagination-sm > li:first-child > a,
.pagination-sm > li:first-child > span {
  border-bottom-left-radius: 1px;
  border-top-left-radius: 1px;
}
.pagination-sm > li:last-child > a,
.pagination-sm > li:last-child > span {
  border-bottom-right-radius: 1px;
  border-top-right-radius: 1px;
}
.pager {
  padding-left: 0;
  margin: 18px 0;
  list-style: none;
  text-align: center;
}
.pager li {
  display: inline;
}
.pager li > a,
.pager li > span {
  display: inline-block;
  padding: 5px 14px;
  background-color: #fff;
  border: 1px solid #ddd;
  border-radius: 15px;
}
.pager li > a:hover,
.pager li > a:focus {
  text-decoration: none;
  background-color: #eeeeee;
}
.pager .next > a,
.pager .next > span {
  float: right;
}
.pager .previous > a,
.pager .previous > span {
  float: left;
}
.pager .disabled > a,
.pager .disabled > a:hover,
.pager .disabled > a:focus,
.pager .disabled > span {
  color: #777777;
  background-color: #fff;
  cursor: not-allowed;
}
.label {
  display: inline;
  padding: .2em .6em .3em;
  font-size: 75%;
  font-weight: bold;
  line-height: 1;
  color: #fff;
  text-align: center;
  white-space: nowrap;
  vertical-align: baseline;
  border-radius: .25em;
}
a.label:hover,
a.label:focus {
  color: #fff;
  text-decoration: none;
  cursor: pointer;
}
.label:empty {
  display: none;
}
.btn .label {
  position: relative;
  top: -1px;
}
.label-default {
  background-color: #777777;
}
.label-default[href]:hover,
.label-default[href]:focus {
  background-color: #5e5e5e;
}
.label-primary {
  background-color: #337ab7;
}
.label-primary[href]:hover,
.label-primary[href]:focus {
  background-color: #286090;
}
.label-success {
  background-color: #5cb85c;
}
.label-success[href]:hover,
.label-success[href]:focus {
  background-color: #449d44;
}
.label-info {
  background-color: #5bc0de;
}
.label-info[href]:hover,
.label-info[href]:focus {
  background-color: #31b0d5;
}
.label-warning {
  background-color: #f0ad4e;
}
.label-warning[href]:hover,
.label-warning[href]:focus {
  background-color: #ec971f;
}
.label-danger {
  background-color: #d9534f;
}
.label-danger[href]:hover,
.label-danger[href]:focus {
  background-color: #c9302c;
}
.badge {
  display: inline-block;
  min-width: 10px;
  padding: 3px 7px;
  font-size: 12px;
  font-weight: bold;
  color: #fff;
  line-height: 1;
  vertical-align: middle;
  white-space: nowrap;
  text-align: center;
  background-color: #777777;
  border-radius: 10px;
}
.badge:empty {
  display: none;
}
.btn .badge {
  position: relative;
  top: -1px;
}
.btn-xs .badge,
.btn-group-xs > .btn .badge {
  top: 0;
  padding: 1px 5px;
}
a.badge:hover,
a.badge:focus {
  color: #fff;
  text-decoration: none;
  cursor: pointer;
}
.list-group-item.active > .badge,
.nav-pills > .active > a > .badge {
  color: #337ab7;
  background-color: #fff;
}
.list-group-item > .badge {
  float: right;
}
.list-group-item > .badge + .badge {
  margin-right: 5px;
}
.nav-pills > li > a > .badge {
  margin-left: 3px;
}
.jumbotron {
  padding-top: 30px;
  padding-bottom: 30px;
  margin-bottom: 30px;
  color: inherit;
  background-color: #eeeeee;
}
.jumbotron h1,
.jumbotron .h1 {
  color: inherit;
}
.jumbotron p {
  margin-bottom: 15px;
  font-size: 20px;
  font-weight: 200;
}
.jumbotron > hr {
  border-top-color: #d5d5d5;
}
.container .jumbotron,
.container-fluid .jumbotron {
  border-radius: 3px;
  padding-left: 0px;
  padding-right: 0px;
}
.jumbotron .container {
  max-width: 100%;
}
@media screen and (min-width: 768px) {
  .jumbotron {
    padding-top: 48px;
    padding-bottom: 48px;
  }
  .container .jumbotron,
  .container-fluid .jumbotron {
    padding-left: 60px;
    padding-right: 60px;
  }
  .jumbotron h1,
  .jumbotron .h1 {
    font-size: 59px;
  }
}
.thumbnail {
  display: block;
  padding: 4px;
  margin-bottom: 18px;
  line-height: 1.42857143;
  background-color: #fff;
  border: 1px solid #ddd;
  border-radius: 2px;
  -webkit-transition: border 0.2s ease-in-out;
  -o-transition: border 0.2s ease-in-out;
  transition: border 0.2s ease-in-out;
}
.thumbnail > img,
.thumbnail a > img {
  margin-left: auto;
  margin-right: auto;
}
a.thumbnail:hover,
a.thumbnail:focus,
a.thumbnail.active {
  border-color: #337ab7;
}
.thumbnail .caption {
  padding: 9px;
  color: #000;
}
.alert {
  padding: 15px;
  margin-bottom: 18px;
  border: 1px solid transparent;
  border-radius: 2px;
}
.alert h4 {
  margin-top: 0;
  color: inherit;
}
.alert .alert-link {
  font-weight: bold;
}
.alert > p,
.alert > ul {
  margin-bottom: 0;
}
.alert > p + p {
  margin-top: 5px;
}
.alert-dismissable,
.alert-dismissible {
  padding-right: 35px;
}
.alert-dismissable .close,
.alert-dismissible .close {
  position: relative;
  top: -2px;
  right: -21px;
  color: inherit;
}
.alert-success {
  background-color: #dff0d8;
  border-color: #d6e9c6;
  color: #3c763d;
}
.alert-success hr {
  border-top-color: #c9e2b3;
}
.alert-success .alert-link {
  color: #2b542c;
}
.alert-info {
  background-color: #d9edf7;
  border-color: #bce8f1;
  color: #31708f;
}
.alert-info hr {
  border-top-color: #a6e1ec;
}
.alert-info .alert-link {
  color: #245269;
}
.alert-warning {
  background-color: #fcf8e3;
  border-color: #faebcc;
  color: #8a6d3b;
}
.alert-warning hr {
  border-top-color: #f7e1b5;
}
.alert-warning .alert-link {
  color: #66512c;
}
.alert-danger {
  background-color: #f2dede;
  border-color: #ebccd1;
  color: #a94442;
}
.alert-danger hr {
  border-top-color: #e4b9c0;
}
.alert-danger .alert-link {
  color: #843534;
}
@-webkit-keyframes progress-bar-stripes {
  from {
    background-position: 40px 0;
  }
  to {
    background-position: 0 0;
  }
}
@keyframes progress-bar-stripes {
  from {
    background-position: 40px 0;
  }
  to {
    background-position: 0 0;
  }
}
.progress {
  overflow: hidden;
  height: 18px;
  margin-bottom: 18px;
  background-color: #f5f5f5;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 1px 2px rgba(0, 0, 0, 0.1);
  box-shadow: inset 0 1px 2px rgba(0, 0, 0, 0.1);
}
.progress-bar {
  float: left;
  width: 0%;
  height: 100%;
  font-size: 12px;
  line-height: 18px;
  color: #fff;
  text-align: center;
  background-color: #337ab7;
  -webkit-box-shadow: inset 0 -1px 0 rgba(0, 0, 0, 0.15);
  box-shadow: inset 0 -1px 0 rgba(0, 0, 0, 0.15);
  -webkit-transition: width 0.6s ease;
  -o-transition: width 0.6s ease;
  transition: width 0.6s ease;
}
.progress-striped .progress-bar,
.progress-bar-striped {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-size: 40px 40px;
}
.progress.active .progress-bar,
.progress-bar.active {
  -webkit-animation: progress-bar-stripes 2s linear infinite;
  -o-animation: progress-bar-stripes 2s linear infinite;
  animation: progress-bar-stripes 2s linear infinite;
}
.progress-bar-success {
  background-color: #5cb85c;
}
.progress-striped .progress-bar-success {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
}
.progress-bar-info {
  background-color: #5bc0de;
}
.progress-striped .progress-bar-info {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
}
.progress-bar-warning {
  background-color: #f0ad4e;
}
.progress-striped .progress-bar-warning {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
}
.progress-bar-danger {
  background-color: #d9534f;
}
.progress-striped .progress-bar-danger {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
}
.media {
  margin-top: 15px;
}
.media:first-child {
  margin-top: 0;
}
.media,
.media-body {
  zoom: 1;
  overflow: hidden;
}
.media-body {
  width: 10000px;
}
.media-object {
  display: block;
}
.media-object.img-thumbnail {
  max-width: none;
}
.media-right,
.media > .pull-right {
  padding-left: 10px;
}
.media-left,
.media > .pull-left {
  padding-right: 10px;
}
.media-left,
.media-right,
.media-body {
  display: table-cell;
  vertical-align: top;
}
.media-middle {
  vertical-align: middle;
}
.media-bottom {
  vertical-align: bottom;
}
.media-heading {
  margin-top: 0;
  margin-bottom: 5px;
}
.media-list {
  padding-left: 0;
  list-style: none;
}
.list-group {
  margin-bottom: 20px;
  padding-left: 0;
}
.list-group-item {
  position: relative;
  display: block;
  padding: 10px 15px;
  margin-bottom: -1px;
  background-color: #fff;
  border: 1px solid #ddd;
}
.list-group-item:first-child {
  border-top-right-radius: 2px;
  border-top-left-radius: 2px;
}
.list-group-item:last-child {
  margin-bottom: 0;
  border-bottom-right-radius: 2px;
  border-bottom-left-radius: 2px;
}
a.list-group-item,
button.list-group-item {
  color: #555;
}
a.list-group-item .list-group-item-heading,
button.list-group-item .list-group-item-heading {
  color: #333;
}
a.list-group-item:hover,
button.list-group-item:hover,
a.list-group-item:focus,
button.list-group-item:focus {
  text-decoration: none;
  color: #555;
  background-color: #f5f5f5;
}
button.list-group-item {
  width: 100%;
  text-align: left;
}
.list-group-item.disabled,
.list-group-item.disabled:hover,
.list-group-item.disabled:focus {
  background-color: #eeeeee;
  color: #777777;
  cursor: not-allowed;
}
.list-group-item.disabled .list-group-item-heading,
.list-group-item.disabled:hover .list-group-item-heading,
.list-group-item.disabled:focus .list-group-item-heading {
  color: inherit;
}
.list-group-item.disabled .list-group-item-text,
.list-group-item.disabled:hover .list-group-item-text,
.list-group-item.disabled:focus .list-group-item-text {
  color: #777777;
}
.list-group-item.active,
.list-group-item.active:hover,
.list-group-item.active:focus {
  z-index: 2;
  color: #fff;
  background-color: #337ab7;
  border-color: #337ab7;
}
.list-group-item.active .list-group-item-heading,
.list-group-item.active:hover .list-group-item-heading,
.list-group-item.active:focus .list-group-item-heading,
.list-group-item.active .list-group-item-heading > small,
.list-group-item.active:hover .list-group-item-heading > small,
.list-group-item.active:focus .list-group-item-heading > small,
.list-group-item.active .list-group-item-heading > .small,
.list-group-item.active:hover .list-group-item-heading > .small,
.list-group-item.active:focus .list-group-item-heading > .small {
  color: inherit;
}
.list-group-item.active .list-group-item-text,
.list-group-item.active:hover .list-group-item-text,
.list-group-item.active:focus .list-group-item-text {
  color: #c7ddef;
}
.list-group-item-success {
  color: #3c763d;
  background-color: #dff0d8;
}
a.list-group-item-success,
button.list-group-item-success {
  color: #3c763d;
}
a.list-group-item-success .list-group-item-heading,
button.list-group-item-success .list-group-item-heading {
  color: inherit;
}
a.list-group-item-success:hover,
button.list-group-item-success:hover,
a.list-group-item-success:focus,
button.list-group-item-success:focus {
  color: #3c763d;
  background-color: #d0e9c6;
}
a.list-group-item-success.active,
button.list-group-item-success.active,
a.list-group-item-success.active:hover,
button.list-group-item-success.active:hover,
a.list-group-item-success.active:focus,
button.list-group-item-success.active:focus {
  color: #fff;
  background-color: #3c763d;
  border-color: #3c763d;
}
.list-group-item-info {
  color: #31708f;
  background-color: #d9edf7;
}
a.list-group-item-info,
button.list-group-item-info {
  color: #31708f;
}
a.list-group-item-info .list-group-item-heading,
button.list-group-item-info .list-group-item-heading {
  color: inherit;
}
a.list-group-item-info:hover,
button.list-group-item-info:hover,
a.list-group-item-info:focus,
button.list-group-item-info:focus {
  color: #31708f;
  background-color: #c4e3f3;
}
a.list-group-item-info.active,
button.list-group-item-info.active,
a.list-group-item-info.active:hover,
button.list-group-item-info.active:hover,
a.list-group-item-info.active:focus,
button.list-group-item-info.active:focus {
  color: #fff;
  background-color: #31708f;
  border-color: #31708f;
}
.list-group-item-warning {
  color: #8a6d3b;
  background-color: #fcf8e3;
}
a.list-group-item-warning,
button.list-group-item-warning {
  color: #8a6d3b;
}
a.list-group-item-warning .list-group-item-heading,
button.list-group-item-warning .list-group-item-heading {
  color: inherit;
}
a.list-group-item-warning:hover,
button.list-group-item-warning:hover,
a.list-group-item-warning:focus,
button.list-group-item-warning:focus {
  color: #8a6d3b;
  background-color: #faf2cc;
}
a.list-group-item-warning.active,
button.list-group-item-warning.active,
a.list-group-item-warning.active:hover,
button.list-group-item-warning.active:hover,
a.list-group-item-warning.active:focus,
button.list-group-item-warning.active:focus {
  color: #fff;
  background-color: #8a6d3b;
  border-color: #8a6d3b;
}
.list-group-item-danger {
  color: #a94442;
  background-color: #f2dede;
}
a.list-group-item-danger,
button.list-group-item-danger {
  color: #a94442;
}
a.list-group-item-danger .list-group-item-heading,
button.list-group-item-danger .list-group-item-heading {
  color: inherit;
}
a.list-group-item-danger:hover,
button.list-group-item-danger:hover,
a.list-group-item-danger:focus,
button.list-group-item-danger:focus {
  color: #a94442;
  background-color: #ebcccc;
}
a.list-group-item-danger.active,
button.list-group-item-danger.active,
a.list-group-item-danger.active:hover,
button.list-group-item-danger.active:hover,
a.list-group-item-danger.active:focus,
button.list-group-item-danger.active:focus {
  color: #fff;
  background-color: #a94442;
  border-color: #a94442;
}
.list-group-item-heading {
  margin-top: 0;
  margin-bottom: 5px;
}
.list-group-item-text {
  margin-bottom: 0;
  line-height: 1.3;
}
.panel {
  margin-bottom: 18px;
  background-color: #fff;
  border: 1px solid transparent;
  border-radius: 2px;
  -webkit-box-shadow: 0 1px 1px rgba(0, 0, 0, 0.05);
  box-shadow: 0 1px 1px rgba(0, 0, 0, 0.05);
}
.panel-body {
  padding: 15px;
}
.panel-heading {
  padding: 10px 15px;
  border-bottom: 1px solid transparent;
  border-top-right-radius: 1px;
  border-top-left-radius: 1px;
}
.panel-heading > .dropdown .dropdown-toggle {
  color: inherit;
}
.panel-title {
  margin-top: 0;
  margin-bottom: 0;
  font-size: 15px;
  color: inherit;
}
.panel-title > a,
.panel-title > small,
.panel-title > .small,
.panel-title > small > a,
.panel-title > .small > a {
  color: inherit;
}
.panel-footer {
  padding: 10px 15px;
  background-color: #f5f5f5;
  border-top: 1px solid #ddd;
  border-bottom-right-radius: 1px;
  border-bottom-left-radius: 1px;
}
.panel > .list-group,
.panel > .panel-collapse > .list-group {
  margin-bottom: 0;
}
.panel > .list-group .list-group-item,
.panel > .panel-collapse > .list-group .list-group-item {
  border-width: 1px 0;
  border-radius: 0;
}
.panel > .list-group:first-child .list-group-item:first-child,
.panel > .panel-collapse > .list-group:first-child .list-group-item:first-child {
  border-top: 0;
  border-top-right-radius: 1px;
  border-top-left-radius: 1px;
}
.panel > .list-group:last-child .list-group-item:last-child,
.panel > .panel-collapse > .list-group:last-child .list-group-item:last-child {
  border-bottom: 0;
  border-bottom-right-radius: 1px;
  border-bottom-left-radius: 1px;
}
.panel > .panel-heading + .panel-collapse > .list-group .list-group-item:first-child {
  border-top-right-radius: 0;
  border-top-left-radius: 0;
}
.panel-heading + .list-group .list-group-item:first-child {
  border-top-width: 0;
}
.list-group + .panel-footer {
  border-top-width: 0;
}
.panel > .table,
.panel > .table-responsive > .table,
.panel > .panel-collapse > .table {
  margin-bottom: 0;
}
.panel > .table caption,
.panel > .table-responsive > .table caption,
.panel > .panel-collapse > .table caption {
  padding-left: 15px;
  padding-right: 15px;
}
.panel > .table:first-child,
.panel > .table-responsive:first-child > .table:first-child {
  border-top-right-radius: 1px;
  border-top-left-radius: 1px;
}
.panel > .table:first-child > thead:first-child > tr:first-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child,
.panel > .table:first-child > tbody:first-child > tr:first-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child {
  border-top-left-radius: 1px;
  border-top-right-radius: 1px;
}
.panel > .table:first-child > thead:first-child > tr:first-child td:first-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child td:first-child,
.panel > .table:first-child > tbody:first-child > tr:first-child td:first-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child td:first-child,
.panel > .table:first-child > thead:first-child > tr:first-child th:first-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child th:first-child,
.panel > .table:first-child > tbody:first-child > tr:first-child th:first-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child th:first-child {
  border-top-left-radius: 1px;
}
.panel > .table:first-child > thead:first-child > tr:first-child td:last-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child td:last-child,
.panel > .table:first-child > tbody:first-child > tr:first-child td:last-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child td:last-child,
.panel > .table:first-child > thead:first-child > tr:first-child th:last-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child th:last-child,
.panel > .table:first-child > tbody:first-child > tr:first-child th:last-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child th:last-child {
  border-top-right-radius: 1px;
}
.panel > .table:last-child,
.panel > .table-responsive:last-child > .table:last-child {
  border-bottom-right-radius: 1px;
  border-bottom-left-radius: 1px;
}
.panel > .table:last-child > tbody:last-child > tr:last-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child {
  border-bottom-left-radius: 1px;
  border-bottom-right-radius: 1px;
}
.panel > .table:last-child > tbody:last-child > tr:last-child td:first-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child td:first-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child td:first-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child td:first-child,
.panel > .table:last-child > tbody:last-child > tr:last-child th:first-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child th:first-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child th:first-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child th:first-child {
  border-bottom-left-radius: 1px;
}
.panel > .table:last-child > tbody:last-child > tr:last-child td:last-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child td:last-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child td:last-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child td:last-child,
.panel > .table:last-child > tbody:last-child > tr:last-child th:last-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child th:last-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child th:last-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child th:last-child {
  border-bottom-right-radius: 1px;
}
.panel > .panel-body + .table,
.panel > .panel-body + .table-responsive,
.panel > .table + .panel-body,
.panel > .table-responsive + .panel-body {
  border-top: 1px solid #ddd;
}
.panel > .table > tbody:first-child > tr:first-child th,
.panel > .table > tbody:first-child > tr:first-child td {
  border-top: 0;
}
.panel > .table-bordered,
.panel > .table-responsive > .table-bordered {
  border: 0;
}
.panel > .table-bordered > thead > tr > th:first-child,
.panel > .table-responsive > .table-bordered > thead > tr > th:first-child,
.panel > .table-bordered > tbody > tr > th:first-child,
.panel > .table-responsive > .table-bordered > tbody > tr > th:first-child,
.panel > .table-bordered > tfoot > tr > th:first-child,
.panel > .table-responsive > .table-bordered > tfoot > tr > th:first-child,
.panel > .table-bordered > thead > tr > td:first-child,
.panel > .table-responsive > .table-bordered > thead > tr > td:first-child,
.panel > .table-bordered > tbody > tr > td:first-child,
.panel > .table-responsive > .table-bordered > tbody > tr > td:first-child,
.panel > .table-bordered > tfoot > tr > td:first-child,
.panel > .table-responsive > .table-bordered > tfoot > tr > td:first-child {
  border-left: 0;
}
.panel > .table-bordered > thead > tr > th:last-child,
.panel > .table-responsive > .table-bordered > thead > tr > th:last-child,
.panel > .table-bordered > tbody > tr > th:last-child,
.panel > .table-responsive > .table-bordered > tbody > tr > th:last-child,
.panel > .table-bordered > tfoot > tr > th:last-child,
.panel > .table-responsive > .table-bordered > tfoot > tr > th:last-child,
.panel > .table-bordered > thead > tr > td:last-child,
.panel > .table-responsive > .table-bordered > thead > tr > td:last-child,
.panel > .table-bordered > tbody > tr > td:last-child,
.panel > .table-responsive > .table-bordered > tbody > tr > td:last-child,
.panel > .table-bordered > tfoot > tr > td:last-child,
.panel > .table-responsive > .table-bordered > tfoot > tr > td:last-child {
  border-right: 0;
}
.panel > .table-bordered > thead > tr:first-child > td,
.panel > .table-responsive > .table-bordered > thead > tr:first-child > td,
.panel > .table-bordered > tbody > tr:first-child > td,
.panel > .table-responsive > .table-bordered > tbody > tr:first-child > td,
.panel > .table-bordered > thead > tr:first-child > th,
.panel > .table-responsive > .table-bordered > thead > tr:first-child > th,
.panel > .table-bordered > tbody > tr:first-child > th,
.panel > .table-responsive > .table-bordered > tbody > tr:first-child > th {
  border-bottom: 0;
}
.panel > .table-bordered > tbody > tr:last-child > td,
.panel > .table-responsive > .table-bordered > tbody > tr:last-child > td,
.panel > .table-bordered > tfoot > tr:last-child > td,
.panel > .table-responsive > .table-bordered > tfoot > tr:last-child > td,
.panel > .table-bordered > tbody > tr:last-child > th,
.panel > .table-responsive > .table-bordered > tbody > tr:last-child > th,
.panel > .table-bordered > tfoot > tr:last-child > th,
.panel > .table-responsive > .table-bordered > tfoot > tr:last-child > th {
  border-bottom: 0;
}
.panel > .table-responsive {
  border: 0;
  margin-bottom: 0;
}
.panel-group {
  margin-bottom: 18px;
}
.panel-group .panel {
  margin-bottom: 0;
  border-radius: 2px;
}
.panel-group .panel + .panel {
  margin-top: 5px;
}
.panel-group .panel-heading {
  border-bottom: 0;
}
.panel-group .panel-heading + .panel-collapse > .panel-body,
.panel-group .panel-heading + .panel-collapse > .list-group {
  border-top: 1px solid #ddd;
}
.panel-group .panel-footer {
  border-top: 0;
}
.panel-group .panel-footer + .panel-collapse .panel-body {
  border-bottom: 1px solid #ddd;
}
.panel-default {
  border-color: #ddd;
}
.panel-default > .panel-heading {
  color: #333333;
  background-color: #f5f5f5;
  border-color: #ddd;
}
.panel-default > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #ddd;
}
.panel-default > .panel-heading .badge {
  color: #f5f5f5;
  background-color: #333333;
}
.panel-default > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #ddd;
}
.panel-primary {
  border-color: #337ab7;
}
.panel-primary > .panel-heading {
  color: #fff;
  background-color: #337ab7;
  border-color: #337ab7;
}
.panel-primary > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #337ab7;
}
.panel-primary > .panel-heading .badge {
  color: #337ab7;
  background-color: #fff;
}
.panel-primary > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #337ab7;
}
.panel-success {
  border-color: #d6e9c6;
}
.panel-success > .panel-heading {
  color: #3c763d;
  background-color: #dff0d8;
  border-color: #d6e9c6;
}
.panel-success > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #d6e9c6;
}
.panel-success > .panel-heading .badge {
  color: #dff0d8;
  background-color: #3c763d;
}
.panel-success > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #d6e9c6;
}
.panel-info {
  border-color: #bce8f1;
}
.panel-info > .panel-heading {
  color: #31708f;
  background-color: #d9edf7;
  border-color: #bce8f1;
}
.panel-info > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #bce8f1;
}
.panel-info > .panel-heading .badge {
  color: #d9edf7;
  background-color: #31708f;
}
.panel-info > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #bce8f1;
}
.panel-warning {
  border-color: #faebcc;
}
.panel-warning > .panel-heading {
  color: #8a6d3b;
  background-color: #fcf8e3;
  border-color: #faebcc;
}
.panel-warning > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #faebcc;
}
.panel-warning > .panel-heading .badge {
  color: #fcf8e3;
  background-color: #8a6d3b;
}
.panel-warning > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #faebcc;
}
.panel-danger {
  border-color: #ebccd1;
}
.panel-danger > .panel-heading {
  color: #a94442;
  background-color: #f2dede;
  border-color: #ebccd1;
}
.panel-danger > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #ebccd1;
}
.panel-danger > .panel-heading .badge {
  color: #f2dede;
  background-color: #a94442;
}
.panel-danger > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #ebccd1;
}
.embed-responsive {
  position: relative;
  display: block;
  height: 0;
  padding: 0;
  overflow: hidden;
}
.embed-responsive .embed-responsive-item,
.embed-responsive iframe,
.embed-responsive embed,
.embed-responsive object,
.embed-responsive video {
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0;
  height: 100%;
  width: 100%;
  border: 0;
}
.embed-responsive-16by9 {
  padding-bottom: 56.25%;
}
.embed-responsive-4by3 {
  padding-bottom: 75%;
}
.well {
  min-height: 20px;
  padding: 19px;
  margin-bottom: 20px;
  background-color: #f5f5f5;
  border: 1px solid #e3e3e3;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.05);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.05);
}
.well blockquote {
  border-color: #ddd;
  border-color: rgba(0, 0, 0, 0.15);
}
.well-lg {
  padding: 24px;
  border-radius: 3px;
}
.well-sm {
  padding: 9px;
  border-radius: 1px;
}
.close {
  float: right;
  font-size: 19.5px;
  font-weight: bold;
  line-height: 1;
  color: #000;
  text-shadow: 0 1px 0 #fff;
  opacity: 0.2;
  filter: alpha(opacity=20);
}
.close:hover,
.close:focus {
  color: #000;
  text-decoration: none;
  cursor: pointer;
  opacity: 0.5;
  filter: alpha(opacity=50);
}
button.close {
  padding: 0;
  cursor: pointer;
  background: transparent;
  border: 0;
  -webkit-appearance: none;
}
.modal-open {
  overflow: hidden;
}
.modal {
  display: none;
  overflow: hidden;
  position: fixed;
  top: 0;
  right: 0;
  bottom: 0;
  left: 0;
  z-index: 1050;
  -webkit-overflow-scrolling: touch;
  outline: 0;
}
.modal.fade .modal-dialog {
  -webkit-transform: translate(0, -25%);
  -ms-transform: translate(0, -25%);
  -o-transform: translate(0, -25%);
  transform: translate(0, -25%);
  -webkit-transition: -webkit-transform 0.3s ease-out;
  -moz-transition: -moz-transform 0.3s ease-out;
  -o-transition: -o-transform 0.3s ease-out;
  transition: transform 0.3s ease-out;
}
.modal.in .modal-dialog {
  -webkit-transform: translate(0, 0);
  -ms-transform: translate(0, 0);
  -o-transform: translate(0, 0);
  transform: translate(0, 0);
}
.modal-open .modal {
  overflow-x: hidden;
  overflow-y: auto;
}
.modal-dialog {
  position: relative;
  width: auto;
  margin: 10px;
}
.modal-content {
  position: relative;
  background-color: #fff;
  border: 1px solid #999;
  border: 1px solid rgba(0, 0, 0, 0.2);
  border-radius: 3px;
  -webkit-box-shadow: 0 3px 9px rgba(0, 0, 0, 0.5);
  box-shadow: 0 3px 9px rgba(0, 0, 0, 0.5);
  background-clip: padding-box;
  outline: 0;
}
.modal-backdrop {
  position: fixed;
  top: 0;
  right: 0;
  bottom: 0;
  left: 0;
  z-index: 1040;
  background-color: #000;
}
.modal-backdrop.fade {
  opacity: 0;
  filter: alpha(opacity=0);
}
.modal-backdrop.in {
  opacity: 0.5;
  filter: alpha(opacity=50);
}
.modal-header {
  padding: 15px;
  border-bottom: 1px solid #e5e5e5;
}
.modal-header .close {
  margin-top: -2px;
}
.modal-title {
  margin: 0;
  line-height: 1.42857143;
}
.modal-body {
  position: relative;
  padding: 15px;
}
.modal-footer {
  padding: 15px;
  text-align: right;
  border-top: 1px solid #e5e5e5;
}
.modal-footer .btn + .btn {
  margin-left: 5px;
  margin-bottom: 0;
}
.modal-footer .btn-group .btn + .btn {
  margin-left: -1px;
}
.modal-footer .btn-block + .btn-block {
  margin-left: 0;
}
.modal-scrollbar-measure {
  position: absolute;
  top: -9999px;
  width: 50px;
  height: 50px;
  overflow: scroll;
}
@media (min-width: 768px) {
  .modal-dialog {
    width: 600px;
    margin: 30px auto;
  }
  .modal-content {
    -webkit-box-shadow: 0 5px 15px rgba(0, 0, 0, 0.5);
    box-shadow: 0 5px 15px rgba(0, 0, 0, 0.5);
  }
  .modal-sm {
    width: 300px;
  }
}
@media (min-width: 992px) {
  .modal-lg {
    width: 900px;
  }
}
.tooltip {
  position: absolute;
  z-index: 1070;
  display: block;
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
  font-style: normal;
  font-weight: normal;
  letter-spacing: normal;
  line-break: auto;
  line-height: 1.42857143;
  text-align: left;
  text-align: start;
  text-decoration: none;
  text-shadow: none;
  text-transform: none;
  white-space: normal;
  word-break: normal;
  word-spacing: normal;
  word-wrap: normal;
  font-size: 12px;
  opacity: 0;
  filter: alpha(opacity=0);
}
.tooltip.in {
  opacity: 0.9;
  filter: alpha(opacity=90);
}
.tooltip.top {
  margin-top: -3px;
  padding: 5px 0;
}
.tooltip.right {
  margin-left: 3px;
  padding: 0 5px;
}
.tooltip.bottom {
  margin-top: 3px;
  padding: 5px 0;
}
.tooltip.left {
  margin-left: -3px;
  padding: 0 5px;
}
.tooltip-inner {
  max-width: 200px;
  padding: 3px 8px;
  color: #fff;
  text-align: center;
  background-color: #000;
  border-radius: 2px;
}
.tooltip-arrow {
  position: absolute;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
}
.tooltip.top .tooltip-arrow {
  bottom: 0;
  left: 50%;
  margin-left: -5px;
  border-width: 5px 5px 0;
  border-top-color: #000;
}
.tooltip.top-left .tooltip-arrow {
  bottom: 0;
  right: 5px;
  margin-bottom: -5px;
  border-width: 5px 5px 0;
  border-top-color: #000;
}
.tooltip.top-right .tooltip-arrow {
  bottom: 0;
  left: 5px;
  margin-bottom: -5px;
  border-width: 5px 5px 0;
  border-top-color: #000;
}
.tooltip.right .tooltip-arrow {
  top: 50%;
  left: 0;
  margin-top: -5px;
  border-width: 5px 5px 5px 0;
  border-right-color: #000;
}
.tooltip.left .tooltip-arrow {
  top: 50%;
  right: 0;
  margin-top: -5px;
  border-width: 5px 0 5px 5px;
  border-left-color: #000;
}
.tooltip.bottom .tooltip-arrow {
  top: 0;
  left: 50%;
  margin-left: -5px;
  border-width: 0 5px 5px;
  border-bottom-color: #000;
}
.tooltip.bottom-left .tooltip-arrow {
  top: 0;
  right: 5px;
  margin-top: -5px;
  border-width: 0 5px 5px;
  border-bottom-color: #000;
}
.tooltip.bottom-right .tooltip-arrow {
  top: 0;
  left: 5px;
  margin-top: -5px;
  border-width: 0 5px 5px;
  border-bottom-color: #000;
}
.popover {
  position: absolute;
  top: 0;
  left: 0;
  z-index: 1060;
  display: none;
  max-width: 276px;
  padding: 1px;
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
  font-style: normal;
  font-weight: normal;
  letter-spacing: normal;
  line-break: auto;
  line-height: 1.42857143;
  text-align: left;
  text-align: start;
  text-decoration: none;
  text-shadow: none;
  text-transform: none;
  white-space: normal;
  word-break: normal;
  word-spacing: normal;
  word-wrap: normal;
  font-size: 13px;
  background-color: #fff;
  background-clip: padding-box;
  border: 1px solid #ccc;
  border: 1px solid rgba(0, 0, 0, 0.2);
  border-radius: 3px;
  -webkit-box-shadow: 0 5px 10px rgba(0, 0, 0, 0.2);
  box-shadow: 0 5px 10px rgba(0, 0, 0, 0.2);
}
.popover.top {
  margin-top: -10px;
}
.popover.right {
  margin-left: 10px;
}
.popover.bottom {
  margin-top: 10px;
}
.popover.left {
  margin-left: -10px;
}
.popover-title {
  margin: 0;
  padding: 8px 14px;
  font-size: 13px;
  background-color: #f7f7f7;
  border-bottom: 1px solid #ebebeb;
  border-radius: 2px 2px 0 0;
}
.popover-content {
  padding: 9px 14px;
}
.popover > .arrow,
.popover > .arrow:after {
  position: absolute;
  display: block;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
}
.popover > .arrow {
  border-width: 11px;
}
.popover > .arrow:after {
  border-width: 10px;
  content: "";
}
.popover.top > .arrow {
  left: 50%;
  margin-left: -11px;
  border-bottom-width: 0;
  border-top-color: #999999;
  border-top-color: rgba(0, 0, 0, 0.25);
  bottom: -11px;
}
.popover.top > .arrow:after {
  content: " ";
  bottom: 1px;
  margin-left: -10px;
  border-bottom-width: 0;
  border-top-color: #fff;
}
.popover.right > .arrow {
  top: 50%;
  left: -11px;
  margin-top: -11px;
  border-left-width: 0;
  border-right-color: #999999;
  border-right-color: rgba(0, 0, 0, 0.25);
}
.popover.right > .arrow:after {
  content: " ";
  left: 1px;
  bottom: -10px;
  border-left-width: 0;
  border-right-color: #fff;
}
.popover.bottom > .arrow {
  left: 50%;
  margin-left: -11px;
  border-top-width: 0;
  border-bottom-color: #999999;
  border-bottom-color: rgba(0, 0, 0, 0.25);
  top: -11px;
}
.popover.bottom > .arrow:after {
  content: " ";
  top: 1px;
  margin-left: -10px;
  border-top-width: 0;
  border-bottom-color: #fff;
}
.popover.left > .arrow {
  top: 50%;
  right: -11px;
  margin-top: -11px;
  border-right-width: 0;
  border-left-color: #999999;
  border-left-color: rgba(0, 0, 0, 0.25);
}
.popover.left > .arrow:after {
  content: " ";
  right: 1px;
  border-right-width: 0;
  border-left-color: #fff;
  bottom: -10px;
}
.carousel {
  position: relative;
}
.carousel-inner {
  position: relative;
  overflow: hidden;
  width: 100%;
}
.carousel-inner > .item {
  display: none;
  position: relative;
  -webkit-transition: 0.6s ease-in-out left;
  -o-transition: 0.6s ease-in-out left;
  transition: 0.6s ease-in-out left;
}
.carousel-inner > .item > img,
.carousel-inner > .item > a > img {
  line-height: 1;
}
@media all and (transform-3d), (-webkit-transform-3d) {
  .carousel-inner > .item {
    -webkit-transition: -webkit-transform 0.6s ease-in-out;
    -moz-transition: -moz-transform 0.6s ease-in-out;
    -o-transition: -o-transform 0.6s ease-in-out;
    transition: transform 0.6s ease-in-out;
    -webkit-backface-visibility: hidden;
    -moz-backface-visibility: hidden;
    backface-visibility: hidden;
    -webkit-perspective: 1000px;
    -moz-perspective: 1000px;
    perspective: 1000px;
  }
  .carousel-inner > .item.next,
  .carousel-inner > .item.active.right {
    -webkit-transform: translate3d(100%, 0, 0);
    transform: translate3d(100%, 0, 0);
    left: 0;
  }
  .carousel-inner > .item.prev,
  .carousel-inner > .item.active.left {
    -webkit-transform: translate3d(-100%, 0, 0);
    transform: translate3d(-100%, 0, 0);
    left: 0;
  }
  .carousel-inner > .item.next.left,
  .carousel-inner > .item.prev.right,
  .carousel-inner > .item.active {
    -webkit-transform: translate3d(0, 0, 0);
    transform: translate3d(0, 0, 0);
    left: 0;
  }
}
.carousel-inner > .active,
.carousel-inner > .next,
.carousel-inner > .prev {
  display: block;
}
.carousel-inner > .active {
  left: 0;
}
.carousel-inner > .next,
.carousel-inner > .prev {
  position: absolute;
  top: 0;
  width: 100%;
}
.carousel-inner > .next {
  left: 100%;
}
.carousel-inner > .prev {
  left: -100%;
}
.carousel-inner > .next.left,
.carousel-inner > .prev.right {
  left: 0;
}
.carousel-inner > .active.left {
  left: -100%;
}
.carousel-inner > .active.right {
  left: 100%;
}
.carousel-control {
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0;
  width: 15%;
  opacity: 0.5;
  filter: alpha(opacity=50);
  font-size: 20px;
  color: #fff;
  text-align: center;
  text-shadow: 0 1px 2px rgba(0, 0, 0, 0.6);
  background-color: rgba(0, 0, 0, 0);
}
.carousel-control.left {
  background-image: -webkit-linear-gradient(left, rgba(0, 0, 0, 0.5) 0%, rgba(0, 0, 0, 0.0001) 100%);
  background-image: -o-linear-gradient(left, rgba(0, 0, 0, 0.5) 0%, rgba(0, 0, 0, 0.0001) 100%);
  background-image: linear-gradient(to right, rgba(0, 0, 0, 0.5) 0%, rgba(0, 0, 0, 0.0001) 100%);
  background-repeat: repeat-x;
  filter: progid:DXImageTransform.Microsoft.gradient(startColorstr='#80000000', endColorstr='#00000000', GradientType=1);
}
.carousel-control.right {
  left: auto;
  right: 0;
  background-image: -webkit-linear-gradient(left, rgba(0, 0, 0, 0.0001) 0%, rgba(0, 0, 0, 0.5) 100%);
  background-image: -o-linear-gradient(left, rgba(0, 0, 0, 0.0001) 0%, rgba(0, 0, 0, 0.5) 100%);
  background-image: linear-gradient(to right, rgba(0, 0, 0, 0.0001) 0%, rgba(0, 0, 0, 0.5) 100%);
  background-repeat: repeat-x;
  filter: progid:DXImageTransform.Microsoft.gradient(startColorstr='#00000000', endColorstr='#80000000', GradientType=1);
}
.carousel-control:hover,
.carousel-control:focus {
  outline: 0;
  color: #fff;
  text-decoration: none;
  opacity: 0.9;
  filter: alpha(opacity=90);
}
.carousel-control .icon-prev,
.carousel-control .icon-next,
.carousel-control .glyphicon-chevron-left,
.carousel-control .glyphicon-chevron-right {
  position: absolute;
  top: 50%;
  margin-top: -10px;
  z-index: 5;
  display: inline-block;
}
.carousel-control .icon-prev,
.carousel-control .glyphicon-chevron-left {
  left: 50%;
  margin-left: -10px;
}
.carousel-control .icon-next,
.carousel-control .glyphicon-chevron-right {
  right: 50%;
  margin-right: -10px;
}
.carousel-control .icon-prev,
.carousel-control .icon-next {
  width: 20px;
  height: 20px;
  line-height: 1;
  font-family: serif;
}
.carousel-control .icon-prev:before {
  content: '\2039';
}
.carousel-control .icon-next:before {
  content: '\203a';
}
.carousel-indicators {
  position: absolute;
  bottom: 10px;
  left: 50%;
  z-index: 15;
  width: 60%;
  margin-left: -30%;
  padding-left: 0;
  list-style: none;
  text-align: center;
}
.carousel-indicators li {
  display: inline-block;
  width: 10px;
  height: 10px;
  margin: 1px;
  text-indent: -999px;
  border: 1px solid #fff;
  border-radius: 10px;
  cursor: pointer;
  background-color: #000 \9;
  background-color: rgba(0, 0, 0, 0);
}
.carousel-indicators .active {
  margin: 0;
  width: 12px;
  height: 12px;
  background-color: #fff;
}
.carousel-caption {
  position: absolute;
  left: 15%;
  right: 15%;
  bottom: 20px;
  z-index: 10;
  padding-top: 20px;
  padding-bottom: 20px;
  color: #fff;
  text-align: center;
  text-shadow: 0 1px 2px rgba(0, 0, 0, 0.6);
}
.carousel-caption .btn {
  text-shadow: none;
}
@media screen and (min-width: 768px) {
  .carousel-control .glyphicon-chevron-left,
  .carousel-control .glyphicon-chevron-right,
  .carousel-control .icon-prev,
  .carousel-control .icon-next {
    width: 30px;
    height: 30px;
    margin-top: -10px;
    font-size: 30px;
  }
  .carousel-control .glyphicon-chevron-left,
  .carousel-control .icon-prev {
    margin-left: -10px;
  }
  .carousel-control .glyphicon-chevron-right,
  .carousel-control .icon-next {
    margin-right: -10px;
  }
  .carousel-caption {
    left: 20%;
    right: 20%;
    padding-bottom: 30px;
  }
  .carousel-indicators {
    bottom: 20px;
  }
}
.clearfix:before,
.clearfix:after,
.dl-horizontal dd:before,
.dl-horizontal dd:after,
.container:before,
.container:after,
.container-fluid:before,
.container-fluid:after,
.row:before,
.row:after,
.form-horizontal .form-group:before,
.form-horizontal .form-group:after,
.btn-toolbar:before,
.btn-toolbar:after,
.btn-group-vertical > .btn-group:before,
.btn-group-vertical > .btn-group:after,
.nav:before,
.nav:after,
.navbar:before,
.navbar:after,
.navbar-header:before,
.navbar-header:after,
.navbar-collapse:before,
.navbar-collapse:after,
.pager:before,
.pager:after,
.panel-body:before,
.panel-body:after,
.modal-header:before,
.modal-header:after,
.modal-footer:before,
.modal-footer:after,
.item_buttons:before,
.item_buttons:after {
  content: " ";
  display: table;
}
.clearfix:after,
.dl-horizontal dd:after,
.container:after,
.container-fluid:after,
.row:after,
.form-horizontal .form-group:after,
.btn-toolbar:after,
.btn-group-vertical > .btn-group:after,
.nav:after,
.navbar:after,
.navbar-header:after,
.navbar-collapse:after,
.pager:after,
.panel-body:after,
.modal-header:after,
.modal-footer:after,
.item_buttons:after {
  clear: both;
}
.center-block {
  display: block;
  margin-left: auto;
  margin-right: auto;
}
.pull-right {
  float: right !important;
}
.pull-left {
  float: left !important;
}
.hide {
  display: none !important;
}
.show {
  display: block !important;
}
.invisible {
  visibility: hidden;
}
.text-hide {
  font: 0/0 a;
  color: transparent;
  text-shadow: none;
  background-color: transparent;
  border: 0;
}
.hidden {
  display: none !important;
}
.affix {
  position: fixed;
}
@-ms-viewport {
  width: device-width;
}
.visible-xs,
.visible-sm,
.visible-md,
.visible-lg {
  display: none !important;
}
.visible-xs-block,
.visible-xs-inline,
.visible-xs-inline-block,
.visible-sm-block,
.visible-sm-inline,
.visible-sm-inline-block,
.visible-md-block,
.visible-md-inline,
.visible-md-inline-block,
.visible-lg-block,
.visible-lg-inline,
.visible-lg-inline-block {
  display: none !important;
}
@media (max-width: 767px) {
  .visible-xs {
    display: block !important;
  }
  table.visible-xs {
    display: table !important;
  }
  tr.visible-xs {
    display: table-row !important;
  }
  th.visible-xs,
  td.visible-xs {
    display: table-cell !important;
  }
}
@media (max-width: 767px) {
  .visible-xs-block {
    display: block !important;
  }
}
@media (max-width: 767px) {
  .visible-xs-inline {
    display: inline !important;
  }
}
@media (max-width: 767px) {
  .visible-xs-inline-block {
    display: inline-block !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .visible-sm {
    display: block !important;
  }
  table.visible-sm {
    display: table !important;
  }
  tr.visible-sm {
    display: table-row !important;
  }
  th.visible-sm,
  td.visible-sm {
    display: table-cell !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .visible-sm-block {
    display: block !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .visible-sm-inline {
    display: inline !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .visible-sm-inline-block {
    display: inline-block !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .visible-md {
    display: block !important;
  }
  table.visible-md {
    display: table !important;
  }
  tr.visible-md {
    display: table-row !important;
  }
  th.visible-md,
  td.visible-md {
    display: table-cell !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .visible-md-block {
    display: block !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .visible-md-inline {
    display: inline !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .visible-md-inline-block {
    display: inline-block !important;
  }
}
@media (min-width: 1200px) {
  .visible-lg {
    display: block !important;
  }
  table.visible-lg {
    display: table !important;
  }
  tr.visible-lg {
    display: table-row !important;
  }
  th.visible-lg,
  td.visible-lg {
    display: table-cell !important;
  }
}
@media (min-width: 1200px) {
  .visible-lg-block {
    display: block !important;
  }
}
@media (min-width: 1200px) {
  .visible-lg-inline {
    display: inline !important;
  }
}
@media (min-width: 1200px) {
  .visible-lg-inline-block {
    display: inline-block !important;
  }
}
@media (max-width: 767px) {
  .hidden-xs {
    display: none !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .hidden-sm {
    display: none !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .hidden-md {
    display: none !important;
  }
}
@media (min-width: 1200px) {
  .hidden-lg {
    display: none !important;
  }
}
.visible-print {
  display: none !important;
}
@media print {
  .visible-print {
    display: block !important;
  }
  table.visible-print {
    display: table !important;
  }
  tr.visible-print {
    display: table-row !important;
  }
  th.visible-print,
  td.visible-print {
    display: table-cell !important;
  }
}
.visible-print-block {
  display: none !important;
}
@media print {
  .visible-print-block {
    display: block !important;
  }
}
.visible-print-inline {
  display: none !important;
}
@media print {
  .visible-print-inline {
    display: inline !important;
  }
}
.visible-print-inline-block {
  display: none !important;
}
@media print {
  .visible-print-inline-block {
    display: inline-block !important;
  }
}
@media print {
  .hidden-print {
    display: none !important;
  }
}
/*!
*
* Font Awesome
*
*/
/*!
 *  Font Awesome 4.2.0 by @davegandy - http://fontawesome.io - @fontawesome
 *  License - http://fontawesome.io/license (Font: SIL OFL 1.1, CSS: MIT License)
 */
/* FONT PATH
 * -------------------------- */
@font-face {
  font-family: 'FontAwesome';
  src: url('../components/font-awesome/fonts/fontawesome-webfont.eot?v=4.2.0');
  src: url('../components/font-awesome/fonts/fontawesome-webfont.eot?#iefix&v=4.2.0') format('embedded-opentype'), url('../components/font-awesome/fonts/fontawesome-webfont.woff?v=4.2.0') format('woff'), url('../components/font-awesome/fonts/fontawesome-webfont.ttf?v=4.2.0') format('truetype'), url('../components/font-awesome/fonts/fontawesome-webfont.svg?v=4.2.0#fontawesomeregular') format('svg');
  font-weight: normal;
  font-style: normal;
}
.fa {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}
/* makes the font 33% larger relative to the icon container */
.fa-lg {
  font-size: 1.33333333em;
  line-height: 0.75em;
  vertical-align: -15%;
}
.fa-2x {
  font-size: 2em;
}
.fa-3x {
  font-size: 3em;
}
.fa-4x {
  font-size: 4em;
}
.fa-5x {
  font-size: 5em;
}
.fa-fw {
  width: 1.28571429em;
  text-align: center;
}
.fa-ul {
  padding-left: 0;
  margin-left: 2.14285714em;
  list-style-type: none;
}
.fa-ul > li {
  position: relative;
}
.fa-li {
  position: absolute;
  left: -2.14285714em;
  width: 2.14285714em;
  top: 0.14285714em;
  text-align: center;
}
.fa-li.fa-lg {
  left: -1.85714286em;
}
.fa-border {
  padding: .2em .25em .15em;
  border: solid 0.08em #eee;
  border-radius: .1em;
}
.pull-right {
  float: right;
}
.pull-left {
  float: left;
}
.fa.pull-left {
  margin-right: .3em;
}
.fa.pull-right {
  margin-left: .3em;
}
.fa-spin {
  -webkit-animation: fa-spin 2s infinite linear;
  animation: fa-spin 2s infinite linear;
}
@-webkit-keyframes fa-spin {
  0% {
    -webkit-transform: rotate(0deg);
    transform: rotate(0deg);
  }
  100% {
    -webkit-transform: rotate(359deg);
    transform: rotate(359deg);
  }
}
@keyframes fa-spin {
  0% {
    -webkit-transform: rotate(0deg);
    transform: rotate(0deg);
  }
  100% {
    -webkit-transform: rotate(359deg);
    transform: rotate(359deg);
  }
}
.fa-rotate-90 {
  filter: progid:DXImageTransform.Microsoft.BasicImage(rotation=1);
  -webkit-transform: rotate(90deg);
  -ms-transform: rotate(90deg);
  transform: rotate(90deg);
}
.fa-rotate-180 {
  filter: progid:DXImageTransform.Microsoft.BasicImage(rotation=2);
  -webkit-transform: rotate(180deg);
  -ms-transform: rotate(180deg);
  transform: rotate(180deg);
}
.fa-rotate-270 {
  filter: progid:DXImageTransform.Microsoft.BasicImage(rotation=3);
  -webkit-transform: rotate(270deg);
  -ms-transform: rotate(270deg);
  transform: rotate(270deg);
}
.fa-flip-horizontal {
  filter: progid:DXImageTransform.Microsoft.BasicImage(rotation=0, mirror=1);
  -webkit-transform: scale(-1, 1);
  -ms-transform: scale(-1, 1);
  transform: scale(-1, 1);
}
.fa-flip-vertical {
  filter: progid:DXImageTransform.Microsoft.BasicImage(rotation=2, mirror=1);
  -webkit-transform: scale(1, -1);
  -ms-transform: scale(1, -1);
  transform: scale(1, -1);
}
:root .fa-rotate-90,
:root .fa-rotate-180,
:root .fa-rotate-270,
:root .fa-flip-horizontal,
:root .fa-flip-vertical {
  filter: none;
}
.fa-stack {
  position: relative;
  display: inline-block;
  width: 2em;
  height: 2em;
  line-height: 2em;
  vertical-align: middle;
}
.fa-stack-1x,
.fa-stack-2x {
  position: absolute;
  left: 0;
  width: 100%;
  text-align: center;
}
.fa-stack-1x {
  line-height: inherit;
}
.fa-stack-2x {
  font-size: 2em;
}
.fa-inverse {
  color: #fff;
}
/* Font Awesome uses the Unicode Private Use Area (PUA) to ensure screen
   readers do not read off random characters that represent icons */
.fa-glass:before {
  content: "\f000";
}
.fa-music:before {
  content: "\f001";
}
.fa-search:before {
  content: "\f002";
}
.fa-envelope-o:before {
  content: "\f003";
}
.fa-heart:before {
  content: "\f004";
}
.fa-star:before {
  content: "\f005";
}
.fa-star-o:before {
  content: "\f006";
}
.fa-user:before {
  content: "\f007";
}
.fa-film:before {
  content: "\f008";
}
.fa-th-large:before {
  content: "\f009";
}
.fa-th:before {
  content: "\f00a";
}
.fa-th-list:before {
  content: "\f00b";
}
.fa-check:before {
  content: "\f00c";
}
.fa-remove:before,
.fa-close:before,
.fa-times:before {
  content: "\f00d";
}
.fa-search-plus:before {
  content: "\f00e";
}
.fa-search-minus:before {
  content: "\f010";
}
.fa-power-off:before {
  content: "\f011";
}
.fa-signal:before {
  content: "\f012";
}
.fa-gear:before,
.fa-cog:before {
  content: "\f013";
}
.fa-trash-o:before {
  content: "\f014";
}
.fa-home:before {
  content: "\f015";
}
.fa-file-o:before {
  content: "\f016";
}
.fa-clock-o:before {
  content: "\f017";
}
.fa-road:before {
  content: "\f018";
}
.fa-download:before {
  content: "\f019";
}
.fa-arrow-circle-o-down:before {
  content: "\f01a";
}
.fa-arrow-circle-o-up:before {
  content: "\f01b";
}
.fa-inbox:before {
  content: "\f01c";
}
.fa-play-circle-o:before {
  content: "\f01d";
}
.fa-rotate-right:before,
.fa-repeat:before {
  content: "\f01e";
}
.fa-refresh:before {
  content: "\f021";
}
.fa-list-alt:before {
  content: "\f022";
}
.fa-lock:before {
  content: "\f023";
}
.fa-flag:before {
  content: "\f024";
}
.fa-headphones:before {
  content: "\f025";
}
.fa-volume-off:before {
  content: "\f026";
}
.fa-volume-down:before {
  content: "\f027";
}
.fa-volume-up:before {
  content: "\f028";
}
.fa-qrcode:before {
  content: "\f029";
}
.fa-barcode:before {
  content: "\f02a";
}
.fa-tag:before {
  content: "\f02b";
}
.fa-tags:before {
  content: "\f02c";
}
.fa-book:before {
  content: "\f02d";
}
.fa-bookmark:before {
  content: "\f02e";
}
.fa-print:before {
  content: "\f02f";
}
.fa-camera:before {
  content: "\f030";
}
.fa-font:before {
  content: "\f031";
}
.fa-bold:before {
  content: "\f032";
}
.fa-italic:before {
  content: "\f033";
}
.fa-text-height:before {
  content: "\f034";
}
.fa-text-width:before {
  content: "\f035";
}
.fa-align-left:before {
  content: "\f036";
}
.fa-align-center:before {
  content: "\f037";
}
.fa-align-right:before {
  content: "\f038";
}
.fa-align-justify:before {
  content: "\f039";
}
.fa-list:before {
  content: "\f03a";
}
.fa-dedent:before,
.fa-outdent:before {
  content: "\f03b";
}
.fa-indent:before {
  content: "\f03c";
}
.fa-video-camera:before {
  content: "\f03d";
}
.fa-photo:before,
.fa-image:before,
.fa-picture-o:before {
  content: "\f03e";
}
.fa-pencil:before {
  content: "\f040";
}
.fa-map-marker:before {
  content: "\f041";
}
.fa-adjust:before {
  content: "\f042";
}
.fa-tint:before {
  content: "\f043";
}
.fa-edit:before,
.fa-pencil-square-o:before {
  content: "\f044";
}
.fa-share-square-o:before {
  content: "\f045";
}
.fa-check-square-o:before {
  content: "\f046";
}
.fa-arrows:before {
  content: "\f047";
}
.fa-step-backward:before {
  content: "\f048";
}
.fa-fast-backward:before {
  content: "\f049";
}
.fa-backward:before {
  content: "\f04a";
}
.fa-play:before {
  content: "\f04b";
}
.fa-pause:before {
  content: "\f04c";
}
.fa-stop:before {
  content: "\f04d";
}
.fa-forward:before {
  content: "\f04e";
}
.fa-fast-forward:before {
  content: "\f050";
}
.fa-step-forward:before {
  content: "\f051";
}
.fa-eject:before {
  content: "\f052";
}
.fa-chevron-left:before {
  content: "\f053";
}
.fa-chevron-right:before {
  content: "\f054";
}
.fa-plus-circle:before {
  content: "\f055";
}
.fa-minus-circle:before {
  content: "\f056";
}
.fa-times-circle:before {
  content: "\f057";
}
.fa-check-circle:before {
  content: "\f058";
}
.fa-question-circle:before {
  content: "\f059";
}
.fa-info-circle:before {
  content: "\f05a";
}
.fa-crosshairs:before {
  content: "\f05b";
}
.fa-times-circle-o:before {
  content: "\f05c";
}
.fa-check-circle-o:before {
  content: "\f05d";
}
.fa-ban:before {
  content: "\f05e";
}
.fa-arrow-left:before {
  content: "\f060";
}
.fa-arrow-right:before {
  content: "\f061";
}
.fa-arrow-up:before {
  content: "\f062";
}
.fa-arrow-down:before {
  content: "\f063";
}
.fa-mail-forward:before,
.fa-share:before {
  content: "\f064";
}
.fa-expand:before {
  content: "\f065";
}
.fa-compress:before {
  content: "\f066";
}
.fa-plus:before {
  content: "\f067";
}
.fa-minus:before {
  content: "\f068";
}
.fa-asterisk:before {
  content: "\f069";
}
.fa-exclamation-circle:before {
  content: "\f06a";
}
.fa-gift:before {
  content: "\f06b";
}
.fa-leaf:before {
  content: "\f06c";
}
.fa-fire:before {
  content: "\f06d";
}
.fa-eye:before {
  content: "\f06e";
}
.fa-eye-slash:before {
  content: "\f070";
}
.fa-warning:before,
.fa-exclamation-triangle:before {
  content: "\f071";
}
.fa-plane:before {
  content: "\f072";
}
.fa-calendar:before {
  content: "\f073";
}
.fa-random:before {
  content: "\f074";
}
.fa-comment:before {
  content: "\f075";
}
.fa-magnet:before {
  content: "\f076";
}
.fa-chevron-up:before {
  content: "\f077";
}
.fa-chevron-down:before {
  content: "\f078";
}
.fa-retweet:before {
  content: "\f079";
}
.fa-shopping-cart:before {
  content: "\f07a";
}
.fa-folder:before {
  content: "\f07b";
}
.fa-folder-open:before {
  content: "\f07c";
}
.fa-arrows-v:before {
  content: "\f07d";
}
.fa-arrows-h:before {
  content: "\f07e";
}
.fa-bar-chart-o:before,
.fa-bar-chart:before {
  content: "\f080";
}
.fa-twitter-square:before {
  content: "\f081";
}
.fa-facebook-square:before {
  content: "\f082";
}
.fa-camera-retro:before {
  content: "\f083";
}
.fa-key:before {
  content: "\f084";
}
.fa-gears:before,
.fa-cogs:before {
  content: "\f085";
}
.fa-comments:before {
  content: "\f086";
}
.fa-thumbs-o-up:before {
  content: "\f087";
}
.fa-thumbs-o-down:before {
  content: "\f088";
}
.fa-star-half:before {
  content: "\f089";
}
.fa-heart-o:before {
  content: "\f08a";
}
.fa-sign-out:before {
  content: "\f08b";
}
.fa-linkedin-square:before {
  content: "\f08c";
}
.fa-thumb-tack:before {
  content: "\f08d";
}
.fa-external-link:before {
  content: "\f08e";
}
.fa-sign-in:before {
  content: "\f090";
}
.fa-trophy:before {
  content: "\f091";
}
.fa-github-square:before {
  content: "\f092";
}
.fa-upload:before {
  content: "\f093";
}
.fa-lemon-o:before {
  content: "\f094";
}
.fa-phone:before {
  content: "\f095";
}
.fa-square-o:before {
  content: "\f096";
}
.fa-bookmark-o:before {
  content: "\f097";
}
.fa-phone-square:before {
  content: "\f098";
}
.fa-twitter:before {
  content: "\f099";
}
.fa-facebook:before {
  content: "\f09a";
}
.fa-github:before {
  content: "\f09b";
}
.fa-unlock:before {
  content: "\f09c";
}
.fa-credit-card:before {
  content: "\f09d";
}
.fa-rss:before {
  content: "\f09e";
}
.fa-hdd-o:before {
  content: "\f0a0";
}
.fa-bullhorn:before {
  content: "\f0a1";
}
.fa-bell:before {
  content: "\f0f3";
}
.fa-certificate:before {
  content: "\f0a3";
}
.fa-hand-o-right:before {
  content: "\f0a4";
}
.fa-hand-o-left:before {
  content: "\f0a5";
}
.fa-hand-o-up:before {
  content: "\f0a6";
}
.fa-hand-o-down:before {
  content: "\f0a7";
}
.fa-arrow-circle-left:before {
  content: "\f0a8";
}
.fa-arrow-circle-right:before {
  content: "\f0a9";
}
.fa-arrow-circle-up:before {
  content: "\f0aa";
}
.fa-arrow-circle-down:before {
  content: "\f0ab";
}
.fa-globe:before {
  content: "\f0ac";
}
.fa-wrench:before {
  content: "\f0ad";
}
.fa-tasks:before {
  content: "\f0ae";
}
.fa-filter:before {
  content: "\f0b0";
}
.fa-briefcase:before {
  content: "\f0b1";
}
.fa-arrows-alt:before {
  content: "\f0b2";
}
.fa-group:before,
.fa-users:before {
  content: "\f0c0";
}
.fa-chain:before,
.fa-link:before {
  content: "\f0c1";
}
.fa-cloud:before {
  content: "\f0c2";
}
.fa-flask:before {
  content: "\f0c3";
}
.fa-cut:before,
.fa-scissors:before {
  content: "\f0c4";
}
.fa-copy:before,
.fa-files-o:before {
  content: "\f0c5";
}
.fa-paperclip:before {
  content: "\f0c6";
}
.fa-save:before,
.fa-floppy-o:before {
  content: "\f0c7";
}
.fa-square:before {
  content: "\f0c8";
}
.fa-navicon:before,
.fa-reorder:before,
.fa-bars:before {
  content: "\f0c9";
}
.fa-list-ul:before {
  content: "\f0ca";
}
.fa-list-ol:before {
  content: "\f0cb";
}
.fa-strikethrough:before {
  content: "\f0cc";
}
.fa-underline:before {
  content: "\f0cd";
}
.fa-table:before {
  content: "\f0ce";
}
.fa-magic:before {
  content: "\f0d0";
}
.fa-truck:before {
  content: "\f0d1";
}
.fa-pinterest:before {
  content: "\f0d2";
}
.fa-pinterest-square:before {
  content: "\f0d3";
}
.fa-google-plus-square:before {
  content: "\f0d4";
}
.fa-google-plus:before {
  content: "\f0d5";
}
.fa-money:before {
  content: "\f0d6";
}
.fa-caret-down:before {
  content: "\f0d7";
}
.fa-caret-up:before {
  content: "\f0d8";
}
.fa-caret-left:before {
  content: "\f0d9";
}
.fa-caret-right:before {
  content: "\f0da";
}
.fa-columns:before {
  content: "\f0db";
}
.fa-unsorted:before,
.fa-sort:before {
  content: "\f0dc";
}
.fa-sort-down:before,
.fa-sort-desc:before {
  content: "\f0dd";
}
.fa-sort-up:before,
.fa-sort-asc:before {
  content: "\f0de";
}
.fa-envelope:before {
  content: "\f0e0";
}
.fa-linkedin:before {
  content: "\f0e1";
}
.fa-rotate-left:before,
.fa-undo:before {
  content: "\f0e2";
}
.fa-legal:before,
.fa-gavel:before {
  content: "\f0e3";
}
.fa-dashboard:before,
.fa-tachometer:before {
  content: "\f0e4";
}
.fa-comment-o:before {
  content: "\f0e5";
}
.fa-comments-o:before {
  content: "\f0e6";
}
.fa-flash:before,
.fa-bolt:before {
  content: "\f0e7";
}
.fa-sitemap:before {
  content: "\f0e8";
}
.fa-umbrella:before {
  content: "\f0e9";
}
.fa-paste:before,
.fa-clipboard:before {
  content: "\f0ea";
}
.fa-lightbulb-o:before {
  content: "\f0eb";
}
.fa-exchange:before {
  content: "\f0ec";
}
.fa-cloud-download:before {
  content: "\f0ed";
}
.fa-cloud-upload:before {
  content: "\f0ee";
}
.fa-user-md:before {
  content: "\f0f0";
}
.fa-stethoscope:before {
  content: "\f0f1";
}
.fa-suitcase:before {
  content: "\f0f2";
}
.fa-bell-o:before {
  content: "\f0a2";
}
.fa-coffee:before {
  content: "\f0f4";
}
.fa-cutlery:before {
  content: "\f0f5";
}
.fa-file-text-o:before {
  content: "\f0f6";
}
.fa-building-o:before {
  content: "\f0f7";
}
.fa-hospital-o:before {
  content: "\f0f8";
}
.fa-ambulance:before {
  content: "\f0f9";
}
.fa-medkit:before {
  content: "\f0fa";
}
.fa-fighter-jet:before {
  content: "\f0fb";
}
.fa-beer:before {
  content: "\f0fc";
}
.fa-h-square:before {
  content: "\f0fd";
}
.fa-plus-square:before {
  content: "\f0fe";
}
.fa-angle-double-left:before {
  content: "\f100";
}
.fa-angle-double-right:before {
  content: "\f101";
}
.fa-angle-double-up:before {
  content: "\f102";
}
.fa-angle-double-down:before {
  content: "\f103";
}
.fa-angle-left:before {
  content: "\f104";
}
.fa-angle-right:before {
  content: "\f105";
}
.fa-angle-up:before {
  content: "\f106";
}
.fa-angle-down:before {
  content: "\f107";
}
.fa-desktop:before {
  content: "\f108";
}
.fa-laptop:before {
  content: "\f109";
}
.fa-tablet:before {
  content: "\f10a";
}
.fa-mobile-phone:before,
.fa-mobile:before {
  content: "\f10b";
}
.fa-circle-o:before {
  content: "\f10c";
}
.fa-quote-left:before {
  content: "\f10d";
}
.fa-quote-right:before {
  content: "\f10e";
}
.fa-spinner:before {
  content: "\f110";
}
.fa-circle:before {
  content: "\f111";
}
.fa-mail-reply:before,
.fa-reply:before {
  content: "\f112";
}
.fa-github-alt:before {
  content: "\f113";
}
.fa-folder-o:before {
  content: "\f114";
}
.fa-folder-open-o:before {
  content: "\f115";
}
.fa-smile-o:before {
  content: "\f118";
}
.fa-frown-o:before {
  content: "\f119";
}
.fa-meh-o:before {
  content: "\f11a";
}
.fa-gamepad:before {
  content: "\f11b";
}
.fa-keyboard-o:before {
  content: "\f11c";
}
.fa-flag-o:before {
  content: "\f11d";
}
.fa-flag-checkered:before {
  content: "\f11e";
}
.fa-terminal:before {
  content: "\f120";
}
.fa-code:before {
  content: "\f121";
}
.fa-mail-reply-all:before,
.fa-reply-all:before {
  content: "\f122";
}
.fa-star-half-empty:before,
.fa-star-half-full:before,
.fa-star-half-o:before {
  content: "\f123";
}
.fa-location-arrow:before {
  content: "\f124";
}
.fa-crop:before {
  content: "\f125";
}
.fa-code-fork:before {
  content: "\f126";
}
.fa-unlink:before,
.fa-chain-broken:before {
  content: "\f127";
}
.fa-question:before {
  content: "\f128";
}
.fa-info:before {
  content: "\f129";
}
.fa-exclamation:before {
  content: "\f12a";
}
.fa-superscript:before {
  content: "\f12b";
}
.fa-subscript:before {
  content: "\f12c";
}
.fa-eraser:before {
  content: "\f12d";
}
.fa-puzzle-piece:before {
  content: "\f12e";
}
.fa-microphone:before {
  content: "\f130";
}
.fa-microphone-slash:before {
  content: "\f131";
}
.fa-shield:before {
  content: "\f132";
}
.fa-calendar-o:before {
  content: "\f133";
}
.fa-fire-extinguisher:before {
  content: "\f134";
}
.fa-rocket:before {
  content: "\f135";
}
.fa-maxcdn:before {
  content: "\f136";
}
.fa-chevron-circle-left:before {
  content: "\f137";
}
.fa-chevron-circle-right:before {
  content: "\f138";
}
.fa-chevron-circle-up:before {
  content: "\f139";
}
.fa-chevron-circle-down:before {
  content: "\f13a";
}
.fa-html5:before {
  content: "\f13b";
}
.fa-css3:before {
  content: "\f13c";
}
.fa-anchor:before {
  content: "\f13d";
}
.fa-unlock-alt:before {
  content: "\f13e";
}
.fa-bullseye:before {
  content: "\f140";
}
.fa-ellipsis-h:before {
  content: "\f141";
}
.fa-ellipsis-v:before {
  content: "\f142";
}
.fa-rss-square:before {
  content: "\f143";
}
.fa-play-circle:before {
  content: "\f144";
}
.fa-ticket:before {
  content: "\f145";
}
.fa-minus-square:before {
  content: "\f146";
}
.fa-minus-square-o:before {
  content: "\f147";
}
.fa-level-up:before {
  content: "\f148";
}
.fa-level-down:before {
  content: "\f149";
}
.fa-check-square:before {
  content: "\f14a";
}
.fa-pencil-square:before {
  content: "\f14b";
}
.fa-external-link-square:before {
  content: "\f14c";
}
.fa-share-square:before {
  content: "\f14d";
}
.fa-compass:before {
  content: "\f14e";
}
.fa-toggle-down:before,
.fa-caret-square-o-down:before {
  content: "\f150";
}
.fa-toggle-up:before,
.fa-caret-square-o-up:before {
  content: "\f151";
}
.fa-toggle-right:before,
.fa-caret-square-o-right:before {
  content: "\f152";
}
.fa-euro:before,
.fa-eur:before {
  content: "\f153";
}
.fa-gbp:before {
  content: "\f154";
}
.fa-dollar:before,
.fa-usd:before {
  content: "\f155";
}
.fa-rupee:before,
.fa-inr:before {
  content: "\f156";
}
.fa-cny:before,
.fa-rmb:before,
.fa-yen:before,
.fa-jpy:before {
  content: "\f157";
}
.fa-ruble:before,
.fa-rouble:before,
.fa-rub:before {
  content: "\f158";
}
.fa-won:before,
.fa-krw:before {
  content: "\f159";
}
.fa-bitcoin:before,
.fa-btc:before {
  content: "\f15a";
}
.fa-file:before {
  content: "\f15b";
}
.fa-file-text:before {
  content: "\f15c";
}
.fa-sort-alpha-asc:before {
  content: "\f15d";
}
.fa-sort-alpha-desc:before {
  content: "\f15e";
}
.fa-sort-amount-asc:before {
  content: "\f160";
}
.fa-sort-amount-desc:before {
  content: "\f161";
}
.fa-sort-numeric-asc:before {
  content: "\f162";
}
.fa-sort-numeric-desc:before {
  content: "\f163";
}
.fa-thumbs-up:before {
  content: "\f164";
}
.fa-thumbs-down:before {
  content: "\f165";
}
.fa-youtube-square:before {
  content: "\f166";
}
.fa-youtube:before {
  content: "\f167";
}
.fa-xing:before {
  content: "\f168";
}
.fa-xing-square:before {
  content: "\f169";
}
.fa-youtube-play:before {
  content: "\f16a";
}
.fa-dropbox:before {
  content: "\f16b";
}
.fa-stack-overflow:before {
  content: "\f16c";
}
.fa-instagram:before {
  content: "\f16d";
}
.fa-flickr:before {
  content: "\f16e";
}
.fa-adn:before {
  content: "\f170";
}
.fa-bitbucket:before {
  content: "\f171";
}
.fa-bitbucket-square:before {
  content: "\f172";
}
.fa-tumblr:before {
  content: "\f173";
}
.fa-tumblr-square:before {
  content: "\f174";
}
.fa-long-arrow-down:before {
  content: "\f175";
}
.fa-long-arrow-up:before {
  content: "\f176";
}
.fa-long-arrow-left:before {
  content: "\f177";
}
.fa-long-arrow-right:before {
  content: "\f178";
}
.fa-apple:before {
  content: "\f179";
}
.fa-windows:before {
  content: "\f17a";
}
.fa-android:before {
  content: "\f17b";
}
.fa-linux:before {
  content: "\f17c";
}
.fa-dribbble:before {
  content: "\f17d";
}
.fa-skype:before {
  content: "\f17e";
}
.fa-foursquare:before {
  content: "\f180";
}
.fa-trello:before {
  content: "\f181";
}
.fa-female:before {
  content: "\f182";
}
.fa-male:before {
  content: "\f183";
}
.fa-gittip:before {
  content: "\f184";
}
.fa-sun-o:before {
  content: "\f185";
}
.fa-moon-o:before {
  content: "\f186";
}
.fa-archive:before {
  content: "\f187";
}
.fa-bug:before {
  content: "\f188";
}
.fa-vk:before {
  content: "\f189";
}
.fa-weibo:before {
  content: "\f18a";
}
.fa-renren:before {
  content: "\f18b";
}
.fa-pagelines:before {
  content: "\f18c";
}
.fa-stack-exchange:before {
  content: "\f18d";
}
.fa-arrow-circle-o-right:before {
  content: "\f18e";
}
.fa-arrow-circle-o-left:before {
  content: "\f190";
}
.fa-toggle-left:before,
.fa-caret-square-o-left:before {
  content: "\f191";
}
.fa-dot-circle-o:before {
  content: "\f192";
}
.fa-wheelchair:before {
  content: "\f193";
}
.fa-vimeo-square:before {
  content: "\f194";
}
.fa-turkish-lira:before,
.fa-try:before {
  content: "\f195";
}
.fa-plus-square-o:before {
  content: "\f196";
}
.fa-space-shuttle:before {
  content: "\f197";
}
.fa-slack:before {
  content: "\f198";
}
.fa-envelope-square:before {
  content: "\f199";
}
.fa-wordpress:before {
  content: "\f19a";
}
.fa-openid:before {
  content: "\f19b";
}
.fa-institution:before,
.fa-bank:before,
.fa-university:before {
  content: "\f19c";
}
.fa-mortar-board:before,
.fa-graduation-cap:before {
  content: "\f19d";
}
.fa-yahoo:before {
  content: "\f19e";
}
.fa-google:before {
  content: "\f1a0";
}
.fa-reddit:before {
  content: "\f1a1";
}
.fa-reddit-square:before {
  content: "\f1a2";
}
.fa-stumbleupon-circle:before {
  content: "\f1a3";
}
.fa-stumbleupon:before {
  content: "\f1a4";
}
.fa-delicious:before {
  content: "\f1a5";
}
.fa-digg:before {
  content: "\f1a6";
}
.fa-pied-piper:before {
  content: "\f1a7";
}
.fa-pied-piper-alt:before {
  content: "\f1a8";
}
.fa-drupal:before {
  content: "\f1a9";
}
.fa-joomla:before {
  content: "\f1aa";
}
.fa-language:before {
  content: "\f1ab";
}
.fa-fax:before {
  content: "\f1ac";
}
.fa-building:before {
  content: "\f1ad";
}
.fa-child:before {
  content: "\f1ae";
}
.fa-paw:before {
  content: "\f1b0";
}
.fa-spoon:before {
  content: "\f1b1";
}
.fa-cube:before {
  content: "\f1b2";
}
.fa-cubes:before {
  content: "\f1b3";
}
.fa-behance:before {
  content: "\f1b4";
}
.fa-behance-square:before {
  content: "\f1b5";
}
.fa-steam:before {
  content: "\f1b6";
}
.fa-steam-square:before {
  content: "\f1b7";
}
.fa-recycle:before {
  content: "\f1b8";
}
.fa-automobile:before,
.fa-car:before {
  content: "\f1b9";
}
.fa-cab:before,
.fa-taxi:before {
  content: "\f1ba";
}
.fa-tree:before {
  content: "\f1bb";
}
.fa-spotify:before {
  content: "\f1bc";
}
.fa-deviantart:before {
  content: "\f1bd";
}
.fa-soundcloud:before {
  content: "\f1be";
}
.fa-database:before {
  content: "\f1c0";
}
.fa-file-pdf-o:before {
  content: "\f1c1";
}
.fa-file-word-o:before {
  content: "\f1c2";
}
.fa-file-excel-o:before {
  content: "\f1c3";
}
.fa-file-powerpoint-o:before {
  content: "\f1c4";
}
.fa-file-photo-o:before,
.fa-file-picture-o:before,
.fa-file-image-o:before {
  content: "\f1c5";
}
.fa-file-zip-o:before,
.fa-file-archive-o:before {
  content: "\f1c6";
}
.fa-file-sound-o:before,
.fa-file-audio-o:before {
  content: "\f1c7";
}
.fa-file-movie-o:before,
.fa-file-video-o:before {
  content: "\f1c8";
}
.fa-file-code-o:before {
  content: "\f1c9";
}
.fa-vine:before {
  content: "\f1ca";
}
.fa-codepen:before {
  content: "\f1cb";
}
.fa-jsfiddle:before {
  content: "\f1cc";
}
.fa-life-bouy:before,
.fa-life-buoy:before,
.fa-life-saver:before,
.fa-support:before,
.fa-life-ring:before {
  content: "\f1cd";
}
.fa-circle-o-notch:before {
  content: "\f1ce";
}
.fa-ra:before,
.fa-rebel:before {
  content: "\f1d0";
}
.fa-ge:before,
.fa-empire:before {
  content: "\f1d1";
}
.fa-git-square:before {
  content: "\f1d2";
}
.fa-git:before {
  content: "\f1d3";
}
.fa-hacker-news:before {
  content: "\f1d4";
}
.fa-tencent-weibo:before {
  content: "\f1d5";
}
.fa-qq:before {
  content: "\f1d6";
}
.fa-wechat:before,
.fa-weixin:before {
  content: "\f1d7";
}
.fa-send:before,
.fa-paper-plane:before {
  content: "\f1d8";
}
.fa-send-o:before,
.fa-paper-plane-o:before {
  content: "\f1d9";
}
.fa-history:before {
  content: "\f1da";
}
.fa-circle-thin:before {
  content: "\f1db";
}
.fa-header:before {
  content: "\f1dc";
}
.fa-paragraph:before {
  content: "\f1dd";
}
.fa-sliders:before {
  content: "\f1de";
}
.fa-share-alt:before {
  content: "\f1e0";
}
.fa-share-alt-square:before {
  content: "\f1e1";
}
.fa-bomb:before {
  content: "\f1e2";
}
.fa-soccer-ball-o:before,
.fa-futbol-o:before {
  content: "\f1e3";
}
.fa-tty:before {
  content: "\f1e4";
}
.fa-binoculars:before {
  content: "\f1e5";
}
.fa-plug:before {
  content: "\f1e6";
}
.fa-slideshare:before {
  content: "\f1e7";
}
.fa-twitch:before {
  content: "\f1e8";
}
.fa-yelp:before {
  content: "\f1e9";
}
.fa-newspaper-o:before {
  content: "\f1ea";
}
.fa-wifi:before {
  content: "\f1eb";
}
.fa-calculator:before {
  content: "\f1ec";
}
.fa-paypal:before {
  content: "\f1ed";
}
.fa-google-wallet:before {
  content: "\f1ee";
}
.fa-cc-visa:before {
  content: "\f1f0";
}
.fa-cc-mastercard:before {
  content: "\f1f1";
}
.fa-cc-discover:before {
  content: "\f1f2";
}
.fa-cc-amex:before {
  content: "\f1f3";
}
.fa-cc-paypal:before {
  content: "\f1f4";
}
.fa-cc-stripe:before {
  content: "\f1f5";
}
.fa-bell-slash:before {
  content: "\f1f6";
}
.fa-bell-slash-o:before {
  content: "\f1f7";
}
.fa-trash:before {
  content: "\f1f8";
}
.fa-copyright:before {
  content: "\f1f9";
}
.fa-at:before {
  content: "\f1fa";
}
.fa-eyedropper:before {
  content: "\f1fb";
}
.fa-paint-brush:before {
  content: "\f1fc";
}
.fa-birthday-cake:before {
  content: "\f1fd";
}
.fa-area-chart:before {
  content: "\f1fe";
}
.fa-pie-chart:before {
  content: "\f200";
}
.fa-line-chart:before {
  content: "\f201";
}
.fa-lastfm:before {
  content: "\f202";
}
.fa-lastfm-square:before {
  content: "\f203";
}
.fa-toggle-off:before {
  content: "\f204";
}
.fa-toggle-on:before {
  content: "\f205";
}
.fa-bicycle:before {
  content: "\f206";
}
.fa-bus:before {
  content: "\f207";
}
.fa-ioxhost:before {
  content: "\f208";
}
.fa-angellist:before {
  content: "\f209";
}
.fa-cc:before {
  content: "\f20a";
}
.fa-shekel:before,
.fa-sheqel:before,
.fa-ils:before {
  content: "\f20b";
}
.fa-meanpath:before {
  content: "\f20c";
}
/*!
*
* IPython base
*
*/
.modal.fade .modal-dialog {
  -webkit-transform: translate(0, 0);
  -ms-transform: translate(0, 0);
  -o-transform: translate(0, 0);
  transform: translate(0, 0);
}
code {
  color: #000;
}
pre {
  font-size: inherit;
  line-height: inherit;
}
label {
  font-weight: normal;
}
/* Make the page background atleast 100% the height of the view port */
/* Make the page itself atleast 70% the height of the view port */
.border-box-sizing {
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
.corner-all {
  border-radius: 2px;
}
.no-padding {
  padding: 0px;
}
/* Flexible box model classes */
/* Taken from Alex Russell http://infrequently.org/2009/08/css-3-progress/ */
/* This file is a compatability layer.  It allows the usage of flexible box 
model layouts accross multiple browsers, including older browsers.  The newest,
universal implementation of the flexible box model is used when available (see
`Modern browsers` comments below).  Browsers that are known to implement this 
new spec completely include:

    Firefox 28.0+
    Chrome 29.0+
    Internet Explorer 11+ 
    Opera 17.0+

Browsers not listed, including Safari, are supported via the styling under the
`Old browsers` comments below.
*/
.hbox {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
.hbox > * {
  /* Old browsers */
  -webkit-box-flex: 0;
  -moz-box-flex: 0;
  box-flex: 0;
  /* Modern browsers */
  flex: none;
}
.vbox {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
.vbox > * {
  /* Old browsers */
  -webkit-box-flex: 0;
  -moz-box-flex: 0;
  box-flex: 0;
  /* Modern browsers */
  flex: none;
}
.hbox.reverse,
.vbox.reverse,
.reverse {
  /* Old browsers */
  -webkit-box-direction: reverse;
  -moz-box-direction: reverse;
  box-direction: reverse;
  /* Modern browsers */
  flex-direction: row-reverse;
}
.hbox.box-flex0,
.vbox.box-flex0,
.box-flex0 {
  /* Old browsers */
  -webkit-box-flex: 0;
  -moz-box-flex: 0;
  box-flex: 0;
  /* Modern browsers */
  flex: none;
  width: auto;
}
.hbox.box-flex1,
.vbox.box-flex1,
.box-flex1 {
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
.hbox.box-flex,
.vbox.box-flex,
.box-flex {
  /* Old browsers */
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
.hbox.box-flex2,
.vbox.box-flex2,
.box-flex2 {
  /* Old browsers */
  -webkit-box-flex: 2;
  -moz-box-flex: 2;
  box-flex: 2;
  /* Modern browsers */
  flex: 2;
}
.box-group1 {
  /*  Deprecated */
  -webkit-box-flex-group: 1;
  -moz-box-flex-group: 1;
  box-flex-group: 1;
}
.box-group2 {
  /* Deprecated */
  -webkit-box-flex-group: 2;
  -moz-box-flex-group: 2;
  box-flex-group: 2;
}
.hbox.start,
.vbox.start,
.start {
  /* Old browsers */
  -webkit-box-pack: start;
  -moz-box-pack: start;
  box-pack: start;
  /* Modern browsers */
  justify-content: flex-start;
}
.hbox.end,
.vbox.end,
.end {
  /* Old browsers */
  -webkit-box-pack: end;
  -moz-box-pack: end;
  box-pack: end;
  /* Modern browsers */
  justify-content: flex-end;
}
.hbox.center,
.vbox.center,
.center {
  /* Old browsers */
  -webkit-box-pack: center;
  -moz-box-pack: center;
  box-pack: center;
  /* Modern browsers */
  justify-content: center;
}
.hbox.baseline,
.vbox.baseline,
.baseline {
  /* Old browsers */
  -webkit-box-pack: baseline;
  -moz-box-pack: baseline;
  box-pack: baseline;
  /* Modern browsers */
  justify-content: baseline;
}
.hbox.stretch,
.vbox.stretch,
.stretch {
  /* Old browsers */
  -webkit-box-pack: stretch;
  -moz-box-pack: stretch;
  box-pack: stretch;
  /* Modern browsers */
  justify-content: stretch;
}
.hbox.align-start,
.vbox.align-start,
.align-start {
  /* Old browsers */
  -webkit-box-align: start;
  -moz-box-align: start;
  box-align: start;
  /* Modern browsers */
  align-items: flex-start;
}
.hbox.align-end,
.vbox.align-end,
.align-end {
  /* Old browsers */
  -webkit-box-align: end;
  -moz-box-align: end;
  box-align: end;
  /* Modern browsers */
  align-items: flex-end;
}
.hbox.align-center,
.vbox.align-center,
.align-center {
  /* Old browsers */
  -webkit-box-align: center;
  -moz-box-align: center;
  box-align: center;
  /* Modern browsers */
  align-items: center;
}
.hbox.align-baseline,
.vbox.align-baseline,
.align-baseline {
  /* Old browsers */
  -webkit-box-align: baseline;
  -moz-box-align: baseline;
  box-align: baseline;
  /* Modern browsers */
  align-items: baseline;
}
.hbox.align-stretch,
.vbox.align-stretch,
.align-stretch {
  /* Old browsers */
  -webkit-box-align: stretch;
  -moz-box-align: stretch;
  box-align: stretch;
  /* Modern browsers */
  align-items: stretch;
}
div.error {
  margin: 2em;
  text-align: center;
}
div.error > h1 {
  font-size: 500%;
  line-height: normal;
}
div.error > p {
  font-size: 200%;
  line-height: normal;
}
div.traceback-wrapper {
  text-align: left;
  max-width: 800px;
  margin: auto;
}
/**
 * Primary styles
 *
 * Author: Jupyter Development Team
 */
body {
  background-color: #fff;
  /* This makes sure that the body covers the entire window and needs to
       be in a different element than the display: box in wrapper below */
  position: absolute;
  left: 0px;
  right: 0px;
  top: 0px;
  bottom: 0px;
  overflow: visible;
}
body > #header {
  /* Initially hidden to prevent FLOUC */
  display: none;
  background-color: #fff;
  /* Display over codemirror */
  position: relative;
  z-index: 100;
}
body > #header #header-container {
  padding-bottom: 5px;
  padding-top: 5px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
body > #header .header-bar {
  width: 100%;
  height: 1px;
  background: #e7e7e7;
  margin-bottom: -1px;
}
@media print {
  body > #header {
    display: none !important;
  }
}
#header-spacer {
  width: 100%;
  visibility: hidden;
}
@media print {
  #header-spacer {
    display: none;
  }
}
#ipython_notebook {
  padding-left: 0px;
  padding-top: 1px;
  padding-bottom: 1px;
}
@media (max-width: 991px) {
  #ipython_notebook {
    margin-left: 10px;
  }
}
[dir="rtl"] #ipython_notebook {
  float: right !important;
}
#noscript {
  width: auto;
  padding-top: 16px;
  padding-bottom: 16px;
  text-align: center;
  font-size: 22px;
  color: red;
  font-weight: bold;
}
#ipython_notebook img {
  height: 28px;
}
#site {
  width: 100%;
  display: none;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  overflow: auto;
}
@media print {
  #site {
    height: auto !important;
  }
}
/* Smaller buttons */
.ui-button .ui-button-text {
  padding: 0.2em 0.8em;
  font-size: 77%;
}
input.ui-button {
  padding: 0.3em 0.9em;
}
span#login_widget {
  float: right;
}
span#login_widget > .button,
#logout {
  color: #333;
  background-color: #fff;
  border-color: #ccc;
}
span#login_widget > .button:focus,
#logout:focus,
span#login_widget > .button.focus,
#logout.focus {
  color: #333;
  background-color: #e6e6e6;
  border-color: #8c8c8c;
}
span#login_widget > .button:hover,
#logout:hover {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
span#login_widget > .button:active,
#logout:active,
span#login_widget > .button.active,
#logout.active,
.open > .dropdown-togglespan#login_widget > .button,
.open > .dropdown-toggle#logout {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
span#login_widget > .button:active:hover,
#logout:active:hover,
span#login_widget > .button.active:hover,
#logout.active:hover,
.open > .dropdown-togglespan#login_widget > .button:hover,
.open > .dropdown-toggle#logout:hover,
span#login_widget > .button:active:focus,
#logout:active:focus,
span#login_widget > .button.active:focus,
#logout.active:focus,
.open > .dropdown-togglespan#login_widget > .button:focus,
.open > .dropdown-toggle#logout:focus,
span#login_widget > .button:active.focus,
#logout:active.focus,
span#login_widget > .button.active.focus,
#logout.active.focus,
.open > .dropdown-togglespan#login_widget > .button.focus,
.open > .dropdown-toggle#logout.focus {
  color: #333;
  background-color: #d4d4d4;
  border-color: #8c8c8c;
}
span#login_widget > .button:active,
#logout:active,
span#login_widget > .button.active,
#logout.active,
.open > .dropdown-togglespan#login_widget > .button,
.open > .dropdown-toggle#logout {
  background-image: none;
}
span#login_widget > .button.disabled:hover,
#logout.disabled:hover,
span#login_widget > .button[disabled]:hover,
#logout[disabled]:hover,
fieldset[disabled] span#login_widget > .button:hover,
fieldset[disabled] #logout:hover,
span#login_widget > .button.disabled:focus,
#logout.disabled:focus,
span#login_widget > .button[disabled]:focus,
#logout[disabled]:focus,
fieldset[disabled] span#login_widget > .button:focus,
fieldset[disabled] #logout:focus,
span#login_widget > .button.disabled.focus,
#logout.disabled.focus,
span#login_widget > .button[disabled].focus,
#logout[disabled].focus,
fieldset[disabled] span#login_widget > .button.focus,
fieldset[disabled] #logout.focus {
  background-color: #fff;
  border-color: #ccc;
}
span#login_widget > .button .badge,
#logout .badge {
  color: #fff;
  background-color: #333;
}
.nav-header {
  text-transform: none;
}
#header > span {
  margin-top: 10px;
}
.modal_stretch .modal-dialog {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  min-height: 80vh;
}
.modal_stretch .modal-dialog .modal-body {
  max-height: calc(100vh - 200px);
  overflow: auto;
  flex: 1;
}
@media (min-width: 768px) {
  .modal .modal-dialog {
    width: 700px;
  }
}
@media (min-width: 768px) {
  select.form-control {
    margin-left: 12px;
    margin-right: 12px;
  }
}
/*!
*
* IPython auth
*
*/
.center-nav {
  display: inline-block;
  margin-bottom: -4px;
}
/*!
*
* IPython tree view
*
*/
/* We need an invisible input field on top of the sentense*/
/* "Drag file onto the list ..." */
.alternate_upload {
  background-color: none;
  display: inline;
}
.alternate_upload.form {
  padding: 0;
  margin: 0;
}
.alternate_upload input.fileinput {
  text-align: center;
  vertical-align: middle;
  display: inline;
  opacity: 0;
  z-index: 2;
  width: 12ex;
  margin-right: -12ex;
}
.alternate_upload .btn-upload {
  height: 22px;
}
/**
 * Primary styles
 *
 * Author: Jupyter Development Team
 */
[dir="rtl"] #tabs li {
  float: right;
}
ul#tabs {
  margin-bottom: 4px;
}
[dir="rtl"] ul#tabs {
  margin-right: 0px;
}
ul#tabs a {
  padding-top: 6px;
  padding-bottom: 4px;
}
ul.breadcrumb a:focus,
ul.breadcrumb a:hover {
  text-decoration: none;
}
ul.breadcrumb i.icon-home {
  font-size: 16px;
  margin-right: 4px;
}
ul.breadcrumb span {
  color: #5e5e5e;
}
.list_toolbar {
  padding: 4px 0 4px 0;
  vertical-align: middle;
}
.list_toolbar .tree-buttons {
  padding-top: 1px;
}
[dir="rtl"] .list_toolbar .tree-buttons {
  float: left !important;
}
[dir="rtl"] .list_toolbar .pull-right {
  padding-top: 1px;
  float: left !important;
}
[dir="rtl"] .list_toolbar .pull-left {
  float: right !important;
}
.dynamic-buttons {
  padding-top: 3px;
  display: inline-block;
}
.list_toolbar [class*="span"] {
  min-height: 24px;
}
.list_header {
  font-weight: bold;
  background-color: #EEE;
}
.list_placeholder {
  font-weight: bold;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 7px;
  padding-right: 7px;
}
.list_container {
  margin-top: 4px;
  margin-bottom: 20px;
  border: 1px solid #ddd;
  border-radius: 2px;
}
.list_container > div {
  border-bottom: 1px solid #ddd;
}
.list_container > div:hover .list-item {
  background-color: red;
}
.list_container > div:last-child {
  border: none;
}
.list_item:hover .list_item {
  background-color: #ddd;
}
.list_item a {
  text-decoration: none;
}
.list_item:hover {
  background-color: #fafafa;
}
.list_header > div,
.list_item > div {
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 7px;
  padding-right: 7px;
  line-height: 22px;
}
.list_header > div input,
.list_item > div input {
  margin-right: 7px;
  margin-left: 14px;
  vertical-align: baseline;
  line-height: 22px;
  position: relative;
  top: -1px;
}
.list_header > div .item_link,
.list_item > div .item_link {
  margin-left: -1px;
  vertical-align: baseline;
  line-height: 22px;
}
.new-file input[type=checkbox] {
  visibility: hidden;
}
.item_name {
  line-height: 22px;
  height: 24px;
}
.item_icon {
  font-size: 14px;
  color: #5e5e5e;
  margin-right: 7px;
  margin-left: 7px;
  line-height: 22px;
  vertical-align: baseline;
}
.item_buttons {
  line-height: 1em;
  margin-left: -5px;
}
.item_buttons .btn,
.item_buttons .btn-group,
.item_buttons .input-group {
  float: left;
}
.item_buttons > .btn,
.item_buttons > .btn-group,
.item_buttons > .input-group {
  margin-left: 5px;
}
.item_buttons .btn {
  min-width: 13ex;
}
.item_buttons .running-indicator {
  padding-top: 4px;
  color: #5cb85c;
}
.item_buttons .kernel-name {
  padding-top: 4px;
  color: #5bc0de;
  margin-right: 7px;
  float: left;
}
.toolbar_info {
  height: 24px;
  line-height: 24px;
}
.list_item input:not([type=checkbox]) {
  padding-top: 3px;
  padding-bottom: 3px;
  height: 22px;
  line-height: 14px;
  margin: 0px;
}
.highlight_text {
  color: blue;
}
#project_name {
  display: inline-block;
  padding-left: 7px;
  margin-left: -2px;
}
#project_name > .breadcrumb {
  padding: 0px;
  margin-bottom: 0px;
  background-color: transparent;
  font-weight: bold;
}
#tree-selector {
  padding-right: 0px;
}
[dir="rtl"] #tree-selector a {
  float: right;
}
#button-select-all {
  min-width: 50px;
}
#select-all {
  margin-left: 7px;
  margin-right: 2px;
}
.menu_icon {
  margin-right: 2px;
}
.tab-content .row {
  margin-left: 0px;
  margin-right: 0px;
}
.folder_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f114";
}
.folder_icon:before.pull-left {
  margin-right: .3em;
}
.folder_icon:before.pull-right {
  margin-left: .3em;
}
.notebook_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f02d";
  position: relative;
  top: -1px;
}
.notebook_icon:before.pull-left {
  margin-right: .3em;
}
.notebook_icon:before.pull-right {
  margin-left: .3em;
}
.running_notebook_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f02d";
  position: relative;
  top: -1px;
  color: #5cb85c;
}
.running_notebook_icon:before.pull-left {
  margin-right: .3em;
}
.running_notebook_icon:before.pull-right {
  margin-left: .3em;
}
.file_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f016";
  position: relative;
  top: -2px;
}
.file_icon:before.pull-left {
  margin-right: .3em;
}
.file_icon:before.pull-right {
  margin-left: .3em;
}
#notebook_toolbar .pull-right {
  padding-top: 0px;
  margin-right: -1px;
}
ul#new-menu {
  left: auto;
  right: 0;
}
[dir="rtl"] #new-menu {
  text-align: right;
}
.kernel-menu-icon {
  padding-right: 12px;
  width: 24px;
  content: "\f096";
}
.kernel-menu-icon:before {
  content: "\f096";
}
.kernel-menu-icon-current:before {
  content: "\f00c";
}
#tab_content {
  padding-top: 20px;
}
#running .panel-group .panel {
  margin-top: 3px;
  margin-bottom: 1em;
}
#running .panel-group .panel .panel-heading {
  background-color: #EEE;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 7px;
  padding-right: 7px;
  line-height: 22px;
}
#running .panel-group .panel .panel-heading a:focus,
#running .panel-group .panel .panel-heading a:hover {
  text-decoration: none;
}
#running .panel-group .panel .panel-body {
  padding: 0px;
}
#running .panel-group .panel .panel-body .list_container {
  margin-top: 0px;
  margin-bottom: 0px;
  border: 0px;
  border-radius: 0px;
}
#running .panel-group .panel .panel-body .list_container .list_item {
  border-bottom: 1px solid #ddd;
}
#running .panel-group .panel .panel-body .list_container .list_item:last-child {
  border-bottom: 0px;
}
[dir="rtl"] #running .col-sm-8 {
  float: right !important;
}
.delete-button {
  display: none;
}
.duplicate-button {
  display: none;
}
.rename-button {
  display: none;
}
.shutdown-button {
  display: none;
}
.dynamic-instructions {
  display: inline-block;
  padding-top: 4px;
}
/*!
*
* IPython text editor webapp
*
*/
.selected-keymap i.fa {
  padding: 0px 5px;
}
.selected-keymap i.fa:before {
  content: "\f00c";
}
#mode-menu {
  overflow: auto;
  max-height: 20em;
}
.edit_app #header {
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
}
.edit_app #menubar .navbar {
  /* Use a negative 1 bottom margin, so the border overlaps the border of the
    header */
  margin-bottom: -1px;
}
.dirty-indicator {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  width: 20px;
}
.dirty-indicator.pull-left {
  margin-right: .3em;
}
.dirty-indicator.pull-right {
  margin-left: .3em;
}
.dirty-indicator-dirty {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  width: 20px;
}
.dirty-indicator-dirty.pull-left {
  margin-right: .3em;
}
.dirty-indicator-dirty.pull-right {
  margin-left: .3em;
}
.dirty-indicator-clean {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  width: 20px;
}
.dirty-indicator-clean.pull-left {
  margin-right: .3em;
}
.dirty-indicator-clean.pull-right {
  margin-left: .3em;
}
.dirty-indicator-clean:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f00c";
}
.dirty-indicator-clean:before.pull-left {
  margin-right: .3em;
}
.dirty-indicator-clean:before.pull-right {
  margin-left: .3em;
}
#filename {
  font-size: 16pt;
  display: table;
  padding: 0px 5px;
}
#current-mode {
  padding-left: 5px;
  padding-right: 5px;
}
#texteditor-backdrop {
  padding-top: 20px;
  padding-bottom: 20px;
}
@media not print {
  #texteditor-backdrop {
    background-color: #EEE;
  }
}
@media print {
  #texteditor-backdrop #texteditor-container .CodeMirror-gutter,
  #texteditor-backdrop #texteditor-container .CodeMirror-gutters {
    background-color: #fff;
  }
}
@media not print {
  #texteditor-backdrop #texteditor-container .CodeMirror-gutter,
  #texteditor-backdrop #texteditor-container .CodeMirror-gutters {
    background-color: #fff;
  }
}
@media not print {
  #texteditor-backdrop #texteditor-container {
    padding: 0px;
    background-color: #fff;
    -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
    box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  }
}
/*!
*
* IPython notebook
*
*/
/* CSS font colors for translated ANSI colors. */
.ansibold {
  font-weight: bold;
}
/* use dark versions for foreground, to improve visibility */
.ansiblack {
  color: black;
}
.ansired {
  color: darkred;
}
.ansigreen {
  color: darkgreen;
}
.ansiyellow {
  color: #c4a000;
}
.ansiblue {
  color: darkblue;
}
.ansipurple {
  color: darkviolet;
}
.ansicyan {
  color: steelblue;
}
.ansigray {
  color: gray;
}
/* and light for background, for the same reason */
.ansibgblack {
  background-color: black;
}
.ansibgred {
  background-color: red;
}
.ansibggreen {
  background-color: green;
}
.ansibgyellow {
  background-color: yellow;
}
.ansibgblue {
  background-color: blue;
}
.ansibgpurple {
  background-color: magenta;
}
.ansibgcyan {
  background-color: cyan;
}
.ansibggray {
  background-color: gray;
}
div.cell {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  border-radius: 2px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  border-width: 1px;
  border-style: solid;
  border-color: transparent;
  width: 100%;
  padding: 5px;
  /* This acts as a spacer between cells, that is outside the border */
  margin: 0px;
  outline: none;
  border-left-width: 1px;
  padding-left: 5px;
  background: linear-gradient(to right, transparent -40px, transparent 1px, transparent 1px, transparent 100%);
}
div.cell.jupyter-soft-selected {
  border-left-color: #90CAF9;
  border-left-color: #E3F2FD;
  border-left-width: 1px;
  padding-left: 5px;
  border-right-color: #E3F2FD;
  border-right-width: 1px;
  background: #E3F2FD;
}
@media print {
  div.cell.jupyter-soft-selected {
    border-color: transparent;
  }
}
div.cell.selected {
  border-color: #ababab;
  border-left-width: 0px;
  padding-left: 6px;
  background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 5px, transparent 5px, transparent 100%);
}
@media print {
  div.cell.selected {
    border-color: transparent;
  }
}
div.cell.selected.jupyter-soft-selected {
  border-left-width: 0;
  padding-left: 6px;
  background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 7px, #E3F2FD 7px, #E3F2FD 100%);
}
.edit_mode div.cell.selected {
  border-color: #66BB6A;
  border-left-width: 0px;
  padding-left: 6px;
  background: linear-gradient(to right, #66BB6A -40px, #66BB6A 5px, transparent 5px, transparent 100%);
}
@media print {
  .edit_mode div.cell.selected {
    border-color: transparent;
  }
}
.prompt {
  /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */
  min-width: 14ex;
  /* This padding is tuned to match the padding on the CodeMirror editor. */
  padding: 0.4em;
  margin: 0px;
  font-family: monospace;
  text-align: right;
  /* This has to match that of the the CodeMirror class line-height below */
  line-height: 1.21429em;
  /* Don't highlight prompt number selection */
  -webkit-touch-callout: none;
  -webkit-user-select: none;
  -khtml-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
  /* Use default cursor */
  cursor: default;
}
@media (max-width: 540px) {
  .prompt {
    text-align: left;
  }
}
div.inner_cell {
  min-width: 0;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
/* input_area and input_prompt must match in top border and margin for alignment */
div.input_area {
  border: 1px solid #cfcfcf;
  border-radius: 2px;
  background: #f7f7f7;
  line-height: 1.21429em;
}
/* This is needed so that empty prompt areas can collapse to zero height when there
   is no content in the output_subarea and the prompt. The main purpose of this is
   to make sure that empty JavaScript output_subareas have no height. */
div.prompt:empty {
  padding-top: 0;
  padding-bottom: 0;
}
div.unrecognized_cell {
  padding: 5px 5px 5px 0px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
div.unrecognized_cell .inner_cell {
  border-radius: 2px;
  padding: 5px;
  font-weight: bold;
  color: red;
  border: 1px solid #cfcfcf;
  background: #eaeaea;
}
div.unrecognized_cell .inner_cell a {
  color: inherit;
  text-decoration: none;
}
div.unrecognized_cell .inner_cell a:hover {
  color: inherit;
  text-decoration: none;
}
@media (max-width: 540px) {
  div.unrecognized_cell > div.prompt {
    display: none;
  }
}
div.code_cell {
  /* avoid page breaking on code cells when printing */
}
@media print {
  div.code_cell {
    page-break-inside: avoid;
  }
}
/* any special styling for code cells that are currently running goes here */
div.input {
  page-break-inside: avoid;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.input {
    /* Old browsers */
    display: -webkit-box;
    -webkit-box-orient: vertical;
    -webkit-box-align: stretch;
    display: -moz-box;
    -moz-box-orient: vertical;
    -moz-box-align: stretch;
    display: box;
    box-orient: vertical;
    box-align: stretch;
    /* Modern browsers */
    display: flex;
    flex-direction: column;
    align-items: stretch;
  }
}
/* input_area and input_prompt must match in top border and margin for alignment */
div.input_prompt {
  color: #303F9F;
  border-top: 1px solid transparent;
}
div.input_area > div.highlight {
  margin: 0.4em;
  border: none;
  padding: 0px;
  background-color: transparent;
}
div.input_area > div.highlight > pre {
  margin: 0px;
  border: none;
  padding: 0px;
  background-color: transparent;
}
/* The following gets added to the <head> if it is detected that the user has a
 * monospace font with inconsistent normal/bold/italic height.  See
 * notebookmain.js.  Such fonts will have keywords vertically offset with
 * respect to the rest of the text.  The user should select a better font.
 * See: https://github.com/ipython/ipython/issues/1503
 *
 * .CodeMirror span {
 *      vertical-align: bottom;
 * }
 */
.CodeMirror {
  line-height: 1.21429em;
  /* Changed from 1em to our global default */
  font-size: 14px;
  height: auto;
  /* Changed to auto to autogrow */
  background: none;
  /* Changed from white to allow our bg to show through */
}
.CodeMirror-scroll {
  /*  The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/
  /*  We have found that if it is visible, vertical scrollbars appear with font size changes.*/
  overflow-y: hidden;
  overflow-x: auto;
}
.CodeMirror-lines {
  /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */
  /* we have set a different line-height and want this to scale with that. */
  padding: 0.4em;
}
.CodeMirror-linenumber {
  padding: 0 8px 0 4px;
}
.CodeMirror-gutters {
  border-bottom-left-radius: 2px;
  border-top-left-radius: 2px;
}
.CodeMirror pre {
  /* In CM3 this went to 4px from 0 in CM2. We need the 0 value because of how we size */
  /* .CodeMirror-lines */
  padding: 0;
  border: 0;
  border-radius: 0;
}
/*

Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org>
Adapted from GitHub theme

*/
.highlight-base {
  color: #000;
}
.highlight-variable {
  color: #000;
}
.highlight-variable-2 {
  color: #1a1a1a;
}
.highlight-variable-3 {
  color: #333333;
}
.highlight-string {
  color: #BA2121;
}
.highlight-comment {
  color: #408080;
  font-style: italic;
}
.highlight-number {
  color: #080;
}
.highlight-atom {
  color: #88F;
}
.highlight-keyword {
  color: #008000;
  font-weight: bold;
}
.highlight-builtin {
  color: #008000;
}
.highlight-error {
  color: #f00;
}
.highlight-operator {
  color: #AA22FF;
  font-weight: bold;
}
.highlight-meta {
  color: #AA22FF;
}
/* previously not defined, copying from default codemirror */
.highlight-def {
  color: #00f;
}
.highlight-string-2 {
  color: #f50;
}
.highlight-qualifier {
  color: #555;
}
.highlight-bracket {
  color: #997;
}
.highlight-tag {
  color: #170;
}
.highlight-attribute {
  color: #00c;
}
.highlight-header {
  color: blue;
}
.highlight-quote {
  color: #090;
}
.highlight-link {
  color: #00c;
}
/* apply the same style to codemirror */
.cm-s-ipython span.cm-keyword {
  color: #008000;
  font-weight: bold;
}
.cm-s-ipython span.cm-atom {
  color: #88F;
}
.cm-s-ipython span.cm-number {
  color: #080;
}
.cm-s-ipython span.cm-def {
  color: #00f;
}
.cm-s-ipython span.cm-variable {
  color: #000;
}
.cm-s-ipython span.cm-operator {
  color: #AA22FF;
  font-weight: bold;
}
.cm-s-ipython span.cm-variable-2 {
  color: #1a1a1a;
}
.cm-s-ipython span.cm-variable-3 {
  color: #333333;
}
.cm-s-ipython span.cm-comment {
  color: #408080;
  font-style: italic;
}
.cm-s-ipython span.cm-string {
  color: #BA2121;
}
.cm-s-ipython span.cm-string-2 {
  color: #f50;
}
.cm-s-ipython span.cm-meta {
  color: #AA22FF;
}
.cm-s-ipython span.cm-qualifier {
  color: #555;
}
.cm-s-ipython span.cm-builtin {
  color: #008000;
}
.cm-s-ipython span.cm-bracket {
  color: #997;
}
.cm-s-ipython span.cm-tag {
  color: #170;
}
.cm-s-ipython span.cm-attribute {
  color: #00c;
}
.cm-s-ipython span.cm-header {
  color: blue;
}
.cm-s-ipython span.cm-quote {
  color: #090;
}
.cm-s-ipython span.cm-link {
  color: #00c;
}
.cm-s-ipython span.cm-error {
  color: #f00;
}
.cm-s-ipython span.cm-tab {
  background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=);
  background-position: right;
  background-repeat: no-repeat;
}
div.output_wrapper {
  /* this position must be relative to enable descendents to be absolute within it */
  position: relative;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  z-index: 1;
}
/* class for the output area when it should be height-limited */
div.output_scroll {
  /* ideally, this would be max-height, but FF barfs all over that */
  height: 24em;
  /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */
  width: 100%;
  overflow: auto;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8);
  box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8);
  display: block;
}
/* output div while it is collapsed */
div.output_collapsed {
  margin: 0px;
  padding: 0px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
div.out_prompt_overlay {
  height: 100%;
  padding: 0px 0.4em;
  position: absolute;
  border-radius: 2px;
}
div.out_prompt_overlay:hover {
  /* use inner shadow to get border that is computed the same on WebKit/FF */
  -webkit-box-shadow: inset 0 0 1px #000;
  box-shadow: inset 0 0 1px #000;
  background: rgba(240, 240, 240, 0.5);
}
div.output_prompt {
  color: #D84315;
}
/* This class is the outer container of all output sections. */
div.output_area {
  padding: 0px;
  page-break-inside: avoid;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
div.output_area .MathJax_Display {
  text-align: left !important;
}
div.output_area .rendered_html table {
  margin-left: 0;
  margin-right: 0;
}
div.output_area .rendered_html img {
  margin-left: 0;
  margin-right: 0;
}
div.output_area img,
div.output_area svg {
  max-width: 100%;
  height: auto;
}
div.output_area img.unconfined,
div.output_area svg.unconfined {
  max-width: none;
}
/* This is needed to protect the pre formating from global settings such
   as that of bootstrap */
.output {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.output_area {
    /* Old browsers */
    display: -webkit-box;
    -webkit-box-orient: vertical;
    -webkit-box-align: stretch;
    display: -moz-box;
    -moz-box-orient: vertical;
    -moz-box-align: stretch;
    display: box;
    box-orient: vertical;
    box-align: stretch;
    /* Modern browsers */
    display: flex;
    flex-direction: column;
    align-items: stretch;
  }
}
div.output_area pre {
  margin: 0;
  padding: 0;
  border: 0;
  vertical-align: baseline;
  color: black;
  background-color: transparent;
  border-radius: 0;
}
/* This class is for the output subarea inside the output_area and after
   the prompt div. */
div.output_subarea {
  overflow-x: auto;
  padding: 0.4em;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
  max-width: calc(100% - 14ex);
}
div.output_scroll div.output_subarea {
  overflow-x: visible;
}
/* The rest of the output_* classes are for special styling of the different
   output types */
/* all text output has this class: */
div.output_text {
  text-align: left;
  color: #000;
  /* This has to match that of the the CodeMirror class line-height below */
  line-height: 1.21429em;
}
/* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */
div.output_stderr {
  background: #fdd;
  /* very light red background for stderr */
}
div.output_latex {
  text-align: left;
}
/* Empty output_javascript divs should have no height */
div.output_javascript:empty {
  padding: 0;
}
.js-error {
  color: darkred;
}
/* raw_input styles */
div.raw_input_container {
  line-height: 1.21429em;
  padding-top: 5px;
}
pre.raw_input_prompt {
  /* nothing needed here. */
}
input.raw_input {
  font-family: monospace;
  font-size: inherit;
  color: inherit;
  width: auto;
  /* make sure input baseline aligns with prompt */
  vertical-align: baseline;
  /* padding + margin = 0.5em between prompt and cursor */
  padding: 0em 0.25em;
  margin: 0em 0.25em;
}
input.raw_input:focus {
  box-shadow: none;
}
p.p-space {
  margin-bottom: 10px;
}
div.output_unrecognized {
  padding: 5px;
  font-weight: bold;
  color: red;
}
div.output_unrecognized a {
  color: inherit;
  text-decoration: none;
}
div.output_unrecognized a:hover {
  color: inherit;
  text-decoration: none;
}
.rendered_html {
  color: #000;
  /* any extras will just be numbers: */
}
.rendered_html em {
  font-style: italic;
}
.rendered_html strong {
  font-weight: bold;
}
.rendered_html u {
  text-decoration: underline;
}
.rendered_html :link {
  text-decoration: underline;
}
.rendered_html :visited {
  text-decoration: underline;
}
.rendered_html h1 {
  font-size: 185.7%;
  margin: 1.08em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
}
.rendered_html h2 {
  font-size: 157.1%;
  margin: 1.27em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
}
.rendered_html h3 {
  font-size: 128.6%;
  margin: 1.55em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
}
.rendered_html h4 {
  font-size: 100%;
  margin: 2em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
}
.rendered_html h5 {
  font-size: 100%;
  margin: 2em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
  font-style: italic;
}
.rendered_html h6 {
  font-size: 100%;
  margin: 2em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
  font-style: italic;
}
.rendered_html h1:first-child {
  margin-top: 0.538em;
}
.rendered_html h2:first-child {
  margin-top: 0.636em;
}
.rendered_html h3:first-child {
  margin-top: 0.777em;
}
.rendered_html h4:first-child {
  margin-top: 1em;
}
.rendered_html h5:first-child {
  margin-top: 1em;
}
.rendered_html h6:first-child {
  margin-top: 1em;
}
.rendered_html ul {
  list-style: disc;
  margin: 0em 2em;
  padding-left: 0px;
}
.rendered_html ul ul {
  list-style: square;
  margin: 0em 2em;
}
.rendered_html ul ul ul {
  list-style: circle;
  margin: 0em 2em;
}
.rendered_html ol {
  list-style: decimal;
  margin: 0em 2em;
  padding-left: 0px;
}
.rendered_html ol ol {
  list-style: upper-alpha;
  margin: 0em 2em;
}
.rendered_html ol ol ol {
  list-style: lower-alpha;
  margin: 0em 2em;
}
.rendered_html ol ol ol ol {
  list-style: lower-roman;
  margin: 0em 2em;
}
.rendered_html ol ol ol ol ol {
  list-style: decimal;
  margin: 0em 2em;
}
.rendered_html * + ul {
  margin-top: 1em;
}
.rendered_html * + ol {
  margin-top: 1em;
}
.rendered_html hr {
  color: black;
  background-color: black;
}
.rendered_html pre {
  margin: 1em 2em;
}
.rendered_html pre,
.rendered_html code {
  border: 0;
  background-color: #fff;
  color: #000;
  font-size: 100%;
  padding: 0px;
}
.rendered_html blockquote {
  margin: 1em 2em;
}
.rendered_html table {
  margin-left: auto;
  margin-right: auto;
  border: 1px solid black;
  border-collapse: collapse;
}
.rendered_html tr,
.rendered_html th,
.rendered_html td {
  border: 1px solid black;
  border-collapse: collapse;
  margin: 1em 2em;
}
.rendered_html td,
.rendered_html th {
  text-align: left;
  vertical-align: middle;
  padding: 4px;
}
.rendered_html th {
  font-weight: bold;
}
.rendered_html * + table {
  margin-top: 1em;
}
.rendered_html p {
  text-align: left;
}
.rendered_html * + p {
  margin-top: 1em;
}
.rendered_html img {
  display: block;
  margin-left: auto;
  margin-right: auto;
}
.rendered_html * + img {
  margin-top: 1em;
}
.rendered_html img,
.rendered_html svg {
  max-width: 100%;
  height: auto;
}
.rendered_html img.unconfined,
.rendered_html svg.unconfined {
  max-width: none;
}
div.text_cell {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.text_cell > div.prompt {
    display: none;
  }
}
div.text_cell_render {
  /*font-family: "Helvetica Neue", Arial, Helvetica, Geneva, sans-serif;*/
  outline: none;
  resize: none;
  width: inherit;
  border-style: none;
  padding: 0.5em 0.5em 0.5em 0.4em;
  color: #000;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
a.anchor-link:link {
  text-decoration: none;
  padding: 0px 20px;
  visibility: hidden;
}
h1:hover .anchor-link,
h2:hover .anchor-link,
h3:hover .anchor-link,
h4:hover .anchor-link,
h5:hover .anchor-link,
h6:hover .anchor-link {
  visibility: visible;
}
.text_cell.rendered .input_area {
  display: none;
}
.text_cell.rendered .rendered_html {
  overflow-x: auto;
  overflow-y: hidden;
}
.text_cell.unrendered .text_cell_render {
  display: none;
}
.cm-header-1,
.cm-header-2,
.cm-header-3,
.cm-header-4,
.cm-header-5,
.cm-header-6 {
  font-weight: bold;
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
}
.cm-header-1 {
  font-size: 185.7%;
}
.cm-header-2 {
  font-size: 157.1%;
}
.cm-header-3 {
  font-size: 128.6%;
}
.cm-header-4 {
  font-size: 110%;
}
.cm-header-5 {
  font-size: 100%;
  font-style: italic;
}
.cm-header-6 {
  font-size: 100%;
  font-style: italic;
}
/*!
*
* IPython notebook webapp
*
*/
@media (max-width: 767px) {
  .notebook_app {
    padding-left: 0px;
    padding-right: 0px;
  }
}
#ipython-main-app {
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  height: 100%;
}
div#notebook_panel {
  margin: 0px;
  padding: 0px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  height: 100%;
}
div#notebook {
  font-size: 14px;
  line-height: 20px;
  overflow-y: hidden;
  overflow-x: auto;
  width: 100%;
  /* This spaces the page away from the edge of the notebook area */
  padding-top: 20px;
  margin: 0px;
  outline: none;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  min-height: 100%;
}
@media not print {
  #notebook-container {
    padding: 15px;
    background-color: #fff;
    min-height: 0;
    -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
    box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  }
}
@media print {
  #notebook-container {
    width: 100%;
  }
}
div.ui-widget-content {
  border: 1px solid #ababab;
  outline: none;
}
pre.dialog {
  background-color: #f7f7f7;
  border: 1px solid #ddd;
  border-radius: 2px;
  padding: 0.4em;
  padding-left: 2em;
}
p.dialog {
  padding: 0.2em;
}
/* Word-wrap output correctly.  This is the CSS3 spelling, though Firefox seems
   to not honor it correctly.  Webkit browsers (Chrome, rekonq, Safari) do.
 */
pre,
code,
kbd,
samp {
  white-space: pre-wrap;
}
#fonttest {
  font-family: monospace;
}
p {
  margin-bottom: 0;
}
.end_space {
  min-height: 100px;
  transition: height .2s ease;
}
.notebook_app > #header {
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
}
@media not print {
  .notebook_app {
    background-color: #EEE;
  }
}
kbd {
  border-style: solid;
  border-width: 1px;
  box-shadow: none;
  margin: 2px;
  padding-left: 2px;
  padding-right: 2px;
  padding-top: 1px;
  padding-bottom: 1px;
}
/* CSS for the cell toolbar */
.celltoolbar {
  border: thin solid #CFCFCF;
  border-bottom: none;
  background: #EEE;
  border-radius: 2px 2px 0px 0px;
  width: 100%;
  height: 29px;
  padding-right: 4px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
  /* Old browsers */
  -webkit-box-pack: end;
  -moz-box-pack: end;
  box-pack: end;
  /* Modern browsers */
  justify-content: flex-end;
  display: -webkit-flex;
}
@media print {
  .celltoolbar {
    display: none;
  }
}
.ctb_hideshow {
  display: none;
  vertical-align: bottom;
}
/* ctb_show is added to the ctb_hideshow div to show the cell toolbar.
   Cell toolbars are only shown when the ctb_global_show class is also set.
*/
.ctb_global_show .ctb_show.ctb_hideshow {
  display: block;
}
.ctb_global_show .ctb_show + .input_area,
.ctb_global_show .ctb_show + div.text_cell_input,
.ctb_global_show .ctb_show ~ div.text_cell_render {
  border-top-right-radius: 0px;
  border-top-left-radius: 0px;
}
.ctb_global_show .ctb_show ~ div.text_cell_render {
  border: 1px solid #cfcfcf;
}
.celltoolbar {
  font-size: 87%;
  padding-top: 3px;
}
.celltoolbar select {
  display: block;
  width: 100%;
  height: 32px;
  padding: 6px 12px;
  font-size: 13px;
  line-height: 1.42857143;
  color: #555555;
  background-color: #fff;
  background-image: none;
  border: 1px solid #ccc;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  -webkit-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  -o-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  height: 30px;
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
  width: inherit;
  font-size: inherit;
  height: 22px;
  padding: 0px;
  display: inline-block;
}
.celltoolbar select:focus {
  border-color: #66afe9;
  outline: 0;
  -webkit-box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
  box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
}
.celltoolbar select::-moz-placeholder {
  color: #999;
  opacity: 1;
}
.celltoolbar select:-ms-input-placeholder {
  color: #999;
}
.celltoolbar select::-webkit-input-placeholder {
  color: #999;
}
.celltoolbar select::-ms-expand {
  border: 0;
  background-color: transparent;
}
.celltoolbar select[disabled],
.celltoolbar select[readonly],
fieldset[disabled] .celltoolbar select {
  background-color: #eeeeee;
  opacity: 1;
}
.celltoolbar select[disabled],
fieldset[disabled] .celltoolbar select {
  cursor: not-allowed;
}
textarea.celltoolbar select {
  height: auto;
}
select.celltoolbar select {
  height: 30px;
  line-height: 30px;
}
textarea.celltoolbar select,
select[multiple].celltoolbar select {
  height: auto;
}
.celltoolbar label {
  margin-left: 5px;
  margin-right: 5px;
}
.completions {
  position: absolute;
  z-index: 110;
  overflow: hidden;
  border: 1px solid #ababab;
  border-radius: 2px;
  -webkit-box-shadow: 0px 6px 10px -1px #adadad;
  box-shadow: 0px 6px 10px -1px #adadad;
  line-height: 1;
}
.completions select {
  background: white;
  outline: none;
  border: none;
  padding: 0px;
  margin: 0px;
  overflow: auto;
  font-family: monospace;
  font-size: 110%;
  color: #000;
  width: auto;
}
.completions select option.context {
  color: #286090;
}
#kernel_logo_widget {
  float: right !important;
  float: right;
}
#kernel_logo_widget .current_kernel_logo {
  display: none;
  margin-top: -1px;
  margin-bottom: -1px;
  width: 32px;
  height: 32px;
}
#menubar {
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  margin-top: 1px;
}
#menubar .navbar {
  border-top: 1px;
  border-radius: 0px 0px 2px 2px;
  margin-bottom: 0px;
}
#menubar .navbar-toggle {
  float: left;
  padding-top: 7px;
  padding-bottom: 7px;
  border: none;
}
#menubar .navbar-collapse {
  clear: left;
}
.nav-wrapper {
  border-bottom: 1px solid #e7e7e7;
}
i.menu-icon {
  padding-top: 4px;
}
ul#help_menu li a {
  overflow: hidden;
  padding-right: 2.2em;
}
ul#help_menu li a i {
  margin-right: -1.2em;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu > .dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
}
.dropdown-submenu:hover > .dropdown-menu {
  display: block;
}
.dropdown-submenu > a:after {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  display: block;
  content: "\f0da";
  float: right;
  color: #333333;
  margin-top: 2px;
  margin-right: -10px;
}
.dropdown-submenu > a:after.pull-left {
  margin-right: .3em;
}
.dropdown-submenu > a:after.pull-right {
  margin-left: .3em;
}
.dropdown-submenu:hover > a:after {
  color: #262626;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left > .dropdown-menu {
  left: -100%;
  margin-left: 10px;
}
#notification_area {
  float: right !important;
  float: right;
  z-index: 10;
}
.indicator_area {
  float: right !important;
  float: right;
  color: #777;
  margin-left: 5px;
  margin-right: 5px;
  width: 11px;
  z-index: 10;
  text-align: center;
  width: auto;
}
#kernel_indicator {
  float: right !important;
  float: right;
  color: #777;
  margin-left: 5px;
  margin-right: 5px;
  width: 11px;
  z-index: 10;
  text-align: center;
  width: auto;
  border-left: 1px solid;
}
#kernel_indicator .kernel_indicator_name {
  padding-left: 5px;
  padding-right: 5px;
}
#modal_indicator {
  float: right !important;
  float: right;
  color: #777;
  margin-left: 5px;
  margin-right: 5px;
  width: 11px;
  z-index: 10;
  text-align: center;
  width: auto;
}
#readonly-indicator {
  float: right !important;
  float: right;
  color: #777;
  margin-left: 5px;
  margin-right: 5px;
  width: 11px;
  z-index: 10;
  text-align: center;
  width: auto;
  margin-top: 2px;
  margin-bottom: 0px;
  margin-left: 0px;
  margin-right: 0px;
  display: none;
}
.modal_indicator:before {
  width: 1.28571429em;
  text-align: center;
}
.edit_mode .modal_indicator:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f040";
}
.edit_mode .modal_indicator:before.pull-left {
  margin-right: .3em;
}
.edit_mode .modal_indicator:before.pull-right {
  margin-left: .3em;
}
.command_mode .modal_indicator:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: ' ';
}
.command_mode .modal_indicator:before.pull-left {
  margin-right: .3em;
}
.command_mode .modal_indicator:before.pull-right {
  margin-left: .3em;
}
.kernel_idle_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f10c";
}
.kernel_idle_icon:before.pull-left {
  margin-right: .3em;
}
.kernel_idle_icon:before.pull-right {
  margin-left: .3em;
}
.kernel_busy_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f111";
}
.kernel_busy_icon:before.pull-left {
  margin-right: .3em;
}
.kernel_busy_icon:before.pull-right {
  margin-left: .3em;
}
.kernel_dead_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f1e2";
}
.kernel_dead_icon:before.pull-left {
  margin-right: .3em;
}
.kernel_dead_icon:before.pull-right {
  margin-left: .3em;
}
.kernel_disconnected_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f127";
}
.kernel_disconnected_icon:before.pull-left {
  margin-right: .3em;
}
.kernel_disconnected_icon:before.pull-right {
  margin-left: .3em;
}
.notification_widget {
  color: #777;
  z-index: 10;
  background: rgba(240, 240, 240, 0.5);
  margin-right: 4px;
  color: #333;
  background-color: #fff;
  border-color: #ccc;
}
.notification_widget:focus,
.notification_widget.focus {
  color: #333;
  background-color: #e6e6e6;
  border-color: #8c8c8c;
}
.notification_widget:hover {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
.notification_widget:active,
.notification_widget.active,
.open > .dropdown-toggle.notification_widget {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
.notification_widget:active:hover,
.notification_widget.active:hover,
.open > .dropdown-toggle.notification_widget:hover,
.notification_widget:active:focus,
.notification_widget.active:focus,
.open > .dropdown-toggle.notification_widget:focus,
.notification_widget:active.focus,
.notification_widget.active.focus,
.open > .dropdown-toggle.notification_widget.focus {
  color: #333;
  background-color: #d4d4d4;
  border-color: #8c8c8c;
}
.notification_widget:active,
.notification_widget.active,
.open > .dropdown-toggle.notification_widget {
  background-image: none;
}
.notification_widget.disabled:hover,
.notification_widget[disabled]:hover,
fieldset[disabled] .notification_widget:hover,
.notification_widget.disabled:focus,
.notification_widget[disabled]:focus,
fieldset[disabled] .notification_widget:focus,
.notification_widget.disabled.focus,
.notification_widget[disabled].focus,
fieldset[disabled] .notification_widget.focus {
  background-color: #fff;
  border-color: #ccc;
}
.notification_widget .badge {
  color: #fff;
  background-color: #333;
}
.notification_widget.warning {
  color: #fff;
  background-color: #f0ad4e;
  border-color: #eea236;
}
.notification_widget.warning:focus,
.notification_widget.warning.focus {
  color: #fff;
  background-color: #ec971f;
  border-color: #985f0d;
}
.notification_widget.warning:hover {
  color: #fff;
  background-color: #ec971f;
  border-color: #d58512;
}
.notification_widget.warning:active,
.notification_widget.warning.active,
.open > .dropdown-toggle.notification_widget.warning {
  color: #fff;
  background-color: #ec971f;
  border-color: #d58512;
}
.notification_widget.warning:active:hover,
.notification_widget.warning.active:hover,
.open > .dropdown-toggle.notification_widget.warning:hover,
.notification_widget.warning:active:focus,
.notification_widget.warning.active:focus,
.open > .dropdown-toggle.notification_widget.warning:focus,
.notification_widget.warning:active.focus,
.notification_widget.warning.active.focus,
.open > .dropdown-toggle.notification_widget.warning.focus {
  color: #fff;
  background-color: #d58512;
  border-color: #985f0d;
}
.notification_widget.warning:active,
.notification_widget.warning.active,
.open > .dropdown-toggle.notification_widget.warning {
  background-image: none;
}
.notification_widget.warning.disabled:hover,
.notification_widget.warning[disabled]:hover,
fieldset[disabled] .notification_widget.warning:hover,
.notification_widget.warning.disabled:focus,
.notification_widget.warning[disabled]:focus,
fieldset[disabled] .notification_widget.warning:focus,
.notification_widget.warning.disabled.focus,
.notification_widget.warning[disabled].focus,
fieldset[disabled] .notification_widget.warning.focus {
  background-color: #f0ad4e;
  border-color: #eea236;
}
.notification_widget.warning .badge {
  color: #f0ad4e;
  background-color: #fff;
}
.notification_widget.success {
  color: #fff;
  background-color: #5cb85c;
  border-color: #4cae4c;
}
.notification_widget.success:focus,
.notification_widget.success.focus {
  color: #fff;
  background-color: #449d44;
  border-color: #255625;
}
.notification_widget.success:hover {
  color: #fff;
  background-color: #449d44;
  border-color: #398439;
}
.notification_widget.success:active,
.notification_widget.success.active,
.open > .dropdown-toggle.notification_widget.success {
  color: #fff;
  background-color: #449d44;
  border-color: #398439;
}
.notification_widget.success:active:hover,
.notification_widget.success.active:hover,
.open > .dropdown-toggle.notification_widget.success:hover,
.notification_widget.success:active:focus,
.notification_widget.success.active:focus,
.open > .dropdown-toggle.notification_widget.success:focus,
.notification_widget.success:active.focus,
.notification_widget.success.active.focus,
.open > .dropdown-toggle.notification_widget.success.focus {
  color: #fff;
  background-color: #398439;
  border-color: #255625;
}
.notification_widget.success:active,
.notification_widget.success.active,
.open > .dropdown-toggle.notification_widget.success {
  background-image: none;
}
.notification_widget.success.disabled:hover,
.notification_widget.success[disabled]:hover,
fieldset[disabled] .notification_widget.success:hover,
.notification_widget.success.disabled:focus,
.notification_widget.success[disabled]:focus,
fieldset[disabled] .notification_widget.success:focus,
.notification_widget.success.disabled.focus,
.notification_widget.success[disabled].focus,
fieldset[disabled] .notification_widget.success.focus {
  background-color: #5cb85c;
  border-color: #4cae4c;
}
.notification_widget.success .badge {
  color: #5cb85c;
  background-color: #fff;
}
.notification_widget.info {
  color: #fff;
  background-color: #5bc0de;
  border-color: #46b8da;
}
.notification_widget.info:focus,
.notification_widget.info.focus {
  color: #fff;
  background-color: #31b0d5;
  border-color: #1b6d85;
}
.notification_widget.info:hover {
  color: #fff;
  background-color: #31b0d5;
  border-color: #269abc;
}
.notification_widget.info:active,
.notification_widget.info.active,
.open > .dropdown-toggle.notification_widget.info {
  color: #fff;
  background-color: #31b0d5;
  border-color: #269abc;
}
.notification_widget.info:active:hover,
.notification_widget.info.active:hover,
.open > .dropdown-toggle.notification_widget.info:hover,
.notification_widget.info:active:focus,
.notification_widget.info.active:focus,
.open > .dropdown-toggle.notification_widget.info:focus,
.notification_widget.info:active.focus,
.notification_widget.info.active.focus,
.open > .dropdown-toggle.notification_widget.info.focus {
  color: #fff;
  background-color: #269abc;
  border-color: #1b6d85;
}
.notification_widget.info:active,
.notification_widget.info.active,
.open > .dropdown-toggle.notification_widget.info {
  background-image: none;
}
.notification_widget.info.disabled:hover,
.notification_widget.info[disabled]:hover,
fieldset[disabled] .notification_widget.info:hover,
.notification_widget.info.disabled:focus,
.notification_widget.info[disabled]:focus,
fieldset[disabled] .notification_widget.info:focus,
.notification_widget.info.disabled.focus,
.notification_widget.info[disabled].focus,
fieldset[disabled] .notification_widget.info.focus {
  background-color: #5bc0de;
  border-color: #46b8da;
}
.notification_widget.info .badge {
  color: #5bc0de;
  background-color: #fff;
}
.notification_widget.danger {
  color: #fff;
  background-color: #d9534f;
  border-color: #d43f3a;
}
.notification_widget.danger:focus,
.notification_widget.danger.focus {
  color: #fff;
  background-color: #c9302c;
  border-color: #761c19;
}
.notification_widget.danger:hover {
  color: #fff;
  background-color: #c9302c;
  border-color: #ac2925;
}
.notification_widget.danger:active,
.notification_widget.danger.active,
.open > .dropdown-toggle.notification_widget.danger {
  color: #fff;
  background-color: #c9302c;
  border-color: #ac2925;
}
.notification_widget.danger:active:hover,
.notification_widget.danger.active:hover,
.open > .dropdown-toggle.notification_widget.danger:hover,
.notification_widget.danger:active:focus,
.notification_widget.danger.active:focus,
.open > .dropdown-toggle.notification_widget.danger:focus,
.notification_widget.danger:active.focus,
.notification_widget.danger.active.focus,
.open > .dropdown-toggle.notification_widget.danger.focus {
  color: #fff;
  background-color: #ac2925;
  border-color: #761c19;
}
.notification_widget.danger:active,
.notification_widget.danger.active,
.open > .dropdown-toggle.notification_widget.danger {
  background-image: none;
}
.notification_widget.danger.disabled:hover,
.notification_widget.danger[disabled]:hover,
fieldset[disabled] .notification_widget.danger:hover,
.notification_widget.danger.disabled:focus,
.notification_widget.danger[disabled]:focus,
fieldset[disabled] .notification_widget.danger:focus,
.notification_widget.danger.disabled.focus,
.notification_widget.danger[disabled].focus,
fieldset[disabled] .notification_widget.danger.focus {
  background-color: #d9534f;
  border-color: #d43f3a;
}
.notification_widget.danger .badge {
  color: #d9534f;
  background-color: #fff;
}
div#pager {
  background-color: #fff;
  font-size: 14px;
  line-height: 20px;
  overflow: hidden;
  display: none;
  position: fixed;
  bottom: 0px;
  width: 100%;
  max-height: 50%;
  padding-top: 8px;
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  /* Display over codemirror */
  z-index: 100;
  /* Hack which prevents jquery ui resizable from changing top. */
  top: auto !important;
}
div#pager pre {
  line-height: 1.21429em;
  color: #000;
  background-color: #f7f7f7;
  padding: 0.4em;
}
div#pager #pager-button-area {
  position: absolute;
  top: 8px;
  right: 20px;
}
div#pager #pager-contents {
  position: relative;
  overflow: auto;
  width: 100%;
  height: 100%;
}
div#pager #pager-contents #pager-container {
  position: relative;
  padding: 15px 0px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
div#pager .ui-resizable-handle {
  top: 0px;
  height: 8px;
  background: #f7f7f7;
  border-top: 1px solid #cfcfcf;
  border-bottom: 1px solid #cfcfcf;
  /* This injects handle bars (a short, wide = symbol) for 
        the resize handle. */
}
div#pager .ui-resizable-handle::after {
  content: '';
  top: 2px;
  left: 50%;
  height: 3px;
  width: 30px;
  margin-left: -15px;
  position: absolute;
  border-top: 1px solid #cfcfcf;
}
.quickhelp {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
  line-height: 1.8em;
}
.shortcut_key {
  display: inline-block;
  width: 21ex;
  text-align: right;
  font-family: monospace;
}
.shortcut_descr {
  display: inline-block;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
span.save_widget {
  margin-top: 6px;
}
span.save_widget span.filename {
  height: 1em;
  line-height: 1em;
  padding: 3px;
  margin-left: 16px;
  border: none;
  font-size: 146.5%;
  border-radius: 2px;
}
span.save_widget span.filename:hover {
  background-color: #e6e6e6;
}
span.checkpoint_status,
span.autosave_status {
  font-size: small;
}
@media (max-width: 767px) {
  span.save_widget {
    font-size: small;
  }
  span.checkpoint_status,
  span.autosave_status {
    display: none;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  span.checkpoint_status {
    display: none;
  }
  span.autosave_status {
    font-size: x-small;
  }
}
.toolbar {
  padding: 0px;
  margin-left: -5px;
  margin-top: 2px;
  margin-bottom: 5px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
.toolbar select,
.toolbar label {
  width: auto;
  vertical-align: middle;
  margin-right: 2px;
  margin-bottom: 0px;
  display: inline;
  font-size: 92%;
  margin-left: 0.3em;
  margin-right: 0.3em;
  padding: 0px;
  padding-top: 3px;
}
.toolbar .btn {
  padding: 2px 8px;
}
.toolbar .btn-group {
  margin-top: 0px;
  margin-left: 5px;
}
#maintoolbar {
  margin-bottom: -3px;
  margin-top: -8px;
  border: 0px;
  min-height: 27px;
  margin-left: 0px;
  padding-top: 11px;
  padding-bottom: 3px;
}
#maintoolbar .navbar-text {
  float: none;
  vertical-align: middle;
  text-align: right;
  margin-left: 5px;
  margin-right: 0px;
  margin-top: 0px;
}
.select-xs {
  height: 24px;
}
.pulse,
.dropdown-menu > li > a.pulse,
li.pulse > a.dropdown-toggle,
li.pulse.open > a.dropdown-toggle {
  background-color: #F37626;
  color: white;
}
/**
 * Primary styles
 *
 * Author: Jupyter Development Team
 */
/** WARNING IF YOU ARE EDITTING THIS FILE, if this is a .css file, It has a lot
 * of chance of beeing generated from the ../less/[samename].less file, you can
 * try to get back the less file by reverting somme commit in history
 **/
/*
 * We'll try to get something pretty, so we
 * have some strange css to have the scroll bar on
 * the left with fix button on the top right of the tooltip
 */
@-moz-keyframes fadeOut {
  from {
    opacity: 1;
  }
  to {
    opacity: 0;
  }
}
@-webkit-keyframes fadeOut {
  from {
    opacity: 1;
  }
  to {
    opacity: 0;
  }
}
@-moz-keyframes fadeIn {
  from {
    opacity: 0;
  }
  to {
    opacity: 1;
  }
}
@-webkit-keyframes fadeIn {
  from {
    opacity: 0;
  }
  to {
    opacity: 1;
  }
}
/*properties of tooltip after "expand"*/
.bigtooltip {
  overflow: auto;
  height: 200px;
  -webkit-transition-property: height;
  -webkit-transition-duration: 500ms;
  -moz-transition-property: height;
  -moz-transition-duration: 500ms;
  transition-property: height;
  transition-duration: 500ms;
}
/*properties of tooltip before "expand"*/
.smalltooltip {
  -webkit-transition-property: height;
  -webkit-transition-duration: 500ms;
  -moz-transition-property: height;
  -moz-transition-duration: 500ms;
  transition-property: height;
  transition-duration: 500ms;
  text-overflow: ellipsis;
  overflow: hidden;
  height: 80px;
}
.tooltipbuttons {
  position: absolute;
  padding-right: 15px;
  top: 0px;
  right: 0px;
}
.tooltiptext {
  /*avoid the button to overlap on some docstring*/
  padding-right: 30px;
}
.ipython_tooltip {
  max-width: 700px;
  /*fade-in animation when inserted*/
  -webkit-animation: fadeOut 400ms;
  -moz-animation: fadeOut 400ms;
  animation: fadeOut 400ms;
  -webkit-animation: fadeIn 400ms;
  -moz-animation: fadeIn 400ms;
  animation: fadeIn 400ms;
  vertical-align: middle;
  background-color: #f7f7f7;
  overflow: visible;
  border: #ababab 1px solid;
  outline: none;
  padding: 3px;
  margin: 0px;
  padding-left: 7px;
  font-family: monospace;
  min-height: 50px;
  -moz-box-shadow: 0px 6px 10px -1px #adadad;
  -webkit-box-shadow: 0px 6px 10px -1px #adadad;
  box-shadow: 0px 6px 10px -1px #adadad;
  border-radius: 2px;
  position: absolute;
  z-index: 1000;
}
.ipython_tooltip a {
  float: right;
}
.ipython_tooltip .tooltiptext pre {
  border: 0;
  border-radius: 0;
  font-size: 100%;
  background-color: #f7f7f7;
}
.pretooltiparrow {
  left: 0px;
  margin: 0px;
  top: -16px;
  width: 40px;
  height: 16px;
  overflow: hidden;
  position: absolute;
}
.pretooltiparrow:before {
  background-color: #f7f7f7;
  border: 1px #ababab solid;
  z-index: 11;
  content: "";
  position: absolute;
  left: 15px;
  top: 10px;
  width: 25px;
  height: 25px;
  -webkit-transform: rotate(45deg);
  -moz-transform: rotate(45deg);
  -ms-transform: rotate(45deg);
  -o-transform: rotate(45deg);
}
ul.typeahead-list i {
  margin-left: -10px;
  width: 18px;
}
ul.typeahead-list {
  max-height: 80vh;
  overflow: auto;
}
ul.typeahead-list > li > a {
  /** Firefox bug **/
  /* see https://github.com/jupyter/notebook/issues/559 */
  white-space: normal;
}
.cmd-palette .modal-body {
  padding: 7px;
}
.cmd-palette form {
  background: white;
}
.cmd-palette input {
  outline: none;
}
.no-shortcut {
  display: none;
}
.command-shortcut:before {
  content: "(command)";
  padding-right: 3px;
  color: #777777;
}
.edit-shortcut:before {
  content: "(edit)";
  padding-right: 3px;
  color: #777777;
}
#find-and-replace #replace-preview .match,
#find-and-replace #replace-preview .insert {
  background-color: #BBDEFB;
  border-color: #90CAF9;
  border-style: solid;
  border-width: 1px;
  border-radius: 0px;
}
#find-and-replace #replace-preview .replace .match {
  background-color: #FFCDD2;
  border-color: #EF9A9A;
  border-radius: 0px;
}
#find-and-replace #replace-preview .replace .insert {
  background-color: #C8E6C9;
  border-color: #A5D6A7;
  border-radius: 0px;
}
#find-and-replace #replace-preview {
  max-height: 60vh;
  overflow: auto;
}
#find-and-replace #replace-preview pre {
  padding: 5px 10px;
}
.terminal-app {
  background: #EEE;
}
.terminal-app #header {
  background: #fff;
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
}
.terminal-app .terminal {
  width: 100%;
  float: left;
  font-family: monospace;
  color: white;
  background: black;
  padding: 0.4em;
  border-radius: 2px;
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.4);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.4);
}
.terminal-app .terminal,
.terminal-app .terminal dummy-screen {
  line-height: 1em;
  font-size: 14px;
}
.terminal-app .terminal .xterm-rows {
  padding: 10px;
}
.terminal-app .terminal-cursor {
  color: black;
  background: white;
}
.terminal-app #terminado-container {
  margin-top: 20px;
}
/*# sourceMappingURL=style.min.css.map */
    </style>
<style type="text/css">
    .highlight .hll { background-color: #ffffcc }
.highlight  { background: #f8f8f8; }
.highlight .c { color: #408080; font-style: italic } /* Comment */
.highlight .err { border: 1px solid #FF0000 } /* Error */
.highlight .k { color: #008000; font-weight: bold } /* Keyword */
.highlight .o { color: #666666 } /* Operator */
.highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */
.highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */
.highlight .cp { color: #BC7A00 } /* Comment.Preproc */
.highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */
.highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */
.highlight .cs { color: #408080; font-style: italic } /* Comment.Special */
.highlight .gd { color: #A00000 } /* Generic.Deleted */
.highlight .ge { font-style: italic } /* Generic.Emph */
.highlight .gr { color: #FF0000 } /* Generic.Error */
.highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */
.highlight .gi { color: #00A000 } /* Generic.Inserted */
.highlight .go { color: #888888 } /* Generic.Output */
.highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */
.highlight .gs { font-weight: bold } /* Generic.Strong */
.highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */
.highlight .gt { color: #0044DD } /* Generic.Traceback */
.highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */
.highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */
.highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */
.highlight .kp { color: #008000 } /* Keyword.Pseudo */
.highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */
.highlight .kt { color: #B00040 } /* Keyword.Type */
.highlight .m { color: #666666 } /* Literal.Number */
.highlight .s { color: #BA2121 } /* Literal.String */
.highlight .na { color: #7D9029 } /* Name.Attribute */
.highlight .nb { color: #008000 } /* Name.Builtin */
.highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */
.highlight .no { color: #880000 } /* Name.Constant */
.highlight .nd { color: #AA22FF } /* Name.Decorator */
.highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */
.highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */
.highlight .nf { color: #0000FF } /* Name.Function */
.highlight .nl { color: #A0A000 } /* Name.Label */
.highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */
.highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */
.highlight .nv { color: #19177C } /* Name.Variable */
.highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */
.highlight .w { color: #bbbbbb } /* Text.Whitespace */
.highlight .mb { color: #666666 } /* Literal.Number.Bin */
.highlight .mf { color: #666666 } /* Literal.Number.Float */
.highlight .mh { color: #666666 } /* Literal.Number.Hex */
.highlight .mi { color: #666666 } /* Literal.Number.Integer */
.highlight .mo { color: #666666 } /* Literal.Number.Oct */
.highlight .sa { color: #BA2121 } /* Literal.String.Affix */
.highlight .sb { color: #BA2121 } /* Literal.String.Backtick */
.highlight .sc { color: #BA2121 } /* Literal.String.Char */
.highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */
.highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */
.highlight .s2 { color: #BA2121 } /* Literal.String.Double */
.highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */
.highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */
.highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */
.highlight .sx { color: #008000 } /* Literal.String.Other */
.highlight .sr { color: #BB6688 } /* Literal.String.Regex */
.highlight .s1 { color: #BA2121 } /* Literal.String.Single */
.highlight .ss { color: #19177C } /* Literal.String.Symbol */
.highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */
.highlight .fm { color: #0000FF } /* Name.Function.Magic */
.highlight .vc { color: #19177C } /* Name.Variable.Class */
.highlight .vg { color: #19177C } /* Name.Variable.Global */
.highlight .vi { color: #19177C } /* Name.Variable.Instance */
.highlight .vm { color: #19177C } /* Name.Variable.Magic */
.highlight .il { color: #666666 } /* Literal.Number.Integer.Long */
    </style>
<style type="text/css">
    
/* Temporary definitions which will become obsolete with Notebook release 5.0 */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-bold { font-weight: bold; }

    </style>


<style type="text/css">
/* Overrides of notebook CSS for static HTML export */
body {
  overflow: visible;
  padding: 8px;
}

div#notebook {
  overflow: visible;
  border-top: none;
}@media print {
  div.cell {
    display: block;
    page-break-inside: avoid;
  } 
  div.output_wrapper { 
    display: block;
    page-break-inside: avoid; 
  }
  div.output { 
    display: block;
    page-break-inside: avoid; 
  }
}
</style>

<!-- Custom stylesheet, it must be in the same directory as the html file -->
<link rel="stylesheet" href="custom.css">

<!-- Loading mathjax macro -->
<!-- Load mathjax -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_HTML"></script>
    <!-- MathJax configuration -->
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ['$','$'], ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
            processEscapes: true,
            processEnvironments: true
        },
        // Center justify equations in code and markdown cells. Elsewhere
        // we use CSS to left justify single line equations in code cells.
        displayAlign: 'center',
        "HTML-CSS": {
            styles: {'.MathJax_Display': {"margin": 0}},
            linebreaks: { automatic: true }
        }
    });
    </script>
    <!-- End of mathjax configuration --></head>
<body>
  <div tabindex="-1" id="notebook" class="border-box-sizing">
    <div class="container" id="notebook-container">

<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<html>

<head>
<meta http-equiv=Content-Type content="text/html; charset=windows-1252">
<meta name=Generator content="Microsoft Word 15 (filtered)">
<style>
<!--
 /* Font Definitions */
 @font-face
    {font-family:"Cambria Math";
    panose-1:2 4 5 3 5 4 6 3 2 4;}
 /* Style Definitions */
 p.MsoNormal, li.MsoNormal, div.MsoNormal
    {margin:0in;
    margin-bottom:.0001pt;
    line-height:115%;
    font-size:11.0pt;
    font-family:"Arial",sans-serif;
    color:black;}
h1
    {margin-top:20.0pt;
    margin-right:0in;
    margin-bottom:6.0pt;
    margin-left:0in;
    line-height:115%;
    page-break-after:avoid;
    font-size:20.0pt;
    font-family:"Arial",sans-serif;
    color:black;
    font-weight:normal;}
h2
    {margin-top:.25in;
    margin-right:0in;
    margin-bottom:6.0pt;
    margin-left:0in;
    line-height:115%;
    page-break-after:avoid;
    font-size:16.0pt;
    font-family:"Arial",sans-serif;
    color:black;
    font-weight:normal;}
h3
    {margin-top:16.0pt;
    margin-right:0in;
    margin-bottom:4.0pt;
    margin-left:0in;
    line-height:115%;
    page-break-after:avoid;
    font-size:14.0pt;
    font-family:"Arial",sans-serif;
    color:#434343;
    font-weight:normal;}
h4
    {margin-top:14.0pt;
    margin-right:0in;
    margin-bottom:4.0pt;
    margin-left:0in;
    line-height:115%;
    page-break-after:avoid;
    font-size:12.0pt;
    font-family:"Arial",sans-serif;
    color:#666666;
    font-weight:normal;}
h5
    {margin-top:12.0pt;
    margin-right:0in;
    margin-bottom:4.0pt;
    margin-left:0in;
    line-height:115%;
    page-break-after:avoid;
    font-size:11.0pt;
    font-family:"Arial",sans-serif;
    color:#666666;
    font-weight:normal;}
h6
    {margin-top:12.0pt;
    margin-right:0in;
    margin-bottom:4.0pt;
    margin-left:0in;
    line-height:115%;
    page-break-after:avoid;
    font-size:11.0pt;
    font-family:"Arial",sans-serif;
    color:#666666;
    font-weight:normal;
    font-style:italic;}
p.MsoHeader, li.MsoHeader, div.MsoHeader
    {mso-style-link:"Header Char";
    margin:0in;
    margin-bottom:.0001pt;
    font-size:11.0pt;
    font-family:"Arial",sans-serif;
    color:black;}
p.MsoFooter, li.MsoFooter, div.MsoFooter
    {mso-style-link:"Footer Char";
    margin:0in;
    margin-bottom:.0001pt;
    font-size:11.0pt;
    font-family:"Arial",sans-serif;
    color:black;}
p.MsoTitle, li.MsoTitle, div.MsoTitle
    {margin-top:0in;
    margin-right:0in;
    margin-bottom:3.0pt;
    margin-left:0in;
    line-height:115%;
    page-break-after:avoid;
    font-size:26.0pt;
    font-family:"Arial",sans-serif;
    color:black;}
p.MsoSubtitle, li.MsoSubtitle, div.MsoSubtitle
    {margin-top:0in;
    margin-right:0in;
    margin-bottom:16.0pt;
    margin-left:0in;
    line-height:115%;
    page-break-after:avoid;
    font-size:15.0pt;
    font-family:"Arial",sans-serif;
    color:#666666;}
a:link, span.MsoHyperlink
    {color:blue;
    text-decoration:underline;}
a:visited, span.MsoHyperlinkFollowed
    {color:purple;
    text-decoration:underline;}
p.MsoListParagraph, li.MsoListParagraph, div.MsoListParagraph
    {margin-top:0in;
    margin-right:0in;
    margin-bottom:0in;
    margin-left:.5in;
    margin-bottom:.0001pt;
    line-height:115%;
    font-size:11.0pt;
    font-family:"Arial",sans-serif;
    color:black;}
p.MsoListParagraphCxSpFirst, li.MsoListParagraphCxSpFirst, div.MsoListParagraphCxSpFirst
    {margin-top:0in;
    margin-right:0in;
    margin-bottom:0in;
    margin-left:.5in;
    margin-bottom:.0001pt;
    line-height:115%;
    font-size:11.0pt;
    font-family:"Arial",sans-serif;
    color:black;}
p.MsoListParagraphCxSpMiddle, li.MsoListParagraphCxSpMiddle, div.MsoListParagraphCxSpMiddle
    {margin-top:0in;
    margin-right:0in;
    margin-bottom:0in;
    margin-left:.5in;
    margin-bottom:.0001pt;
    line-height:115%;
    font-size:11.0pt;
    font-family:"Arial",sans-serif;
    color:black;}
p.MsoListParagraphCxSpLast, li.MsoListParagraphCxSpLast, div.MsoListParagraphCxSpLast
    {margin-top:0in;
    margin-right:0in;
    margin-bottom:0in;
    margin-left:.5in;
    margin-bottom:.0001pt;
    line-height:115%;
    font-size:11.0pt;
    font-family:"Arial",sans-serif;
    color:black;}
span.HeaderChar
    {mso-style-name:"Header Char";
    mso-style-link:Header;}
span.FooterChar
    {mso-style-name:"Footer Char";
    mso-style-link:Footer;}
.MsoChpDefault
    {font-family:"Arial",sans-serif;
    color:black;}
.MsoPapDefault
    {line-height:115%;}
 /* Page Definitions */
 @page WordSection1
    {size:8.5in 11.0in;
    margin:1.0in 1.0in 1.0in 1.0in;}
div.WordSection1
    {page:WordSection1;}
 /* List Definitions */
 ol
    {margin-bottom:0in;}
ul
    {margin-bottom:0in;}
-->
</style>

</head>

<body lang=EN-US link=blue vlink=purple>

<div class=WordSection1>

<p class=MsoNormal align=center style='text-align:center'><span lang=EN
style='font-size:24.0pt;line-height:115%;font-family:"Times New Roman",serif'>Analyzing
Parameter Performance on Deep Belief Networks</span></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>&nbsp;</span></p>

<p class=MsoNormal><b><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>Andy Thai                                                                                          </span></b><span
lang=EN style='font-size:12.0pt;line-height:115%;font-family:"Times New Roman",serif'>ANDYTHAI@UCSD.EDU</span></p>

<p class=MsoNormal><i><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>Department of Cognitive Science,
Department of Mathematics</span></i></p>

<p class=MsoNormal><i><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>University of California, San Diego</span></i></p>

<p class=MsoNormal><i><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>La Jolla, CA</span></i></p>

<p class=MsoNormal><b><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>&nbsp;</span></b></p>

<p class=MsoNormal><b><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>&nbsp;</span></b></p>

<p class=MsoNormal align=center style='margin-bottom:6.0pt;text-align:center'><b><span
lang=EN style='font-size:16.0pt;line-height:115%;font-family:"Times New Roman",serif'>Abstract</span></b></p>

<p class=MsoNormal style='margin-top:0in;margin-right:.6in;margin-bottom:6.0pt;
margin-left:.6in'><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>The rise of deep learning in recent years
has led to adaptations of many different supervised learning models, each made
to tackle subsets of problems. Deep belief networks are a unique type of model,
made up of restricted Boltzmann machines (RBMs) stacked upon each other to
enable hierarchical representations of data. For these networks to achieve peak
performance on datasets, tuning parameters to be optimal is extremely key. This
paper will explore parameter optimization within deep belief networks. Using
MNIST, I look to find the best possible parameters for a deep belief network
and compare its performance against other standard models used throughout
supervised classification. </span></p>

<p class=MsoNormal style='margin-top:0in;margin-right:.6in;margin-bottom:0in;
margin-left:.6in;margin-bottom:.0001pt'><b><span lang=EN style='font-size:12.0pt;
line-height:115%;font-family:"Times New Roman",serif'>Keywords:</span></b><span
lang=EN style='font-size:12.0pt;line-height:115%;font-family:"Times New Roman",serif'>
deep belief networks, SVM, MNIST, supervised learning, neural networks, deep
learning</span></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>&nbsp;</span></p>

<p class=MsoNormal style='margin-bottom:6.0pt'><b><span lang=EN
style='font-size:16.0pt;line-height:115%;font-family:"Times New Roman",serif'>1.
Introduction</span></b></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>Recent upgrades in hardware and computing
have led to an explosion of innovation and development within the fields of
machine learning. Faster, more efficient hardware components have allowed deep
learning models, previously too slow to run on most computers, to be more
easily adapted to everyday general use. </span></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>&nbsp;</span></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>Early models using backpropagation and
gradient-based learning methods came across the vanishing gradient problem,
where gradient updates to network weights could potentially get the weight
stuck on a value so small to the point where weights could no longer be
updated. This would lead to significant issues in training where affected
weights remained constant for an entire training session.</span></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>&nbsp;</span></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>The introduction of deep belief networks
(Hinton et al., 2006) offered a potential solution to this vanishing gradient
problem. This would be done by pre-training each layer, originally set as a
Restricted Boltzmann Machine, via unsupervised training to initialize the
weights, then training them again via the supervised learning method. These
deep belief networks cemented the foundation for future work on neural
networks, and ultimately were a huge factor in the explosion of deep learning
for the near future. </span></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>&nbsp;</span></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>However, to properly utilize these deep
belief networks to their fullest extent, careful attention must be paid to
their parameters, which dictate how the model behaves when training itself with
training datasets. I take a formulaic approach to tuning the parameters on the
MNIST dataset, iteratively testing each parameter on a model and gauging
accuracy to achieve a set of parameters that output the best overall
performance. Results can potentially lead to optimal parameters for training on
sets similar to the MNIST handwritten digit set, such as the Stanford OCR. </span></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>&nbsp;</span></p>

<p class=MsoNormal style='margin-bottom:6.0pt'><b><span lang=EN
style='font-size:16.0pt;line-height:115%;font-family:"Times New Roman",serif'>2.
Method</span></b></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>The deep belief networks library used here
were found at albertbup’s deep-belief-network GitHub repository. The dataset
these networks were trained on was taken from the MNIST handwritten digits. The
dataset was loaded into a Jupyter Python file, where each handwritten digit
data point is represented by a NumPy array holding 784 integers from 0-255
representing a pixel hue (0 being white, and 255 being the darkest black). The
dataset was normalized from 0 to 1 by dividing each pixel value by 256 to
obtain floating point numbers. To cut down on training time, the dataset was
cut down to 1000 data points made up of 10 classes. The first 100 data points
for each class is taken and put into the final set used for this experiment.
The set is then split into a training set and test set with an 80/20 split.  </span></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>&nbsp;</span></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>To tune the hyperparameters, a series of
tests were run gauging performance on the default deep belief network settings
with the one parameter changed. The parameters tested were the number of layers
(1, 2, or 3), pre-training learning rates (RBM), learning rate, pre-training
lengths, and backpropagation iterations. For each parameter, the best performing
setting aws saved and noted for future continued benchmarks.</span></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>&nbsp;</span></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>The first test was run on networks with
one hidden layer, with varying units: 100, 200, 300, 400, and 500 units. The next
test was run on networks with two hidden layers, with a varying number of units
in each layer. The first layer had either 100 or 500 units, while the second
layer had either 200 or 500 units. The specific settings tested were: [100,
200], [100, 500], [500, 200], [500, 500]. The last test for the number of
layers was run with three hidden layers. The first layer had either 100 or 500
units, the second layer had either 200 or 500 units, and the third layer had
either 300 or 500 units. The specific settings tested were: [100, 200, 300],
[500, 200, 300], [100, 500, 300], [100, 200, 500], [500, 500, 500], [500, 200,
500], [100, 500, 500], and [500, 500, 300]. </span></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>&nbsp;</span></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>The next parameter tested was the
pre-training (RBM) learning rates. This parameter determines the learning rate
of the network in the pre-training stage. The learning rates tested were 0.01,
0.05, 0.10, 0.50, and 1.00. </span></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>&nbsp;</span></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>Afterwards, the learning rate parameter
was tested. This learning rate determines the learning rate of the network in
the fine-tuning stage. Like the pre-training learning rate, a similar process
will be run for the network’s learning rate. The learning rates tested will be
0.01, 0.05, 0.10, 0.50, and 1.00. </span></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>&nbsp;</span></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>Next was the pre-training length, which
determines the number of epochs each layer is run in the pre-training stage.
The number of epochs tested was 1, 2, 5, 10, 20, 30, 40, 50, 100, and 200. </span></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>&nbsp;</span></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>After that, the fine-tuning / backprop
lengths, which determine the number of epochs the network will be run in the
fine-tuning stage, was tested. The specific lengths tested were 1, 5, 10, 20,
50, 100, and 200. </span></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>&nbsp;</span></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>After running through all these
parameters, the best performing settings for each parameter was chosen for the
first benchmark tests. These tests consisted of the best performing parameters
with either a one hidden layer, two hidden-layer, or three hidden-layer network
setup. The best performing model out of all the first initial benchmark tests
was taken. Those settings would then be run on either the sigmoidal or ReLu
activation function, with varying dropout settings for each model. Eleven
dropout settings will be run on the models: 0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6,
0.7, 0.8, 0.9, and 1.0, for a total of 22 tests in the second benchmark. The
test with the highest accuracy of these 22 tests was chosen as the final model.
</span></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>&nbsp;</span></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>After choosing the best model, a series of
alternate models was run on the MNIST set. The accuracy of these models was
compared with the final model. The models chosen are taken from the SKLearn
library: LinearSVC, DecisionTreeClassifier, AdaBoostClassifier,
RandomForestClassifier, and four different SVC models. The linearSVC model used
four different C-values. The SVC models were made up of the default parameters,
using a 3<sup>rd</sup> degree polynomial with an rbf kernel. The C parameters
for both the linearSVC and the SVC were 0.1, 1, 10, and 100. </span></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>&nbsp;</span></p>

<b><span lang=EN style='font-size:16.0pt;line-height:115%;font-family:"Times New Roman",serif'><br
clear=all style='page-break-before:always'>
</span></b>

<p class=MsoNormal><b><span lang=EN style='font-size:16.0pt;line-height:115%;
font-family:"Times New Roman",serif'>&nbsp;</span></b></p>

<p class=MsoNormal><b><span lang=EN style='font-size:16.0pt;line-height:115%;
font-family:"Times New Roman",serif'>3. Experiment</span></b></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>&nbsp;</span></p>

<p class=MsoNormal><b><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>One-layer</span></b></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>&nbsp;</span></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'><img width=366 height=262 id="Picture 2"
src="FINAL%20PROJECT_files/image023.gif"></span></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>Running the tests on a one-layer network, it
was found that the most accurate number of units are 200 and 300, which were
tied for 91% classification accuracy. </span></p>

<p class=MsoNormal><b><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>&nbsp;</span></b></p>

<p class=MsoNormal><b><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>Two-layer</span></b></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'><img width=365 height=261 id="Picture 3"
src="FINAL%20PROJECT_files/image024.gif"></span></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>Running the tests on a two-layer network, it
was found that the most accurate setting is 100 units in the first layer, and
500 units in the second layer: [100, 500], with a classification accuracy of
92% accuracy. </span></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>&nbsp;</span></p>

<b><span lang=EN style='font-size:12.0pt;line-height:115%;font-family:"Times New Roman",serif'><br
clear=all style='page-break-before:always'>
</span></b>

<p class=MsoNormal><b><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>&nbsp;</span></b></p>

<p class=MsoNormal><b><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>Three-layer</span></b></p>

<p class=MsoNormal><b><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'><img width=418 height=335 id="Picture 4"
src="FINAL%20PROJECT_files/image025.gif"></span></b></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>Running the tests on a three-layer
network, it was found that the most accurate setting is 500 units in the first
layer, 200 units in the second layer, and 300 units in the third layer: [500,
200, 300], with a classification accuracy of 92.5%. </span></p>

<p class=MsoNormal><b><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>&nbsp;</span></b></p>

<p class=MsoNormal><b><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>Pre-training (RBM) Learning Rates</span></b></p>

<p class=MsoNormal><b><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'><img width=413 height=306 id="Picture 5"
src="FINAL%20PROJECT_files/image026.gif"></span></b></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>Running the tests on the pre-training
learning rates, it was found that the highest accuracy was with
learning_rate_rbm=0.05, with a 91% classification accuracy. </span></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>&nbsp;</span></p>

<p class=MsoNormal><b><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>&nbsp;</span></b></p>

<p class=MsoNormal><b><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>Learning Rate</span></b></p>

<p class=MsoNormal><b><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'><img width=372 height=276 id="Picture 6"
src="FINAL%20PROJECT_files/image027.gif"></span></b></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>Running the tests on the learning rates,
it was found that the highest accuracy was with learning_rate=0.1, with a 91.5%
classification accuracy. </span></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>&nbsp;</span></p>

<p class=MsoNormal><b><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>Pre-training Lengths</span></b></p>

<p class=MsoNormal><b><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'><img width=441 height=324 id="Picture 7"
src="FINAL%20PROJECT_files/image028.gif"></span></b></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>Running the tests on the pre-training lengths,
it was found that the highest accuracy was with n_epochs_rbm=40, with a 93%
classification accuracy. </span></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>&nbsp;</span></p>

<b><span lang=EN style='font-size:12.0pt;line-height:115%;font-family:"Times New Roman",serif'><br
clear=all style='page-break-before:always'>
</span></b>

<p class=MsoNormal><b><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>&nbsp;</span></b></p>

<p class=MsoNormal><b><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>Fine-tuning / Backprop Lengths</span></b></p>

<p class=MsoNormal><b><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'><img width=390 height=287 id="Picture 8"
src="FINAL%20PROJECT_files/image029.gif"></span></b></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>Running the tests on the backpropagation
epochs, it was found that the highest accuracy was with n_iter_backprop=200,
with a 91% classification accuracy. </span></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>&nbsp;</span></p>

<p class=MsoNormal><b><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>First Benchmark</span></b></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>Given the results given in previous tests,
these following models were benchmarked:</span></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>&nbsp;</span></p>

<p class=MsoNormal><i><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>Model 1 (1-layer):</span></i></p>

<p class=MsoNormal><i><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>deep_belief_net(hidden_layers_structure=[200],
</span></i></p>

<p class=MsoNormal><i><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>                      
learning_rate_rbm=0.05,</span></i></p>

<p class=MsoNormal><i><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>                       learning_rate=0.1,</span></i></p>

<p class=MsoNormal><i><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>                       n_epochs_rbm=40,</span></i></p>

<p class=MsoNormal><i><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>                      
n_iter_backprop=200,</span></i></p>

<p class=MsoNormal><i><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>                       batch_size=32,</span></i></p>

<p class=MsoNormal><i><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>                      
activation_function='relu',</span></i></p>

<p class=MsoNormal><i><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>                       dropout_p=0.2)</span></i></p>

<p class=MsoNormal><i><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>&nbsp;</span></i></p>

<p class=MsoNormal><i><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>Model 2 (1-layer):</span></i></p>

<p class=MsoNormal><i><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>deep_belief_net(hidden_layers_structure=[300],
</span></i></p>

<p class=MsoNormal><i><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>                      
learning_rate_rbm=0.05,</span></i></p>

<p class=MsoNormal><i><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>                       learning_rate=0.1,</span></i></p>

<p class=MsoNormal><i><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>                       n_epochs_rbm=40,</span></i></p>

<p class=MsoNormal><i><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>                      
n_iter_backprop=200,</span></i></p>

<p class=MsoNormal><i><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>                       batch_size=32,</span></i></p>

<p class=MsoNormal><i><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>                      
activation_function='relu',</span></i></p>

<p class=MsoNormal><i><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>                       dropout_p=0.2)</span></i></p>

<p class=MsoNormal><i><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>&nbsp;</span></i></p>

<p class=MsoNormal><i><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>Model 3 (2-layer):</span></i></p>

<p class=MsoNormal><i><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>deep_belief_net(hidden_layers_structure=[100,
500], </span></i></p>

<p class=MsoNormal><i><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>                      
learning_rate_rbm=0.05,</span></i></p>

<p class=MsoNormal><i><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>                       learning_rate=0.1,</span></i></p>

<p class=MsoNormal><i><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>                       n_epochs_rbm=40,</span></i></p>

<p class=MsoNormal><i><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>                       n_iter_backprop=200,</span></i></p>

<p class=MsoNormal><i><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>                       batch_size=32,</span></i></p>

<p class=MsoNormal><i><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>                      
activation_function='relu',</span></i></p>

<p class=MsoNormal><i><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>                       dropout_p=0.2)</span></i></p>

<p class=MsoNormal><i><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>&nbsp;</span></i></p>

<p class=MsoNormal><i><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>Model 4 (3-layer):</span></i></p>

<p class=MsoNormal><i><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>deep_belief_net(hidden_layers_structure=[500,
200, 300], </span></i></p>

<p class=MsoNormal><i><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>                      
learning_rate_rbm=0.05,</span></i></p>

<p class=MsoNormal><i><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>                       learning_rate=0.1,</span></i></p>

<p class=MsoNormal><i><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>                       n_epochs_rbm=40,</span></i></p>

<p class=MsoNormal><i><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>                      
n_iter_backprop=200,</span></i></p>

<p class=MsoNormal><i><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>                       batch_size=32,</span></i></p>

<p class=MsoNormal><i><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>                      
activation_function='relu',</span></i></p>

<p class=MsoNormal><i><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>                       dropout_p=0.2)</span></i></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>&nbsp;</span></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>&nbsp;</span></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'><img width=340 height=275 id="Picture 9"
src="FINAL%20PROJECT_files/image030.gif"></span></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>Results from the benchmark indicate that: </span></p>

<p class=MsoNormal style='text-indent:.5in'><span lang=EN style='font-size:
12.0pt;line-height:115%;font-family:"Times New Roman",serif'>Model 1 gives a
90% classification accuracy, </span></p>

<p class=MsoNormal style='text-indent:.5in'><span lang=EN style='font-size:
12.0pt;line-height:115%;font-family:"Times New Roman",serif'>Model 2 gives a
92% classification accuracy, </span></p>

<p class=MsoNormal style='text-indent:.5in'><span lang=EN style='font-size:
12.0pt;line-height:115%;font-family:"Times New Roman",serif'>Model 3 gives a
90.5% classification accuracy.</span></p>

<p class=MsoNormal style='text-indent:.5in'><span lang=EN style='font-size:
12.0pt;line-height:115%;font-family:"Times New Roman",serif'>Model 4 gives a
90.5% classification accuracy.</span></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>&nbsp;</span></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>Results from above indicate that the best
performing model is model 2, containing one hidden layer with 300 units. </span></p>

<p class=MsoNormal><b><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>&nbsp;</span></b></p>

<p class=MsoNormal><b><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>Second Benchmark</span></b></p>

<p class=MsoNormal><b><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'><img width=375 height=276 id="Picture 10"
src="FINAL%20PROJECT_files/image031.gif"></span></b></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>Running through the second benchmark (ReLu
activation) indicated the following results: </span></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>            dropout = 0.0  :           88.0%</span></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>            dropout = 0.1  :           89.0%</span></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>            dropout = 0.2  :           92.0%</span></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>            dropout = 0.3  :           90.0%</span></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>            dropout = 0.4  :           89.0%</span></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>            dropout = 0.5  :           89.0%</span></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>            dropout = 0.6  :           87.0%</span></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>            dropout = 0.7  :           90.0%</span></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>            dropout = 0.8  :           80.0%</span></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>            dropout = 0.9  :           15.5%</span></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>            dropout = 1.0  :           10.0%</span></p>

<p class=MsoNormal><b><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'><img width=374 height=275 id="Picture 11"
src="FINAL%20PROJECT_files/image032.gif"></span></b></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>Running with sigmoidal activation indicated
the following results:</span></p>

<p class=MsoNormal style='text-indent:.5in'><span lang=EN style='font-size:
12.0pt;line-height:115%;font-family:"Times New Roman",serif'>dropout = 0.0  :           88.5%</span></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>            dropout = 0.1  :           92.0%</span></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>            dropout = 0.2  :           89.5%</span></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>            dropout = 0.3  :           88.5%</span></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>            dropout = 0.4  :           88.5%</span></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>            dropout = 0.5  :           87.5%</span></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>            dropout = 0.6  :           88.0%</span></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>            dropout = 0.7  :           85.0%</span></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>            dropout = 0.8  :           84.5%</span></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>            dropout = 0.9  :           70.0%</span></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>            dropout = 1.0  :           10.0%</span></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>            </span></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>&nbsp;</span></p>

<b><span lang=EN style='font-size:12.0pt;line-height:115%;font-family:"Times New Roman",serif'><br
clear=all style='page-break-before:always'>
</span></b>

<p class=MsoNormal><b><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>&nbsp;</span></b></p>

<p class=MsoNormal><b><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>Alternate models</span></b></p>

<p class=MsoNormal><b><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'><img width=519 height=466 id="Picture 12"
src="FINAL%20PROJECT_files/image033.gif"></span></b></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>Running on the alternative models, they
outputted:</span></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>&nbsp;</span></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>            LinearSVC                  ( C
= 0.1  )      :            86.0%</span></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>            LinearSVC                  ( C
= 1     )      :            84.5%</span></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>            LinearSVC                  ( C
= 10   )      :            84.0%</span></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>            LinearSVC                  ( C
= 100 )      :            84.0%</span></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>            Decision Tree                                     :            66.5%</span></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>            Ada Boost                                           :            39.5%</span></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>            Random Forest                                   :            79.5%</span></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>            SVC                            (
C = 0.1  )      :            11.5%</span></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>            SVC                            (
C = 1     )      :            80.5%</span></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>            SVC                            (
C = 10   )      :            88.5%</span></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>            SVC                            (
C = 100 )      :            89.5%</span></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>            </span></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>&nbsp;</span></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>&nbsp;</span></p>

<p class=MsoNormal style='margin-bottom:6.0pt'><b><span lang=EN
style='font-size:16.0pt;line-height:115%;font-family:"Times New Roman",serif'>&nbsp;</span></b></p>

<p class=MsoNormal style='margin-bottom:6.0pt'><b><span lang=EN
style='font-size:16.0pt;line-height:115%;font-family:"Times New Roman",serif'>4.
Conclusion</span></b></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>From the given results, I find that the
parameters chosen using the process noted outputs two deep belief network
settings tied for highest performance at 92% classification accuracy. Using
this process results in deep belief networks that significantly outperform
other methods of supervised learning as seen with boosting, random forests,
decision trees, and the above support vector machines. From the parameter
tuning process, we also find that the parameter settings taken from this
process have resulted in deep belief network accuracies generally higher than
other deep belief network settings, including the default parameters. </span></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>&nbsp;</span></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>On another note, one of the parameters
done in the testing phase, specifically the parameter n_epochs_rbm=40, with
otherwise default parameters, had a classification accuracy of 93%. This is the
highest accuracy performance seen in any of the models tested, including the
two final models.  However, this is not as big of an improvement over the final
models as it seems, given the rather small training and test set size. </span></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>&nbsp;</span></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>Hopefully, the results found in these
experiments will prompt further investigation into parameter tuning for future
applications of deep belief networks, especially on similar datasets to MNIST. </span></p>

<p class=MsoNormal><span lang=EN style='font-size:12.0pt;line-height:115%;
font-family:"Times New Roman",serif'>&nbsp;</span></p>

<p class=MsoNormal style='margin-bottom:6.0pt'><b><span lang=EN
style='font-size:16.0pt;line-height:115%;font-family:"Times New Roman",serif'>5.
References</span></b></p>

<p class=MsoNormal style='margin-bottom:6.0pt'><span lang=EN style='font-size:
12.0pt;line-height:115%;font-family:"Times New Roman",serif'>H Geoffrey E., O
Simon, and T Yee-Whye. A fast learning algorithm for deep belief nets. <i>Neural
Comput</i>, 2006.</span></p>

</div>

</body>

</html>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Code">Code<a class="anchor-link" href="#Code">&#182;</a></h1><hr>
<p>Deep belief networks from G. Hinton<br>
(<a href="http://www.cs.toronto.edu/~hinton/csc2515/projects/deep1.html">http://www.cs.toronto.edu/~hinton/csc2515/projects/deep1.html</a>)</p>
<p>DBN code is available at <a href="https://github.com/albertbup/deep-belief-network">https://github.com/albertbup/deep-belief-network</a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Set-up</strong></p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[44]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Import NumPy</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>  <span class="c1"># Set random seed</span>

<span class="c1"># Import scikit-learn DBN dependency modules</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="k">import</span> <span class="n">load_digits</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="k">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics.classification</span> <span class="k">import</span> <span class="n">accuracy_score</span>

<span class="c1"># Import deep-belief network</span>
<span class="c1"># Found at https://github.com/albertbup/deep-belief-network</span>
<span class="kn">from</span> <span class="nn">dbn.tensorflow</span> <span class="k">import</span> <span class="n">SupervisedDBNClassification</span>

<span class="c1"># Graphs</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Load and format dataset</strong></p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[3]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Load MNIST dataset</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="k">import</span> <span class="n">fetch_mldata</span>
<span class="n">mnist</span> <span class="o">=</span> <span class="n">fetch_mldata</span><span class="p">(</span><span class="s1">&#39;MNIST original&#39;</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[4]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Create set of 1000 MNIST datapoints</span>
<span class="n">class_count</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">X</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">Y</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># Get 100 of each class</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">mnist</span><span class="o">.</span><span class="n">data</span><span class="p">)):</span>
    <span class="n">target_value</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">mnist</span><span class="o">.</span><span class="n">target</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">class_count</span><span class="p">[</span><span class="n">target_value</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mi">100</span><span class="p">:</span>
        <span class="n">X</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mnist</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
        <span class="n">Y</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">target_value</span><span class="p">)</span>
        <span class="n">class_count</span><span class="p">[</span><span class="n">target_value</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">if</span> <span class="nb">sum</span><span class="p">(</span><span class="n">class_count</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1000</span><span class="p">:</span>
        <span class="k">break</span>

<span class="c1"># Convert to NumPy</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>

<span class="c1"># Scale data for MNIST</span>
<span class="n">X</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span> <span class="o">/</span> <span class="mi">256</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>DBN function</strong></p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[5]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">deep_belief_net</span><span class="p">(</span><span class="n">hidden_layers_structure</span><span class="o">=</span><span class="p">[</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">],</span> 
                    <span class="n">learning_rate_rbm</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span>
                    <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                    <span class="n">n_epochs_rbm</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                    <span class="n">n_iter_backprop</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
                    <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
                    <span class="n">activation_function</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span>
                    <span class="n">dropout_p</span><span class="o">=</span><span class="mf">0.2</span><span class="p">):</span>

    <span class="c1"># Splitting data</span>
    <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">Y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="c1"># Training</span>
    <span class="n">classifier</span> <span class="o">=</span> <span class="n">SupervisedDBNClassification</span><span class="p">(</span><span class="n">hidden_layers_structure</span><span class="o">=</span><span class="n">hidden_layers_structure</span><span class="p">,</span>
                                             <span class="n">learning_rate_rbm</span><span class="o">=</span><span class="n">learning_rate_rbm</span><span class="p">,</span>
                                             <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
                                             <span class="n">n_epochs_rbm</span><span class="o">=</span><span class="n">n_epochs_rbm</span><span class="p">,</span>
                                             <span class="n">n_iter_backprop</span><span class="o">=</span><span class="n">n_iter_backprop</span><span class="p">,</span>
                                             <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                                             <span class="n">activation_function</span><span class="o">=</span><span class="n">activation_function</span><span class="p">,</span>
                                             <span class="n">dropout_p</span><span class="o">=</span><span class="n">dropout_p</span><span class="p">)</span>
    <span class="n">classifier</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">)</span>

    <span class="c1"># Save the model</span>
    <span class="n">classifier</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s1">&#39;model.pkl&#39;</span><span class="p">)</span>

    <span class="c1"># Restore</span>
    <span class="n">classifier</span> <span class="o">=</span> <span class="n">SupervisedDBNClassification</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;model.pkl&#39;</span><span class="p">)</span>

    <span class="c1"># Test</span>
    <span class="n">Y_pred</span> <span class="o">=</span> <span class="n">classifier</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="n">acc</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">Y_test</span><span class="p">,</span> <span class="n">Y_pred</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Done.</span><span class="se">\n</span><span class="s1">Accuracy: </span><span class="si">%f</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">acc</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">acc</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Run-DBN-tests-below:">Run DBN tests below:<a class="anchor-link" href="#Run-DBN-tests-below:">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Default setting (base performance)</strong></p>

<pre><code>hidden_layers_structure=[256, 256]
learning_rate_rbm=0.05
learning_rate=0.1
n_epochs_rbm=10
n_iter_backprop=100
batch_size=32
activation_function='relu'
dropout_p=0.2</code></pre>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[5]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Run DBN</span>
<span class="n">default_acc</span> <span class="o">=</span> <span class="n">deep_belief_net</span><span class="p">()</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[START] Pre-training step:
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 74.742325
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 41.013561
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 29.765366
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 39.022728
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 38.707474
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 45.363823
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 35.756557
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 37.316841
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 34.732811
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 26.490023
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 112.534027
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 114.069450
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 75.408974
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 76.183395
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 86.979401
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 107.212440
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 92.068924
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 101.160736
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 83.630653
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 109.224358
[END] Pre-training step
[START] Fine tuning step:
&gt;&gt; Epoch 0 finished 	ANN training loss 0.661431
&gt;&gt; Epoch 1 finished 	ANN training loss 0.473681
&gt;&gt; Epoch 2 finished 	ANN training loss 0.381107
&gt;&gt; Epoch 3 finished 	ANN training loss 0.329544
&gt;&gt; Epoch 4 finished 	ANN training loss 0.277102
&gt;&gt; Epoch 5 finished 	ANN training loss 0.237960
&gt;&gt; Epoch 6 finished 	ANN training loss 0.201121
&gt;&gt; Epoch 7 finished 	ANN training loss 0.176852
&gt;&gt; Epoch 8 finished 	ANN training loss 0.175910
&gt;&gt; Epoch 9 finished 	ANN training loss 0.150462
&gt;&gt; Epoch 10 finished 	ANN training loss 0.134221
&gt;&gt; Epoch 11 finished 	ANN training loss 0.106897
&gt;&gt; Epoch 12 finished 	ANN training loss 0.101303
&gt;&gt; Epoch 13 finished 	ANN training loss 0.097185
&gt;&gt; Epoch 14 finished 	ANN training loss 0.090578
&gt;&gt; Epoch 15 finished 	ANN training loss 0.081134
&gt;&gt; Epoch 16 finished 	ANN training loss 0.062650
&gt;&gt; Epoch 17 finished 	ANN training loss 0.054735
&gt;&gt; Epoch 18 finished 	ANN training loss 0.052028
&gt;&gt; Epoch 19 finished 	ANN training loss 0.051379
&gt;&gt; Epoch 20 finished 	ANN training loss 0.041975
&gt;&gt; Epoch 21 finished 	ANN training loss 0.036709
&gt;&gt; Epoch 22 finished 	ANN training loss 0.038421
&gt;&gt; Epoch 23 finished 	ANN training loss 0.034796
&gt;&gt; Epoch 24 finished 	ANN training loss 0.030439
&gt;&gt; Epoch 25 finished 	ANN training loss 0.028146
&gt;&gt; Epoch 26 finished 	ANN training loss 0.024311
&gt;&gt; Epoch 27 finished 	ANN training loss 0.027123
&gt;&gt; Epoch 28 finished 	ANN training loss 0.021096
&gt;&gt; Epoch 29 finished 	ANN training loss 0.022384
&gt;&gt; Epoch 30 finished 	ANN training loss 0.018908
&gt;&gt; Epoch 31 finished 	ANN training loss 0.016520
&gt;&gt; Epoch 32 finished 	ANN training loss 0.016011
&gt;&gt; Epoch 33 finished 	ANN training loss 0.014118
&gt;&gt; Epoch 34 finished 	ANN training loss 0.012290
&gt;&gt; Epoch 35 finished 	ANN training loss 0.011765
&gt;&gt; Epoch 36 finished 	ANN training loss 0.010858
&gt;&gt; Epoch 37 finished 	ANN training loss 0.010637
&gt;&gt; Epoch 38 finished 	ANN training loss 0.011058
&gt;&gt; Epoch 39 finished 	ANN training loss 0.010356
&gt;&gt; Epoch 40 finished 	ANN training loss 0.008754
&gt;&gt; Epoch 41 finished 	ANN training loss 0.008657
&gt;&gt; Epoch 42 finished 	ANN training loss 0.008185
&gt;&gt; Epoch 43 finished 	ANN training loss 0.007592
&gt;&gt; Epoch 44 finished 	ANN training loss 0.007276
&gt;&gt; Epoch 45 finished 	ANN training loss 0.007210
&gt;&gt; Epoch 46 finished 	ANN training loss 0.006566
&gt;&gt; Epoch 47 finished 	ANN training loss 0.005973
&gt;&gt; Epoch 48 finished 	ANN training loss 0.006141
&gt;&gt; Epoch 49 finished 	ANN training loss 0.005940
&gt;&gt; Epoch 50 finished 	ANN training loss 0.005364
&gt;&gt; Epoch 51 finished 	ANN training loss 0.005246
&gt;&gt; Epoch 52 finished 	ANN training loss 0.004899
&gt;&gt; Epoch 53 finished 	ANN training loss 0.004868
&gt;&gt; Epoch 54 finished 	ANN training loss 0.004056
&gt;&gt; Epoch 55 finished 	ANN training loss 0.004099
&gt;&gt; Epoch 56 finished 	ANN training loss 0.003540
&gt;&gt; Epoch 57 finished 	ANN training loss 0.003511
&gt;&gt; Epoch 58 finished 	ANN training loss 0.003346
&gt;&gt; Epoch 59 finished 	ANN training loss 0.003240
&gt;&gt; Epoch 60 finished 	ANN training loss 0.003048
&gt;&gt; Epoch 61 finished 	ANN training loss 0.003064
&gt;&gt; Epoch 62 finished 	ANN training loss 0.003173
&gt;&gt; Epoch 63 finished 	ANN training loss 0.003033
&gt;&gt; Epoch 64 finished 	ANN training loss 0.003287
&gt;&gt; Epoch 65 finished 	ANN training loss 0.003656
&gt;&gt; Epoch 66 finished 	ANN training loss 0.002517
&gt;&gt; Epoch 67 finished 	ANN training loss 0.002400
&gt;&gt; Epoch 68 finished 	ANN training loss 0.002507
&gt;&gt; Epoch 69 finished 	ANN training loss 0.002370
&gt;&gt; Epoch 70 finished 	ANN training loss 0.002416
&gt;&gt; Epoch 71 finished 	ANN training loss 0.002390
&gt;&gt; Epoch 72 finished 	ANN training loss 0.002906
&gt;&gt; Epoch 73 finished 	ANN training loss 0.002098
&gt;&gt; Epoch 74 finished 	ANN training loss 0.002158
&gt;&gt; Epoch 75 finished 	ANN training loss 0.001844
&gt;&gt; Epoch 76 finished 	ANN training loss 0.002086
&gt;&gt; Epoch 77 finished 	ANN training loss 0.002633
&gt;&gt; Epoch 78 finished 	ANN training loss 0.002115
&gt;&gt; Epoch 79 finished 	ANN training loss 0.001528
&gt;&gt; Epoch 80 finished 	ANN training loss 0.001490
&gt;&gt; Epoch 81 finished 	ANN training loss 0.001441
&gt;&gt; Epoch 82 finished 	ANN training loss 0.001383
&gt;&gt; Epoch 83 finished 	ANN training loss 0.001412
&gt;&gt; Epoch 84 finished 	ANN training loss 0.001336
&gt;&gt; Epoch 85 finished 	ANN training loss 0.001183
&gt;&gt; Epoch 86 finished 	ANN training loss 0.001278
&gt;&gt; Epoch 87 finished 	ANN training loss 0.001399
&gt;&gt; Epoch 88 finished 	ANN training loss 0.001311
&gt;&gt; Epoch 89 finished 	ANN training loss 0.001214
&gt;&gt; Epoch 90 finished 	ANN training loss 0.001467
&gt;&gt; Epoch 91 finished 	ANN training loss 0.001324
&gt;&gt; Epoch 92 finished 	ANN training loss 0.001243
&gt;&gt; Epoch 93 finished 	ANN training loss 0.001247
&gt;&gt; Epoch 94 finished 	ANN training loss 0.001068
&gt;&gt; Epoch 95 finished 	ANN training loss 0.001037
&gt;&gt; Epoch 96 finished 	ANN training loss 0.001000
&gt;&gt; Epoch 97 finished 	ANN training loss 0.001057
&gt;&gt; Epoch 98 finished 	ANN training loss 0.000968
&gt;&gt; Epoch 99 finished 	ANN training loss 0.000883
[END] Fine tuning step
Done.
Accuracy: 0.910000
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[6]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ACCURACY: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">default_acc</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>ACCURACY: 0.91
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Different-1-layer-structures">Different 1-layer structures<a class="anchor-link" href="#Different-1-layer-structures">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[7]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Store accuracies</span>
<span class="n">onelayer_acc</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Setting 1</strong></p>

<pre><code>hidden_layers_structure=[100]</code></pre>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[8]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">onelayer_acc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">deep_belief_net</span><span class="p">(</span><span class="n">hidden_layers_structure</span><span class="o">=</span><span class="p">[</span><span class="mi">100</span><span class="p">])</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[START] Pre-training step:
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 86.976990
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 63.835304
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 47.721466
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 48.712292
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 55.124664
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 55.460400
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 53.136978
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 58.151356
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 63.297531
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 60.489033
[END] Pre-training step
[START] Fine tuning step:
&gt;&gt; Epoch 0 finished 	ANN training loss 0.787348
&gt;&gt; Epoch 1 finished 	ANN training loss 0.577817
&gt;&gt; Epoch 2 finished 	ANN training loss 0.461953
&gt;&gt; Epoch 3 finished 	ANN training loss 0.410259
&gt;&gt; Epoch 4 finished 	ANN training loss 0.370284
&gt;&gt; Epoch 5 finished 	ANN training loss 0.330365
&gt;&gt; Epoch 6 finished 	ANN training loss 0.296728
&gt;&gt; Epoch 7 finished 	ANN training loss 0.282827
&gt;&gt; Epoch 8 finished 	ANN training loss 0.254772
&gt;&gt; Epoch 9 finished 	ANN training loss 0.230176
&gt;&gt; Epoch 10 finished 	ANN training loss 0.229763
&gt;&gt; Epoch 11 finished 	ANN training loss 0.192113
&gt;&gt; Epoch 12 finished 	ANN training loss 0.183068
&gt;&gt; Epoch 13 finished 	ANN training loss 0.172171
&gt;&gt; Epoch 14 finished 	ANN training loss 0.153478
&gt;&gt; Epoch 15 finished 	ANN training loss 0.153599
&gt;&gt; Epoch 16 finished 	ANN training loss 0.138848
&gt;&gt; Epoch 17 finished 	ANN training loss 0.123464
&gt;&gt; Epoch 18 finished 	ANN training loss 0.117612
&gt;&gt; Epoch 19 finished 	ANN training loss 0.106769
&gt;&gt; Epoch 20 finished 	ANN training loss 0.099512
&gt;&gt; Epoch 21 finished 	ANN training loss 0.090460
&gt;&gt; Epoch 22 finished 	ANN training loss 0.088617
&gt;&gt; Epoch 23 finished 	ANN training loss 0.078987
&gt;&gt; Epoch 24 finished 	ANN training loss 0.079952
&gt;&gt; Epoch 25 finished 	ANN training loss 0.071017
&gt;&gt; Epoch 26 finished 	ANN training loss 0.068963
&gt;&gt; Epoch 27 finished 	ANN training loss 0.063790
&gt;&gt; Epoch 28 finished 	ANN training loss 0.059158
&gt;&gt; Epoch 29 finished 	ANN training loss 0.054030
&gt;&gt; Epoch 30 finished 	ANN training loss 0.051049
&gt;&gt; Epoch 31 finished 	ANN training loss 0.047558
&gt;&gt; Epoch 32 finished 	ANN training loss 0.045558
&gt;&gt; Epoch 33 finished 	ANN training loss 0.042773
&gt;&gt; Epoch 34 finished 	ANN training loss 0.040962
&gt;&gt; Epoch 35 finished 	ANN training loss 0.037335
&gt;&gt; Epoch 36 finished 	ANN training loss 0.035929
&gt;&gt; Epoch 37 finished 	ANN training loss 0.036479
&gt;&gt; Epoch 38 finished 	ANN training loss 0.031472
&gt;&gt; Epoch 39 finished 	ANN training loss 0.032730
&gt;&gt; Epoch 40 finished 	ANN training loss 0.028346
&gt;&gt; Epoch 41 finished 	ANN training loss 0.027368
&gt;&gt; Epoch 42 finished 	ANN training loss 0.027131
&gt;&gt; Epoch 43 finished 	ANN training loss 0.025665
&gt;&gt; Epoch 44 finished 	ANN training loss 0.024435
&gt;&gt; Epoch 45 finished 	ANN training loss 0.022174
&gt;&gt; Epoch 46 finished 	ANN training loss 0.021516
&gt;&gt; Epoch 47 finished 	ANN training loss 0.020932
&gt;&gt; Epoch 48 finished 	ANN training loss 0.020583
&gt;&gt; Epoch 49 finished 	ANN training loss 0.019465
&gt;&gt; Epoch 50 finished 	ANN training loss 0.019192
&gt;&gt; Epoch 51 finished 	ANN training loss 0.017391
&gt;&gt; Epoch 52 finished 	ANN training loss 0.016926
&gt;&gt; Epoch 53 finished 	ANN training loss 0.016037
&gt;&gt; Epoch 54 finished 	ANN training loss 0.014989
&gt;&gt; Epoch 55 finished 	ANN training loss 0.016574
&gt;&gt; Epoch 56 finished 	ANN training loss 0.015238
&gt;&gt; Epoch 57 finished 	ANN training loss 0.013714
&gt;&gt; Epoch 58 finished 	ANN training loss 0.013706
&gt;&gt; Epoch 59 finished 	ANN training loss 0.013606
&gt;&gt; Epoch 60 finished 	ANN training loss 0.013038
&gt;&gt; Epoch 61 finished 	ANN training loss 0.012507
&gt;&gt; Epoch 62 finished 	ANN training loss 0.012005
&gt;&gt; Epoch 63 finished 	ANN training loss 0.011266
&gt;&gt; Epoch 64 finished 	ANN training loss 0.010778
&gt;&gt; Epoch 65 finished 	ANN training loss 0.010342
&gt;&gt; Epoch 66 finished 	ANN training loss 0.010704
&gt;&gt; Epoch 67 finished 	ANN training loss 0.009574
&gt;&gt; Epoch 68 finished 	ANN training loss 0.009193
&gt;&gt; Epoch 69 finished 	ANN training loss 0.009129
&gt;&gt; Epoch 70 finished 	ANN training loss 0.009406
&gt;&gt; Epoch 71 finished 	ANN training loss 0.009048
&gt;&gt; Epoch 72 finished 	ANN training loss 0.008465
&gt;&gt; Epoch 73 finished 	ANN training loss 0.009038
&gt;&gt; Epoch 74 finished 	ANN training loss 0.007922
&gt;&gt; Epoch 75 finished 	ANN training loss 0.007606
&gt;&gt; Epoch 76 finished 	ANN training loss 0.007918
&gt;&gt; Epoch 77 finished 	ANN training loss 0.007046
&gt;&gt; Epoch 78 finished 	ANN training loss 0.008020
&gt;&gt; Epoch 79 finished 	ANN training loss 0.006778
&gt;&gt; Epoch 80 finished 	ANN training loss 0.007059
&gt;&gt; Epoch 81 finished 	ANN training loss 0.006585
&gt;&gt; Epoch 82 finished 	ANN training loss 0.006527
&gt;&gt; Epoch 83 finished 	ANN training loss 0.006702
&gt;&gt; Epoch 84 finished 	ANN training loss 0.006153
&gt;&gt; Epoch 85 finished 	ANN training loss 0.006052
&gt;&gt; Epoch 86 finished 	ANN training loss 0.005641
&gt;&gt; Epoch 87 finished 	ANN training loss 0.005605
&gt;&gt; Epoch 88 finished 	ANN training loss 0.005523
&gt;&gt; Epoch 89 finished 	ANN training loss 0.005322
&gt;&gt; Epoch 90 finished 	ANN training loss 0.005478
&gt;&gt; Epoch 91 finished 	ANN training loss 0.005106
&gt;&gt; Epoch 92 finished 	ANN training loss 0.005030
&gt;&gt; Epoch 93 finished 	ANN training loss 0.005018
&gt;&gt; Epoch 94 finished 	ANN training loss 0.005037
&gt;&gt; Epoch 95 finished 	ANN training loss 0.004526
&gt;&gt; Epoch 96 finished 	ANN training loss 0.004715
&gt;&gt; Epoch 97 finished 	ANN training loss 0.004673
&gt;&gt; Epoch 98 finished 	ANN training loss 0.004583
&gt;&gt; Epoch 99 finished 	ANN training loss 0.004510
[END] Fine tuning step
Done.
Accuracy: 0.900000
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[9]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ACCURACY: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">onelayer_acc</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>ACCURACY: 0.9
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Setting 2</strong></p>

<pre><code>hidden_layers_structure=[200]</code></pre>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[10]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">onelayer_acc</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">deep_belief_net</span><span class="p">(</span><span class="n">hidden_layers_structure</span><span class="o">=</span><span class="p">[</span><span class="mi">200</span><span class="p">])</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[START] Pre-training step:
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 52.349266
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 72.684662
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 46.626049
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 43.839184
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 40.715866
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 38.150314
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 40.179611
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 44.788387
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 38.358856
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 36.342644
[END] Pre-training step
[START] Fine tuning step:
&gt;&gt; Epoch 0 finished 	ANN training loss 0.856542
&gt;&gt; Epoch 1 finished 	ANN training loss 0.590754
&gt;&gt; Epoch 2 finished 	ANN training loss 0.478319
&gt;&gt; Epoch 3 finished 	ANN training loss 0.415798
&gt;&gt; Epoch 4 finished 	ANN training loss 0.363241
&gt;&gt; Epoch 5 finished 	ANN training loss 0.342769
&gt;&gt; Epoch 6 finished 	ANN training loss 0.290627
&gt;&gt; Epoch 7 finished 	ANN training loss 0.278313
&gt;&gt; Epoch 8 finished 	ANN training loss 0.241844
&gt;&gt; Epoch 9 finished 	ANN training loss 0.221732
&gt;&gt; Epoch 10 finished 	ANN training loss 0.208090
&gt;&gt; Epoch 11 finished 	ANN training loss 0.195600
&gt;&gt; Epoch 12 finished 	ANN training loss 0.175948
&gt;&gt; Epoch 13 finished 	ANN training loss 0.176113
&gt;&gt; Epoch 14 finished 	ANN training loss 0.144608
&gt;&gt; Epoch 15 finished 	ANN training loss 0.132851
&gt;&gt; Epoch 16 finished 	ANN training loss 0.123206
&gt;&gt; Epoch 17 finished 	ANN training loss 0.118496
&gt;&gt; Epoch 18 finished 	ANN training loss 0.115063
&gt;&gt; Epoch 19 finished 	ANN training loss 0.101604
&gt;&gt; Epoch 20 finished 	ANN training loss 0.094504
&gt;&gt; Epoch 21 finished 	ANN training loss 0.086143
&gt;&gt; Epoch 22 finished 	ANN training loss 0.082999
&gt;&gt; Epoch 23 finished 	ANN training loss 0.074577
&gt;&gt; Epoch 24 finished 	ANN training loss 0.068166
&gt;&gt; Epoch 25 finished 	ANN training loss 0.064587
&gt;&gt; Epoch 26 finished 	ANN training loss 0.063026
&gt;&gt; Epoch 27 finished 	ANN training loss 0.056789
&gt;&gt; Epoch 28 finished 	ANN training loss 0.051912
&gt;&gt; Epoch 29 finished 	ANN training loss 0.048790
&gt;&gt; Epoch 30 finished 	ANN training loss 0.046831
&gt;&gt; Epoch 31 finished 	ANN training loss 0.043765
&gt;&gt; Epoch 32 finished 	ANN training loss 0.041510
&gt;&gt; Epoch 33 finished 	ANN training loss 0.037993
&gt;&gt; Epoch 34 finished 	ANN training loss 0.036890
&gt;&gt; Epoch 35 finished 	ANN training loss 0.034095
&gt;&gt; Epoch 36 finished 	ANN training loss 0.033277
&gt;&gt; Epoch 37 finished 	ANN training loss 0.030609
&gt;&gt; Epoch 38 finished 	ANN training loss 0.028846
&gt;&gt; Epoch 39 finished 	ANN training loss 0.027414
&gt;&gt; Epoch 40 finished 	ANN training loss 0.025866
&gt;&gt; Epoch 41 finished 	ANN training loss 0.025596
&gt;&gt; Epoch 42 finished 	ANN training loss 0.022377
&gt;&gt; Epoch 43 finished 	ANN training loss 0.021889
&gt;&gt; Epoch 44 finished 	ANN training loss 0.021763
&gt;&gt; Epoch 45 finished 	ANN training loss 0.020090
&gt;&gt; Epoch 46 finished 	ANN training loss 0.019128
&gt;&gt; Epoch 47 finished 	ANN training loss 0.019097
&gt;&gt; Epoch 48 finished 	ANN training loss 0.018722
&gt;&gt; Epoch 49 finished 	ANN training loss 0.016980
&gt;&gt; Epoch 50 finished 	ANN training loss 0.015873
&gt;&gt; Epoch 51 finished 	ANN training loss 0.015356
&gt;&gt; Epoch 52 finished 	ANN training loss 0.015273
&gt;&gt; Epoch 53 finished 	ANN training loss 0.014214
&gt;&gt; Epoch 54 finished 	ANN training loss 0.014201
&gt;&gt; Epoch 55 finished 	ANN training loss 0.013499
&gt;&gt; Epoch 56 finished 	ANN training loss 0.012644
&gt;&gt; Epoch 57 finished 	ANN training loss 0.012240
&gt;&gt; Epoch 58 finished 	ANN training loss 0.011818
&gt;&gt; Epoch 59 finished 	ANN training loss 0.011327
&gt;&gt; Epoch 60 finished 	ANN training loss 0.011356
&gt;&gt; Epoch 61 finished 	ANN training loss 0.010827
&gt;&gt; Epoch 62 finished 	ANN training loss 0.010376
&gt;&gt; Epoch 63 finished 	ANN training loss 0.010062
&gt;&gt; Epoch 64 finished 	ANN training loss 0.009569
&gt;&gt; Epoch 65 finished 	ANN training loss 0.009572
&gt;&gt; Epoch 66 finished 	ANN training loss 0.009667
&gt;&gt; Epoch 67 finished 	ANN training loss 0.009007
&gt;&gt; Epoch 68 finished 	ANN training loss 0.008775
&gt;&gt; Epoch 69 finished 	ANN training loss 0.008365
&gt;&gt; Epoch 70 finished 	ANN training loss 0.008614
&gt;&gt; Epoch 71 finished 	ANN training loss 0.007897
&gt;&gt; Epoch 72 finished 	ANN training loss 0.007492
&gt;&gt; Epoch 73 finished 	ANN training loss 0.007543
&gt;&gt; Epoch 74 finished 	ANN training loss 0.007750
&gt;&gt; Epoch 75 finished 	ANN training loss 0.007785
&gt;&gt; Epoch 76 finished 	ANN training loss 0.007707
&gt;&gt; Epoch 77 finished 	ANN training loss 0.007456
&gt;&gt; Epoch 78 finished 	ANN training loss 0.007066
&gt;&gt; Epoch 79 finished 	ANN training loss 0.006705
&gt;&gt; Epoch 80 finished 	ANN training loss 0.006266
&gt;&gt; Epoch 81 finished 	ANN training loss 0.006087
&gt;&gt; Epoch 82 finished 	ANN training loss 0.006256
&gt;&gt; Epoch 83 finished 	ANN training loss 0.005950
&gt;&gt; Epoch 84 finished 	ANN training loss 0.005855
&gt;&gt; Epoch 85 finished 	ANN training loss 0.005483
&gt;&gt; Epoch 86 finished 	ANN training loss 0.005128
&gt;&gt; Epoch 87 finished 	ANN training loss 0.005136
&gt;&gt; Epoch 88 finished 	ANN training loss 0.004829
&gt;&gt; Epoch 89 finished 	ANN training loss 0.004799
&gt;&gt; Epoch 90 finished 	ANN training loss 0.004780
&gt;&gt; Epoch 91 finished 	ANN training loss 0.004908
&gt;&gt; Epoch 92 finished 	ANN training loss 0.004728
&gt;&gt; Epoch 93 finished 	ANN training loss 0.004494
&gt;&gt; Epoch 94 finished 	ANN training loss 0.004456
&gt;&gt; Epoch 95 finished 	ANN training loss 0.004336
&gt;&gt; Epoch 96 finished 	ANN training loss 0.004266
&gt;&gt; Epoch 97 finished 	ANN training loss 0.004331
&gt;&gt; Epoch 98 finished 	ANN training loss 0.004199
&gt;&gt; Epoch 99 finished 	ANN training loss 0.004174
[END] Fine tuning step
Done.
Accuracy: 0.910000
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[11]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ACCURACY: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">onelayer_acc</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>ACCURACY: 0.91
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Setting 3</strong></p>

<pre><code>hidden_layers_structure=[300]</code></pre>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[12]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">onelayer_acc</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">deep_belief_net</span><span class="p">(</span><span class="n">hidden_layers_structure</span><span class="o">=</span><span class="p">[</span><span class="mi">300</span><span class="p">])</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[START] Pre-training step:
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 87.902206
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 55.003399
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 40.669594
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 37.418659
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 39.941753
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 47.157059
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 31.197338
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 39.757397
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 33.218170
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 46.521252
[END] Pre-training step
[START] Fine tuning step:
&gt;&gt; Epoch 0 finished 	ANN training loss 0.809349
&gt;&gt; Epoch 1 finished 	ANN training loss 0.550912
&gt;&gt; Epoch 2 finished 	ANN training loss 0.458730
&gt;&gt; Epoch 3 finished 	ANN training loss 0.408621
&gt;&gt; Epoch 4 finished 	ANN training loss 0.343763
&gt;&gt; Epoch 5 finished 	ANN training loss 0.327247
&gt;&gt; Epoch 6 finished 	ANN training loss 0.292876
&gt;&gt; Epoch 7 finished 	ANN training loss 0.264524
&gt;&gt; Epoch 8 finished 	ANN training loss 0.235765
&gt;&gt; Epoch 9 finished 	ANN training loss 0.215654
&gt;&gt; Epoch 10 finished 	ANN training loss 0.202415
&gt;&gt; Epoch 11 finished 	ANN training loss 0.187902
&gt;&gt; Epoch 12 finished 	ANN training loss 0.166092
&gt;&gt; Epoch 13 finished 	ANN training loss 0.160441
&gt;&gt; Epoch 14 finished 	ANN training loss 0.142402
&gt;&gt; Epoch 15 finished 	ANN training loss 0.130498
&gt;&gt; Epoch 16 finished 	ANN training loss 0.122930
&gt;&gt; Epoch 17 finished 	ANN training loss 0.111313
&gt;&gt; Epoch 18 finished 	ANN training loss 0.106092
&gt;&gt; Epoch 19 finished 	ANN training loss 0.092249
&gt;&gt; Epoch 20 finished 	ANN training loss 0.089241
&gt;&gt; Epoch 21 finished 	ANN training loss 0.083307
&gt;&gt; Epoch 22 finished 	ANN training loss 0.077107
&gt;&gt; Epoch 23 finished 	ANN training loss 0.070437
&gt;&gt; Epoch 24 finished 	ANN training loss 0.064246
&gt;&gt; Epoch 25 finished 	ANN training loss 0.059722
&gt;&gt; Epoch 26 finished 	ANN training loss 0.056275
&gt;&gt; Epoch 27 finished 	ANN training loss 0.054445
&gt;&gt; Epoch 28 finished 	ANN training loss 0.049925
&gt;&gt; Epoch 29 finished 	ANN training loss 0.048585
&gt;&gt; Epoch 30 finished 	ANN training loss 0.043062
&gt;&gt; Epoch 31 finished 	ANN training loss 0.040427
&gt;&gt; Epoch 32 finished 	ANN training loss 0.037998
&gt;&gt; Epoch 33 finished 	ANN training loss 0.036308
&gt;&gt; Epoch 34 finished 	ANN training loss 0.034671
&gt;&gt; Epoch 35 finished 	ANN training loss 0.031982
&gt;&gt; Epoch 36 finished 	ANN training loss 0.031280
&gt;&gt; Epoch 37 finished 	ANN training loss 0.027919
&gt;&gt; Epoch 38 finished 	ANN training loss 0.027858
&gt;&gt; Epoch 39 finished 	ANN training loss 0.028015
&gt;&gt; Epoch 40 finished 	ANN training loss 0.023874
&gt;&gt; Epoch 41 finished 	ANN training loss 0.024393
&gt;&gt; Epoch 42 finished 	ANN training loss 0.022408
&gt;&gt; Epoch 43 finished 	ANN training loss 0.023429
&gt;&gt; Epoch 44 finished 	ANN training loss 0.019635
&gt;&gt; Epoch 45 finished 	ANN training loss 0.019911
&gt;&gt; Epoch 46 finished 	ANN training loss 0.018749
&gt;&gt; Epoch 47 finished 	ANN training loss 0.018279
&gt;&gt; Epoch 48 finished 	ANN training loss 0.016492
&gt;&gt; Epoch 49 finished 	ANN training loss 0.016776
&gt;&gt; Epoch 50 finished 	ANN training loss 0.016031
&gt;&gt; Epoch 51 finished 	ANN training loss 0.014930
&gt;&gt; Epoch 52 finished 	ANN training loss 0.014278
&gt;&gt; Epoch 53 finished 	ANN training loss 0.014475
&gt;&gt; Epoch 54 finished 	ANN training loss 0.013630
&gt;&gt; Epoch 55 finished 	ANN training loss 0.013575
&gt;&gt; Epoch 56 finished 	ANN training loss 0.013000
&gt;&gt; Epoch 57 finished 	ANN training loss 0.011852
&gt;&gt; Epoch 58 finished 	ANN training loss 0.011353
&gt;&gt; Epoch 59 finished 	ANN training loss 0.010982
&gt;&gt; Epoch 60 finished 	ANN training loss 0.010344
&gt;&gt; Epoch 61 finished 	ANN training loss 0.010204
&gt;&gt; Epoch 62 finished 	ANN training loss 0.009919
&gt;&gt; Epoch 63 finished 	ANN training loss 0.009515
&gt;&gt; Epoch 64 finished 	ANN training loss 0.009078
&gt;&gt; Epoch 65 finished 	ANN training loss 0.008570
&gt;&gt; Epoch 66 finished 	ANN training loss 0.008352
&gt;&gt; Epoch 67 finished 	ANN training loss 0.008462
&gt;&gt; Epoch 68 finished 	ANN training loss 0.007986
&gt;&gt; Epoch 69 finished 	ANN training loss 0.007929
&gt;&gt; Epoch 70 finished 	ANN training loss 0.007431
&gt;&gt; Epoch 71 finished 	ANN training loss 0.007339
&gt;&gt; Epoch 72 finished 	ANN training loss 0.007707
&gt;&gt; Epoch 73 finished 	ANN training loss 0.006715
&gt;&gt; Epoch 74 finished 	ANN training loss 0.006569
&gt;&gt; Epoch 75 finished 	ANN training loss 0.006192
&gt;&gt; Epoch 76 finished 	ANN training loss 0.006381
&gt;&gt; Epoch 77 finished 	ANN training loss 0.006126
&gt;&gt; Epoch 78 finished 	ANN training loss 0.006186
&gt;&gt; Epoch 79 finished 	ANN training loss 0.005830
&gt;&gt; Epoch 80 finished 	ANN training loss 0.005903
&gt;&gt; Epoch 81 finished 	ANN training loss 0.005633
&gt;&gt; Epoch 82 finished 	ANN training loss 0.005361
&gt;&gt; Epoch 83 finished 	ANN training loss 0.005482
&gt;&gt; Epoch 84 finished 	ANN training loss 0.005286
&gt;&gt; Epoch 85 finished 	ANN training loss 0.004962
&gt;&gt; Epoch 86 finished 	ANN training loss 0.004915
&gt;&gt; Epoch 87 finished 	ANN training loss 0.004904
&gt;&gt; Epoch 88 finished 	ANN training loss 0.004724
&gt;&gt; Epoch 89 finished 	ANN training loss 0.004845
&gt;&gt; Epoch 90 finished 	ANN training loss 0.004529
&gt;&gt; Epoch 91 finished 	ANN training loss 0.004636
&gt;&gt; Epoch 92 finished 	ANN training loss 0.004444
&gt;&gt; Epoch 93 finished 	ANN training loss 0.004189
&gt;&gt; Epoch 94 finished 	ANN training loss 0.004262
&gt;&gt; Epoch 95 finished 	ANN training loss 0.004398
&gt;&gt; Epoch 96 finished 	ANN training loss 0.003976
&gt;&gt; Epoch 97 finished 	ANN training loss 0.004088
&gt;&gt; Epoch 98 finished 	ANN training loss 0.003930
&gt;&gt; Epoch 99 finished 	ANN training loss 0.003776
[END] Fine tuning step
Done.
Accuracy: 0.910000
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[13]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ACCURACY: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">onelayer_acc</span><span class="p">[</span><span class="mi">2</span><span class="p">]))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>ACCURACY: 0.91
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Setting 4</strong></p>

<pre><code>hidden_layers_structure=[400]</code></pre>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[14]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">onelayer_acc</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">=</span> <span class="n">deep_belief_net</span><span class="p">(</span><span class="n">hidden_layers_structure</span><span class="o">=</span><span class="p">[</span><span class="mi">400</span><span class="p">])</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[START] Pre-training step:
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 49.054512
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 90.852394
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 54.142891
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 68.925636
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 59.368515
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 53.105038
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 49.493587
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 54.296677
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 41.562691
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 35.099659
[END] Pre-training step
[START] Fine tuning step:
&gt;&gt; Epoch 0 finished 	ANN training loss 0.792912
&gt;&gt; Epoch 1 finished 	ANN training loss 0.560390
&gt;&gt; Epoch 2 finished 	ANN training loss 0.456119
&gt;&gt; Epoch 3 finished 	ANN training loss 0.393785
&gt;&gt; Epoch 4 finished 	ANN training loss 0.362233
&gt;&gt; Epoch 5 finished 	ANN training loss 0.321579
&gt;&gt; Epoch 6 finished 	ANN training loss 0.302900
&gt;&gt; Epoch 7 finished 	ANN training loss 0.277325
&gt;&gt; Epoch 8 finished 	ANN training loss 0.240903
&gt;&gt; Epoch 9 finished 	ANN training loss 0.226532
&gt;&gt; Epoch 10 finished 	ANN training loss 0.202836
&gt;&gt; Epoch 11 finished 	ANN training loss 0.185821
&gt;&gt; Epoch 12 finished 	ANN training loss 0.176060
&gt;&gt; Epoch 13 finished 	ANN training loss 0.158493
&gt;&gt; Epoch 14 finished 	ANN training loss 0.147478
&gt;&gt; Epoch 15 finished 	ANN training loss 0.136997
&gt;&gt; Epoch 16 finished 	ANN training loss 0.131882
&gt;&gt; Epoch 17 finished 	ANN training loss 0.120900
&gt;&gt; Epoch 18 finished 	ANN training loss 0.107460
&gt;&gt; Epoch 19 finished 	ANN training loss 0.101513
&gt;&gt; Epoch 20 finished 	ANN training loss 0.091230
&gt;&gt; Epoch 21 finished 	ANN training loss 0.082328
&gt;&gt; Epoch 22 finished 	ANN training loss 0.083925
&gt;&gt; Epoch 23 finished 	ANN training loss 0.075263
&gt;&gt; Epoch 24 finished 	ANN training loss 0.069971
&gt;&gt; Epoch 25 finished 	ANN training loss 0.066040
&gt;&gt; Epoch 26 finished 	ANN training loss 0.059458
&gt;&gt; Epoch 27 finished 	ANN training loss 0.054597
&gt;&gt; Epoch 28 finished 	ANN training loss 0.051358
&gt;&gt; Epoch 29 finished 	ANN training loss 0.049066
&gt;&gt; Epoch 30 finished 	ANN training loss 0.045689
&gt;&gt; Epoch 31 finished 	ANN training loss 0.043978
&gt;&gt; Epoch 32 finished 	ANN training loss 0.040109
&gt;&gt; Epoch 33 finished 	ANN training loss 0.038521
&gt;&gt; Epoch 34 finished 	ANN training loss 0.035331
&gt;&gt; Epoch 35 finished 	ANN training loss 0.033475
&gt;&gt; Epoch 36 finished 	ANN training loss 0.031607
&gt;&gt; Epoch 37 finished 	ANN training loss 0.029440
&gt;&gt; Epoch 38 finished 	ANN training loss 0.028656
&gt;&gt; Epoch 39 finished 	ANN training loss 0.030513
&gt;&gt; Epoch 40 finished 	ANN training loss 0.026177
&gt;&gt; Epoch 41 finished 	ANN training loss 0.025393
&gt;&gt; Epoch 42 finished 	ANN training loss 0.026700
&gt;&gt; Epoch 43 finished 	ANN training loss 0.021883
&gt;&gt; Epoch 44 finished 	ANN training loss 0.021498
&gt;&gt; Epoch 45 finished 	ANN training loss 0.021088
&gt;&gt; Epoch 46 finished 	ANN training loss 0.019419
&gt;&gt; Epoch 47 finished 	ANN training loss 0.018494
&gt;&gt; Epoch 48 finished 	ANN training loss 0.017926
&gt;&gt; Epoch 49 finished 	ANN training loss 0.017273
&gt;&gt; Epoch 50 finished 	ANN training loss 0.017615
&gt;&gt; Epoch 51 finished 	ANN training loss 0.015977
&gt;&gt; Epoch 52 finished 	ANN training loss 0.014584
&gt;&gt; Epoch 53 finished 	ANN training loss 0.013821
&gt;&gt; Epoch 54 finished 	ANN training loss 0.013653
&gt;&gt; Epoch 55 finished 	ANN training loss 0.012896
&gt;&gt; Epoch 56 finished 	ANN training loss 0.012485
&gt;&gt; Epoch 57 finished 	ANN training loss 0.012098
&gt;&gt; Epoch 58 finished 	ANN training loss 0.011524
&gt;&gt; Epoch 59 finished 	ANN training loss 0.011108
&gt;&gt; Epoch 60 finished 	ANN training loss 0.010978
&gt;&gt; Epoch 61 finished 	ANN training loss 0.010789
&gt;&gt; Epoch 62 finished 	ANN training loss 0.010164
&gt;&gt; Epoch 63 finished 	ANN training loss 0.009842
&gt;&gt; Epoch 64 finished 	ANN training loss 0.009203
&gt;&gt; Epoch 65 finished 	ANN training loss 0.008949
&gt;&gt; Epoch 66 finished 	ANN training loss 0.008519
&gt;&gt; Epoch 67 finished 	ANN training loss 0.008319
&gt;&gt; Epoch 68 finished 	ANN training loss 0.008626
&gt;&gt; Epoch 69 finished 	ANN training loss 0.008294
&gt;&gt; Epoch 70 finished 	ANN training loss 0.007995
&gt;&gt; Epoch 71 finished 	ANN training loss 0.007694
&gt;&gt; Epoch 72 finished 	ANN training loss 0.007618
&gt;&gt; Epoch 73 finished 	ANN training loss 0.007048
&gt;&gt; Epoch 74 finished 	ANN training loss 0.006974
&gt;&gt; Epoch 75 finished 	ANN training loss 0.007001
&gt;&gt; Epoch 76 finished 	ANN training loss 0.006803
&gt;&gt; Epoch 77 finished 	ANN training loss 0.006330
&gt;&gt; Epoch 78 finished 	ANN training loss 0.006589
&gt;&gt; Epoch 79 finished 	ANN training loss 0.006382
&gt;&gt; Epoch 80 finished 	ANN training loss 0.006140
&gt;&gt; Epoch 81 finished 	ANN training loss 0.005898
&gt;&gt; Epoch 82 finished 	ANN training loss 0.005944
&gt;&gt; Epoch 83 finished 	ANN training loss 0.005474
&gt;&gt; Epoch 84 finished 	ANN training loss 0.005404
&gt;&gt; Epoch 85 finished 	ANN training loss 0.005225
&gt;&gt; Epoch 86 finished 	ANN training loss 0.005078
&gt;&gt; Epoch 87 finished 	ANN training loss 0.005013
&gt;&gt; Epoch 88 finished 	ANN training loss 0.004891
&gt;&gt; Epoch 89 finished 	ANN training loss 0.005059
&gt;&gt; Epoch 90 finished 	ANN training loss 0.005261
&gt;&gt; Epoch 91 finished 	ANN training loss 0.004610
&gt;&gt; Epoch 92 finished 	ANN training loss 0.004801
&gt;&gt; Epoch 93 finished 	ANN training loss 0.004589
&gt;&gt; Epoch 94 finished 	ANN training loss 0.004381
&gt;&gt; Epoch 95 finished 	ANN training loss 0.004123
&gt;&gt; Epoch 96 finished 	ANN training loss 0.003979
&gt;&gt; Epoch 97 finished 	ANN training loss 0.003998
&gt;&gt; Epoch 98 finished 	ANN training loss 0.003940
&gt;&gt; Epoch 99 finished 	ANN training loss 0.004037
[END] Fine tuning step
Done.
Accuracy: 0.905000
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[15]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ACCURACY: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">onelayer_acc</span><span class="p">[</span><span class="mi">3</span><span class="p">]))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>ACCURACY: 0.905
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Setting 5</strong></p>

<pre><code>hidden_layers_structure=[500]</code></pre>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[16]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">onelayer_acc</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span> <span class="o">=</span> <span class="n">deep_belief_net</span><span class="p">(</span><span class="n">hidden_layers_structure</span><span class="o">=</span><span class="p">[</span><span class="mi">500</span><span class="p">])</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[START] Pre-training step:
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 42.600475
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 38.644325
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 50.377831
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 37.820583
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 95.727844
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 49.393799
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 49.923161
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 27.079639
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 30.330034
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 31.138687
[END] Pre-training step
[START] Fine tuning step:
&gt;&gt; Epoch 0 finished 	ANN training loss 0.798674
&gt;&gt; Epoch 1 finished 	ANN training loss 0.554613
&gt;&gt; Epoch 2 finished 	ANN training loss 0.452316
&gt;&gt; Epoch 3 finished 	ANN training loss 0.381789
&gt;&gt; Epoch 4 finished 	ANN training loss 0.329494
&gt;&gt; Epoch 5 finished 	ANN training loss 0.295961
&gt;&gt; Epoch 6 finished 	ANN training loss 0.267486
&gt;&gt; Epoch 7 finished 	ANN training loss 0.241804
&gt;&gt; Epoch 8 finished 	ANN training loss 0.235162
&gt;&gt; Epoch 9 finished 	ANN training loss 0.204400
&gt;&gt; Epoch 10 finished 	ANN training loss 0.190618
&gt;&gt; Epoch 11 finished 	ANN training loss 0.173614
&gt;&gt; Epoch 12 finished 	ANN training loss 0.164421
&gt;&gt; Epoch 13 finished 	ANN training loss 0.147224
&gt;&gt; Epoch 14 finished 	ANN training loss 0.130776
&gt;&gt; Epoch 15 finished 	ANN training loss 0.121806
&gt;&gt; Epoch 16 finished 	ANN training loss 0.111645
&gt;&gt; Epoch 17 finished 	ANN training loss 0.114588
&gt;&gt; Epoch 18 finished 	ANN training loss 0.103425
&gt;&gt; Epoch 19 finished 	ANN training loss 0.091205
&gt;&gt; Epoch 20 finished 	ANN training loss 0.081668
&gt;&gt; Epoch 21 finished 	ANN training loss 0.074841
&gt;&gt; Epoch 22 finished 	ANN training loss 0.070446
&gt;&gt; Epoch 23 finished 	ANN training loss 0.064421
&gt;&gt; Epoch 24 finished 	ANN training loss 0.059264
&gt;&gt; Epoch 25 finished 	ANN training loss 0.054818
&gt;&gt; Epoch 26 finished 	ANN training loss 0.052259
&gt;&gt; Epoch 27 finished 	ANN training loss 0.049628
&gt;&gt; Epoch 28 finished 	ANN training loss 0.045563
&gt;&gt; Epoch 29 finished 	ANN training loss 0.043829
&gt;&gt; Epoch 30 finished 	ANN training loss 0.041754
&gt;&gt; Epoch 31 finished 	ANN training loss 0.038039
&gt;&gt; Epoch 32 finished 	ANN training loss 0.035826
&gt;&gt; Epoch 33 finished 	ANN training loss 0.034101
&gt;&gt; Epoch 34 finished 	ANN training loss 0.032588
&gt;&gt; Epoch 35 finished 	ANN training loss 0.037622
&gt;&gt; Epoch 36 finished 	ANN training loss 0.027654
&gt;&gt; Epoch 37 finished 	ANN training loss 0.026445
&gt;&gt; Epoch 38 finished 	ANN training loss 0.025119
&gt;&gt; Epoch 39 finished 	ANN training loss 0.023703
&gt;&gt; Epoch 40 finished 	ANN training loss 0.022303
&gt;&gt; Epoch 41 finished 	ANN training loss 0.021529
&gt;&gt; Epoch 42 finished 	ANN training loss 0.021216
&gt;&gt; Epoch 43 finished 	ANN training loss 0.019514
&gt;&gt; Epoch 44 finished 	ANN training loss 0.018915
&gt;&gt; Epoch 45 finished 	ANN training loss 0.017770
&gt;&gt; Epoch 46 finished 	ANN training loss 0.018500
&gt;&gt; Epoch 47 finished 	ANN training loss 0.016693
&gt;&gt; Epoch 48 finished 	ANN training loss 0.015336
&gt;&gt; Epoch 49 finished 	ANN training loss 0.015308
&gt;&gt; Epoch 50 finished 	ANN training loss 0.014903
&gt;&gt; Epoch 51 finished 	ANN training loss 0.013932
&gt;&gt; Epoch 52 finished 	ANN training loss 0.014617
&gt;&gt; Epoch 53 finished 	ANN training loss 0.013093
&gt;&gt; Epoch 54 finished 	ANN training loss 0.012403
&gt;&gt; Epoch 55 finished 	ANN training loss 0.011971
&gt;&gt; Epoch 56 finished 	ANN training loss 0.011769
&gt;&gt; Epoch 57 finished 	ANN training loss 0.010903
&gt;&gt; Epoch 58 finished 	ANN training loss 0.011059
&gt;&gt; Epoch 59 finished 	ANN training loss 0.010350
&gt;&gt; Epoch 60 finished 	ANN training loss 0.010125
&gt;&gt; Epoch 61 finished 	ANN training loss 0.009666
&gt;&gt; Epoch 62 finished 	ANN training loss 0.010602
&gt;&gt; Epoch 63 finished 	ANN training loss 0.009305
&gt;&gt; Epoch 64 finished 	ANN training loss 0.009050
&gt;&gt; Epoch 65 finished 	ANN training loss 0.008513
&gt;&gt; Epoch 66 finished 	ANN training loss 0.008434
&gt;&gt; Epoch 67 finished 	ANN training loss 0.007942
&gt;&gt; Epoch 68 finished 	ANN training loss 0.008066
&gt;&gt; Epoch 69 finished 	ANN training loss 0.008938
&gt;&gt; Epoch 70 finished 	ANN training loss 0.007838
&gt;&gt; Epoch 71 finished 	ANN training loss 0.007490
&gt;&gt; Epoch 72 finished 	ANN training loss 0.007038
&gt;&gt; Epoch 73 finished 	ANN training loss 0.007209
&gt;&gt; Epoch 74 finished 	ANN training loss 0.006680
&gt;&gt; Epoch 75 finished 	ANN training loss 0.006607
&gt;&gt; Epoch 76 finished 	ANN training loss 0.006495
&gt;&gt; Epoch 77 finished 	ANN training loss 0.005989
&gt;&gt; Epoch 78 finished 	ANN training loss 0.006087
&gt;&gt; Epoch 79 finished 	ANN training loss 0.005715
&gt;&gt; Epoch 80 finished 	ANN training loss 0.006298
&gt;&gt; Epoch 81 finished 	ANN training loss 0.005512
&gt;&gt; Epoch 82 finished 	ANN training loss 0.005520
&gt;&gt; Epoch 83 finished 	ANN training loss 0.005228
&gt;&gt; Epoch 84 finished 	ANN training loss 0.005405
&gt;&gt; Epoch 85 finished 	ANN training loss 0.004982
&gt;&gt; Epoch 86 finished 	ANN training loss 0.005140
&gt;&gt; Epoch 87 finished 	ANN training loss 0.005102
&gt;&gt; Epoch 88 finished 	ANN training loss 0.004754
&gt;&gt; Epoch 89 finished 	ANN training loss 0.004679
&gt;&gt; Epoch 90 finished 	ANN training loss 0.004527
&gt;&gt; Epoch 91 finished 	ANN training loss 0.004462
&gt;&gt; Epoch 92 finished 	ANN training loss 0.004584
&gt;&gt; Epoch 93 finished 	ANN training loss 0.004337
&gt;&gt; Epoch 94 finished 	ANN training loss 0.004321
&gt;&gt; Epoch 95 finished 	ANN training loss 0.003943
&gt;&gt; Epoch 96 finished 	ANN training loss 0.004028
&gt;&gt; Epoch 97 finished 	ANN training loss 0.003730
&gt;&gt; Epoch 98 finished 	ANN training loss 0.003666
&gt;&gt; Epoch 99 finished 	ANN training loss 0.003641
[END] Fine tuning step
Done.
Accuracy: 0.885000
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[17]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ACCURACY: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">onelayer_acc</span><span class="p">[</span><span class="mi">4</span><span class="p">]))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>ACCURACY: 0.885
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[18]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Collated results</span>
<span class="nb">print</span><span class="p">(</span><span class="n">onelayer_acc</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Most accurate 1-layer setting is Setting &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">onelayer_acc</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="n">onelayer_acc</span><span class="p">))</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[0.90000000000000002, 0.91000000000000003, 0.91000000000000003, 0.90500000000000003, 0.88500000000000001]
Most accurate 1-layer setting is Setting 2
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[45]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Create bar graph</span>
<span class="n">plt</span><span class="o">.</span><span class="n">clf</span><span class="p">()</span>
<span class="n">objects</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;[100]&#39;</span><span class="p">,</span> <span class="s1">&#39;[200]&#39;</span><span class="p">,</span> <span class="s1">&#39;[300]&#39;</span><span class="p">,</span> <span class="s1">&#39;[400]&#39;</span><span class="p">,</span> <span class="s1">&#39;[500]&#39;</span><span class="p">)</span>
<span class="n">y_pos</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">objects</span><span class="p">))</span>
<span class="n">performance</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.9</span><span class="p">,</span><span class="mf">0.91</span><span class="p">,</span><span class="mf">0.91</span><span class="p">,</span><span class="mf">0.905</span><span class="p">,</span><span class="mf">0.885</span><span class="p">]</span>
 
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">y_pos</span><span class="p">,</span> <span class="n">performance</span><span class="p">,</span> <span class="n">align</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">y_pos</span><span class="p">,</span> <span class="n">objects</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Layer Settings&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Accuracy&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;One-layer Settings&#39;</span><span class="p">)</span>
 
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">clf</span><span class="p">()</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>




<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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==
"
>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Different-2-layer-structures">Different 2-layer structures<a class="anchor-link" href="#Different-2-layer-structures">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[19]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Store accuracies</span>
<span class="n">twolayer_acc</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Testing layer_1=100 performance:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Setting 1 – (100) on 200</strong></p>

<pre><code>hidden_layers_structure=[100, 200]</code></pre>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[20]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">twolayer_acc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">deep_belief_net</span><span class="p">(</span><span class="n">hidden_layers_structure</span><span class="o">=</span><span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">200</span><span class="p">])</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[START] Pre-training step:
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 43.735626
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 69.860291
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 52.413918
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 61.957706
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 58.830509
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 55.932449
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 53.635048
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 79.695244
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 65.988762
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 81.554207
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 360.266235
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 431.059296
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 448.964172
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 555.941589
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 502.545776
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 604.778809
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 668.629089
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 631.405762
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 762.210938
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 780.738342
[END] Pre-training step
[START] Fine tuning step:
&gt;&gt; Epoch 0 finished 	ANN training loss 0.710166
&gt;&gt; Epoch 1 finished 	ANN training loss 0.513591
&gt;&gt; Epoch 2 finished 	ANN training loss 0.398980
&gt;&gt; Epoch 3 finished 	ANN training loss 0.339667
&gt;&gt; Epoch 4 finished 	ANN training loss 0.307840
&gt;&gt; Epoch 5 finished 	ANN training loss 0.261378
&gt;&gt; Epoch 6 finished 	ANN training loss 0.264133
&gt;&gt; Epoch 7 finished 	ANN training loss 0.238512
&gt;&gt; Epoch 8 finished 	ANN training loss 0.195845
&gt;&gt; Epoch 9 finished 	ANN training loss 0.176364
&gt;&gt; Epoch 10 finished 	ANN training loss 0.152304
&gt;&gt; Epoch 11 finished 	ANN training loss 0.137684
&gt;&gt; Epoch 12 finished 	ANN training loss 0.128414
&gt;&gt; Epoch 13 finished 	ANN training loss 0.117642
&gt;&gt; Epoch 14 finished 	ANN training loss 0.105372
&gt;&gt; Epoch 15 finished 	ANN training loss 0.103730
&gt;&gt; Epoch 16 finished 	ANN training loss 0.087785
&gt;&gt; Epoch 17 finished 	ANN training loss 0.073750
&gt;&gt; Epoch 18 finished 	ANN training loss 0.066918
&gt;&gt; Epoch 19 finished 	ANN training loss 0.071870
&gt;&gt; Epoch 20 finished 	ANN training loss 0.057518
&gt;&gt; Epoch 21 finished 	ANN training loss 0.051202
&gt;&gt; Epoch 22 finished 	ANN training loss 0.055200
&gt;&gt; Epoch 23 finished 	ANN training loss 0.048699
&gt;&gt; Epoch 24 finished 	ANN training loss 0.040021
&gt;&gt; Epoch 25 finished 	ANN training loss 0.037071
&gt;&gt; Epoch 26 finished 	ANN training loss 0.034395
&gt;&gt; Epoch 27 finished 	ANN training loss 0.030486
&gt;&gt; Epoch 28 finished 	ANN training loss 0.037661
&gt;&gt; Epoch 29 finished 	ANN training loss 0.028354
&gt;&gt; Epoch 30 finished 	ANN training loss 0.028711
&gt;&gt; Epoch 31 finished 	ANN training loss 0.022794
&gt;&gt; Epoch 32 finished 	ANN training loss 0.021876
&gt;&gt; Epoch 33 finished 	ANN training loss 0.022532
&gt;&gt; Epoch 34 finished 	ANN training loss 0.018598
&gt;&gt; Epoch 35 finished 	ANN training loss 0.021609
&gt;&gt; Epoch 36 finished 	ANN training loss 0.021850
&gt;&gt; Epoch 37 finished 	ANN training loss 0.017889
&gt;&gt; Epoch 38 finished 	ANN training loss 0.017366
&gt;&gt; Epoch 39 finished 	ANN training loss 0.018041
&gt;&gt; Epoch 40 finished 	ANN training loss 0.016547
&gt;&gt; Epoch 41 finished 	ANN training loss 0.015665
&gt;&gt; Epoch 42 finished 	ANN training loss 0.014554
&gt;&gt; Epoch 43 finished 	ANN training loss 0.012336
&gt;&gt; Epoch 44 finished 	ANN training loss 0.010441
&gt;&gt; Epoch 45 finished 	ANN training loss 0.010772
&gt;&gt; Epoch 46 finished 	ANN training loss 0.011054
&gt;&gt; Epoch 47 finished 	ANN training loss 0.010722
&gt;&gt; Epoch 48 finished 	ANN training loss 0.010356
&gt;&gt; Epoch 49 finished 	ANN training loss 0.010196
&gt;&gt; Epoch 50 finished 	ANN training loss 0.009778
&gt;&gt; Epoch 51 finished 	ANN training loss 0.011873
&gt;&gt; Epoch 52 finished 	ANN training loss 0.008751
&gt;&gt; Epoch 53 finished 	ANN training loss 0.008284
&gt;&gt; Epoch 54 finished 	ANN training loss 0.007183
&gt;&gt; Epoch 55 finished 	ANN training loss 0.007568
&gt;&gt; Epoch 56 finished 	ANN training loss 0.006002
&gt;&gt; Epoch 57 finished 	ANN training loss 0.005474
&gt;&gt; Epoch 58 finished 	ANN training loss 0.007555
&gt;&gt; Epoch 59 finished 	ANN training loss 0.006181
&gt;&gt; Epoch 60 finished 	ANN training loss 0.004627
&gt;&gt; Epoch 61 finished 	ANN training loss 0.005255
&gt;&gt; Epoch 62 finished 	ANN training loss 0.005082
&gt;&gt; Epoch 63 finished 	ANN training loss 0.004754
&gt;&gt; Epoch 64 finished 	ANN training loss 0.004437
&gt;&gt; Epoch 65 finished 	ANN training loss 0.004095
&gt;&gt; Epoch 66 finished 	ANN training loss 0.003809
&gt;&gt; Epoch 67 finished 	ANN training loss 0.003604
&gt;&gt; Epoch 68 finished 	ANN training loss 0.003691
&gt;&gt; Epoch 69 finished 	ANN training loss 0.003116
&gt;&gt; Epoch 70 finished 	ANN training loss 0.003221
&gt;&gt; Epoch 71 finished 	ANN training loss 0.003310
&gt;&gt; Epoch 72 finished 	ANN training loss 0.003188
&gt;&gt; Epoch 73 finished 	ANN training loss 0.003459
&gt;&gt; Epoch 74 finished 	ANN training loss 0.003284
&gt;&gt; Epoch 75 finished 	ANN training loss 0.002931
&gt;&gt; Epoch 76 finished 	ANN training loss 0.003030
&gt;&gt; Epoch 77 finished 	ANN training loss 0.003313
&gt;&gt; Epoch 78 finished 	ANN training loss 0.002980
&gt;&gt; Epoch 79 finished 	ANN training loss 0.002975
&gt;&gt; Epoch 80 finished 	ANN training loss 0.002494
&gt;&gt; Epoch 81 finished 	ANN training loss 0.002394
&gt;&gt; Epoch 82 finished 	ANN training loss 0.002521
&gt;&gt; Epoch 83 finished 	ANN training loss 0.003384
&gt;&gt; Epoch 84 finished 	ANN training loss 0.002047
&gt;&gt; Epoch 85 finished 	ANN training loss 0.002407
&gt;&gt; Epoch 86 finished 	ANN training loss 0.002197
&gt;&gt; Epoch 87 finished 	ANN training loss 0.001881
&gt;&gt; Epoch 88 finished 	ANN training loss 0.001671
&gt;&gt; Epoch 89 finished 	ANN training loss 0.002061
&gt;&gt; Epoch 90 finished 	ANN training loss 0.001710
&gt;&gt; Epoch 91 finished 	ANN training loss 0.002201
&gt;&gt; Epoch 92 finished 	ANN training loss 0.001971
&gt;&gt; Epoch 93 finished 	ANN training loss 0.002462
&gt;&gt; Epoch 94 finished 	ANN training loss 0.001763
&gt;&gt; Epoch 95 finished 	ANN training loss 0.001852
&gt;&gt; Epoch 96 finished 	ANN training loss 0.001465
&gt;&gt; Epoch 97 finished 	ANN training loss 0.001718
&gt;&gt; Epoch 98 finished 	ANN training loss 0.001340
&gt;&gt; Epoch 99 finished 	ANN training loss 0.001242
[END] Fine tuning step
Done.
Accuracy: 0.905000
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[21]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ACCURACY: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">twolayer_acc</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>ACCURACY: 0.905
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Setting 2 – (100) on 500</strong></p>

<pre><code>hidden_layers_structure=[100, 500]</code></pre>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[22]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">twolayer_acc</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">deep_belief_net</span><span class="p">(</span><span class="n">hidden_layers_structure</span><span class="o">=</span><span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">500</span><span class="p">])</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[START] Pre-training step:
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 77.752686
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 52.723736
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 53.163139
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 52.002865
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 67.212433
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 57.364613
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 70.963165
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 53.970383
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 76.276886
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 66.696815
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 142.798584
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 265.637207
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 189.706680
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 182.475662
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 246.715027
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 322.739990
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 283.961273
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 416.864136
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 503.759216
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 375.786560
[END] Pre-training step
[START] Fine tuning step:
&gt;&gt; Epoch 0 finished 	ANN training loss 0.725721
&gt;&gt; Epoch 1 finished 	ANN training loss 0.555961
&gt;&gt; Epoch 2 finished 	ANN training loss 0.455945
&gt;&gt; Epoch 3 finished 	ANN training loss 0.362099
&gt;&gt; Epoch 4 finished 	ANN training loss 0.304426
&gt;&gt; Epoch 5 finished 	ANN training loss 0.287528
&gt;&gt; Epoch 6 finished 	ANN training loss 0.266063
&gt;&gt; Epoch 7 finished 	ANN training loss 0.209929
&gt;&gt; Epoch 8 finished 	ANN training loss 0.197395
&gt;&gt; Epoch 9 finished 	ANN training loss 0.192417
&gt;&gt; Epoch 10 finished 	ANN training loss 0.157767
&gt;&gt; Epoch 11 finished 	ANN training loss 0.152469
&gt;&gt; Epoch 12 finished 	ANN training loss 0.123194
&gt;&gt; Epoch 13 finished 	ANN training loss 0.117787
&gt;&gt; Epoch 14 finished 	ANN training loss 0.103535
&gt;&gt; Epoch 15 finished 	ANN training loss 0.099334
&gt;&gt; Epoch 16 finished 	ANN training loss 0.088828
&gt;&gt; Epoch 17 finished 	ANN training loss 0.084695
&gt;&gt; Epoch 18 finished 	ANN training loss 0.070887
&gt;&gt; Epoch 19 finished 	ANN training loss 0.071093
&gt;&gt; Epoch 20 finished 	ANN training loss 0.066874
&gt;&gt; Epoch 21 finished 	ANN training loss 0.058676
&gt;&gt; Epoch 22 finished 	ANN training loss 0.047950
&gt;&gt; Epoch 23 finished 	ANN training loss 0.040768
&gt;&gt; Epoch 24 finished 	ANN training loss 0.041859
&gt;&gt; Epoch 25 finished 	ANN training loss 0.038428
&gt;&gt; Epoch 26 finished 	ANN training loss 0.034559
&gt;&gt; Epoch 27 finished 	ANN training loss 0.034939
&gt;&gt; Epoch 28 finished 	ANN training loss 0.034862
&gt;&gt; Epoch 29 finished 	ANN training loss 0.031442
&gt;&gt; Epoch 30 finished 	ANN training loss 0.026234
&gt;&gt; Epoch 31 finished 	ANN training loss 0.024811
&gt;&gt; Epoch 32 finished 	ANN training loss 0.024139
&gt;&gt; Epoch 33 finished 	ANN training loss 0.025639
&gt;&gt; Epoch 34 finished 	ANN training loss 0.018450
&gt;&gt; Epoch 35 finished 	ANN training loss 0.019408
&gt;&gt; Epoch 36 finished 	ANN training loss 0.015877
&gt;&gt; Epoch 37 finished 	ANN training loss 0.014950
&gt;&gt; Epoch 38 finished 	ANN training loss 0.015253
&gt;&gt; Epoch 39 finished 	ANN training loss 0.014468
&gt;&gt; Epoch 40 finished 	ANN training loss 0.028219
&gt;&gt; Epoch 41 finished 	ANN training loss 0.012077
&gt;&gt; Epoch 42 finished 	ANN training loss 0.012430
&gt;&gt; Epoch 43 finished 	ANN training loss 0.011440
&gt;&gt; Epoch 44 finished 	ANN training loss 0.009415
&gt;&gt; Epoch 45 finished 	ANN training loss 0.010727
&gt;&gt; Epoch 46 finished 	ANN training loss 0.009735
&gt;&gt; Epoch 47 finished 	ANN training loss 0.009666
&gt;&gt; Epoch 48 finished 	ANN training loss 0.008429
&gt;&gt; Epoch 49 finished 	ANN training loss 0.011000
&gt;&gt; Epoch 50 finished 	ANN training loss 0.007966
&gt;&gt; Epoch 51 finished 	ANN training loss 0.009881
&gt;&gt; Epoch 52 finished 	ANN training loss 0.008241
&gt;&gt; Epoch 53 finished 	ANN training loss 0.009118
&gt;&gt; Epoch 54 finished 	ANN training loss 0.007182
&gt;&gt; Epoch 55 finished 	ANN training loss 0.005784
&gt;&gt; Epoch 56 finished 	ANN training loss 0.005853
&gt;&gt; Epoch 57 finished 	ANN training loss 0.005524
&gt;&gt; Epoch 58 finished 	ANN training loss 0.005711
&gt;&gt; Epoch 59 finished 	ANN training loss 0.005407
&gt;&gt; Epoch 60 finished 	ANN training loss 0.005191
&gt;&gt; Epoch 61 finished 	ANN training loss 0.004779
&gt;&gt; Epoch 62 finished 	ANN training loss 0.004540
&gt;&gt; Epoch 63 finished 	ANN training loss 0.004728
&gt;&gt; Epoch 64 finished 	ANN training loss 0.003845
&gt;&gt; Epoch 65 finished 	ANN training loss 0.004285
&gt;&gt; Epoch 66 finished 	ANN training loss 0.004571
&gt;&gt; Epoch 67 finished 	ANN training loss 0.004412
&gt;&gt; Epoch 68 finished 	ANN training loss 0.003766
&gt;&gt; Epoch 69 finished 	ANN training loss 0.003805
&gt;&gt; Epoch 70 finished 	ANN training loss 0.003945
&gt;&gt; Epoch 71 finished 	ANN training loss 0.003950
&gt;&gt; Epoch 72 finished 	ANN training loss 0.003225
&gt;&gt; Epoch 73 finished 	ANN training loss 0.003514
&gt;&gt; Epoch 74 finished 	ANN training loss 0.003312
&gt;&gt; Epoch 75 finished 	ANN training loss 0.003424
&gt;&gt; Epoch 76 finished 	ANN training loss 0.002760
&gt;&gt; Epoch 77 finished 	ANN training loss 0.002403
&gt;&gt; Epoch 78 finished 	ANN training loss 0.003159
&gt;&gt; Epoch 79 finished 	ANN training loss 0.002689
&gt;&gt; Epoch 80 finished 	ANN training loss 0.001958
&gt;&gt; Epoch 81 finished 	ANN training loss 0.002782
&gt;&gt; Epoch 82 finished 	ANN training loss 0.002177
&gt;&gt; Epoch 83 finished 	ANN training loss 0.002044
&gt;&gt; Epoch 84 finished 	ANN training loss 0.002169
&gt;&gt; Epoch 85 finished 	ANN training loss 0.001879
&gt;&gt; Epoch 86 finished 	ANN training loss 0.002688
&gt;&gt; Epoch 87 finished 	ANN training loss 0.003280
&gt;&gt; Epoch 88 finished 	ANN training loss 0.002178
&gt;&gt; Epoch 89 finished 	ANN training loss 0.002332
&gt;&gt; Epoch 90 finished 	ANN training loss 0.002143
&gt;&gt; Epoch 91 finished 	ANN training loss 0.001624
&gt;&gt; Epoch 92 finished 	ANN training loss 0.001732
&gt;&gt; Epoch 93 finished 	ANN training loss 0.001514
&gt;&gt; Epoch 94 finished 	ANN training loss 0.001734
&gt;&gt; Epoch 95 finished 	ANN training loss 0.001677
&gt;&gt; Epoch 96 finished 	ANN training loss 0.001513
&gt;&gt; Epoch 97 finished 	ANN training loss 0.001382
&gt;&gt; Epoch 98 finished 	ANN training loss 0.001374
&gt;&gt; Epoch 99 finished 	ANN training loss 0.001510
[END] Fine tuning step
Done.
Accuracy: 0.920000
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[23]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ACCURACY: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">twolayer_acc</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>ACCURACY: 0.92
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Testing layer_1=500 performance:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Setting 3 – (500) on 200</strong></p>

<pre><code>hidden_layers_structure=[500, 200]</code></pre>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[24]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">twolayer_acc</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">deep_belief_net</span><span class="p">(</span><span class="n">hidden_layers_structure</span><span class="o">=</span><span class="p">[</span><span class="mi">500</span><span class="p">,</span> <span class="mi">200</span><span class="p">])</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[START] Pre-training step:
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 89.590607
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 37.799076
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 57.134327
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 54.106468
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 41.545689
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 28.666601
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 27.435949
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 40.246414
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 21.926275
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 33.239376
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 98.059181
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 174.640564
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 166.056488
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 173.509491
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 190.788391
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 228.330200
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 243.219604
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 243.815216
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 288.703979
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 325.087891
[END] Pre-training step
[START] Fine tuning step:
&gt;&gt; Epoch 0 finished 	ANN training loss 0.739812
&gt;&gt; Epoch 1 finished 	ANN training loss 0.454461
&gt;&gt; Epoch 2 finished 	ANN training loss 0.383898
&gt;&gt; Epoch 3 finished 	ANN training loss 0.338644
&gt;&gt; Epoch 4 finished 	ANN training loss 0.304319
&gt;&gt; Epoch 5 finished 	ANN training loss 0.244198
&gt;&gt; Epoch 6 finished 	ANN training loss 0.224944
&gt;&gt; Epoch 7 finished 	ANN training loss 0.199082
&gt;&gt; Epoch 8 finished 	ANN training loss 0.182059
&gt;&gt; Epoch 9 finished 	ANN training loss 0.151635
&gt;&gt; Epoch 10 finished 	ANN training loss 0.144171
&gt;&gt; Epoch 11 finished 	ANN training loss 0.115367
&gt;&gt; Epoch 12 finished 	ANN training loss 0.117670
&gt;&gt; Epoch 13 finished 	ANN training loss 0.121261
&gt;&gt; Epoch 14 finished 	ANN training loss 0.085923
&gt;&gt; Epoch 15 finished 	ANN training loss 0.075368
&gt;&gt; Epoch 16 finished 	ANN training loss 0.071203
&gt;&gt; Epoch 17 finished 	ANN training loss 0.061213
&gt;&gt; Epoch 18 finished 	ANN training loss 0.053001
&gt;&gt; Epoch 19 finished 	ANN training loss 0.048034
&gt;&gt; Epoch 20 finished 	ANN training loss 0.043710
&gt;&gt; Epoch 21 finished 	ANN training loss 0.039093
&gt;&gt; Epoch 22 finished 	ANN training loss 0.041623
&gt;&gt; Epoch 23 finished 	ANN training loss 0.031440
&gt;&gt; Epoch 24 finished 	ANN training loss 0.029539
&gt;&gt; Epoch 25 finished 	ANN training loss 0.027321
&gt;&gt; Epoch 26 finished 	ANN training loss 0.024297
&gt;&gt; Epoch 27 finished 	ANN training loss 0.021936
&gt;&gt; Epoch 28 finished 	ANN training loss 0.021124
&gt;&gt; Epoch 29 finished 	ANN training loss 0.024465
&gt;&gt; Epoch 30 finished 	ANN training loss 0.018379
&gt;&gt; Epoch 31 finished 	ANN training loss 0.016287
&gt;&gt; Epoch 32 finished 	ANN training loss 0.014820
&gt;&gt; Epoch 33 finished 	ANN training loss 0.015014
&gt;&gt; Epoch 34 finished 	ANN training loss 0.012345
&gt;&gt; Epoch 35 finished 	ANN training loss 0.012518
&gt;&gt; Epoch 36 finished 	ANN training loss 0.012737
&gt;&gt; Epoch 37 finished 	ANN training loss 0.011847
&gt;&gt; Epoch 38 finished 	ANN training loss 0.010565
&gt;&gt; Epoch 39 finished 	ANN training loss 0.009242
&gt;&gt; Epoch 40 finished 	ANN training loss 0.009036
&gt;&gt; Epoch 41 finished 	ANN training loss 0.009443
&gt;&gt; Epoch 42 finished 	ANN training loss 0.008202
&gt;&gt; Epoch 43 finished 	ANN training loss 0.011443
&gt;&gt; Epoch 44 finished 	ANN training loss 0.006845
&gt;&gt; Epoch 45 finished 	ANN training loss 0.006734
&gt;&gt; Epoch 46 finished 	ANN training loss 0.005756
&gt;&gt; Epoch 47 finished 	ANN training loss 0.005248
&gt;&gt; Epoch 48 finished 	ANN training loss 0.006864
&gt;&gt; Epoch 49 finished 	ANN training loss 0.005188
&gt;&gt; Epoch 50 finished 	ANN training loss 0.005877
&gt;&gt; Epoch 51 finished 	ANN training loss 0.004740
&gt;&gt; Epoch 52 finished 	ANN training loss 0.004359
&gt;&gt; Epoch 53 finished 	ANN training loss 0.004120
&gt;&gt; Epoch 54 finished 	ANN training loss 0.004402
&gt;&gt; Epoch 55 finished 	ANN training loss 0.004480
&gt;&gt; Epoch 56 finished 	ANN training loss 0.003810
&gt;&gt; Epoch 57 finished 	ANN training loss 0.003917
&gt;&gt; Epoch 58 finished 	ANN training loss 0.003295
&gt;&gt; Epoch 59 finished 	ANN training loss 0.002996
&gt;&gt; Epoch 60 finished 	ANN training loss 0.002722
&gt;&gt; Epoch 61 finished 	ANN training loss 0.002538
&gt;&gt; Epoch 62 finished 	ANN training loss 0.002431
&gt;&gt; Epoch 63 finished 	ANN training loss 0.002842
&gt;&gt; Epoch 64 finished 	ANN training loss 0.002769
&gt;&gt; Epoch 65 finished 	ANN training loss 0.002154
&gt;&gt; Epoch 66 finished 	ANN training loss 0.002520
&gt;&gt; Epoch 67 finished 	ANN training loss 0.001927
&gt;&gt; Epoch 68 finished 	ANN training loss 0.001995
&gt;&gt; Epoch 69 finished 	ANN training loss 0.002096
&gt;&gt; Epoch 70 finished 	ANN training loss 0.001781
&gt;&gt; Epoch 71 finished 	ANN training loss 0.001815
&gt;&gt; Epoch 72 finished 	ANN training loss 0.001487
&gt;&gt; Epoch 73 finished 	ANN training loss 0.001612
&gt;&gt; Epoch 74 finished 	ANN training loss 0.001554
&gt;&gt; Epoch 75 finished 	ANN training loss 0.001294
&gt;&gt; Epoch 76 finished 	ANN training loss 0.001397
&gt;&gt; Epoch 77 finished 	ANN training loss 0.002263
&gt;&gt; Epoch 78 finished 	ANN training loss 0.001267
&gt;&gt; Epoch 79 finished 	ANN training loss 0.001496
&gt;&gt; Epoch 80 finished 	ANN training loss 0.001183
&gt;&gt; Epoch 81 finished 	ANN training loss 0.001197
&gt;&gt; Epoch 82 finished 	ANN training loss 0.001516
&gt;&gt; Epoch 83 finished 	ANN training loss 0.001149
&gt;&gt; Epoch 84 finished 	ANN training loss 0.001015
&gt;&gt; Epoch 85 finished 	ANN training loss 0.001055
&gt;&gt; Epoch 86 finished 	ANN training loss 0.000982
&gt;&gt; Epoch 87 finished 	ANN training loss 0.000952
&gt;&gt; Epoch 88 finished 	ANN training loss 0.001089
&gt;&gt; Epoch 89 finished 	ANN training loss 0.001359
&gt;&gt; Epoch 90 finished 	ANN training loss 0.000992
&gt;&gt; Epoch 91 finished 	ANN training loss 0.001130
&gt;&gt; Epoch 92 finished 	ANN training loss 0.000978
&gt;&gt; Epoch 93 finished 	ANN training loss 0.000927
&gt;&gt; Epoch 94 finished 	ANN training loss 0.001100
&gt;&gt; Epoch 95 finished 	ANN training loss 0.000900
&gt;&gt; Epoch 96 finished 	ANN training loss 0.001113
&gt;&gt; Epoch 97 finished 	ANN training loss 0.000988
&gt;&gt; Epoch 98 finished 	ANN training loss 0.001640
&gt;&gt; Epoch 99 finished 	ANN training loss 0.001042
[END] Fine tuning step
Done.
Accuracy: 0.890000
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[25]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ACCURACY: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">twolayer_acc</span><span class="p">[</span><span class="mi">2</span><span class="p">]))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>ACCURACY: 0.89
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Setting 4 – (500) on 500</strong></p>

<pre><code>hidden_layers_structure=[500, 500]</code></pre>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[26]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">twolayer_acc</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">=</span> <span class="n">deep_belief_net</span><span class="p">(</span><span class="n">hidden_layers_structure</span><span class="o">=</span><span class="p">[</span><span class="mi">500</span><span class="p">,</span> <span class="mi">500</span><span class="p">])</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[START] Pre-training step:
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 46.529064
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 66.733582
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 51.008633
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 34.017078
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 30.529028
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 27.276186
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 28.727407
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 43.306202
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 33.214878
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 35.673023
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 113.897949
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 122.613113
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 76.815407
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 100.304230
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 176.164490
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 117.903763
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 118.125267
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 63.958897
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 135.292709
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 98.156700
[END] Pre-training step
[START] Fine tuning step:
&gt;&gt; Epoch 0 finished 	ANN training loss 0.678442
&gt;&gt; Epoch 1 finished 	ANN training loss 0.424808
&gt;&gt; Epoch 2 finished 	ANN training loss 0.375037
&gt;&gt; Epoch 3 finished 	ANN training loss 0.296220
&gt;&gt; Epoch 4 finished 	ANN training loss 0.246093
&gt;&gt; Epoch 5 finished 	ANN training loss 0.224821
&gt;&gt; Epoch 6 finished 	ANN training loss 0.198668
&gt;&gt; Epoch 7 finished 	ANN training loss 0.177970
&gt;&gt; Epoch 8 finished 	ANN training loss 0.159623
&gt;&gt; Epoch 9 finished 	ANN training loss 0.133888
&gt;&gt; Epoch 10 finished 	ANN training loss 0.128539
&gt;&gt; Epoch 11 finished 	ANN training loss 0.107228
&gt;&gt; Epoch 12 finished 	ANN training loss 0.100005
&gt;&gt; Epoch 13 finished 	ANN training loss 0.088832
&gt;&gt; Epoch 14 finished 	ANN training loss 0.082100
&gt;&gt; Epoch 15 finished 	ANN training loss 0.067345
&gt;&gt; Epoch 16 finished 	ANN training loss 0.059807
&gt;&gt; Epoch 17 finished 	ANN training loss 0.052954
&gt;&gt; Epoch 18 finished 	ANN training loss 0.047900
&gt;&gt; Epoch 19 finished 	ANN training loss 0.045992
&gt;&gt; Epoch 20 finished 	ANN training loss 0.039242
&gt;&gt; Epoch 21 finished 	ANN training loss 0.038365
&gt;&gt; Epoch 22 finished 	ANN training loss 0.032795
&gt;&gt; Epoch 23 finished 	ANN training loss 0.029510
&gt;&gt; Epoch 24 finished 	ANN training loss 0.026360
&gt;&gt; Epoch 25 finished 	ANN training loss 0.026042
&gt;&gt; Epoch 26 finished 	ANN training loss 0.027286
&gt;&gt; Epoch 27 finished 	ANN training loss 0.020565
&gt;&gt; Epoch 28 finished 	ANN training loss 0.019415
&gt;&gt; Epoch 29 finished 	ANN training loss 0.016214
&gt;&gt; Epoch 30 finished 	ANN training loss 0.016160
&gt;&gt; Epoch 31 finished 	ANN training loss 0.017187
&gt;&gt; Epoch 32 finished 	ANN training loss 0.014296
&gt;&gt; Epoch 33 finished 	ANN training loss 0.014186
&gt;&gt; Epoch 34 finished 	ANN training loss 0.011290
&gt;&gt; Epoch 35 finished 	ANN training loss 0.010938
&gt;&gt; Epoch 36 finished 	ANN training loss 0.011059
&gt;&gt; Epoch 37 finished 	ANN training loss 0.009930
&gt;&gt; Epoch 38 finished 	ANN training loss 0.010398
&gt;&gt; Epoch 39 finished 	ANN training loss 0.008298
&gt;&gt; Epoch 40 finished 	ANN training loss 0.007541
&gt;&gt; Epoch 41 finished 	ANN training loss 0.007224
&gt;&gt; Epoch 42 finished 	ANN training loss 0.006695
&gt;&gt; Epoch 43 finished 	ANN training loss 0.008653
&gt;&gt; Epoch 44 finished 	ANN training loss 0.006183
&gt;&gt; Epoch 45 finished 	ANN training loss 0.006785
&gt;&gt; Epoch 46 finished 	ANN training loss 0.005258
&gt;&gt; Epoch 47 finished 	ANN training loss 0.005046
&gt;&gt; Epoch 48 finished 	ANN training loss 0.005256
&gt;&gt; Epoch 49 finished 	ANN training loss 0.005673
&gt;&gt; Epoch 50 finished 	ANN training loss 0.004570
&gt;&gt; Epoch 51 finished 	ANN training loss 0.004244
&gt;&gt; Epoch 52 finished 	ANN training loss 0.004006
&gt;&gt; Epoch 53 finished 	ANN training loss 0.003882
&gt;&gt; Epoch 54 finished 	ANN training loss 0.003236
&gt;&gt; Epoch 55 finished 	ANN training loss 0.003414
&gt;&gt; Epoch 56 finished 	ANN training loss 0.003563
&gt;&gt; Epoch 57 finished 	ANN training loss 0.003114
&gt;&gt; Epoch 58 finished 	ANN training loss 0.003044
&gt;&gt; Epoch 59 finished 	ANN training loss 0.003371
&gt;&gt; Epoch 60 finished 	ANN training loss 0.002759
&gt;&gt; Epoch 61 finished 	ANN training loss 0.003110
&gt;&gt; Epoch 62 finished 	ANN training loss 0.002729
&gt;&gt; Epoch 63 finished 	ANN training loss 0.002817
&gt;&gt; Epoch 64 finished 	ANN training loss 0.002660
&gt;&gt; Epoch 65 finished 	ANN training loss 0.002363
&gt;&gt; Epoch 66 finished 	ANN training loss 0.002338
&gt;&gt; Epoch 67 finished 	ANN training loss 0.002152
&gt;&gt; Epoch 68 finished 	ANN training loss 0.002214
&gt;&gt; Epoch 69 finished 	ANN training loss 0.002184
&gt;&gt; Epoch 70 finished 	ANN training loss 0.002136
&gt;&gt; Epoch 71 finished 	ANN training loss 0.002225
&gt;&gt; Epoch 72 finished 	ANN training loss 0.002041
&gt;&gt; Epoch 73 finished 	ANN training loss 0.001975
&gt;&gt; Epoch 74 finished 	ANN training loss 0.002181
&gt;&gt; Epoch 75 finished 	ANN training loss 0.001712
&gt;&gt; Epoch 76 finished 	ANN training loss 0.001743
&gt;&gt; Epoch 77 finished 	ANN training loss 0.001605
&gt;&gt; Epoch 78 finished 	ANN training loss 0.001575
&gt;&gt; Epoch 79 finished 	ANN training loss 0.001453
&gt;&gt; Epoch 80 finished 	ANN training loss 0.001423
&gt;&gt; Epoch 81 finished 	ANN training loss 0.001296
&gt;&gt; Epoch 82 finished 	ANN training loss 0.001214
&gt;&gt; Epoch 83 finished 	ANN training loss 0.001397
&gt;&gt; Epoch 84 finished 	ANN training loss 0.001072
&gt;&gt; Epoch 85 finished 	ANN training loss 0.001148
&gt;&gt; Epoch 86 finished 	ANN training loss 0.001103
&gt;&gt; Epoch 87 finished 	ANN training loss 0.001128
&gt;&gt; Epoch 88 finished 	ANN training loss 0.000970
&gt;&gt; Epoch 89 finished 	ANN training loss 0.000958
&gt;&gt; Epoch 90 finished 	ANN training loss 0.001050
&gt;&gt; Epoch 91 finished 	ANN training loss 0.000907
&gt;&gt; Epoch 92 finished 	ANN training loss 0.001002
&gt;&gt; Epoch 93 finished 	ANN training loss 0.000934
&gt;&gt; Epoch 94 finished 	ANN training loss 0.000908
&gt;&gt; Epoch 95 finished 	ANN training loss 0.000919
&gt;&gt; Epoch 96 finished 	ANN training loss 0.000895
&gt;&gt; Epoch 97 finished 	ANN training loss 0.001143
&gt;&gt; Epoch 98 finished 	ANN training loss 0.001023
&gt;&gt; Epoch 99 finished 	ANN training loss 0.000763
[END] Fine tuning step
Done.
Accuracy: 0.890000
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[27]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ACCURACY: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">twolayer_acc</span><span class="p">[</span><span class="mi">3</span><span class="p">]))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>ACCURACY: 0.89
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[32]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Collated results</span>
<span class="nb">print</span><span class="p">(</span><span class="n">twolayer_acc</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Most accurate 2-layer setting is Setting &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">twolayer_acc</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="n">twolayer_acc</span><span class="p">))</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[0.90500000000000003, 0.92000000000000004, 0.89000000000000001, 0.89000000000000001, 0.90500000000000003, 0.89000000000000001, 0.92000000000000004, 0.89000000000000001]
Most accurate 2-layer setting is Setting 2
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[47]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Create bar graph</span>
<span class="n">plt</span><span class="o">.</span><span class="n">clf</span><span class="p">()</span>
<span class="n">objects</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;[100, 200]&#39;</span><span class="p">,</span> <span class="s1">&#39;[100, 500]&#39;</span><span class="p">,</span> <span class="s1">&#39;[500, 200]&#39;</span><span class="p">,</span> <span class="s1">&#39;[500, 500]&#39;</span><span class="p">)</span>
<span class="n">y_pos</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">objects</span><span class="p">))</span>
<span class="n">performance</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.905</span><span class="p">,</span><span class="mf">0.92</span><span class="p">,</span><span class="mf">0.89</span><span class="p">,</span><span class="mf">0.89</span><span class="p">]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">y_pos</span><span class="p">,</span> <span class="n">performance</span><span class="p">,</span> <span class="n">align</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">y_pos</span><span class="p">,</span> <span class="n">objects</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Layer Settings&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Accuracy&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Two-layer Settings&#39;</span><span class="p">)</span>
 
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">clf</span><span class="p">()</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>




<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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"
>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Different-3-layer-structures">Different 3-layer structures<a class="anchor-link" href="#Different-3-layer-structures">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[33]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Store accuracies</span>
<span class="n">threelayer_acc</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Setting 1 – Baseline</strong></p>

<pre><code>hidden_layers_structure=[100, 200, 300]</code></pre>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[34]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">threelayer_acc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">deep_belief_net</span><span class="p">(</span><span class="n">hidden_layers_structure</span><span class="o">=</span><span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">300</span><span class="p">])</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[START] Pre-training step:
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 48.776649
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 77.958260
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 96.530106
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 82.910736
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 116.125664
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 90.105782
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 66.176308
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 96.009384
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 87.633675
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 70.791771
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 247.282806
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 292.829712
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 302.180573
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 377.232025
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 518.424988
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 440.123657
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 448.164154
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 539.442139
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 556.956604
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 612.908630
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 18618.408203
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 17771.822266
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 16998.207031
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 21979.873047
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 27382.714844
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 39169.394531
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 44458.195312
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 41281.765625
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 48748.398438
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 37989.843750
[END] Pre-training step
[START] Fine tuning step:
&gt;&gt; Epoch 0 finished 	ANN training loss 1.048735
&gt;&gt; Epoch 1 finished 	ANN training loss 0.718359
&gt;&gt; Epoch 2 finished 	ANN training loss 0.557308
&gt;&gt; Epoch 3 finished 	ANN training loss 0.523559
&gt;&gt; Epoch 4 finished 	ANN training loss 0.390225
&gt;&gt; Epoch 5 finished 	ANN training loss 0.318191
&gt;&gt; Epoch 6 finished 	ANN training loss 0.276111
&gt;&gt; Epoch 7 finished 	ANN training loss 0.239038
&gt;&gt; Epoch 8 finished 	ANN training loss 0.222058
&gt;&gt; Epoch 9 finished 	ANN training loss 0.190678
&gt;&gt; Epoch 10 finished 	ANN training loss 0.147539
&gt;&gt; Epoch 11 finished 	ANN training loss 0.140231
&gt;&gt; Epoch 12 finished 	ANN training loss 0.120642
&gt;&gt; Epoch 13 finished 	ANN training loss 0.108311
&gt;&gt; Epoch 14 finished 	ANN training loss 0.081935
&gt;&gt; Epoch 15 finished 	ANN training loss 0.076665
&gt;&gt; Epoch 16 finished 	ANN training loss 0.084998
&gt;&gt; Epoch 17 finished 	ANN training loss 0.067256
&gt;&gt; Epoch 18 finished 	ANN training loss 0.057310
&gt;&gt; Epoch 19 finished 	ANN training loss 0.047843
&gt;&gt; Epoch 20 finished 	ANN training loss 0.053459
&gt;&gt; Epoch 21 finished 	ANN training loss 0.044084
&gt;&gt; Epoch 22 finished 	ANN training loss 0.053099
&gt;&gt; Epoch 23 finished 	ANN training loss 0.035367
&gt;&gt; Epoch 24 finished 	ANN training loss 0.033165
&gt;&gt; Epoch 25 finished 	ANN training loss 0.037883
&gt;&gt; Epoch 26 finished 	ANN training loss 0.029208
&gt;&gt; Epoch 27 finished 	ANN training loss 0.047251
&gt;&gt; Epoch 28 finished 	ANN training loss 0.024188
&gt;&gt; Epoch 29 finished 	ANN training loss 0.026445
&gt;&gt; Epoch 30 finished 	ANN training loss 0.019255
&gt;&gt; Epoch 31 finished 	ANN training loss 0.024286
&gt;&gt; Epoch 32 finished 	ANN training loss 0.019467
&gt;&gt; Epoch 33 finished 	ANN training loss 0.017768
&gt;&gt; Epoch 34 finished 	ANN training loss 0.022086
&gt;&gt; Epoch 35 finished 	ANN training loss 0.016425
&gt;&gt; Epoch 36 finished 	ANN training loss 0.017146
&gt;&gt; Epoch 37 finished 	ANN training loss 0.020152
&gt;&gt; Epoch 38 finished 	ANN training loss 0.011766
&gt;&gt; Epoch 39 finished 	ANN training loss 0.009566
&gt;&gt; Epoch 40 finished 	ANN training loss 0.015626
&gt;&gt; Epoch 41 finished 	ANN training loss 0.010250
&gt;&gt; Epoch 42 finished 	ANN training loss 0.011436
&gt;&gt; Epoch 43 finished 	ANN training loss 0.012583
&gt;&gt; Epoch 44 finished 	ANN training loss 0.026032
&gt;&gt; Epoch 45 finished 	ANN training loss 0.012718
&gt;&gt; Epoch 46 finished 	ANN training loss 0.008813
&gt;&gt; Epoch 47 finished 	ANN training loss 0.012629
&gt;&gt; Epoch 48 finished 	ANN training loss 0.007918
&gt;&gt; Epoch 49 finished 	ANN training loss 0.008212
&gt;&gt; Epoch 50 finished 	ANN training loss 0.011280
&gt;&gt; Epoch 51 finished 	ANN training loss 0.010441
&gt;&gt; Epoch 52 finished 	ANN training loss 0.005996
&gt;&gt; Epoch 53 finished 	ANN training loss 0.005710
&gt;&gt; Epoch 54 finished 	ANN training loss 0.006742
&gt;&gt; Epoch 55 finished 	ANN training loss 0.006059
&gt;&gt; Epoch 56 finished 	ANN training loss 0.006146
&gt;&gt; Epoch 57 finished 	ANN training loss 0.004724
&gt;&gt; Epoch 58 finished 	ANN training loss 0.006122
&gt;&gt; Epoch 59 finished 	ANN training loss 0.004437
&gt;&gt; Epoch 60 finished 	ANN training loss 0.003719
&gt;&gt; Epoch 61 finished 	ANN training loss 0.003949
&gt;&gt; Epoch 62 finished 	ANN training loss 0.003071
&gt;&gt; Epoch 63 finished 	ANN training loss 0.003466
&gt;&gt; Epoch 64 finished 	ANN training loss 0.003300
&gt;&gt; Epoch 65 finished 	ANN training loss 0.004619
&gt;&gt; Epoch 66 finished 	ANN training loss 0.005675
&gt;&gt; Epoch 67 finished 	ANN training loss 0.003084
&gt;&gt; Epoch 68 finished 	ANN training loss 0.002994
&gt;&gt; Epoch 69 finished 	ANN training loss 0.003212
&gt;&gt; Epoch 70 finished 	ANN training loss 0.007424
&gt;&gt; Epoch 71 finished 	ANN training loss 0.011309
&gt;&gt; Epoch 72 finished 	ANN training loss 0.003106
&gt;&gt; Epoch 73 finished 	ANN training loss 0.003080
&gt;&gt; Epoch 74 finished 	ANN training loss 0.002585
&gt;&gt; Epoch 75 finished 	ANN training loss 0.002552
&gt;&gt; Epoch 76 finished 	ANN training loss 0.002590
&gt;&gt; Epoch 77 finished 	ANN training loss 0.004292
&gt;&gt; Epoch 78 finished 	ANN training loss 0.002162
&gt;&gt; Epoch 79 finished 	ANN training loss 0.003070
&gt;&gt; Epoch 80 finished 	ANN training loss 0.001826
&gt;&gt; Epoch 81 finished 	ANN training loss 0.001511
&gt;&gt; Epoch 82 finished 	ANN training loss 0.002201
&gt;&gt; Epoch 83 finished 	ANN training loss 0.002240
&gt;&gt; Epoch 84 finished 	ANN training loss 0.002084
&gt;&gt; Epoch 85 finished 	ANN training loss 0.001534
&gt;&gt; Epoch 86 finished 	ANN training loss 0.001456
&gt;&gt; Epoch 87 finished 	ANN training loss 0.001200
&gt;&gt; Epoch 88 finished 	ANN training loss 0.000910
&gt;&gt; Epoch 89 finished 	ANN training loss 0.001506
&gt;&gt; Epoch 90 finished 	ANN training loss 0.001334
&gt;&gt; Epoch 91 finished 	ANN training loss 0.001055
&gt;&gt; Epoch 92 finished 	ANN training loss 0.000998
&gt;&gt; Epoch 93 finished 	ANN training loss 0.001161
&gt;&gt; Epoch 94 finished 	ANN training loss 0.001203
&gt;&gt; Epoch 95 finished 	ANN training loss 0.001659
&gt;&gt; Epoch 96 finished 	ANN training loss 0.000997
&gt;&gt; Epoch 97 finished 	ANN training loss 0.001004
&gt;&gt; Epoch 98 finished 	ANN training loss 0.001538
&gt;&gt; Epoch 99 finished 	ANN training loss 0.000857
[END] Fine tuning step
Done.
Accuracy: 0.900000
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[35]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ACCURACY: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">threelayer_acc</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>ACCURACY: 0.9
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Setting 2 – 1st Layer = 500</strong></p>

<pre><code>hidden_layers_structure=[500, 200, 300]</code></pre>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[36]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">threelayer_acc</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">deep_belief_net</span><span class="p">(</span><span class="n">hidden_layers_structure</span><span class="o">=</span><span class="p">[</span><span class="mi">500</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">300</span><span class="p">])</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[START] Pre-training step:
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 57.344833
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 65.222092
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 36.743771
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 38.844582
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 44.889423
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 23.421108
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 30.931402
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 40.723019
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 30.118111
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 40.697941
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 281.745117
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 238.984146
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 232.955353
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 247.143829
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 279.849365
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 174.890121
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 195.232620
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 218.132187
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 241.617111
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 261.642883
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 2737.657471
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 3818.996338
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 5488.172363
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 5364.857422
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 7427.385742
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 8336.666992
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 10444.168945
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 11059.750977
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 9636.936523
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 11250.191406
[END] Pre-training step
[START] Fine tuning step:
&gt;&gt; Epoch 0 finished 	ANN training loss 0.703324
&gt;&gt; Epoch 1 finished 	ANN training loss 0.478931
&gt;&gt; Epoch 2 finished 	ANN training loss 0.396747
&gt;&gt; Epoch 3 finished 	ANN training loss 0.310771
&gt;&gt; Epoch 4 finished 	ANN training loss 0.271994
&gt;&gt; Epoch 5 finished 	ANN training loss 0.212138
&gt;&gt; Epoch 6 finished 	ANN training loss 0.169806
&gt;&gt; Epoch 7 finished 	ANN training loss 0.140385
&gt;&gt; Epoch 8 finished 	ANN training loss 0.138881
&gt;&gt; Epoch 9 finished 	ANN training loss 0.128901
&gt;&gt; Epoch 10 finished 	ANN training loss 0.088348
&gt;&gt; Epoch 11 finished 	ANN training loss 0.080008
&gt;&gt; Epoch 12 finished 	ANN training loss 0.069058
&gt;&gt; Epoch 13 finished 	ANN training loss 0.053698
&gt;&gt; Epoch 14 finished 	ANN training loss 0.059593
&gt;&gt; Epoch 15 finished 	ANN training loss 0.044812
&gt;&gt; Epoch 16 finished 	ANN training loss 0.033827
&gt;&gt; Epoch 17 finished 	ANN training loss 0.029094
&gt;&gt; Epoch 18 finished 	ANN training loss 0.033973
&gt;&gt; Epoch 19 finished 	ANN training loss 0.026899
&gt;&gt; Epoch 20 finished 	ANN training loss 0.023098
&gt;&gt; Epoch 21 finished 	ANN training loss 0.045296
&gt;&gt; Epoch 22 finished 	ANN training loss 0.019485
&gt;&gt; Epoch 23 finished 	ANN training loss 0.014655
&gt;&gt; Epoch 24 finished 	ANN training loss 0.014383
&gt;&gt; Epoch 25 finished 	ANN training loss 0.014686
&gt;&gt; Epoch 26 finished 	ANN training loss 0.014275
&gt;&gt; Epoch 27 finished 	ANN training loss 0.010776
&gt;&gt; Epoch 28 finished 	ANN training loss 0.008901
&gt;&gt; Epoch 29 finished 	ANN training loss 0.008485
&gt;&gt; Epoch 30 finished 	ANN training loss 0.007667
&gt;&gt; Epoch 31 finished 	ANN training loss 0.006525
&gt;&gt; Epoch 32 finished 	ANN training loss 0.006767
&gt;&gt; Epoch 33 finished 	ANN training loss 0.006382
&gt;&gt; Epoch 34 finished 	ANN training loss 0.006764
&gt;&gt; Epoch 35 finished 	ANN training loss 0.004568
&gt;&gt; Epoch 36 finished 	ANN training loss 0.005110
&gt;&gt; Epoch 37 finished 	ANN training loss 0.005352
&gt;&gt; Epoch 38 finished 	ANN training loss 0.004307
&gt;&gt; Epoch 39 finished 	ANN training loss 0.003712
&gt;&gt; Epoch 40 finished 	ANN training loss 0.003149
&gt;&gt; Epoch 41 finished 	ANN training loss 0.004384
&gt;&gt; Epoch 42 finished 	ANN training loss 0.002963
&gt;&gt; Epoch 43 finished 	ANN training loss 0.003058
&gt;&gt; Epoch 44 finished 	ANN training loss 0.004003
&gt;&gt; Epoch 45 finished 	ANN training loss 0.002533
&gt;&gt; Epoch 46 finished 	ANN training loss 0.002238
&gt;&gt; Epoch 47 finished 	ANN training loss 0.001924
&gt;&gt; Epoch 48 finished 	ANN training loss 0.002303
&gt;&gt; Epoch 49 finished 	ANN training loss 0.001635
&gt;&gt; Epoch 50 finished 	ANN training loss 0.001745
&gt;&gt; Epoch 51 finished 	ANN training loss 0.001279
&gt;&gt; Epoch 52 finished 	ANN training loss 0.001394
&gt;&gt; Epoch 53 finished 	ANN training loss 0.005309
&gt;&gt; Epoch 54 finished 	ANN training loss 0.002727
&gt;&gt; Epoch 55 finished 	ANN training loss 0.001293
&gt;&gt; Epoch 56 finished 	ANN training loss 0.001096
&gt;&gt; Epoch 57 finished 	ANN training loss 0.001456
&gt;&gt; Epoch 58 finished 	ANN training loss 0.000962
&gt;&gt; Epoch 59 finished 	ANN training loss 0.001269
&gt;&gt; Epoch 60 finished 	ANN training loss 0.003433
&gt;&gt; Epoch 61 finished 	ANN training loss 0.001575
&gt;&gt; Epoch 62 finished 	ANN training loss 0.000889
&gt;&gt; Epoch 63 finished 	ANN training loss 0.001236
&gt;&gt; Epoch 64 finished 	ANN training loss 0.001697
&gt;&gt; Epoch 65 finished 	ANN training loss 0.001059
&gt;&gt; Epoch 66 finished 	ANN training loss 0.001085
&gt;&gt; Epoch 67 finished 	ANN training loss 0.001179
&gt;&gt; Epoch 68 finished 	ANN training loss 0.001136
&gt;&gt; Epoch 69 finished 	ANN training loss 0.001136
&gt;&gt; Epoch 70 finished 	ANN training loss 0.000898
&gt;&gt; Epoch 71 finished 	ANN training loss 0.000630
&gt;&gt; Epoch 72 finished 	ANN training loss 0.000571
&gt;&gt; Epoch 73 finished 	ANN training loss 0.000813
&gt;&gt; Epoch 74 finished 	ANN training loss 0.000590
&gt;&gt; Epoch 75 finished 	ANN training loss 0.000545
&gt;&gt; Epoch 76 finished 	ANN training loss 0.000517
&gt;&gt; Epoch 77 finished 	ANN training loss 0.000598
&gt;&gt; Epoch 78 finished 	ANN training loss 0.000684
&gt;&gt; Epoch 79 finished 	ANN training loss 0.000607
&gt;&gt; Epoch 80 finished 	ANN training loss 0.000638
&gt;&gt; Epoch 81 finished 	ANN training loss 0.000651
&gt;&gt; Epoch 82 finished 	ANN training loss 0.000378
&gt;&gt; Epoch 83 finished 	ANN training loss 0.000516
&gt;&gt; Epoch 84 finished 	ANN training loss 0.003198
&gt;&gt; Epoch 85 finished 	ANN training loss 0.000389
&gt;&gt; Epoch 86 finished 	ANN training loss 0.000882
&gt;&gt; Epoch 87 finished 	ANN training loss 0.000349
&gt;&gt; Epoch 88 finished 	ANN training loss 0.000710
&gt;&gt; Epoch 89 finished 	ANN training loss 0.000895
&gt;&gt; Epoch 90 finished 	ANN training loss 0.000631
&gt;&gt; Epoch 91 finished 	ANN training loss 0.000354
&gt;&gt; Epoch 92 finished 	ANN training loss 0.000271
&gt;&gt; Epoch 93 finished 	ANN training loss 0.000350
&gt;&gt; Epoch 94 finished 	ANN training loss 0.000304
&gt;&gt; Epoch 95 finished 	ANN training loss 0.000320
&gt;&gt; Epoch 96 finished 	ANN training loss 0.000403
&gt;&gt; Epoch 97 finished 	ANN training loss 0.000309
&gt;&gt; Epoch 98 finished 	ANN training loss 0.000244
&gt;&gt; Epoch 99 finished 	ANN training loss 0.000229
[END] Fine tuning step
Done.
Accuracy: 0.925000
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[37]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ACCURACY: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">threelayer_acc</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>ACCURACY: 0.925
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Setting 3 – 2nd Layer = 500</strong></p>

<pre><code>hidden_layers_structure=[100, 500, 300]</code></pre>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[38]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">threelayer_acc</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">deep_belief_net</span><span class="p">(</span><span class="n">hidden_layers_structure</span><span class="o">=</span><span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">500</span><span class="p">,</span> <span class="mi">300</span><span class="p">])</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[START] Pre-training step:
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 59.564102
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 59.761078
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 57.921844
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 46.745922
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 65.336388
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 107.946800
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 93.640976
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 101.977051
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 98.326408
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 103.741455
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 329.913818
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 587.393494
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 238.993225
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 555.764648
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 475.240692
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 475.708588
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 442.185699
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 499.192139
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 555.016052
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 750.540283
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 45197.039062
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 42511.648438
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 57938.121094
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 45343.339844
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 52561.078125
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 95396.632812
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 74806.039062
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 77988.851562
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 120865.539062
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 94011.882812
[END] Pre-training step
[START] Fine tuning step:
&gt;&gt; Epoch 0 finished 	ANN training loss 1.533851
&gt;&gt; Epoch 1 finished 	ANN training loss 1.276689
&gt;&gt; Epoch 2 finished 	ANN training loss 0.949397
&gt;&gt; Epoch 3 finished 	ANN training loss 0.714508
&gt;&gt; Epoch 4 finished 	ANN training loss 0.554281
&gt;&gt; Epoch 5 finished 	ANN training loss 0.477935
&gt;&gt; Epoch 6 finished 	ANN training loss 0.420389
&gt;&gt; Epoch 7 finished 	ANN training loss 0.354417
&gt;&gt; Epoch 8 finished 	ANN training loss 0.330924
&gt;&gt; Epoch 9 finished 	ANN training loss 0.291550
&gt;&gt; Epoch 10 finished 	ANN training loss 0.225266
&gt;&gt; Epoch 11 finished 	ANN training loss 0.193930
&gt;&gt; Epoch 12 finished 	ANN training loss 0.200787
&gt;&gt; Epoch 13 finished 	ANN training loss 0.158300
&gt;&gt; Epoch 14 finished 	ANN training loss 0.148867
&gt;&gt; Epoch 15 finished 	ANN training loss 0.137767
&gt;&gt; Epoch 16 finished 	ANN training loss 0.122214
&gt;&gt; Epoch 17 finished 	ANN training loss 0.120279
&gt;&gt; Epoch 18 finished 	ANN training loss 0.109280
&gt;&gt; Epoch 19 finished 	ANN training loss 0.103567
&gt;&gt; Epoch 20 finished 	ANN training loss 0.091581
&gt;&gt; Epoch 21 finished 	ANN training loss 0.066786
&gt;&gt; Epoch 22 finished 	ANN training loss 0.073343
&gt;&gt; Epoch 23 finished 	ANN training loss 0.066734
&gt;&gt; Epoch 24 finished 	ANN training loss 0.076449
&gt;&gt; Epoch 25 finished 	ANN training loss 0.048777
&gt;&gt; Epoch 26 finished 	ANN training loss 0.044279
&gt;&gt; Epoch 27 finished 	ANN training loss 0.051107
&gt;&gt; Epoch 28 finished 	ANN training loss 0.043139
&gt;&gt; Epoch 29 finished 	ANN training loss 0.040017
&gt;&gt; Epoch 30 finished 	ANN training loss 0.038079
&gt;&gt; Epoch 31 finished 	ANN training loss 0.029705
&gt;&gt; Epoch 32 finished 	ANN training loss 0.029876
&gt;&gt; Epoch 33 finished 	ANN training loss 0.031328
&gt;&gt; Epoch 34 finished 	ANN training loss 0.028740
&gt;&gt; Epoch 35 finished 	ANN training loss 0.025744
&gt;&gt; Epoch 36 finished 	ANN training loss 0.028364
&gt;&gt; Epoch 37 finished 	ANN training loss 0.025740
&gt;&gt; Epoch 38 finished 	ANN training loss 0.027433
&gt;&gt; Epoch 39 finished 	ANN training loss 0.016523
&gt;&gt; Epoch 40 finished 	ANN training loss 0.017461
&gt;&gt; Epoch 41 finished 	ANN training loss 0.021345
&gt;&gt; Epoch 42 finished 	ANN training loss 0.024730
&gt;&gt; Epoch 43 finished 	ANN training loss 0.021354
&gt;&gt; Epoch 44 finished 	ANN training loss 0.014698
&gt;&gt; Epoch 45 finished 	ANN training loss 0.013888
&gt;&gt; Epoch 46 finished 	ANN training loss 0.014201
&gt;&gt; Epoch 47 finished 	ANN training loss 0.036601
&gt;&gt; Epoch 48 finished 	ANN training loss 0.013088
&gt;&gt; Epoch 49 finished 	ANN training loss 0.010723
&gt;&gt; Epoch 50 finished 	ANN training loss 0.016645
&gt;&gt; Epoch 51 finished 	ANN training loss 0.012544
&gt;&gt; Epoch 52 finished 	ANN training loss 0.012440
&gt;&gt; Epoch 53 finished 	ANN training loss 0.011306
&gt;&gt; Epoch 54 finished 	ANN training loss 0.008115
&gt;&gt; Epoch 55 finished 	ANN training loss 0.009665
&gt;&gt; Epoch 56 finished 	ANN training loss 0.008852
&gt;&gt; Epoch 57 finished 	ANN training loss 0.005930
&gt;&gt; Epoch 58 finished 	ANN training loss 0.006430
&gt;&gt; Epoch 59 finished 	ANN training loss 0.007473
&gt;&gt; Epoch 60 finished 	ANN training loss 0.008734
&gt;&gt; Epoch 61 finished 	ANN training loss 0.005969
&gt;&gt; Epoch 62 finished 	ANN training loss 0.006821
&gt;&gt; Epoch 63 finished 	ANN training loss 0.012818
&gt;&gt; Epoch 64 finished 	ANN training loss 0.006060
&gt;&gt; Epoch 65 finished 	ANN training loss 0.009088
&gt;&gt; Epoch 66 finished 	ANN training loss 0.005121
&gt;&gt; Epoch 67 finished 	ANN training loss 0.017593
&gt;&gt; Epoch 68 finished 	ANN training loss 0.005724
&gt;&gt; Epoch 69 finished 	ANN training loss 0.004105
&gt;&gt; Epoch 70 finished 	ANN training loss 0.004846
&gt;&gt; Epoch 71 finished 	ANN training loss 0.004525
&gt;&gt; Epoch 72 finished 	ANN training loss 0.005610
&gt;&gt; Epoch 73 finished 	ANN training loss 0.016978
&gt;&gt; Epoch 74 finished 	ANN training loss 0.004154
&gt;&gt; Epoch 75 finished 	ANN training loss 0.005548
&gt;&gt; Epoch 76 finished 	ANN training loss 0.004250
&gt;&gt; Epoch 77 finished 	ANN training loss 0.005915
&gt;&gt; Epoch 78 finished 	ANN training loss 0.005880
&gt;&gt; Epoch 79 finished 	ANN training loss 0.005053
&gt;&gt; Epoch 80 finished 	ANN training loss 0.003347
&gt;&gt; Epoch 81 finished 	ANN training loss 0.003937
&gt;&gt; Epoch 82 finished 	ANN training loss 0.004959
&gt;&gt; Epoch 83 finished 	ANN training loss 0.003024
&gt;&gt; Epoch 84 finished 	ANN training loss 0.002971
&gt;&gt; Epoch 85 finished 	ANN training loss 0.002826
&gt;&gt; Epoch 86 finished 	ANN training loss 0.002263
&gt;&gt; Epoch 87 finished 	ANN training loss 0.002154
&gt;&gt; Epoch 88 finished 	ANN training loss 0.002168
&gt;&gt; Epoch 89 finished 	ANN training loss 0.002966
&gt;&gt; Epoch 90 finished 	ANN training loss 0.004537
&gt;&gt; Epoch 91 finished 	ANN training loss 0.003439
&gt;&gt; Epoch 92 finished 	ANN training loss 0.002118
&gt;&gt; Epoch 93 finished 	ANN training loss 0.002449
&gt;&gt; Epoch 94 finished 	ANN training loss 0.001787
&gt;&gt; Epoch 95 finished 	ANN training loss 0.001540
&gt;&gt; Epoch 96 finished 	ANN training loss 0.002248
&gt;&gt; Epoch 97 finished 	ANN training loss 0.001464
&gt;&gt; Epoch 98 finished 	ANN training loss 0.008587
&gt;&gt; Epoch 99 finished 	ANN training loss 0.004158
[END] Fine tuning step
Done.
Accuracy: 0.895000
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[39]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ACCURACY: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">threelayer_acc</span><span class="p">[</span><span class="mi">2</span><span class="p">]))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>ACCURACY: 0.895
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Setting 4 – 3rd Layer = 500</strong></p>

<pre><code>hidden_layers_structure=[100, 200, 500]</code></pre>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[40]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">threelayer_acc</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">=</span> <span class="n">deep_belief_net</span><span class="p">(</span><span class="n">hidden_layers_structure</span><span class="o">=</span><span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">500</span><span class="p">])</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[START] Pre-training step:
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 75.150208
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 61.374603
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 64.751137
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 63.384277
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 54.535011
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 56.678871
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 75.155716
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 88.478317
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 77.702446
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 84.700813
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 577.839172
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 415.941925
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 600.284607
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 793.621643
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 637.713135
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 843.325439
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 760.770752
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 810.339233
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 955.818298
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 911.069092
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 11445.860352
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 25202.421875
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 37259.140625
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 39311.695312
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 40274.070312
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 59256.753906
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 59931.890625
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 61257.945312
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 54933.531250
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 63609.621094
[END] Pre-training step
[START] Fine tuning step:
&gt;&gt; Epoch 0 finished 	ANN training loss 1.258452
&gt;&gt; Epoch 1 finished 	ANN training loss 1.083188
&gt;&gt; Epoch 2 finished 	ANN training loss 0.613726
&gt;&gt; Epoch 3 finished 	ANN training loss 0.504612
&gt;&gt; Epoch 4 finished 	ANN training loss 0.415223
&gt;&gt; Epoch 5 finished 	ANN training loss 0.370487
&gt;&gt; Epoch 6 finished 	ANN training loss 0.348263
&gt;&gt; Epoch 7 finished 	ANN training loss 0.269174
&gt;&gt; Epoch 8 finished 	ANN training loss 0.226006
&gt;&gt; Epoch 9 finished 	ANN training loss 0.210821
&gt;&gt; Epoch 10 finished 	ANN training loss 0.169759
&gt;&gt; Epoch 11 finished 	ANN training loss 0.165086
&gt;&gt; Epoch 12 finished 	ANN training loss 0.147889
&gt;&gt; Epoch 13 finished 	ANN training loss 0.136483
&gt;&gt; Epoch 14 finished 	ANN training loss 0.116200
&gt;&gt; Epoch 15 finished 	ANN training loss 0.107202
&gt;&gt; Epoch 16 finished 	ANN training loss 0.117964
&gt;&gt; Epoch 17 finished 	ANN training loss 0.088797
&gt;&gt; Epoch 18 finished 	ANN training loss 0.070382
&gt;&gt; Epoch 19 finished 	ANN training loss 0.077285
&gt;&gt; Epoch 20 finished 	ANN training loss 0.070289
&gt;&gt; Epoch 21 finished 	ANN training loss 0.069854
&gt;&gt; Epoch 22 finished 	ANN training loss 0.058113
&gt;&gt; Epoch 23 finished 	ANN training loss 0.048474
&gt;&gt; Epoch 24 finished 	ANN training loss 0.049994
&gt;&gt; Epoch 25 finished 	ANN training loss 0.043107
&gt;&gt; Epoch 26 finished 	ANN training loss 0.039130
&gt;&gt; Epoch 27 finished 	ANN training loss 0.034399
&gt;&gt; Epoch 28 finished 	ANN training loss 0.047503
&gt;&gt; Epoch 29 finished 	ANN training loss 0.029602
&gt;&gt; Epoch 30 finished 	ANN training loss 0.027452
&gt;&gt; Epoch 31 finished 	ANN training loss 0.025084
&gt;&gt; Epoch 32 finished 	ANN training loss 0.026054
&gt;&gt; Epoch 33 finished 	ANN training loss 0.023706
&gt;&gt; Epoch 34 finished 	ANN training loss 0.031561
&gt;&gt; Epoch 35 finished 	ANN training loss 0.018627
&gt;&gt; Epoch 36 finished 	ANN training loss 0.018784
&gt;&gt; Epoch 37 finished 	ANN training loss 0.015933
&gt;&gt; Epoch 38 finished 	ANN training loss 0.151235
&gt;&gt; Epoch 39 finished 	ANN training loss 0.027518
&gt;&gt; Epoch 40 finished 	ANN training loss 0.022655
&gt;&gt; Epoch 41 finished 	ANN training loss 0.021426
&gt;&gt; Epoch 42 finished 	ANN training loss 0.018086
&gt;&gt; Epoch 43 finished 	ANN training loss 0.014521
&gt;&gt; Epoch 44 finished 	ANN training loss 0.014864
&gt;&gt; Epoch 45 finished 	ANN training loss 0.017260
&gt;&gt; Epoch 46 finished 	ANN training loss 0.026693
&gt;&gt; Epoch 47 finished 	ANN training loss 0.013925
&gt;&gt; Epoch 48 finished 	ANN training loss 0.012750
&gt;&gt; Epoch 49 finished 	ANN training loss 0.008987
&gt;&gt; Epoch 50 finished 	ANN training loss 0.009832
&gt;&gt; Epoch 51 finished 	ANN training loss 0.010033
&gt;&gt; Epoch 52 finished 	ANN training loss 0.012602
&gt;&gt; Epoch 53 finished 	ANN training loss 0.008046
&gt;&gt; Epoch 54 finished 	ANN training loss 0.010736
&gt;&gt; Epoch 55 finished 	ANN training loss 0.006261
&gt;&gt; Epoch 56 finished 	ANN training loss 0.021140
&gt;&gt; Epoch 57 finished 	ANN training loss 0.006422
&gt;&gt; Epoch 58 finished 	ANN training loss 0.006872
&gt;&gt; Epoch 59 finished 	ANN training loss 0.007119
&gt;&gt; Epoch 60 finished 	ANN training loss 0.007198
&gt;&gt; Epoch 61 finished 	ANN training loss 0.005168
&gt;&gt; Epoch 62 finished 	ANN training loss 0.007424
&gt;&gt; Epoch 63 finished 	ANN training loss 0.006021
&gt;&gt; Epoch 64 finished 	ANN training loss 0.005989
&gt;&gt; Epoch 65 finished 	ANN training loss 0.005185
&gt;&gt; Epoch 66 finished 	ANN training loss 0.004433
&gt;&gt; Epoch 67 finished 	ANN training loss 0.003414
&gt;&gt; Epoch 68 finished 	ANN training loss 0.004208
&gt;&gt; Epoch 69 finished 	ANN training loss 0.007795
&gt;&gt; Epoch 70 finished 	ANN training loss 0.005307
&gt;&gt; Epoch 71 finished 	ANN training loss 0.004187
&gt;&gt; Epoch 72 finished 	ANN training loss 0.004377
&gt;&gt; Epoch 73 finished 	ANN training loss 0.003942
&gt;&gt; Epoch 74 finished 	ANN training loss 0.003532
&gt;&gt; Epoch 75 finished 	ANN training loss 0.003018
&gt;&gt; Epoch 76 finished 	ANN training loss 0.003464
&gt;&gt; Epoch 77 finished 	ANN training loss 0.003108
&gt;&gt; Epoch 78 finished 	ANN training loss 0.002644
&gt;&gt; Epoch 79 finished 	ANN training loss 0.002728
&gt;&gt; Epoch 80 finished 	ANN training loss 0.002868
&gt;&gt; Epoch 81 finished 	ANN training loss 0.005210
&gt;&gt; Epoch 82 finished 	ANN training loss 0.003154
&gt;&gt; Epoch 83 finished 	ANN training loss 0.002397
&gt;&gt; Epoch 84 finished 	ANN training loss 0.003985
&gt;&gt; Epoch 85 finished 	ANN training loss 0.002626
&gt;&gt; Epoch 86 finished 	ANN training loss 0.001937
&gt;&gt; Epoch 87 finished 	ANN training loss 0.001281
&gt;&gt; Epoch 88 finished 	ANN training loss 0.001930
&gt;&gt; Epoch 89 finished 	ANN training loss 0.001422
&gt;&gt; Epoch 90 finished 	ANN training loss 0.001669
&gt;&gt; Epoch 91 finished 	ANN training loss 0.001492
&gt;&gt; Epoch 92 finished 	ANN training loss 0.001529
&gt;&gt; Epoch 93 finished 	ANN training loss 0.002800
&gt;&gt; Epoch 94 finished 	ANN training loss 0.002059
&gt;&gt; Epoch 95 finished 	ANN training loss 0.002790
&gt;&gt; Epoch 96 finished 	ANN training loss 0.001995
&gt;&gt; Epoch 97 finished 	ANN training loss 0.001121
&gt;&gt; Epoch 98 finished 	ANN training loss 0.000980
&gt;&gt; Epoch 99 finished 	ANN training loss 0.001216
[END] Fine tuning step
Done.
Accuracy: 0.870000
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[41]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ACCURACY: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">threelayer_acc</span><span class="p">[</span><span class="mi">3</span><span class="p">]))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>ACCURACY: 0.87
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Setting 5 – 1st Layer = 500, 2nd Layer = 500</strong></p>

<pre><code>hidden_layers_structure=[500, 500, 300]</code></pre>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[42]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">threelayer_acc</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span> <span class="o">=</span> <span class="n">deep_belief_net</span><span class="p">(</span><span class="n">hidden_layers_structure</span><span class="o">=</span><span class="p">[</span><span class="mi">500</span><span class="p">,</span> <span class="mi">500</span><span class="p">,</span> <span class="mi">300</span><span class="p">])</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[START] Pre-training step:
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 68.491608
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 56.644928
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 53.951054
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 44.317753
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 37.582027
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 26.515465
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 31.068565
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 28.296555
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 26.028257
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 25.418171
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 152.616257
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 63.525909
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 43.410030
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 52.196709
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 96.214417
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 58.725548
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 39.154755
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 90.772125
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 41.436943
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 65.333107
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 315.171967
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 704.780334
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 604.968628
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 875.380310
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 853.201416
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 839.794739
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 960.186707
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 959.082336
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 1235.092163
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 1277.223877
[END] Pre-training step
[START] Fine tuning step:
&gt;&gt; Epoch 0 finished 	ANN training loss 0.642280
&gt;&gt; Epoch 1 finished 	ANN training loss 0.512841
&gt;&gt; Epoch 2 finished 	ANN training loss 0.311047
&gt;&gt; Epoch 3 finished 	ANN training loss 0.261270
&gt;&gt; Epoch 4 finished 	ANN training loss 0.237309
&gt;&gt; Epoch 5 finished 	ANN training loss 0.194324
&gt;&gt; Epoch 6 finished 	ANN training loss 0.203730
&gt;&gt; Epoch 7 finished 	ANN training loss 0.144550
&gt;&gt; Epoch 8 finished 	ANN training loss 0.121052
&gt;&gt; Epoch 9 finished 	ANN training loss 0.117892
&gt;&gt; Epoch 10 finished 	ANN training loss 0.083755
&gt;&gt; Epoch 11 finished 	ANN training loss 0.073921
&gt;&gt; Epoch 12 finished 	ANN training loss 0.068480
&gt;&gt; Epoch 13 finished 	ANN training loss 0.050775
&gt;&gt; Epoch 14 finished 	ANN training loss 0.044401
&gt;&gt; Epoch 15 finished 	ANN training loss 0.039711
&gt;&gt; Epoch 16 finished 	ANN training loss 0.031740
&gt;&gt; Epoch 17 finished 	ANN training loss 0.028511
&gt;&gt; Epoch 18 finished 	ANN training loss 0.025597
&gt;&gt; Epoch 19 finished 	ANN training loss 0.021941
&gt;&gt; Epoch 20 finished 	ANN training loss 0.016981
&gt;&gt; Epoch 21 finished 	ANN training loss 0.019400
&gt;&gt; Epoch 22 finished 	ANN training loss 0.019374
&gt;&gt; Epoch 23 finished 	ANN training loss 0.031542
&gt;&gt; Epoch 24 finished 	ANN training loss 0.016698
&gt;&gt; Epoch 25 finished 	ANN training loss 0.011849
&gt;&gt; Epoch 26 finished 	ANN training loss 0.011747
&gt;&gt; Epoch 27 finished 	ANN training loss 0.009666
&gt;&gt; Epoch 28 finished 	ANN training loss 0.007762
&gt;&gt; Epoch 29 finished 	ANN training loss 0.009268
&gt;&gt; Epoch 30 finished 	ANN training loss 0.009688
&gt;&gt; Epoch 31 finished 	ANN training loss 0.009016
&gt;&gt; Epoch 32 finished 	ANN training loss 0.006719
&gt;&gt; Epoch 33 finished 	ANN training loss 0.007408
&gt;&gt; Epoch 34 finished 	ANN training loss 0.006457
&gt;&gt; Epoch 35 finished 	ANN training loss 0.005747
&gt;&gt; Epoch 36 finished 	ANN training loss 0.004501
&gt;&gt; Epoch 37 finished 	ANN training loss 0.004402
&gt;&gt; Epoch 38 finished 	ANN training loss 0.003900
&gt;&gt; Epoch 39 finished 	ANN training loss 0.003608
&gt;&gt; Epoch 40 finished 	ANN training loss 0.002960
&gt;&gt; Epoch 41 finished 	ANN training loss 0.002813
&gt;&gt; Epoch 42 finished 	ANN training loss 0.002161
&gt;&gt; Epoch 43 finished 	ANN training loss 0.002619
&gt;&gt; Epoch 44 finished 	ANN training loss 0.003579
&gt;&gt; Epoch 45 finished 	ANN training loss 0.002943
&gt;&gt; Epoch 46 finished 	ANN training loss 0.002186
&gt;&gt; Epoch 47 finished 	ANN training loss 0.001951
&gt;&gt; Epoch 48 finished 	ANN training loss 0.004580
&gt;&gt; Epoch 49 finished 	ANN training loss 0.003021
&gt;&gt; Epoch 50 finished 	ANN training loss 0.003405
&gt;&gt; Epoch 51 finished 	ANN training loss 0.001844
&gt;&gt; Epoch 52 finished 	ANN training loss 0.001807
&gt;&gt; Epoch 53 finished 	ANN training loss 0.001755
&gt;&gt; Epoch 54 finished 	ANN training loss 0.001252
&gt;&gt; Epoch 55 finished 	ANN training loss 0.001320
&gt;&gt; Epoch 56 finished 	ANN training loss 0.001371
&gt;&gt; Epoch 57 finished 	ANN training loss 0.001298
&gt;&gt; Epoch 58 finished 	ANN training loss 0.000955
&gt;&gt; Epoch 59 finished 	ANN training loss 0.001305
&gt;&gt; Epoch 60 finished 	ANN training loss 0.001030
&gt;&gt; Epoch 61 finished 	ANN training loss 0.001128
&gt;&gt; Epoch 62 finished 	ANN training loss 0.000973
&gt;&gt; Epoch 63 finished 	ANN training loss 0.000982
&gt;&gt; Epoch 64 finished 	ANN training loss 0.001252
&gt;&gt; Epoch 65 finished 	ANN training loss 0.001098
&gt;&gt; Epoch 66 finished 	ANN training loss 0.000788
&gt;&gt; Epoch 67 finished 	ANN training loss 0.000999
&gt;&gt; Epoch 68 finished 	ANN training loss 0.001126
&gt;&gt; Epoch 69 finished 	ANN training loss 0.000856
&gt;&gt; Epoch 70 finished 	ANN training loss 0.000873
&gt;&gt; Epoch 71 finished 	ANN training loss 0.000717
&gt;&gt; Epoch 72 finished 	ANN training loss 0.000775
&gt;&gt; Epoch 73 finished 	ANN training loss 0.000858
&gt;&gt; Epoch 74 finished 	ANN training loss 0.000935
&gt;&gt; Epoch 75 finished 	ANN training loss 0.000911
&gt;&gt; Epoch 76 finished 	ANN training loss 0.000481
&gt;&gt; Epoch 77 finished 	ANN training loss 0.000534
&gt;&gt; Epoch 78 finished 	ANN training loss 0.000715
&gt;&gt; Epoch 79 finished 	ANN training loss 0.000511
&gt;&gt; Epoch 80 finished 	ANN training loss 0.000445
&gt;&gt; Epoch 81 finished 	ANN training loss 0.000527
&gt;&gt; Epoch 82 finished 	ANN training loss 0.000599
&gt;&gt; Epoch 83 finished 	ANN training loss 0.000532
&gt;&gt; Epoch 84 finished 	ANN training loss 0.000725
&gt;&gt; Epoch 85 finished 	ANN training loss 0.000393
&gt;&gt; Epoch 86 finished 	ANN training loss 0.000442
&gt;&gt; Epoch 87 finished 	ANN training loss 0.000435
&gt;&gt; Epoch 88 finished 	ANN training loss 0.000510
&gt;&gt; Epoch 89 finished 	ANN training loss 0.000390
&gt;&gt; Epoch 90 finished 	ANN training loss 0.000425
&gt;&gt; Epoch 91 finished 	ANN training loss 0.000335
&gt;&gt; Epoch 92 finished 	ANN training loss 0.000317
&gt;&gt; Epoch 93 finished 	ANN training loss 0.000315
&gt;&gt; Epoch 94 finished 	ANN training loss 0.000415
&gt;&gt; Epoch 95 finished 	ANN training loss 0.000503
&gt;&gt; Epoch 96 finished 	ANN training loss 0.000396
&gt;&gt; Epoch 97 finished 	ANN training loss 0.000376
&gt;&gt; Epoch 98 finished 	ANN training loss 0.000273
&gt;&gt; Epoch 99 finished 	ANN training loss 0.000261
[END] Fine tuning step
Done.
Accuracy: 0.910000
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[43]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ACCURACY: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">threelayer_acc</span><span class="p">[</span><span class="mi">4</span><span class="p">]))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>ACCURACY: 0.91
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Setting 6 – 1st Layer = 500, 3rd Layer = 500</strong></p>

<pre><code>hidden_layers_structure=[500, 200, 500]</code></pre>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[44]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">threelayer_acc</span><span class="p">[</span><span class="mi">5</span><span class="p">]</span> <span class="o">=</span> <span class="n">deep_belief_net</span><span class="p">(</span><span class="n">hidden_layers_structure</span><span class="o">=</span><span class="p">[</span><span class="mi">500</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">500</span><span class="p">])</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[START] Pre-training step:
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 78.036766
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 52.033066
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 32.189144
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 37.730347
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 38.358086
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 45.428146
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 39.048912
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 56.111622
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 23.523413
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 37.212223
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 123.392220
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 132.450577
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 221.457428
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 173.573120
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 181.331360
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 233.512924
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 192.164215
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 229.156311
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 271.157501
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 276.509216
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 1931.465210
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 3652.421631
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 4494.917969
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 5545.651855
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 4545.560547
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 4857.675781
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 5941.788574
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 6031.382324
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 5178.402344
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 6309.609863
[END] Pre-training step
[START] Fine tuning step:
&gt;&gt; Epoch 0 finished 	ANN training loss 0.713952
&gt;&gt; Epoch 1 finished 	ANN training loss 0.632705
&gt;&gt; Epoch 2 finished 	ANN training loss 0.366221
&gt;&gt; Epoch 3 finished 	ANN training loss 0.287641
&gt;&gt; Epoch 4 finished 	ANN training loss 0.274968
&gt;&gt; Epoch 5 finished 	ANN training loss 0.207943
&gt;&gt; Epoch 6 finished 	ANN training loss 0.183766
&gt;&gt; Epoch 7 finished 	ANN training loss 0.139385
&gt;&gt; Epoch 8 finished 	ANN training loss 0.133136
&gt;&gt; Epoch 9 finished 	ANN training loss 0.128391
&gt;&gt; Epoch 10 finished 	ANN training loss 0.115430
&gt;&gt; Epoch 11 finished 	ANN training loss 0.134924
&gt;&gt; Epoch 12 finished 	ANN training loss 0.080716
&gt;&gt; Epoch 13 finished 	ANN training loss 0.070322
&gt;&gt; Epoch 14 finished 	ANN training loss 0.081943
&gt;&gt; Epoch 15 finished 	ANN training loss 0.052324
&gt;&gt; Epoch 16 finished 	ANN training loss 0.054598
&gt;&gt; Epoch 17 finished 	ANN training loss 0.047250
&gt;&gt; Epoch 18 finished 	ANN training loss 0.038195
&gt;&gt; Epoch 19 finished 	ANN training loss 0.039413
&gt;&gt; Epoch 20 finished 	ANN training loss 0.026421
&gt;&gt; Epoch 21 finished 	ANN training loss 0.022356
&gt;&gt; Epoch 22 finished 	ANN training loss 0.019117
&gt;&gt; Epoch 23 finished 	ANN training loss 0.015601
&gt;&gt; Epoch 24 finished 	ANN training loss 0.017516
&gt;&gt; Epoch 25 finished 	ANN training loss 0.017139
&gt;&gt; Epoch 26 finished 	ANN training loss 0.017296
&gt;&gt; Epoch 27 finished 	ANN training loss 0.012669
&gt;&gt; Epoch 28 finished 	ANN training loss 0.012581
&gt;&gt; Epoch 29 finished 	ANN training loss 0.012118
&gt;&gt; Epoch 30 finished 	ANN training loss 0.008638
&gt;&gt; Epoch 31 finished 	ANN training loss 0.007331
&gt;&gt; Epoch 32 finished 	ANN training loss 0.008692
&gt;&gt; Epoch 33 finished 	ANN training loss 0.008452
&gt;&gt; Epoch 34 finished 	ANN training loss 0.005836
&gt;&gt; Epoch 35 finished 	ANN training loss 0.007319
&gt;&gt; Epoch 36 finished 	ANN training loss 0.005158
&gt;&gt; Epoch 37 finished 	ANN training loss 0.005184
&gt;&gt; Epoch 38 finished 	ANN training loss 0.003938
&gt;&gt; Epoch 39 finished 	ANN training loss 0.008897
&gt;&gt; Epoch 40 finished 	ANN training loss 0.004148
&gt;&gt; Epoch 41 finished 	ANN training loss 0.003720
&gt;&gt; Epoch 42 finished 	ANN training loss 0.002842
&gt;&gt; Epoch 43 finished 	ANN training loss 0.002704
&gt;&gt; Epoch 44 finished 	ANN training loss 0.004034
&gt;&gt; Epoch 45 finished 	ANN training loss 0.003140
&gt;&gt; Epoch 46 finished 	ANN training loss 0.003658
&gt;&gt; Epoch 47 finished 	ANN training loss 0.002641
&gt;&gt; Epoch 48 finished 	ANN training loss 0.002161
&gt;&gt; Epoch 49 finished 	ANN training loss 0.002023
&gt;&gt; Epoch 50 finished 	ANN training loss 0.002653
&gt;&gt; Epoch 51 finished 	ANN training loss 0.001711
&gt;&gt; Epoch 52 finished 	ANN training loss 0.001963
&gt;&gt; Epoch 53 finished 	ANN training loss 0.002100
&gt;&gt; Epoch 54 finished 	ANN training loss 0.001490
&gt;&gt; Epoch 55 finished 	ANN training loss 0.001593
&gt;&gt; Epoch 56 finished 	ANN training loss 0.001803
&gt;&gt; Epoch 57 finished 	ANN training loss 0.001548
&gt;&gt; Epoch 58 finished 	ANN training loss 0.002671
&gt;&gt; Epoch 59 finished 	ANN training loss 0.002320
&gt;&gt; Epoch 60 finished 	ANN training loss 0.001416
&gt;&gt; Epoch 61 finished 	ANN training loss 0.001142
&gt;&gt; Epoch 62 finished 	ANN training loss 0.001347
&gt;&gt; Epoch 63 finished 	ANN training loss 0.001051
&gt;&gt; Epoch 64 finished 	ANN training loss 0.001429
&gt;&gt; Epoch 65 finished 	ANN training loss 0.001294
&gt;&gt; Epoch 66 finished 	ANN training loss 0.000963
&gt;&gt; Epoch 67 finished 	ANN training loss 0.000795
&gt;&gt; Epoch 68 finished 	ANN training loss 0.000796
&gt;&gt; Epoch 69 finished 	ANN training loss 0.000556
&gt;&gt; Epoch 70 finished 	ANN training loss 0.000769
&gt;&gt; Epoch 71 finished 	ANN training loss 0.000558
&gt;&gt; Epoch 72 finished 	ANN training loss 0.000450
&gt;&gt; Epoch 73 finished 	ANN training loss 0.000430
&gt;&gt; Epoch 74 finished 	ANN training loss 0.000453
&gt;&gt; Epoch 75 finished 	ANN training loss 0.000546
&gt;&gt; Epoch 76 finished 	ANN training loss 0.000422
&gt;&gt; Epoch 77 finished 	ANN training loss 0.000460
&gt;&gt; Epoch 78 finished 	ANN training loss 0.000437
&gt;&gt; Epoch 79 finished 	ANN training loss 0.000516
&gt;&gt; Epoch 80 finished 	ANN training loss 0.000723
&gt;&gt; Epoch 81 finished 	ANN training loss 0.000454
&gt;&gt; Epoch 82 finished 	ANN training loss 0.000410
&gt;&gt; Epoch 83 finished 	ANN training loss 0.000487
&gt;&gt; Epoch 84 finished 	ANN training loss 0.000508
&gt;&gt; Epoch 85 finished 	ANN training loss 0.001063
&gt;&gt; Epoch 86 finished 	ANN training loss 0.000693
&gt;&gt; Epoch 87 finished 	ANN training loss 0.000534
&gt;&gt; Epoch 88 finished 	ANN training loss 0.000694
&gt;&gt; Epoch 89 finished 	ANN training loss 0.000495
&gt;&gt; Epoch 90 finished 	ANN training loss 0.000531
&gt;&gt; Epoch 91 finished 	ANN training loss 0.000385
&gt;&gt; Epoch 92 finished 	ANN training loss 0.000477
&gt;&gt; Epoch 93 finished 	ANN training loss 0.000299
&gt;&gt; Epoch 94 finished 	ANN training loss 0.001446
&gt;&gt; Epoch 95 finished 	ANN training loss 0.000973
&gt;&gt; Epoch 96 finished 	ANN training loss 0.000481
&gt;&gt; Epoch 97 finished 	ANN training loss 0.000551
&gt;&gt; Epoch 98 finished 	ANN training loss 0.000467
&gt;&gt; Epoch 99 finished 	ANN training loss 0.000395
[END] Fine tuning step
Done.
Accuracy: 0.875000
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[45]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ACCURACY: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">threelayer_acc</span><span class="p">[</span><span class="mi">5</span><span class="p">]))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>ACCURACY: 0.875
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Setting 7 – 2nd Layer = 500, 3rd Layer = 500</strong></p>

<pre><code>hidden_layers_structure=[100, 500, 500]</code></pre>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[46]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">threelayer_acc</span><span class="p">[</span><span class="mi">6</span><span class="p">]</span> <span class="o">=</span> <span class="n">deep_belief_net</span><span class="p">(</span><span class="n">hidden_layers_structure</span><span class="o">=</span><span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">500</span><span class="p">,</span> <span class="mi">500</span><span class="p">])</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[START] Pre-training step:
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 82.229980
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 46.168163
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 51.044132
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 76.949280
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 87.207382
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 85.926399
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 81.613663
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 87.550720
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 112.198029
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 93.661072
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 303.795593
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 469.277771
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 314.409546
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 261.004852
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 333.697815
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 439.731628
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 470.187653
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 351.625122
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 363.454834
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 631.564026
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 28168.574219
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 18858.269531
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 21156.072266
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 24177.750000
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 40059.156250
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 48131.300781
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 62412.929688
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 79066.500000
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 71365.078125
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 86572.859375
[END] Pre-training step
[START] Fine tuning step:
&gt;&gt; Epoch 0 finished 	ANN training loss 1.422574
&gt;&gt; Epoch 1 finished 	ANN training loss 1.026839
&gt;&gt; Epoch 2 finished 	ANN training loss 0.847494
&gt;&gt; Epoch 3 finished 	ANN training loss 0.729024
&gt;&gt; Epoch 4 finished 	ANN training loss 0.589623
&gt;&gt; Epoch 5 finished 	ANN training loss 0.555054
&gt;&gt; Epoch 6 finished 	ANN training loss 0.436554
&gt;&gt; Epoch 7 finished 	ANN training loss 0.393142
&gt;&gt; Epoch 8 finished 	ANN training loss 0.325833
&gt;&gt; Epoch 9 finished 	ANN training loss 0.298387
&gt;&gt; Epoch 10 finished 	ANN training loss 0.253825
&gt;&gt; Epoch 11 finished 	ANN training loss 0.244105
&gt;&gt; Epoch 12 finished 	ANN training loss 0.203849
&gt;&gt; Epoch 13 finished 	ANN training loss 0.195522
&gt;&gt; Epoch 14 finished 	ANN training loss 0.173128
&gt;&gt; Epoch 15 finished 	ANN training loss 0.150871
&gt;&gt; Epoch 16 finished 	ANN training loss 0.145122
&gt;&gt; Epoch 17 finished 	ANN training loss 0.114257
&gt;&gt; Epoch 18 finished 	ANN training loss 0.119007
&gt;&gt; Epoch 19 finished 	ANN training loss 0.105456
&gt;&gt; Epoch 20 finished 	ANN training loss 0.104219
&gt;&gt; Epoch 21 finished 	ANN training loss 0.080118
&gt;&gt; Epoch 22 finished 	ANN training loss 0.063676
&gt;&gt; Epoch 23 finished 	ANN training loss 0.077698
&gt;&gt; Epoch 24 finished 	ANN training loss 0.065270
&gt;&gt; Epoch 25 finished 	ANN training loss 0.063665
&gt;&gt; Epoch 26 finished 	ANN training loss 0.062546
&gt;&gt; Epoch 27 finished 	ANN training loss 0.049267
&gt;&gt; Epoch 28 finished 	ANN training loss 0.053107
&gt;&gt; Epoch 29 finished 	ANN training loss 0.039250
&gt;&gt; Epoch 30 finished 	ANN training loss 0.035018
&gt;&gt; Epoch 31 finished 	ANN training loss 0.036830
&gt;&gt; Epoch 32 finished 	ANN training loss 0.030265
&gt;&gt; Epoch 33 finished 	ANN training loss 0.034772
&gt;&gt; Epoch 34 finished 	ANN training loss 0.030809
&gt;&gt; Epoch 35 finished 	ANN training loss 0.038304
&gt;&gt; Epoch 36 finished 	ANN training loss 0.030574
&gt;&gt; Epoch 37 finished 	ANN training loss 0.022830
&gt;&gt; Epoch 38 finished 	ANN training loss 0.024702
&gt;&gt; Epoch 39 finished 	ANN training loss 0.021393
&gt;&gt; Epoch 40 finished 	ANN training loss 0.017328
&gt;&gt; Epoch 41 finished 	ANN training loss 0.019605
&gt;&gt; Epoch 42 finished 	ANN training loss 0.020833
&gt;&gt; Epoch 43 finished 	ANN training loss 0.023375
&gt;&gt; Epoch 44 finished 	ANN training loss 0.018717
&gt;&gt; Epoch 45 finished 	ANN training loss 0.020408
&gt;&gt; Epoch 46 finished 	ANN training loss 0.015725
&gt;&gt; Epoch 47 finished 	ANN training loss 0.016227
&gt;&gt; Epoch 48 finished 	ANN training loss 0.027124
&gt;&gt; Epoch 49 finished 	ANN training loss 0.018069
&gt;&gt; Epoch 50 finished 	ANN training loss 0.011587
&gt;&gt; Epoch 51 finished 	ANN training loss 0.014348
&gt;&gt; Epoch 52 finished 	ANN training loss 0.014379
&gt;&gt; Epoch 53 finished 	ANN training loss 0.017997
&gt;&gt; Epoch 54 finished 	ANN training loss 0.014161
&gt;&gt; Epoch 55 finished 	ANN training loss 0.009151
&gt;&gt; Epoch 56 finished 	ANN training loss 0.010320
&gt;&gt; Epoch 57 finished 	ANN training loss 0.008837
&gt;&gt; Epoch 58 finished 	ANN training loss 0.006893
&gt;&gt; Epoch 59 finished 	ANN training loss 0.010685
&gt;&gt; Epoch 60 finished 	ANN training loss 0.007026
&gt;&gt; Epoch 61 finished 	ANN training loss 0.006646
&gt;&gt; Epoch 62 finished 	ANN training loss 0.007347
&gt;&gt; Epoch 63 finished 	ANN training loss 0.006674
&gt;&gt; Epoch 64 finished 	ANN training loss 0.010059
&gt;&gt; Epoch 65 finished 	ANN training loss 0.008058
&gt;&gt; Epoch 66 finished 	ANN training loss 0.006482
&gt;&gt; Epoch 67 finished 	ANN training loss 0.005870
&gt;&gt; Epoch 68 finished 	ANN training loss 0.006228
&gt;&gt; Epoch 69 finished 	ANN training loss 0.006360
&gt;&gt; Epoch 70 finished 	ANN training loss 0.008768
&gt;&gt; Epoch 71 finished 	ANN training loss 0.005981
&gt;&gt; Epoch 72 finished 	ANN training loss 0.007048
&gt;&gt; Epoch 73 finished 	ANN training loss 0.006741
&gt;&gt; Epoch 74 finished 	ANN training loss 0.007541
&gt;&gt; Epoch 75 finished 	ANN training loss 0.010282
&gt;&gt; Epoch 76 finished 	ANN training loss 0.008161
&gt;&gt; Epoch 77 finished 	ANN training loss 0.006051
&gt;&gt; Epoch 78 finished 	ANN training loss 0.005904
&gt;&gt; Epoch 79 finished 	ANN training loss 0.006657
&gt;&gt; Epoch 80 finished 	ANN training loss 0.005100
&gt;&gt; Epoch 81 finished 	ANN training loss 0.005631
&gt;&gt; Epoch 82 finished 	ANN training loss 0.007407
&gt;&gt; Epoch 83 finished 	ANN training loss 0.005248
&gt;&gt; Epoch 84 finished 	ANN training loss 0.004808
&gt;&gt; Epoch 85 finished 	ANN training loss 0.004339
&gt;&gt; Epoch 86 finished 	ANN training loss 0.006076
&gt;&gt; Epoch 87 finished 	ANN training loss 0.004384
&gt;&gt; Epoch 88 finished 	ANN training loss 0.004184
&gt;&gt; Epoch 89 finished 	ANN training loss 0.004218
&gt;&gt; Epoch 90 finished 	ANN training loss 0.003687
&gt;&gt; Epoch 91 finished 	ANN training loss 0.004417
&gt;&gt; Epoch 92 finished 	ANN training loss 0.004836
&gt;&gt; Epoch 93 finished 	ANN training loss 0.004638
&gt;&gt; Epoch 94 finished 	ANN training loss 0.004646
&gt;&gt; Epoch 95 finished 	ANN training loss 0.003542
&gt;&gt; Epoch 96 finished 	ANN training loss 0.005069
&gt;&gt; Epoch 97 finished 	ANN training loss 0.003526
&gt;&gt; Epoch 98 finished 	ANN training loss 0.003974
&gt;&gt; Epoch 99 finished 	ANN training loss 0.004375
[END] Fine tuning step
Done.
Accuracy: 0.870000
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[47]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ACCURACY: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">threelayer_acc</span><span class="p">[</span><span class="mi">6</span><span class="p">]))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>ACCURACY: 0.87
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[48]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Collated results</span>
<span class="nb">print</span><span class="p">(</span><span class="n">threelayer_acc</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Most accurate 3-layer setting is Setting &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">threelayer_acc</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="n">threelayer_acc</span><span class="p">))</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[0.90000000000000002, 0.92500000000000004, 0.89500000000000002, 0.87, 0.91000000000000003, 0.875, 0.87]
Most accurate 3-layer setting is Setting 2
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[56]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Create bar graph</span>
<span class="n">plt</span><span class="o">.</span><span class="n">clf</span><span class="p">()</span>
<span class="n">objects</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;[100, 200, 300]&#39;</span><span class="p">,</span> <span class="s1">&#39;[500, 200, 300]&#39;</span><span class="p">,</span> <span class="s1">&#39;[100, 500, 300]&#39;</span><span class="p">,</span> <span class="s1">&#39;[100, 200, 500]&#39;</span><span class="p">,</span> <span class="s1">&#39;[500, 500, 300]&#39;</span><span class="p">,</span> <span class="s1">&#39;[500, 200, 500]&#39;</span><span class="p">,</span> <span class="s1">&#39;[100, 500, 500]&#39;</span><span class="p">)</span>
<span class="n">y_pos</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">objects</span><span class="p">))</span>
<span class="n">performance</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.9</span><span class="p">,</span><span class="mf">0.925</span><span class="p">,</span><span class="mf">0.895</span><span class="p">,</span><span class="mf">0.87</span><span class="p">,</span> <span class="mf">0.91</span><span class="p">,</span> <span class="mf">0.875</span><span class="p">,</span> <span class="mf">0.87</span><span class="p">]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">y_pos</span><span class="p">,</span> <span class="n">performance</span><span class="p">,</span> <span class="n">align</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">y_pos</span><span class="p">,</span> <span class="n">objects</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Layer Settings&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Accuracy&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Three-layer Settings&#39;</span><span class="p">)</span>

 
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">clf</span><span class="p">()</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>




<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAYoAAAE8CAYAAADe7fZ4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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"
>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Different-Pre-training-(RBM)-Learning-Rates:">Different Pre-training (RBM) Learning Rates:<a class="anchor-link" href="#Different-Pre-training-(RBM)-Learning-Rates:">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[49]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Store accuracies</span>
<span class="n">rbm_acc</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Setting 1</strong></p>

<pre><code>learning_rate_rbm=0.01</code></pre>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[50]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">rbm_acc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">deep_belief_net</span><span class="p">(</span><span class="n">learning_rate_rbm</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[START] Pre-training step:
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 48.429943
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 44.728573
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 37.250076
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 31.907465
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 28.755913
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 26.740873
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 25.324945
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 23.896397
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 22.649555
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 21.804190
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 25.185154
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 18.686335
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 10.709770
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 7.455266
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 5.747530
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 4.696509
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 3.895706
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 3.339420
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 3.075150
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 2.861675
[END] Pre-training step
[START] Fine tuning step:
&gt;&gt; Epoch 0 finished 	ANN training loss 1.026684
&gt;&gt; Epoch 1 finished 	ANN training loss 0.738023
&gt;&gt; Epoch 2 finished 	ANN training loss 0.599452
&gt;&gt; Epoch 3 finished 	ANN training loss 0.489238
&gt;&gt; Epoch 4 finished 	ANN training loss 0.437557
&gt;&gt; Epoch 5 finished 	ANN training loss 0.374682
&gt;&gt; Epoch 6 finished 	ANN training loss 0.335208
&gt;&gt; Epoch 7 finished 	ANN training loss 0.306487
&gt;&gt; Epoch 8 finished 	ANN training loss 0.257842
&gt;&gt; Epoch 9 finished 	ANN training loss 0.245150
&gt;&gt; Epoch 10 finished 	ANN training loss 0.201488
&gt;&gt; Epoch 11 finished 	ANN training loss 0.204628
&gt;&gt; Epoch 12 finished 	ANN training loss 0.157771
&gt;&gt; Epoch 13 finished 	ANN training loss 0.138897
&gt;&gt; Epoch 14 finished 	ANN training loss 0.129000
&gt;&gt; Epoch 15 finished 	ANN training loss 0.123893
&gt;&gt; Epoch 16 finished 	ANN training loss 0.107592
&gt;&gt; Epoch 17 finished 	ANN training loss 0.087158
&gt;&gt; Epoch 18 finished 	ANN training loss 0.083540
&gt;&gt; Epoch 19 finished 	ANN training loss 0.117613
&gt;&gt; Epoch 20 finished 	ANN training loss 0.075871
&gt;&gt; Epoch 21 finished 	ANN training loss 0.071079
&gt;&gt; Epoch 22 finished 	ANN training loss 0.073331
&gt;&gt; Epoch 23 finished 	ANN training loss 0.054254
&gt;&gt; Epoch 24 finished 	ANN training loss 0.047814
&gt;&gt; Epoch 25 finished 	ANN training loss 0.040908
&gt;&gt; Epoch 26 finished 	ANN training loss 0.036101
&gt;&gt; Epoch 27 finished 	ANN training loss 0.031959
&gt;&gt; Epoch 28 finished 	ANN training loss 0.033934
&gt;&gt; Epoch 29 finished 	ANN training loss 0.026952
&gt;&gt; Epoch 30 finished 	ANN training loss 0.023128
&gt;&gt; Epoch 31 finished 	ANN training loss 0.026336
&gt;&gt; Epoch 32 finished 	ANN training loss 0.026706
&gt;&gt; Epoch 33 finished 	ANN training loss 0.018220
&gt;&gt; Epoch 34 finished 	ANN training loss 0.015882
&gt;&gt; Epoch 35 finished 	ANN training loss 0.016173
&gt;&gt; Epoch 36 finished 	ANN training loss 0.016196
&gt;&gt; Epoch 37 finished 	ANN training loss 0.013575
&gt;&gt; Epoch 38 finished 	ANN training loss 0.015625
&gt;&gt; Epoch 39 finished 	ANN training loss 0.012308
&gt;&gt; Epoch 40 finished 	ANN training loss 0.010823
&gt;&gt; Epoch 41 finished 	ANN training loss 0.011302
&gt;&gt; Epoch 42 finished 	ANN training loss 0.010032
&gt;&gt; Epoch 43 finished 	ANN training loss 0.011795
&gt;&gt; Epoch 44 finished 	ANN training loss 0.008577
&gt;&gt; Epoch 45 finished 	ANN training loss 0.008509
&gt;&gt; Epoch 46 finished 	ANN training loss 0.007353
&gt;&gt; Epoch 47 finished 	ANN training loss 0.007781
&gt;&gt; Epoch 48 finished 	ANN training loss 0.007838
&gt;&gt; Epoch 49 finished 	ANN training loss 0.006890
&gt;&gt; Epoch 50 finished 	ANN training loss 0.007011
&gt;&gt; Epoch 51 finished 	ANN training loss 0.005143
&gt;&gt; Epoch 52 finished 	ANN training loss 0.005654
&gt;&gt; Epoch 53 finished 	ANN training loss 0.006066
&gt;&gt; Epoch 54 finished 	ANN training loss 0.008010
&gt;&gt; Epoch 55 finished 	ANN training loss 0.005409
&gt;&gt; Epoch 56 finished 	ANN training loss 0.004631
&gt;&gt; Epoch 57 finished 	ANN training loss 0.004697
&gt;&gt; Epoch 58 finished 	ANN training loss 0.004014
&gt;&gt; Epoch 59 finished 	ANN training loss 0.004065
&gt;&gt; Epoch 60 finished 	ANN training loss 0.003648
&gt;&gt; Epoch 61 finished 	ANN training loss 0.003955
&gt;&gt; Epoch 62 finished 	ANN training loss 0.003463
&gt;&gt; Epoch 63 finished 	ANN training loss 0.003597
&gt;&gt; Epoch 64 finished 	ANN training loss 0.003128
&gt;&gt; Epoch 65 finished 	ANN training loss 0.002815
&gt;&gt; Epoch 66 finished 	ANN training loss 0.002781
&gt;&gt; Epoch 67 finished 	ANN training loss 0.003304
&gt;&gt; Epoch 68 finished 	ANN training loss 0.003071
&gt;&gt; Epoch 69 finished 	ANN training loss 0.002651
&gt;&gt; Epoch 70 finished 	ANN training loss 0.002896
&gt;&gt; Epoch 71 finished 	ANN training loss 0.002207
&gt;&gt; Epoch 72 finished 	ANN training loss 0.002331
&gt;&gt; Epoch 73 finished 	ANN training loss 0.002288
&gt;&gt; Epoch 74 finished 	ANN training loss 0.002224
&gt;&gt; Epoch 75 finished 	ANN training loss 0.002153
&gt;&gt; Epoch 76 finished 	ANN training loss 0.002389
&gt;&gt; Epoch 77 finished 	ANN training loss 0.001850
&gt;&gt; Epoch 78 finished 	ANN training loss 0.002531
&gt;&gt; Epoch 79 finished 	ANN training loss 0.001833
&gt;&gt; Epoch 80 finished 	ANN training loss 0.002138
&gt;&gt; Epoch 81 finished 	ANN training loss 0.001977
&gt;&gt; Epoch 82 finished 	ANN training loss 0.001566
&gt;&gt; Epoch 83 finished 	ANN training loss 0.002432
&gt;&gt; Epoch 84 finished 	ANN training loss 0.001725
&gt;&gt; Epoch 85 finished 	ANN training loss 0.001382
&gt;&gt; Epoch 86 finished 	ANN training loss 0.001534
&gt;&gt; Epoch 87 finished 	ANN training loss 0.001267
&gt;&gt; Epoch 88 finished 	ANN training loss 0.001290
&gt;&gt; Epoch 89 finished 	ANN training loss 0.001479
&gt;&gt; Epoch 90 finished 	ANN training loss 0.001209
&gt;&gt; Epoch 91 finished 	ANN training loss 0.001558
&gt;&gt; Epoch 92 finished 	ANN training loss 0.001149
&gt;&gt; Epoch 93 finished 	ANN training loss 0.001014
&gt;&gt; Epoch 94 finished 	ANN training loss 0.002997
&gt;&gt; Epoch 95 finished 	ANN training loss 0.001005
&gt;&gt; Epoch 96 finished 	ANN training loss 0.001130
&gt;&gt; Epoch 97 finished 	ANN training loss 0.001200
&gt;&gt; Epoch 98 finished 	ANN training loss 0.000986
&gt;&gt; Epoch 99 finished 	ANN training loss 0.000986
[END] Fine tuning step
Done.
Accuracy: 0.895000
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[51]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ACCURACY: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">rbm_acc</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>ACCURACY: 0.895
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Setting 2</strong></p>

<pre><code>learning_rate_rbm=0.05</code></pre>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[52]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">rbm_acc</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">deep_belief_net</span><span class="p">(</span><span class="n">learning_rate_rbm</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[START] Pre-training step:
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 49.428436
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 38.784439
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 36.508812
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 32.487255
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 30.027876
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 27.512949
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 28.220713
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 36.800270
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 26.080055
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 27.156790
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 127.447762
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 60.762264
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 132.231400
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 136.848663
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 105.110397
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 122.694519
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 115.857346
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 184.296173
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 178.169037
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 167.853561
[END] Pre-training step
[START] Fine tuning step:
&gt;&gt; Epoch 0 finished 	ANN training loss 0.635793
&gt;&gt; Epoch 1 finished 	ANN training loss 0.481509
&gt;&gt; Epoch 2 finished 	ANN training loss 0.373607
&gt;&gt; Epoch 3 finished 	ANN training loss 0.311064
&gt;&gt; Epoch 4 finished 	ANN training loss 0.288711
&gt;&gt; Epoch 5 finished 	ANN training loss 0.226046
&gt;&gt; Epoch 6 finished 	ANN training loss 0.197164
&gt;&gt; Epoch 7 finished 	ANN training loss 0.192715
&gt;&gt; Epoch 8 finished 	ANN training loss 0.162547
&gt;&gt; Epoch 9 finished 	ANN training loss 0.137399
&gt;&gt; Epoch 10 finished 	ANN training loss 0.124895
&gt;&gt; Epoch 11 finished 	ANN training loss 0.106781
&gt;&gt; Epoch 12 finished 	ANN training loss 0.095121
&gt;&gt; Epoch 13 finished 	ANN training loss 0.085233
&gt;&gt; Epoch 14 finished 	ANN training loss 0.081535
&gt;&gt; Epoch 15 finished 	ANN training loss 0.063347
&gt;&gt; Epoch 16 finished 	ANN training loss 0.055384
&gt;&gt; Epoch 17 finished 	ANN training loss 0.052163
&gt;&gt; Epoch 18 finished 	ANN training loss 0.049630
&gt;&gt; Epoch 19 finished 	ANN training loss 0.042883
&gt;&gt; Epoch 20 finished 	ANN training loss 0.040378
&gt;&gt; Epoch 21 finished 	ANN training loss 0.032656
&gt;&gt; Epoch 22 finished 	ANN training loss 0.029055
&gt;&gt; Epoch 23 finished 	ANN training loss 0.028338
&gt;&gt; Epoch 24 finished 	ANN training loss 0.025327
&gt;&gt; Epoch 25 finished 	ANN training loss 0.023662
&gt;&gt; Epoch 26 finished 	ANN training loss 0.020544
&gt;&gt; Epoch 27 finished 	ANN training loss 0.021189
&gt;&gt; Epoch 28 finished 	ANN training loss 0.017175
&gt;&gt; Epoch 29 finished 	ANN training loss 0.018268
&gt;&gt; Epoch 30 finished 	ANN training loss 0.015474
&gt;&gt; Epoch 31 finished 	ANN training loss 0.015518
&gt;&gt; Epoch 32 finished 	ANN training loss 0.017359
&gt;&gt; Epoch 33 finished 	ANN training loss 0.012366
&gt;&gt; Epoch 34 finished 	ANN training loss 0.010943
&gt;&gt; Epoch 35 finished 	ANN training loss 0.011026
&gt;&gt; Epoch 36 finished 	ANN training loss 0.009873
&gt;&gt; Epoch 37 finished 	ANN training loss 0.009909
&gt;&gt; Epoch 38 finished 	ANN training loss 0.008420
&gt;&gt; Epoch 39 finished 	ANN training loss 0.009114
&gt;&gt; Epoch 40 finished 	ANN training loss 0.007491
&gt;&gt; Epoch 41 finished 	ANN training loss 0.007807
&gt;&gt; Epoch 42 finished 	ANN training loss 0.006912
&gt;&gt; Epoch 43 finished 	ANN training loss 0.006836
&gt;&gt; Epoch 44 finished 	ANN training loss 0.006192
&gt;&gt; Epoch 45 finished 	ANN training loss 0.005974
&gt;&gt; Epoch 46 finished 	ANN training loss 0.005571
&gt;&gt; Epoch 47 finished 	ANN training loss 0.005055
&gt;&gt; Epoch 48 finished 	ANN training loss 0.005414
&gt;&gt; Epoch 49 finished 	ANN training loss 0.004368
&gt;&gt; Epoch 50 finished 	ANN training loss 0.004063
&gt;&gt; Epoch 51 finished 	ANN training loss 0.004289
&gt;&gt; Epoch 52 finished 	ANN training loss 0.004410
&gt;&gt; Epoch 53 finished 	ANN training loss 0.003731
&gt;&gt; Epoch 54 finished 	ANN training loss 0.003492
&gt;&gt; Epoch 55 finished 	ANN training loss 0.003437
&gt;&gt; Epoch 56 finished 	ANN training loss 0.003974
&gt;&gt; Epoch 57 finished 	ANN training loss 0.003274
&gt;&gt; Epoch 58 finished 	ANN training loss 0.003593
&gt;&gt; Epoch 59 finished 	ANN training loss 0.003117
&gt;&gt; Epoch 60 finished 	ANN training loss 0.003042
&gt;&gt; Epoch 61 finished 	ANN training loss 0.002655
&gt;&gt; Epoch 62 finished 	ANN training loss 0.002706
&gt;&gt; Epoch 63 finished 	ANN training loss 0.002677
&gt;&gt; Epoch 64 finished 	ANN training loss 0.002640
&gt;&gt; Epoch 65 finished 	ANN training loss 0.002286
&gt;&gt; Epoch 66 finished 	ANN training loss 0.002343
&gt;&gt; Epoch 67 finished 	ANN training loss 0.002021
&gt;&gt; Epoch 68 finished 	ANN training loss 0.001908
&gt;&gt; Epoch 69 finished 	ANN training loss 0.001922
&gt;&gt; Epoch 70 finished 	ANN training loss 0.002169
&gt;&gt; Epoch 71 finished 	ANN training loss 0.001971
&gt;&gt; Epoch 72 finished 	ANN training loss 0.001913
&gt;&gt; Epoch 73 finished 	ANN training loss 0.001818
&gt;&gt; Epoch 74 finished 	ANN training loss 0.001914
&gt;&gt; Epoch 75 finished 	ANN training loss 0.001864
&gt;&gt; Epoch 76 finished 	ANN training loss 0.001706
&gt;&gt; Epoch 77 finished 	ANN training loss 0.001824
&gt;&gt; Epoch 78 finished 	ANN training loss 0.001557
&gt;&gt; Epoch 79 finished 	ANN training loss 0.001725
&gt;&gt; Epoch 80 finished 	ANN training loss 0.001677
&gt;&gt; Epoch 81 finished 	ANN training loss 0.001914
&gt;&gt; Epoch 82 finished 	ANN training loss 0.001608
&gt;&gt; Epoch 83 finished 	ANN training loss 0.001835
&gt;&gt; Epoch 84 finished 	ANN training loss 0.001420
&gt;&gt; Epoch 85 finished 	ANN training loss 0.001372
&gt;&gt; Epoch 86 finished 	ANN training loss 0.001213
&gt;&gt; Epoch 87 finished 	ANN training loss 0.001284
&gt;&gt; Epoch 88 finished 	ANN training loss 0.001029
&gt;&gt; Epoch 89 finished 	ANN training loss 0.001252
&gt;&gt; Epoch 90 finished 	ANN training loss 0.001041
&gt;&gt; Epoch 91 finished 	ANN training loss 0.001204
&gt;&gt; Epoch 92 finished 	ANN training loss 0.001129
&gt;&gt; Epoch 93 finished 	ANN training loss 0.001000
&gt;&gt; Epoch 94 finished 	ANN training loss 0.001000
&gt;&gt; Epoch 95 finished 	ANN training loss 0.000882
&gt;&gt; Epoch 96 finished 	ANN training loss 0.000831
&gt;&gt; Epoch 97 finished 	ANN training loss 0.000905
&gt;&gt; Epoch 98 finished 	ANN training loss 0.000854
&gt;&gt; Epoch 99 finished 	ANN training loss 0.000798
[END] Fine tuning step
Done.
Accuracy: 0.910000
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[53]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ACCURACY: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">rbm_acc</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>ACCURACY: 0.91
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Setting 3</strong></p>

<pre><code>learning_rate_rbm=0.1</code></pre>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[54]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">rbm_acc</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">deep_belief_net</span><span class="p">(</span><span class="n">learning_rate_rbm</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[START] Pre-training step:
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 110.904007
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 125.835686
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 164.992279
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 136.079025
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 230.743042
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 214.991425
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 219.420624
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 186.010040
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 267.364807
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 164.378845
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 3380.159424
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 2683.055664
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 4320.045898
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 5420.953125
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 4888.116211
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 8385.436523
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 5375.637695
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 5713.787109
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 6178.853516
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 5392.146973
[END] Pre-training step
[START] Fine tuning step:
&gt;&gt; Epoch 0 finished 	ANN training loss 0.974992
&gt;&gt; Epoch 1 finished 	ANN training loss 0.661995
&gt;&gt; Epoch 2 finished 	ANN training loss 0.549957
&gt;&gt; Epoch 3 finished 	ANN training loss 0.470238
&gt;&gt; Epoch 4 finished 	ANN training loss 0.430156
&gt;&gt; Epoch 5 finished 	ANN training loss 0.356329
&gt;&gt; Epoch 6 finished 	ANN training loss 0.332026
&gt;&gt; Epoch 7 finished 	ANN training loss 0.296107
&gt;&gt; Epoch 8 finished 	ANN training loss 0.245873
&gt;&gt; Epoch 9 finished 	ANN training loss 0.238457
&gt;&gt; Epoch 10 finished 	ANN training loss 0.200607
&gt;&gt; Epoch 11 finished 	ANN training loss 0.197177
&gt;&gt; Epoch 12 finished 	ANN training loss 0.182111
&gt;&gt; Epoch 13 finished 	ANN training loss 0.160953
&gt;&gt; Epoch 14 finished 	ANN training loss 0.162905
&gt;&gt; Epoch 15 finished 	ANN training loss 0.161946
&gt;&gt; Epoch 16 finished 	ANN training loss 0.133252
&gt;&gt; Epoch 17 finished 	ANN training loss 0.121509
&gt;&gt; Epoch 18 finished 	ANN training loss 0.109463
&gt;&gt; Epoch 19 finished 	ANN training loss 0.106086
&gt;&gt; Epoch 20 finished 	ANN training loss 0.093620
&gt;&gt; Epoch 21 finished 	ANN training loss 0.081935
&gt;&gt; Epoch 22 finished 	ANN training loss 0.081763
&gt;&gt; Epoch 23 finished 	ANN training loss 0.077425
&gt;&gt; Epoch 24 finished 	ANN training loss 0.071794
&gt;&gt; Epoch 25 finished 	ANN training loss 0.065713
&gt;&gt; Epoch 26 finished 	ANN training loss 0.064029
&gt;&gt; Epoch 27 finished 	ANN training loss 0.060664
&gt;&gt; Epoch 28 finished 	ANN training loss 0.056845
&gt;&gt; Epoch 29 finished 	ANN training loss 0.050273
&gt;&gt; Epoch 30 finished 	ANN training loss 0.052669
&gt;&gt; Epoch 31 finished 	ANN training loss 0.047205
&gt;&gt; Epoch 32 finished 	ANN training loss 0.039564
&gt;&gt; Epoch 33 finished 	ANN training loss 0.039708
&gt;&gt; Epoch 34 finished 	ANN training loss 0.040156
&gt;&gt; Epoch 35 finished 	ANN training loss 0.037226
&gt;&gt; Epoch 36 finished 	ANN training loss 0.039703
&gt;&gt; Epoch 37 finished 	ANN training loss 0.036133
&gt;&gt; Epoch 38 finished 	ANN training loss 0.031029
&gt;&gt; Epoch 39 finished 	ANN training loss 0.028533
&gt;&gt; Epoch 40 finished 	ANN training loss 0.027219
&gt;&gt; Epoch 41 finished 	ANN training loss 0.025730
&gt;&gt; Epoch 42 finished 	ANN training loss 0.029454
&gt;&gt; Epoch 43 finished 	ANN training loss 0.023459
&gt;&gt; Epoch 44 finished 	ANN training loss 0.024340
&gt;&gt; Epoch 45 finished 	ANN training loss 0.021740
&gt;&gt; Epoch 46 finished 	ANN training loss 0.025161
&gt;&gt; Epoch 47 finished 	ANN training loss 0.018554
&gt;&gt; Epoch 48 finished 	ANN training loss 0.018711
&gt;&gt; Epoch 49 finished 	ANN training loss 0.019660
&gt;&gt; Epoch 50 finished 	ANN training loss 0.017450
&gt;&gt; Epoch 51 finished 	ANN training loss 0.015244
&gt;&gt; Epoch 52 finished 	ANN training loss 0.014013
&gt;&gt; Epoch 53 finished 	ANN training loss 0.013459
&gt;&gt; Epoch 54 finished 	ANN training loss 0.014766
&gt;&gt; Epoch 55 finished 	ANN training loss 0.012963
&gt;&gt; Epoch 56 finished 	ANN training loss 0.012350
&gt;&gt; Epoch 57 finished 	ANN training loss 0.012016
&gt;&gt; Epoch 58 finished 	ANN training loss 0.012834
&gt;&gt; Epoch 59 finished 	ANN training loss 0.012906
&gt;&gt; Epoch 60 finished 	ANN training loss 0.011110
&gt;&gt; Epoch 61 finished 	ANN training loss 0.011643
&gt;&gt; Epoch 62 finished 	ANN training loss 0.010738
&gt;&gt; Epoch 63 finished 	ANN training loss 0.011479
&gt;&gt; Epoch 64 finished 	ANN training loss 0.010109
&gt;&gt; Epoch 65 finished 	ANN training loss 0.009724
&gt;&gt; Epoch 66 finished 	ANN training loss 0.008283
&gt;&gt; Epoch 67 finished 	ANN training loss 0.008818
&gt;&gt; Epoch 68 finished 	ANN training loss 0.009003
&gt;&gt; Epoch 69 finished 	ANN training loss 0.007449
&gt;&gt; Epoch 70 finished 	ANN training loss 0.007127
&gt;&gt; Epoch 71 finished 	ANN training loss 0.007380
&gt;&gt; Epoch 72 finished 	ANN training loss 0.006818
&gt;&gt; Epoch 73 finished 	ANN training loss 0.005973
&gt;&gt; Epoch 74 finished 	ANN training loss 0.006184
&gt;&gt; Epoch 75 finished 	ANN training loss 0.006140
&gt;&gt; Epoch 76 finished 	ANN training loss 0.005986
&gt;&gt; Epoch 77 finished 	ANN training loss 0.005218
&gt;&gt; Epoch 78 finished 	ANN training loss 0.005357
&gt;&gt; Epoch 79 finished 	ANN training loss 0.005804
&gt;&gt; Epoch 80 finished 	ANN training loss 0.006060
&gt;&gt; Epoch 81 finished 	ANN training loss 0.005213
&gt;&gt; Epoch 82 finished 	ANN training loss 0.005856
&gt;&gt; Epoch 83 finished 	ANN training loss 0.007979
&gt;&gt; Epoch 84 finished 	ANN training loss 0.004489
&gt;&gt; Epoch 85 finished 	ANN training loss 0.004373
&gt;&gt; Epoch 86 finished 	ANN training loss 0.005216
&gt;&gt; Epoch 87 finished 	ANN training loss 0.004771
&gt;&gt; Epoch 88 finished 	ANN training loss 0.005004
&gt;&gt; Epoch 89 finished 	ANN training loss 0.004507
&gt;&gt; Epoch 90 finished 	ANN training loss 0.004008
&gt;&gt; Epoch 91 finished 	ANN training loss 0.004959
&gt;&gt; Epoch 92 finished 	ANN training loss 0.003502
&gt;&gt; Epoch 93 finished 	ANN training loss 0.003327
&gt;&gt; Epoch 94 finished 	ANN training loss 0.003738
&gt;&gt; Epoch 95 finished 	ANN training loss 0.003221
&gt;&gt; Epoch 96 finished 	ANN training loss 0.002786
&gt;&gt; Epoch 97 finished 	ANN training loss 0.002647
&gt;&gt; Epoch 98 finished 	ANN training loss 0.002955
&gt;&gt; Epoch 99 finished 	ANN training loss 0.002613
[END] Fine tuning step
Done.
Accuracy: 0.885000
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[55]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ACCURACY: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">rbm_acc</span><span class="p">[</span><span class="mi">2</span><span class="p">]))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>ACCURACY: 0.885
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Setting 4</strong></p>

<pre><code>learning_rate_rbm=0.5</code></pre>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[56]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">rbm_acc</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">=</span> <span class="n">deep_belief_net</span><span class="p">(</span><span class="n">learning_rate_rbm</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[START] Pre-training step:
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 31779.625000
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 189873.421875
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 180143.687500
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 224904.078125
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 170434.359375
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 164641.656250
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 263771.000000
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 274975.156250
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 245861.359375
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 189474.312500
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 846481653760.000000
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 790017802240.000000
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 3146633969664.000000
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 2016595542016.000000
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 1990187155456.000000
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 983176052736.000000
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 2778203160576.000000
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 1564428730368.000000
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 2989057114112.000000
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 1582784708608.000000
[END] Pre-training step
[START] Fine tuning step:
&gt;&gt; Epoch 0 finished 	ANN training loss 2.300706
&gt;&gt; Epoch 1 finished 	ANN training loss 2.298146
&gt;&gt; Epoch 2 finished 	ANN training loss 2.293770
&gt;&gt; Epoch 3 finished 	ANN training loss 2.284529
&gt;&gt; Epoch 4 finished 	ANN training loss 2.262846
&gt;&gt; Epoch 5 finished 	ANN training loss 2.232202
&gt;&gt; Epoch 6 finished 	ANN training loss 2.208014
&gt;&gt; Epoch 7 finished 	ANN training loss 2.189869
&gt;&gt; Epoch 8 finished 	ANN training loss 2.179913
&gt;&gt; Epoch 9 finished 	ANN training loss 2.171853
&gt;&gt; Epoch 10 finished 	ANN training loss 2.166882
&gt;&gt; Epoch 11 finished 	ANN training loss 2.160546
&gt;&gt; Epoch 12 finished 	ANN training loss 2.154613
&gt;&gt; Epoch 13 finished 	ANN training loss 2.150535
&gt;&gt; Epoch 14 finished 	ANN training loss 2.142323
&gt;&gt; Epoch 15 finished 	ANN training loss 2.138678
&gt;&gt; Epoch 16 finished 	ANN training loss 2.133862
&gt;&gt; Epoch 17 finished 	ANN training loss 2.131676
&gt;&gt; Epoch 18 finished 	ANN training loss 2.130769
&gt;&gt; Epoch 19 finished 	ANN training loss 2.129317
&gt;&gt; Epoch 20 finished 	ANN training loss 2.127237
&gt;&gt; Epoch 21 finished 	ANN training loss 2.125255
&gt;&gt; Epoch 22 finished 	ANN training loss 2.123509
&gt;&gt; Epoch 23 finished 	ANN training loss 2.121984
&gt;&gt; Epoch 24 finished 	ANN training loss 2.121421
&gt;&gt; Epoch 25 finished 	ANN training loss 2.118954
&gt;&gt; Epoch 26 finished 	ANN training loss 2.114867
&gt;&gt; Epoch 27 finished 	ANN training loss 2.107084
&gt;&gt; Epoch 28 finished 	ANN training loss 2.096528
&gt;&gt; Epoch 29 finished 	ANN training loss 2.091860
&gt;&gt; Epoch 30 finished 	ANN training loss 2.085374
&gt;&gt; Epoch 31 finished 	ANN training loss 2.083824
&gt;&gt; Epoch 32 finished 	ANN training loss 2.077761
&gt;&gt; Epoch 33 finished 	ANN training loss 2.065537
&gt;&gt; Epoch 34 finished 	ANN training loss 2.055930
&gt;&gt; Epoch 35 finished 	ANN training loss 2.034392
&gt;&gt; Epoch 36 finished 	ANN training loss 2.007093
&gt;&gt; Epoch 37 finished 	ANN training loss 1.969934
&gt;&gt; Epoch 38 finished 	ANN training loss 1.931400
&gt;&gt; Epoch 39 finished 	ANN training loss 1.913904
&gt;&gt; Epoch 40 finished 	ANN training loss 1.857626
&gt;&gt; Epoch 41 finished 	ANN training loss 1.823647
&gt;&gt; Epoch 42 finished 	ANN training loss 1.778742
&gt;&gt; Epoch 43 finished 	ANN training loss 1.753495
&gt;&gt; Epoch 44 finished 	ANN training loss 1.710771
&gt;&gt; Epoch 45 finished 	ANN training loss 1.720669
&gt;&gt; Epoch 46 finished 	ANN training loss 1.646131
&gt;&gt; Epoch 47 finished 	ANN training loss 1.614182
&gt;&gt; Epoch 48 finished 	ANN training loss 1.551993
&gt;&gt; Epoch 49 finished 	ANN training loss 1.526839
&gt;&gt; Epoch 50 finished 	ANN training loss 1.491676
&gt;&gt; Epoch 51 finished 	ANN training loss 1.468890
&gt;&gt; Epoch 52 finished 	ANN training loss 1.458467
&gt;&gt; Epoch 53 finished 	ANN training loss 1.398917
&gt;&gt; Epoch 54 finished 	ANN training loss 1.390435
&gt;&gt; Epoch 55 finished 	ANN training loss 1.355867
&gt;&gt; Epoch 56 finished 	ANN training loss 1.329674
&gt;&gt; Epoch 57 finished 	ANN training loss 1.339947
&gt;&gt; Epoch 58 finished 	ANN training loss 1.304133
&gt;&gt; Epoch 59 finished 	ANN training loss 1.327821
&gt;&gt; Epoch 60 finished 	ANN training loss 1.268801
&gt;&gt; Epoch 61 finished 	ANN training loss 1.241808
&gt;&gt; Epoch 62 finished 	ANN training loss 1.202110
&gt;&gt; Epoch 63 finished 	ANN training loss 1.185184
&gt;&gt; Epoch 64 finished 	ANN training loss 1.178161
&gt;&gt; Epoch 65 finished 	ANN training loss 1.180296
&gt;&gt; Epoch 66 finished 	ANN training loss 1.135762
&gt;&gt; Epoch 67 finished 	ANN training loss 1.127119
&gt;&gt; Epoch 68 finished 	ANN training loss 1.137036
&gt;&gt; Epoch 69 finished 	ANN training loss 1.104631
&gt;&gt; Epoch 70 finished 	ANN training loss 1.115334
&gt;&gt; Epoch 71 finished 	ANN training loss 1.089910
&gt;&gt; Epoch 72 finished 	ANN training loss 1.119207
&gt;&gt; Epoch 73 finished 	ANN training loss 1.065434
&gt;&gt; Epoch 74 finished 	ANN training loss 0.999465
&gt;&gt; Epoch 75 finished 	ANN training loss 0.991289
&gt;&gt; Epoch 76 finished 	ANN training loss 1.002694
&gt;&gt; Epoch 77 finished 	ANN training loss 1.033328
&gt;&gt; Epoch 78 finished 	ANN training loss 0.946839
&gt;&gt; Epoch 79 finished 	ANN training loss 0.912989
&gt;&gt; Epoch 80 finished 	ANN training loss 0.924845
&gt;&gt; Epoch 81 finished 	ANN training loss 0.935192
&gt;&gt; Epoch 82 finished 	ANN training loss 0.874035
&gt;&gt; Epoch 83 finished 	ANN training loss 0.920369
&gt;&gt; Epoch 84 finished 	ANN training loss 0.896667
&gt;&gt; Epoch 85 finished 	ANN training loss 0.860983
&gt;&gt; Epoch 86 finished 	ANN training loss 0.876610
&gt;&gt; Epoch 87 finished 	ANN training loss 0.844067
&gt;&gt; Epoch 88 finished 	ANN training loss 0.871358
&gt;&gt; Epoch 89 finished 	ANN training loss 0.841526
&gt;&gt; Epoch 90 finished 	ANN training loss 0.862323
&gt;&gt; Epoch 91 finished 	ANN training loss 0.847701
&gt;&gt; Epoch 92 finished 	ANN training loss 0.840739
&gt;&gt; Epoch 93 finished 	ANN training loss 0.871223
&gt;&gt; Epoch 94 finished 	ANN training loss 0.849284
&gt;&gt; Epoch 95 finished 	ANN training loss 0.854567
&gt;&gt; Epoch 96 finished 	ANN training loss 0.837644
&gt;&gt; Epoch 97 finished 	ANN training loss 0.807640
&gt;&gt; Epoch 98 finished 	ANN training loss 0.855461
&gt;&gt; Epoch 99 finished 	ANN training loss 0.807312
[END] Fine tuning step
Done.
Accuracy: 0.595000
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[57]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ACCURACY: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">rbm_acc</span><span class="p">[</span><span class="mi">3</span><span class="p">]))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>ACCURACY: 0.595
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Setting 5</strong></p>

<pre><code>learning_rate_rbm=1.0</code></pre>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[58]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">rbm_acc</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span> <span class="o">=</span> <span class="n">deep_belief_net</span><span class="p">(</span><span class="n">learning_rate_rbm</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[START] Pre-training step:
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 22193574.000000
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 34716188.000000
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 30751990.000000
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 29281052.000000
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 18426390.000000
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 17048326.000000
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 24133926.000000
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 26676414.000000
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 26827366.000000
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 27220286.000000
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 2080750301580623872.000000
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 811598117131517952.000000
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 1798525045622964224.000000
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 3235358044707618816.000000
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 2510459099485831168.000000
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 2100549757217800192.000000
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 2453775701661188096.000000
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 3079025633179729920.000000
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 4631429398971547648.000000
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 2737053877441396736.000000
[END] Pre-training step
[START] Fine tuning step:
&gt;&gt; Epoch 0 finished 	ANN training loss 2.297242
&gt;&gt; Epoch 1 finished 	ANN training loss 2.274268
&gt;&gt; Epoch 2 finished 	ANN training loss 2.262210
&gt;&gt; Epoch 3 finished 	ANN training loss 2.240759
&gt;&gt; Epoch 4 finished 	ANN training loss 2.202708
&gt;&gt; Epoch 5 finished 	ANN training loss 2.157465
&gt;&gt; Epoch 6 finished 	ANN training loss 2.086788
&gt;&gt; Epoch 7 finished 	ANN training loss 2.038932
&gt;&gt; Epoch 8 finished 	ANN training loss 2.001882
&gt;&gt; Epoch 9 finished 	ANN training loss 1.974601
&gt;&gt; Epoch 10 finished 	ANN training loss 1.962472
&gt;&gt; Epoch 11 finished 	ANN training loss 1.934985
&gt;&gt; Epoch 12 finished 	ANN training loss 1.925109
&gt;&gt; Epoch 13 finished 	ANN training loss 1.917198
&gt;&gt; Epoch 14 finished 	ANN training loss 1.889538
&gt;&gt; Epoch 15 finished 	ANN training loss 1.873553
&gt;&gt; Epoch 16 finished 	ANN training loss 1.865580
&gt;&gt; Epoch 17 finished 	ANN training loss 1.859376
&gt;&gt; Epoch 18 finished 	ANN training loss 1.865465
&gt;&gt; Epoch 19 finished 	ANN training loss 1.857389
&gt;&gt; Epoch 20 finished 	ANN training loss 1.832351
&gt;&gt; Epoch 21 finished 	ANN training loss 1.842289
&gt;&gt; Epoch 22 finished 	ANN training loss 1.805307
&gt;&gt; Epoch 23 finished 	ANN training loss 1.806518
&gt;&gt; Epoch 24 finished 	ANN training loss 1.818427
&gt;&gt; Epoch 25 finished 	ANN training loss 1.812820
&gt;&gt; Epoch 26 finished 	ANN training loss 1.800590
&gt;&gt; Epoch 27 finished 	ANN training loss 1.821038
&gt;&gt; Epoch 28 finished 	ANN training loss 1.789669
&gt;&gt; Epoch 29 finished 	ANN training loss 1.787685
&gt;&gt; Epoch 30 finished 	ANN training loss 1.762848
&gt;&gt; Epoch 31 finished 	ANN training loss 1.769935
&gt;&gt; Epoch 32 finished 	ANN training loss 1.770095
&gt;&gt; Epoch 33 finished 	ANN training loss 1.772374
&gt;&gt; Epoch 34 finished 	ANN training loss 1.816199
&gt;&gt; Epoch 35 finished 	ANN training loss 1.799861
&gt;&gt; Epoch 36 finished 	ANN training loss 1.791363
&gt;&gt; Epoch 37 finished 	ANN training loss 1.752728
&gt;&gt; Epoch 38 finished 	ANN training loss 1.755200
&gt;&gt; Epoch 39 finished 	ANN training loss 1.746279
&gt;&gt; Epoch 40 finished 	ANN training loss 1.750928
&gt;&gt; Epoch 41 finished 	ANN training loss 1.775592
&gt;&gt; Epoch 42 finished 	ANN training loss 1.726165
&gt;&gt; Epoch 43 finished 	ANN training loss 1.750028
&gt;&gt; Epoch 44 finished 	ANN training loss 1.756253
&gt;&gt; Epoch 45 finished 	ANN training loss 1.736169
&gt;&gt; Epoch 46 finished 	ANN training loss 1.726813
&gt;&gt; Epoch 47 finished 	ANN training loss 1.746619
&gt;&gt; Epoch 48 finished 	ANN training loss 1.724770
&gt;&gt; Epoch 49 finished 	ANN training loss 1.783655
&gt;&gt; Epoch 50 finished 	ANN training loss 1.711332
&gt;&gt; Epoch 51 finished 	ANN training loss 1.733179
&gt;&gt; Epoch 52 finished 	ANN training loss 1.706090
&gt;&gt; Epoch 53 finished 	ANN training loss 1.755092
&gt;&gt; Epoch 54 finished 	ANN training loss 1.724696
&gt;&gt; Epoch 55 finished 	ANN training loss 1.702846
&gt;&gt; Epoch 56 finished 	ANN training loss 1.700244
&gt;&gt; Epoch 57 finished 	ANN training loss 1.698678
&gt;&gt; Epoch 58 finished 	ANN training loss 1.721877
&gt;&gt; Epoch 59 finished 	ANN training loss 1.772423
&gt;&gt; Epoch 60 finished 	ANN training loss 1.685490
&gt;&gt; Epoch 61 finished 	ANN training loss 1.689682
&gt;&gt; Epoch 62 finished 	ANN training loss 1.691927
&gt;&gt; Epoch 63 finished 	ANN training loss 1.701385
&gt;&gt; Epoch 64 finished 	ANN training loss 1.699016
&gt;&gt; Epoch 65 finished 	ANN training loss 1.684073
&gt;&gt; Epoch 66 finished 	ANN training loss 1.704044
&gt;&gt; Epoch 67 finished 	ANN training loss 1.715825
&gt;&gt; Epoch 68 finished 	ANN training loss 1.675861
&gt;&gt; Epoch 69 finished 	ANN training loss 1.701425
&gt;&gt; Epoch 70 finished 	ANN training loss 1.710229
&gt;&gt; Epoch 71 finished 	ANN training loss 1.676245
&gt;&gt; Epoch 72 finished 	ANN training loss 1.726708
&gt;&gt; Epoch 73 finished 	ANN training loss 1.713694
&gt;&gt; Epoch 74 finished 	ANN training loss 1.668636
&gt;&gt; Epoch 75 finished 	ANN training loss 1.685295
&gt;&gt; Epoch 76 finished 	ANN training loss 1.657891
&gt;&gt; Epoch 77 finished 	ANN training loss 1.718065
&gt;&gt; Epoch 78 finished 	ANN training loss 1.752952
&gt;&gt; Epoch 79 finished 	ANN training loss 1.709124
&gt;&gt; Epoch 80 finished 	ANN training loss 1.661495
&gt;&gt; Epoch 81 finished 	ANN training loss 1.670639
&gt;&gt; Epoch 82 finished 	ANN training loss 1.656527
&gt;&gt; Epoch 83 finished 	ANN training loss 1.761409
&gt;&gt; Epoch 84 finished 	ANN training loss 1.702537
&gt;&gt; Epoch 85 finished 	ANN training loss 1.722679
&gt;&gt; Epoch 86 finished 	ANN training loss 1.662428
&gt;&gt; Epoch 87 finished 	ANN training loss 1.792198
&gt;&gt; Epoch 88 finished 	ANN training loss 1.753308
&gt;&gt; Epoch 89 finished 	ANN training loss 1.743749
&gt;&gt; Epoch 90 finished 	ANN training loss 1.797779
&gt;&gt; Epoch 91 finished 	ANN training loss 1.681832
&gt;&gt; Epoch 92 finished 	ANN training loss 1.715062
&gt;&gt; Epoch 93 finished 	ANN training loss 1.711773
&gt;&gt; Epoch 94 finished 	ANN training loss 1.648828
&gt;&gt; Epoch 95 finished 	ANN training loss 1.763949
&gt;&gt; Epoch 96 finished 	ANN training loss 1.747831
&gt;&gt; Epoch 97 finished 	ANN training loss 1.678146
&gt;&gt; Epoch 98 finished 	ANN training loss 1.671631
&gt;&gt; Epoch 99 finished 	ANN training loss 1.641885
[END] Fine tuning step
Done.
Accuracy: 0.215000
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[59]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ACCURACY: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">rbm_acc</span><span class="p">[</span><span class="mi">4</span><span class="p">]))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>ACCURACY: 0.215
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[60]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Collated results</span>
<span class="nb">print</span><span class="p">(</span><span class="n">rbm_acc</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Most accurate rbm learning_rate setting is Setting &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">rbm_acc</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="n">rbm_acc</span><span class="p">))</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[0.89500000000000002, 0.91000000000000003, 0.88500000000000001, 0.59499999999999997, 0.215]
Most accurate rbm learning_rate setting is Setting 2
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[57]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Create bar graph</span>
<span class="n">plt</span><span class="o">.</span><span class="n">clf</span><span class="p">()</span>
<span class="n">objects</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;0.01&#39;</span><span class="p">,</span> <span class="s1">&#39;0.05&#39;</span><span class="p">,</span> <span class="s1">&#39;0.10&#39;</span><span class="p">,</span> <span class="s1">&#39;0.50&#39;</span><span class="p">,</span> <span class="s1">&#39;1.0&#39;</span><span class="p">)</span>
<span class="n">y_pos</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">objects</span><span class="p">))</span>
<span class="n">performance</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.895</span><span class="p">,</span><span class="mf">0.91</span><span class="p">,</span><span class="mf">0.885</span><span class="p">,</span><span class="mf">0.595</span><span class="p">,</span> <span class="mf">0.215</span><span class="p">]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">y_pos</span><span class="p">,</span> <span class="n">performance</span><span class="p">,</span> <span class="n">align</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">y_pos</span><span class="p">,</span> <span class="n">objects</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Settings&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Accuracy&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;learning_rate_rbm Settings&#39;</span><span class="p">)</span>
 
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">clf</span><span class="p">()</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>




<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAYUAAAEgCAYAAABb8m8VAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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"
>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Different-Learning-Rates:">Different Learning Rates:<a class="anchor-link" href="#Different-Learning-Rates:">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[61]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Store accuracies</span>
<span class="n">learning_rate_acc</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Setting 1</strong></p>

<pre><code>learning_rate=0.01</code></pre>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[62]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learning_rate_acc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">deep_belief_net</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[START] Pre-training step:
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 80.508308
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 56.202160
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 76.771797
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 61.637432
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 73.832054
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 68.935799
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 89.181679
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 76.366074
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 89.140671
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 61.519100
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 143.703171
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 171.010208
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 214.397614
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 192.322067
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 199.107819
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 192.237610
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 207.964966
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 225.997635
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 222.943314
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 298.965149
[END] Pre-training step
[START] Fine tuning step:
&gt;&gt; Epoch 0 finished 	ANN training loss 1.605582
&gt;&gt; Epoch 1 finished 	ANN training loss 1.256691
&gt;&gt; Epoch 2 finished 	ANN training loss 1.041892
&gt;&gt; Epoch 3 finished 	ANN training loss 0.908176
&gt;&gt; Epoch 4 finished 	ANN training loss 0.817185
&gt;&gt; Epoch 5 finished 	ANN training loss 0.746655
&gt;&gt; Epoch 6 finished 	ANN training loss 0.692895
&gt;&gt; Epoch 7 finished 	ANN training loss 0.651748
&gt;&gt; Epoch 8 finished 	ANN training loss 0.615956
&gt;&gt; Epoch 9 finished 	ANN training loss 0.591655
&gt;&gt; Epoch 10 finished 	ANN training loss 0.563548
&gt;&gt; Epoch 11 finished 	ANN training loss 0.540059
&gt;&gt; Epoch 12 finished 	ANN training loss 0.524468
&gt;&gt; Epoch 13 finished 	ANN training loss 0.504133
&gt;&gt; Epoch 14 finished 	ANN training loss 0.490128
&gt;&gt; Epoch 15 finished 	ANN training loss 0.474626
&gt;&gt; Epoch 16 finished 	ANN training loss 0.467938
&gt;&gt; Epoch 17 finished 	ANN training loss 0.452328
&gt;&gt; Epoch 18 finished 	ANN training loss 0.441225
&gt;&gt; Epoch 19 finished 	ANN training loss 0.429239
&gt;&gt; Epoch 20 finished 	ANN training loss 0.423407
&gt;&gt; Epoch 21 finished 	ANN training loss 0.413563
&gt;&gt; Epoch 22 finished 	ANN training loss 0.404902
&gt;&gt; Epoch 23 finished 	ANN training loss 0.395966
&gt;&gt; Epoch 24 finished 	ANN training loss 0.389757
&gt;&gt; Epoch 25 finished 	ANN training loss 0.382781
&gt;&gt; Epoch 26 finished 	ANN training loss 0.375894
&gt;&gt; Epoch 27 finished 	ANN training loss 0.365179
&gt;&gt; Epoch 28 finished 	ANN training loss 0.359713
&gt;&gt; Epoch 29 finished 	ANN training loss 0.352262
&gt;&gt; Epoch 30 finished 	ANN training loss 0.349488
&gt;&gt; Epoch 31 finished 	ANN training loss 0.340875
&gt;&gt; Epoch 32 finished 	ANN training loss 0.338723
&gt;&gt; Epoch 33 finished 	ANN training loss 0.333318
&gt;&gt; Epoch 34 finished 	ANN training loss 0.326745
&gt;&gt; Epoch 35 finished 	ANN training loss 0.322652
&gt;&gt; Epoch 36 finished 	ANN training loss 0.316992
&gt;&gt; Epoch 37 finished 	ANN training loss 0.313476
&gt;&gt; Epoch 38 finished 	ANN training loss 0.306860
&gt;&gt; Epoch 39 finished 	ANN training loss 0.304439
&gt;&gt; Epoch 40 finished 	ANN training loss 0.297698
&gt;&gt; Epoch 41 finished 	ANN training loss 0.295981
&gt;&gt; Epoch 42 finished 	ANN training loss 0.289961
&gt;&gt; Epoch 43 finished 	ANN training loss 0.286322
&gt;&gt; Epoch 44 finished 	ANN training loss 0.280451
&gt;&gt; Epoch 45 finished 	ANN training loss 0.276129
&gt;&gt; Epoch 46 finished 	ANN training loss 0.274126
&gt;&gt; Epoch 47 finished 	ANN training loss 0.268594
&gt;&gt; Epoch 48 finished 	ANN training loss 0.265549
&gt;&gt; Epoch 49 finished 	ANN training loss 0.267172
&gt;&gt; Epoch 50 finished 	ANN training loss 0.259268
&gt;&gt; Epoch 51 finished 	ANN training loss 0.259059
&gt;&gt; Epoch 52 finished 	ANN training loss 0.251981
&gt;&gt; Epoch 53 finished 	ANN training loss 0.256523
&gt;&gt; Epoch 54 finished 	ANN training loss 0.245687
&gt;&gt; Epoch 55 finished 	ANN training loss 0.243593
&gt;&gt; Epoch 56 finished 	ANN training loss 0.241691
&gt;&gt; Epoch 57 finished 	ANN training loss 0.237249
&gt;&gt; Epoch 58 finished 	ANN training loss 0.235186
&gt;&gt; Epoch 59 finished 	ANN training loss 0.233754
&gt;&gt; Epoch 60 finished 	ANN training loss 0.229291
&gt;&gt; Epoch 61 finished 	ANN training loss 0.225640
&gt;&gt; Epoch 62 finished 	ANN training loss 0.223897
&gt;&gt; Epoch 63 finished 	ANN training loss 0.219965
&gt;&gt; Epoch 64 finished 	ANN training loss 0.217309
&gt;&gt; Epoch 65 finished 	ANN training loss 0.214615
&gt;&gt; Epoch 66 finished 	ANN training loss 0.212048
&gt;&gt; Epoch 67 finished 	ANN training loss 0.210142
&gt;&gt; Epoch 68 finished 	ANN training loss 0.206790
&gt;&gt; Epoch 69 finished 	ANN training loss 0.206273
&gt;&gt; Epoch 70 finished 	ANN training loss 0.202590
&gt;&gt; Epoch 71 finished 	ANN training loss 0.198663
&gt;&gt; Epoch 72 finished 	ANN training loss 0.198622
&gt;&gt; Epoch 73 finished 	ANN training loss 0.195035
&gt;&gt; Epoch 74 finished 	ANN training loss 0.192717
&gt;&gt; Epoch 75 finished 	ANN training loss 0.191702
&gt;&gt; Epoch 76 finished 	ANN training loss 0.190267
&gt;&gt; Epoch 77 finished 	ANN training loss 0.186787
&gt;&gt; Epoch 78 finished 	ANN training loss 0.185742
&gt;&gt; Epoch 79 finished 	ANN training loss 0.183775
&gt;&gt; Epoch 80 finished 	ANN training loss 0.181876
&gt;&gt; Epoch 81 finished 	ANN training loss 0.179821
&gt;&gt; Epoch 82 finished 	ANN training loss 0.178532
&gt;&gt; Epoch 83 finished 	ANN training loss 0.174873
&gt;&gt; Epoch 84 finished 	ANN training loss 0.172816
&gt;&gt; Epoch 85 finished 	ANN training loss 0.171760
&gt;&gt; Epoch 86 finished 	ANN training loss 0.169226
&gt;&gt; Epoch 87 finished 	ANN training loss 0.168151
&gt;&gt; Epoch 88 finished 	ANN training loss 0.165527
&gt;&gt; Epoch 89 finished 	ANN training loss 0.164152
&gt;&gt; Epoch 90 finished 	ANN training loss 0.163154
&gt;&gt; Epoch 91 finished 	ANN training loss 0.163119
&gt;&gt; Epoch 92 finished 	ANN training loss 0.159858
&gt;&gt; Epoch 93 finished 	ANN training loss 0.159550
&gt;&gt; Epoch 94 finished 	ANN training loss 0.156982
&gt;&gt; Epoch 95 finished 	ANN training loss 0.155653
&gt;&gt; Epoch 96 finished 	ANN training loss 0.154482
&gt;&gt; Epoch 97 finished 	ANN training loss 0.151421
&gt;&gt; Epoch 98 finished 	ANN training loss 0.148886
&gt;&gt; Epoch 99 finished 	ANN training loss 0.148983
[END] Fine tuning step
Done.
Accuracy: 0.885000
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[63]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ACCURACY: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">learning_rate_acc</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>ACCURACY: 0.885
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Setting 2</strong></p>

<pre><code>learning_rate=0.05</code></pre>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[64]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learning_rate_acc</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">deep_belief_net</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[START] Pre-training step:
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 81.708733
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 55.849056
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 48.589561
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 45.749756
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 44.211746
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 54.903095
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 42.353642
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 43.882450
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 59.503819
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 50.491993
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 222.933167
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 371.112885
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 420.357605
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 200.313187
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 319.664520
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 271.096985
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 297.548096
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 308.967896
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 279.965790
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 336.806732
[END] Pre-training step
[START] Fine tuning step:
&gt;&gt; Epoch 0 finished 	ANN training loss 0.832086
&gt;&gt; Epoch 1 finished 	ANN training loss 0.631094
&gt;&gt; Epoch 2 finished 	ANN training loss 0.499987
&gt;&gt; Epoch 3 finished 	ANN training loss 0.446935
&gt;&gt; Epoch 4 finished 	ANN training loss 0.380466
&gt;&gt; Epoch 5 finished 	ANN training loss 0.343083
&gt;&gt; Epoch 6 finished 	ANN training loss 0.327225
&gt;&gt; Epoch 7 finished 	ANN training loss 0.308603
&gt;&gt; Epoch 8 finished 	ANN training loss 0.269831
&gt;&gt; Epoch 9 finished 	ANN training loss 0.279059
&gt;&gt; Epoch 10 finished 	ANN training loss 0.245699
&gt;&gt; Epoch 11 finished 	ANN training loss 0.228884
&gt;&gt; Epoch 12 finished 	ANN training loss 0.221781
&gt;&gt; Epoch 13 finished 	ANN training loss 0.202035
&gt;&gt; Epoch 14 finished 	ANN training loss 0.191175
&gt;&gt; Epoch 15 finished 	ANN training loss 0.173313
&gt;&gt; Epoch 16 finished 	ANN training loss 0.170683
&gt;&gt; Epoch 17 finished 	ANN training loss 0.160590
&gt;&gt; Epoch 18 finished 	ANN training loss 0.147598
&gt;&gt; Epoch 19 finished 	ANN training loss 0.146364
&gt;&gt; Epoch 20 finished 	ANN training loss 0.136079
&gt;&gt; Epoch 21 finished 	ANN training loss 0.130020
&gt;&gt; Epoch 22 finished 	ANN training loss 0.122344
&gt;&gt; Epoch 23 finished 	ANN training loss 0.106600
&gt;&gt; Epoch 24 finished 	ANN training loss 0.101626
&gt;&gt; Epoch 25 finished 	ANN training loss 0.095792
&gt;&gt; Epoch 26 finished 	ANN training loss 0.091160
&gt;&gt; Epoch 27 finished 	ANN training loss 0.084276
&gt;&gt; Epoch 28 finished 	ANN training loss 0.080266
&gt;&gt; Epoch 29 finished 	ANN training loss 0.081004
&gt;&gt; Epoch 30 finished 	ANN training loss 0.074666
&gt;&gt; Epoch 31 finished 	ANN training loss 0.071260
&gt;&gt; Epoch 32 finished 	ANN training loss 0.064608
&gt;&gt; Epoch 33 finished 	ANN training loss 0.063873
&gt;&gt; Epoch 34 finished 	ANN training loss 0.062643
&gt;&gt; Epoch 35 finished 	ANN training loss 0.056112
&gt;&gt; Epoch 36 finished 	ANN training loss 0.055827
&gt;&gt; Epoch 37 finished 	ANN training loss 0.052442
&gt;&gt; Epoch 38 finished 	ANN training loss 0.046361
&gt;&gt; Epoch 39 finished 	ANN training loss 0.044391
&gt;&gt; Epoch 40 finished 	ANN training loss 0.044546
&gt;&gt; Epoch 41 finished 	ANN training loss 0.046930
&gt;&gt; Epoch 42 finished 	ANN training loss 0.041922
&gt;&gt; Epoch 43 finished 	ANN training loss 0.038258
&gt;&gt; Epoch 44 finished 	ANN training loss 0.036549
&gt;&gt; Epoch 45 finished 	ANN training loss 0.034326
&gt;&gt; Epoch 46 finished 	ANN training loss 0.035937
&gt;&gt; Epoch 47 finished 	ANN training loss 0.034378
&gt;&gt; Epoch 48 finished 	ANN training loss 0.030205
&gt;&gt; Epoch 49 finished 	ANN training loss 0.030137
&gt;&gt; Epoch 50 finished 	ANN training loss 0.027596
&gt;&gt; Epoch 51 finished 	ANN training loss 0.026496
&gt;&gt; Epoch 52 finished 	ANN training loss 0.026889
&gt;&gt; Epoch 53 finished 	ANN training loss 0.023991
&gt;&gt; Epoch 54 finished 	ANN training loss 0.023917
&gt;&gt; Epoch 55 finished 	ANN training loss 0.022650
&gt;&gt; Epoch 56 finished 	ANN training loss 0.023188
&gt;&gt; Epoch 57 finished 	ANN training loss 0.021756
&gt;&gt; Epoch 58 finished 	ANN training loss 0.021397
&gt;&gt; Epoch 59 finished 	ANN training loss 0.021832
&gt;&gt; Epoch 60 finished 	ANN training loss 0.018921
&gt;&gt; Epoch 61 finished 	ANN training loss 0.018720
&gt;&gt; Epoch 62 finished 	ANN training loss 0.017264
&gt;&gt; Epoch 63 finished 	ANN training loss 0.017214
&gt;&gt; Epoch 64 finished 	ANN training loss 0.015186
&gt;&gt; Epoch 65 finished 	ANN training loss 0.016196
&gt;&gt; Epoch 66 finished 	ANN training loss 0.015223
&gt;&gt; Epoch 67 finished 	ANN training loss 0.013936
&gt;&gt; Epoch 68 finished 	ANN training loss 0.014372
&gt;&gt; Epoch 69 finished 	ANN training loss 0.013984
&gt;&gt; Epoch 70 finished 	ANN training loss 0.013033
&gt;&gt; Epoch 71 finished 	ANN training loss 0.014657
&gt;&gt; Epoch 72 finished 	ANN training loss 0.011918
&gt;&gt; Epoch 73 finished 	ANN training loss 0.011618
&gt;&gt; Epoch 74 finished 	ANN training loss 0.012326
&gt;&gt; Epoch 75 finished 	ANN training loss 0.010860
&gt;&gt; Epoch 76 finished 	ANN training loss 0.011137
&gt;&gt; Epoch 77 finished 	ANN training loss 0.010364
&gt;&gt; Epoch 78 finished 	ANN training loss 0.009984
&gt;&gt; Epoch 79 finished 	ANN training loss 0.009691
&gt;&gt; Epoch 80 finished 	ANN training loss 0.009287
&gt;&gt; Epoch 81 finished 	ANN training loss 0.009500
&gt;&gt; Epoch 82 finished 	ANN training loss 0.008652
&gt;&gt; Epoch 83 finished 	ANN training loss 0.009191
&gt;&gt; Epoch 84 finished 	ANN training loss 0.008879
&gt;&gt; Epoch 85 finished 	ANN training loss 0.007862
&gt;&gt; Epoch 86 finished 	ANN training loss 0.007762
&gt;&gt; Epoch 87 finished 	ANN training loss 0.007817
&gt;&gt; Epoch 88 finished 	ANN training loss 0.007047
&gt;&gt; Epoch 89 finished 	ANN training loss 0.006941
&gt;&gt; Epoch 90 finished 	ANN training loss 0.006881
&gt;&gt; Epoch 91 finished 	ANN training loss 0.006879
&gt;&gt; Epoch 92 finished 	ANN training loss 0.006168
&gt;&gt; Epoch 93 finished 	ANN training loss 0.006342
&gt;&gt; Epoch 94 finished 	ANN training loss 0.006365
&gt;&gt; Epoch 95 finished 	ANN training loss 0.005720
&gt;&gt; Epoch 96 finished 	ANN training loss 0.005604
&gt;&gt; Epoch 97 finished 	ANN training loss 0.005754
&gt;&gt; Epoch 98 finished 	ANN training loss 0.005939
&gt;&gt; Epoch 99 finished 	ANN training loss 0.005114
[END] Fine tuning step
Done.
Accuracy: 0.895000
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[65]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ACCURACY: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">learning_rate_acc</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>ACCURACY: 0.895
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Setting 3</strong></p>

<pre><code>learning_rate=0.1</code></pre>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[66]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learning_rate_acc</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">deep_belief_net</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[START] Pre-training step:
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 69.982780
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 36.274624
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 28.071461
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 46.687866
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 21.562672
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 54.380157
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 31.012707
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 36.234512
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 40.408497
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 34.521046
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 232.677383
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 128.166565
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 133.991409
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 131.393631
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 141.272736
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 180.160980
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 186.990906
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 69.389183
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 124.187363
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 177.669907
[END] Pre-training step
[START] Fine tuning step:
&gt;&gt; Epoch 0 finished 	ANN training loss 0.635615
&gt;&gt; Epoch 1 finished 	ANN training loss 0.444051
&gt;&gt; Epoch 2 finished 	ANN training loss 0.358084
&gt;&gt; Epoch 3 finished 	ANN training loss 0.313933
&gt;&gt; Epoch 4 finished 	ANN training loss 0.280643
&gt;&gt; Epoch 5 finished 	ANN training loss 0.218436
&gt;&gt; Epoch 6 finished 	ANN training loss 0.206894
&gt;&gt; Epoch 7 finished 	ANN training loss 0.173023
&gt;&gt; Epoch 8 finished 	ANN training loss 0.158146
&gt;&gt; Epoch 9 finished 	ANN training loss 0.136123
&gt;&gt; Epoch 10 finished 	ANN training loss 0.120227
&gt;&gt; Epoch 11 finished 	ANN training loss 0.113445
&gt;&gt; Epoch 12 finished 	ANN training loss 0.105395
&gt;&gt; Epoch 13 finished 	ANN training loss 0.080144
&gt;&gt; Epoch 14 finished 	ANN training loss 0.076876
&gt;&gt; Epoch 15 finished 	ANN training loss 0.070847
&gt;&gt; Epoch 16 finished 	ANN training loss 0.060769
&gt;&gt; Epoch 17 finished 	ANN training loss 0.054252
&gt;&gt; Epoch 18 finished 	ANN training loss 0.047125
&gt;&gt; Epoch 19 finished 	ANN training loss 0.039209
&gt;&gt; Epoch 20 finished 	ANN training loss 0.037317
&gt;&gt; Epoch 21 finished 	ANN training loss 0.037025
&gt;&gt; Epoch 22 finished 	ANN training loss 0.031349
&gt;&gt; Epoch 23 finished 	ANN training loss 0.029862
&gt;&gt; Epoch 24 finished 	ANN training loss 0.025357
&gt;&gt; Epoch 25 finished 	ANN training loss 0.022290
&gt;&gt; Epoch 26 finished 	ANN training loss 0.022562
&gt;&gt; Epoch 27 finished 	ANN training loss 0.018725
&gt;&gt; Epoch 28 finished 	ANN training loss 0.017980
&gt;&gt; Epoch 29 finished 	ANN training loss 0.015338
&gt;&gt; Epoch 30 finished 	ANN training loss 0.014259
&gt;&gt; Epoch 31 finished 	ANN training loss 0.012136
&gt;&gt; Epoch 32 finished 	ANN training loss 0.012440
&gt;&gt; Epoch 33 finished 	ANN training loss 0.011384
&gt;&gt; Epoch 34 finished 	ANN training loss 0.010551
&gt;&gt; Epoch 35 finished 	ANN training loss 0.011239
&gt;&gt; Epoch 36 finished 	ANN training loss 0.009546
&gt;&gt; Epoch 37 finished 	ANN training loss 0.008967
&gt;&gt; Epoch 38 finished 	ANN training loss 0.007785
&gt;&gt; Epoch 39 finished 	ANN training loss 0.007889
&gt;&gt; Epoch 40 finished 	ANN training loss 0.007497
&gt;&gt; Epoch 41 finished 	ANN training loss 0.006230
&gt;&gt; Epoch 42 finished 	ANN training loss 0.005336
&gt;&gt; Epoch 43 finished 	ANN training loss 0.005462
&gt;&gt; Epoch 44 finished 	ANN training loss 0.005591
&gt;&gt; Epoch 45 finished 	ANN training loss 0.006107
&gt;&gt; Epoch 46 finished 	ANN training loss 0.004970
&gt;&gt; Epoch 47 finished 	ANN training loss 0.004611
&gt;&gt; Epoch 48 finished 	ANN training loss 0.004077
&gt;&gt; Epoch 49 finished 	ANN training loss 0.004403
&gt;&gt; Epoch 50 finished 	ANN training loss 0.004114
&gt;&gt; Epoch 51 finished 	ANN training loss 0.004387
&gt;&gt; Epoch 52 finished 	ANN training loss 0.003674
&gt;&gt; Epoch 53 finished 	ANN training loss 0.003555
&gt;&gt; Epoch 54 finished 	ANN training loss 0.003395
&gt;&gt; Epoch 55 finished 	ANN training loss 0.003470
&gt;&gt; Epoch 56 finished 	ANN training loss 0.002933
&gt;&gt; Epoch 57 finished 	ANN training loss 0.002721
&gt;&gt; Epoch 58 finished 	ANN training loss 0.002661
&gt;&gt; Epoch 59 finished 	ANN training loss 0.002753
&gt;&gt; Epoch 60 finished 	ANN training loss 0.002514
&gt;&gt; Epoch 61 finished 	ANN training loss 0.002459
&gt;&gt; Epoch 62 finished 	ANN training loss 0.002155
&gt;&gt; Epoch 63 finished 	ANN training loss 0.002020
&gt;&gt; Epoch 64 finished 	ANN training loss 0.002050
&gt;&gt; Epoch 65 finished 	ANN training loss 0.002497
&gt;&gt; Epoch 66 finished 	ANN training loss 0.002113
&gt;&gt; Epoch 67 finished 	ANN training loss 0.001868
&gt;&gt; Epoch 68 finished 	ANN training loss 0.001735
&gt;&gt; Epoch 69 finished 	ANN training loss 0.001974
&gt;&gt; Epoch 70 finished 	ANN training loss 0.001601
&gt;&gt; Epoch 71 finished 	ANN training loss 0.001603
&gt;&gt; Epoch 72 finished 	ANN training loss 0.001723
&gt;&gt; Epoch 73 finished 	ANN training loss 0.001551
&gt;&gt; Epoch 74 finished 	ANN training loss 0.001695
&gt;&gt; Epoch 75 finished 	ANN training loss 0.001497
&gt;&gt; Epoch 76 finished 	ANN training loss 0.001321
&gt;&gt; Epoch 77 finished 	ANN training loss 0.001402
&gt;&gt; Epoch 78 finished 	ANN training loss 0.001113
&gt;&gt; Epoch 79 finished 	ANN training loss 0.001264
&gt;&gt; Epoch 80 finished 	ANN training loss 0.001136
&gt;&gt; Epoch 81 finished 	ANN training loss 0.001103
&gt;&gt; Epoch 82 finished 	ANN training loss 0.000961
&gt;&gt; Epoch 83 finished 	ANN training loss 0.001010
&gt;&gt; Epoch 84 finished 	ANN training loss 0.001055
&gt;&gt; Epoch 85 finished 	ANN training loss 0.000914
&gt;&gt; Epoch 86 finished 	ANN training loss 0.000943
&gt;&gt; Epoch 87 finished 	ANN training loss 0.000872
&gt;&gt; Epoch 88 finished 	ANN training loss 0.001109
&gt;&gt; Epoch 89 finished 	ANN training loss 0.001122
&gt;&gt; Epoch 90 finished 	ANN training loss 0.000959
&gt;&gt; Epoch 91 finished 	ANN training loss 0.000929
&gt;&gt; Epoch 92 finished 	ANN training loss 0.000905
&gt;&gt; Epoch 93 finished 	ANN training loss 0.000796
&gt;&gt; Epoch 94 finished 	ANN training loss 0.000740
&gt;&gt; Epoch 95 finished 	ANN training loss 0.000660
&gt;&gt; Epoch 96 finished 	ANN training loss 0.000747
&gt;&gt; Epoch 97 finished 	ANN training loss 0.000708
&gt;&gt; Epoch 98 finished 	ANN training loss 0.000778
&gt;&gt; Epoch 99 finished 	ANN training loss 0.000611
[END] Fine tuning step
Done.
Accuracy: 0.915000
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[67]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ACCURACY: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">learning_rate_acc</span><span class="p">[</span><span class="mi">2</span><span class="p">]))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>ACCURACY: 0.915
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Setting 4</strong></p>

<pre><code>learning_rate=0.5</code></pre>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[68]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learning_rate_acc</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">=</span> <span class="n">deep_belief_net</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[START] Pre-training step:
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 79.112305
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 96.725349
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 51.052197
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 37.876381
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 47.518787
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 33.752762
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 48.474228
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 44.482853
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 37.027935
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 42.175068
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 133.939896
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 197.219604
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 240.424820
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 256.749054
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 258.981415
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 168.490448
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 153.094330
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 211.722504
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 189.015991
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 214.493042
[END] Pre-training step
[START] Fine tuning step:
&gt;&gt; Epoch 0 finished 	ANN training loss 0.592979
&gt;&gt; Epoch 1 finished 	ANN training loss 0.919712
&gt;&gt; Epoch 2 finished 	ANN training loss 0.269604
&gt;&gt; Epoch 3 finished 	ANN training loss 0.243406
&gt;&gt; Epoch 4 finished 	ANN training loss 0.144731
&gt;&gt; Epoch 5 finished 	ANN training loss 0.106821
&gt;&gt; Epoch 6 finished 	ANN training loss 0.155397
&gt;&gt; Epoch 7 finished 	ANN training loss 0.118717
&gt;&gt; Epoch 8 finished 	ANN training loss 0.066168
&gt;&gt; Epoch 9 finished 	ANN training loss 0.050119
&gt;&gt; Epoch 10 finished 	ANN training loss 0.055784
&gt;&gt; Epoch 11 finished 	ANN training loss 0.116684
&gt;&gt; Epoch 12 finished 	ANN training loss 0.018682
&gt;&gt; Epoch 13 finished 	ANN training loss 0.027114
&gt;&gt; Epoch 14 finished 	ANN training loss 0.038821
&gt;&gt; Epoch 15 finished 	ANN training loss 0.019516
&gt;&gt; Epoch 16 finished 	ANN training loss 0.043683
&gt;&gt; Epoch 17 finished 	ANN training loss 0.017078
&gt;&gt; Epoch 18 finished 	ANN training loss 0.013328
&gt;&gt; Epoch 19 finished 	ANN training loss 0.008714
&gt;&gt; Epoch 20 finished 	ANN training loss 0.007808
&gt;&gt; Epoch 21 finished 	ANN training loss 0.009127
&gt;&gt; Epoch 22 finished 	ANN training loss 0.008772
&gt;&gt; Epoch 23 finished 	ANN training loss 0.015420
&gt;&gt; Epoch 24 finished 	ANN training loss 0.006906
&gt;&gt; Epoch 25 finished 	ANN training loss 0.002489
&gt;&gt; Epoch 26 finished 	ANN training loss 0.002595
&gt;&gt; Epoch 27 finished 	ANN training loss 0.016892
&gt;&gt; Epoch 28 finished 	ANN training loss 0.005684
&gt;&gt; Epoch 29 finished 	ANN training loss 0.002046
&gt;&gt; Epoch 30 finished 	ANN training loss 0.009243
&gt;&gt; Epoch 31 finished 	ANN training loss 0.003108
&gt;&gt; Epoch 32 finished 	ANN training loss 0.004623
&gt;&gt; Epoch 33 finished 	ANN training loss 0.004727
&gt;&gt; Epoch 34 finished 	ANN training loss 0.001026
&gt;&gt; Epoch 35 finished 	ANN training loss 0.002531
&gt;&gt; Epoch 36 finished 	ANN training loss 0.010728
&gt;&gt; Epoch 37 finished 	ANN training loss 0.008343
&gt;&gt; Epoch 38 finished 	ANN training loss 0.009291
&gt;&gt; Epoch 39 finished 	ANN training loss 0.001726
&gt;&gt; Epoch 40 finished 	ANN training loss 0.000759
&gt;&gt; Epoch 41 finished 	ANN training loss 0.000726
&gt;&gt; Epoch 42 finished 	ANN training loss 0.002183
&gt;&gt; Epoch 43 finished 	ANN training loss 0.003649
&gt;&gt; Epoch 44 finished 	ANN training loss 0.001090
&gt;&gt; Epoch 45 finished 	ANN training loss 0.000794
&gt;&gt; Epoch 46 finished 	ANN training loss 0.001917
&gt;&gt; Epoch 47 finished 	ANN training loss 0.001134
&gt;&gt; Epoch 48 finished 	ANN training loss 0.002529
&gt;&gt; Epoch 49 finished 	ANN training loss 0.000704
&gt;&gt; Epoch 50 finished 	ANN training loss 0.000694
&gt;&gt; Epoch 51 finished 	ANN training loss 0.000179
&gt;&gt; Epoch 52 finished 	ANN training loss 0.000652
&gt;&gt; Epoch 53 finished 	ANN training loss 0.000684
&gt;&gt; Epoch 54 finished 	ANN training loss 0.000338
&gt;&gt; Epoch 55 finished 	ANN training loss 0.000210
&gt;&gt; Epoch 56 finished 	ANN training loss 0.000185
&gt;&gt; Epoch 57 finished 	ANN training loss 0.002425
&gt;&gt; Epoch 58 finished 	ANN training loss 0.000948
&gt;&gt; Epoch 59 finished 	ANN training loss 0.000192
&gt;&gt; Epoch 60 finished 	ANN training loss 0.000189
&gt;&gt; Epoch 61 finished 	ANN training loss 0.001760
&gt;&gt; Epoch 62 finished 	ANN training loss 0.000620
&gt;&gt; Epoch 63 finished 	ANN training loss 0.000317
&gt;&gt; Epoch 64 finished 	ANN training loss 0.000141
&gt;&gt; Epoch 65 finished 	ANN training loss 0.000130
&gt;&gt; Epoch 66 finished 	ANN training loss 0.000140
&gt;&gt; Epoch 67 finished 	ANN training loss 0.000074
&gt;&gt; Epoch 68 finished 	ANN training loss 0.000058
&gt;&gt; Epoch 69 finished 	ANN training loss 0.000044
&gt;&gt; Epoch 70 finished 	ANN training loss 0.000267
&gt;&gt; Epoch 71 finished 	ANN training loss 0.000171
&gt;&gt; Epoch 72 finished 	ANN training loss 0.001560
&gt;&gt; Epoch 73 finished 	ANN training loss 0.000123
&gt;&gt; Epoch 74 finished 	ANN training loss 0.000052
&gt;&gt; Epoch 75 finished 	ANN training loss 0.000608
&gt;&gt; Epoch 76 finished 	ANN training loss 0.000136
&gt;&gt; Epoch 77 finished 	ANN training loss 0.000064
&gt;&gt; Epoch 78 finished 	ANN training loss 0.000091
&gt;&gt; Epoch 79 finished 	ANN training loss 0.000185
&gt;&gt; Epoch 80 finished 	ANN training loss 0.000050
&gt;&gt; Epoch 81 finished 	ANN training loss 0.000043
&gt;&gt; Epoch 82 finished 	ANN training loss 0.000252
&gt;&gt; Epoch 83 finished 	ANN training loss 0.000112
&gt;&gt; Epoch 84 finished 	ANN training loss 0.000828
&gt;&gt; Epoch 85 finished 	ANN training loss 0.000062
&gt;&gt; Epoch 86 finished 	ANN training loss 0.000045
&gt;&gt; Epoch 87 finished 	ANN training loss 0.000065
&gt;&gt; Epoch 88 finished 	ANN training loss 0.000088
&gt;&gt; Epoch 89 finished 	ANN training loss 0.000945
&gt;&gt; Epoch 90 finished 	ANN training loss 0.000660
&gt;&gt; Epoch 91 finished 	ANN training loss 0.000056
&gt;&gt; Epoch 92 finished 	ANN training loss 0.000073
&gt;&gt; Epoch 93 finished 	ANN training loss 0.000320
&gt;&gt; Epoch 94 finished 	ANN training loss 0.000045
&gt;&gt; Epoch 95 finished 	ANN training loss 0.000358
&gt;&gt; Epoch 96 finished 	ANN training loss 0.000064
&gt;&gt; Epoch 97 finished 	ANN training loss 0.000205
&gt;&gt; Epoch 98 finished 	ANN training loss 0.000103
&gt;&gt; Epoch 99 finished 	ANN training loss 0.000056
[END] Fine tuning step
Done.
Accuracy: 0.895000
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[69]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ACCURACY: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">learning_rate_acc</span><span class="p">[</span><span class="mi">3</span><span class="p">]))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>ACCURACY: 0.895
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Setting 5</strong></p>

<pre><code>learning_rate=1.0</code></pre>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[70]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learning_rate_acc</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span> <span class="o">=</span> <span class="n">deep_belief_net</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[START] Pre-training step:
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 76.779762
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 47.087139
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 59.271000
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 54.856850
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 72.953751
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 34.348270
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 50.360817
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 35.676952
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 40.834705
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 47.222740
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 266.484985
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 177.374817
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 254.176849
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 316.424438
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 188.242752
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 231.718887
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 162.049530
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 193.549850
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 268.331512
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 206.428787
[END] Pre-training step
[START] Fine tuning step:
&gt;&gt; Epoch 0 finished 	ANN training loss 1.864518
&gt;&gt; Epoch 1 finished 	ANN training loss 2.007123
&gt;&gt; Epoch 2 finished 	ANN training loss 1.281737
&gt;&gt; Epoch 3 finished 	ANN training loss 1.052272
&gt;&gt; Epoch 4 finished 	ANN training loss 0.871565
&gt;&gt; Epoch 5 finished 	ANN training loss 0.983157
&gt;&gt; Epoch 6 finished 	ANN training loss 0.714842
&gt;&gt; Epoch 7 finished 	ANN training loss 1.038296
&gt;&gt; Epoch 8 finished 	ANN training loss 1.266252
&gt;&gt; Epoch 9 finished 	ANN training loss 0.776818
&gt;&gt; Epoch 10 finished 	ANN training loss 0.741439
&gt;&gt; Epoch 11 finished 	ANN training loss 0.486001
&gt;&gt; Epoch 12 finished 	ANN training loss 0.419827
&gt;&gt; Epoch 13 finished 	ANN training loss 0.361042
&gt;&gt; Epoch 14 finished 	ANN training loss 0.367671
&gt;&gt; Epoch 15 finished 	ANN training loss 0.268364
&gt;&gt; Epoch 16 finished 	ANN training loss 0.405128
&gt;&gt; Epoch 17 finished 	ANN training loss 0.327894
&gt;&gt; Epoch 18 finished 	ANN training loss 0.199816
&gt;&gt; Epoch 19 finished 	ANN training loss 0.264040
&gt;&gt; Epoch 20 finished 	ANN training loss 0.164132
&gt;&gt; Epoch 21 finished 	ANN training loss 0.158136
&gt;&gt; Epoch 22 finished 	ANN training loss 0.280490
&gt;&gt; Epoch 23 finished 	ANN training loss 0.179689
&gt;&gt; Epoch 24 finished 	ANN training loss 0.166684
&gt;&gt; Epoch 25 finished 	ANN training loss 0.220441
&gt;&gt; Epoch 26 finished 	ANN training loss 0.176274
&gt;&gt; Epoch 27 finished 	ANN training loss 0.108506
&gt;&gt; Epoch 28 finished 	ANN training loss 0.157198
&gt;&gt; Epoch 29 finished 	ANN training loss 0.082274
&gt;&gt; Epoch 30 finished 	ANN training loss 0.101209
&gt;&gt; Epoch 31 finished 	ANN training loss 0.188011
&gt;&gt; Epoch 32 finished 	ANN training loss 0.073861
&gt;&gt; Epoch 33 finished 	ANN training loss 0.106709
&gt;&gt; Epoch 34 finished 	ANN training loss 0.077232
&gt;&gt; Epoch 35 finished 	ANN training loss 0.084905
&gt;&gt; Epoch 36 finished 	ANN training loss 0.072080
&gt;&gt; Epoch 37 finished 	ANN training loss 0.100271
&gt;&gt; Epoch 38 finished 	ANN training loss 0.070289
&gt;&gt; Epoch 39 finished 	ANN training loss 0.078162
&gt;&gt; Epoch 40 finished 	ANN training loss 0.079343
&gt;&gt; Epoch 41 finished 	ANN training loss 0.041180
&gt;&gt; Epoch 42 finished 	ANN training loss 0.068703
&gt;&gt; Epoch 43 finished 	ANN training loss 0.042101
&gt;&gt; Epoch 44 finished 	ANN training loss 0.086688
&gt;&gt; Epoch 45 finished 	ANN training loss 0.070873
&gt;&gt; Epoch 46 finished 	ANN training loss 0.048416
&gt;&gt; Epoch 47 finished 	ANN training loss 0.080061
&gt;&gt; Epoch 48 finished 	ANN training loss 0.063316
&gt;&gt; Epoch 49 finished 	ANN training loss 0.047654
&gt;&gt; Epoch 50 finished 	ANN training loss 0.038760
&gt;&gt; Epoch 51 finished 	ANN training loss 0.064794
&gt;&gt; Epoch 52 finished 	ANN training loss 0.064305
&gt;&gt; Epoch 53 finished 	ANN training loss 0.045841
&gt;&gt; Epoch 54 finished 	ANN training loss 0.058909
&gt;&gt; Epoch 55 finished 	ANN training loss 0.055491
&gt;&gt; Epoch 56 finished 	ANN training loss 0.033093
&gt;&gt; Epoch 57 finished 	ANN training loss 0.028660
&gt;&gt; Epoch 58 finished 	ANN training loss 0.036853
&gt;&gt; Epoch 59 finished 	ANN training loss 0.029701
&gt;&gt; Epoch 60 finished 	ANN training loss 0.035146
&gt;&gt; Epoch 61 finished 	ANN training loss 0.030586
&gt;&gt; Epoch 62 finished 	ANN training loss 0.018859
&gt;&gt; Epoch 63 finished 	ANN training loss 0.025407
&gt;&gt; Epoch 64 finished 	ANN training loss 0.026353
&gt;&gt; Epoch 65 finished 	ANN training loss 0.032093
&gt;&gt; Epoch 66 finished 	ANN training loss 0.015542
&gt;&gt; Epoch 67 finished 	ANN training loss 0.028916
&gt;&gt; Epoch 68 finished 	ANN training loss 0.051072
&gt;&gt; Epoch 69 finished 	ANN training loss 0.032595
&gt;&gt; Epoch 70 finished 	ANN training loss 0.038493
&gt;&gt; Epoch 71 finished 	ANN training loss 0.026300
&gt;&gt; Epoch 72 finished 	ANN training loss 0.029317
&gt;&gt; Epoch 73 finished 	ANN training loss 0.039796
&gt;&gt; Epoch 74 finished 	ANN training loss 0.036456
&gt;&gt; Epoch 75 finished 	ANN training loss 0.043984
&gt;&gt; Epoch 76 finished 	ANN training loss 0.030501
&gt;&gt; Epoch 77 finished 	ANN training loss 0.025206
&gt;&gt; Epoch 78 finished 	ANN training loss 0.021465
&gt;&gt; Epoch 79 finished 	ANN training loss 0.015890
&gt;&gt; Epoch 80 finished 	ANN training loss 0.026133
&gt;&gt; Epoch 81 finished 	ANN training loss 0.030957
&gt;&gt; Epoch 82 finished 	ANN training loss 0.030298
&gt;&gt; Epoch 83 finished 	ANN training loss 0.035556
&gt;&gt; Epoch 84 finished 	ANN training loss 0.026714
&gt;&gt; Epoch 85 finished 	ANN training loss 0.031732
&gt;&gt; Epoch 86 finished 	ANN training loss 0.028883
&gt;&gt; Epoch 87 finished 	ANN training loss 0.025399
&gt;&gt; Epoch 88 finished 	ANN training loss 0.025603
&gt;&gt; Epoch 89 finished 	ANN training loss 0.021756
&gt;&gt; Epoch 90 finished 	ANN training loss 0.021983
&gt;&gt; Epoch 91 finished 	ANN training loss 0.028638
&gt;&gt; Epoch 92 finished 	ANN training loss 0.026257
&gt;&gt; Epoch 93 finished 	ANN training loss 0.020985
&gt;&gt; Epoch 94 finished 	ANN training loss 0.024991
&gt;&gt; Epoch 95 finished 	ANN training loss 0.020566
&gt;&gt; Epoch 96 finished 	ANN training loss 0.032148
&gt;&gt; Epoch 97 finished 	ANN training loss 0.019458
&gt;&gt; Epoch 98 finished 	ANN training loss 0.029209
&gt;&gt; Epoch 99 finished 	ANN training loss 0.022101
[END] Fine tuning step
Done.
Accuracy: 0.875000
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[71]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ACCURACY: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">learning_rate_acc</span><span class="p">[</span><span class="mi">4</span><span class="p">]))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>ACCURACY: 0.875
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[72]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Collated results</span>
<span class="nb">print</span><span class="p">(</span><span class="n">learning_rate_acc</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Most accurate learning_rate setting is Setting &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">learning_rate_acc</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="n">learning_rate_acc</span><span class="p">))</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[0.88500000000000001, 0.89500000000000002, 0.91500000000000004, 0.89500000000000002, 0.875]
Most accurate learning_rate setting is Setting 3
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[59]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Create bar graph</span>
<span class="n">plt</span><span class="o">.</span><span class="n">clf</span><span class="p">()</span>
<span class="n">objects</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;0.01&#39;</span><span class="p">,</span> <span class="s1">&#39;0.05&#39;</span><span class="p">,</span> <span class="s1">&#39;0.10&#39;</span><span class="p">,</span> <span class="s1">&#39;0.50&#39;</span><span class="p">,</span> <span class="s1">&#39;1.0&#39;</span><span class="p">)</span>
<span class="n">y_pos</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">objects</span><span class="p">))</span>
<span class="n">performance</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.885</span><span class="p">,</span><span class="mf">0.895</span><span class="p">,</span><span class="mf">0.915</span><span class="p">,</span><span class="mf">0.895</span><span class="p">,</span> <span class="mf">0.875</span><span class="p">]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">y_pos</span><span class="p">,</span> <span class="n">performance</span><span class="p">,</span> <span class="n">align</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1.</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">y_pos</span><span class="p">,</span> <span class="n">objects</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Settings&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Accuracy&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;learning_rate Settings&#39;</span><span class="p">)</span>
 
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">clf</span><span class="p">()</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>




<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAYUAAAEgCAYAAABb8m8VAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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"
>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Different-Pre-training-Lengths:">Different Pre-training Lengths:<a class="anchor-link" href="#Different-Pre-training-Lengths:">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[73]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Store accuracies</span>
<span class="n">epoch_rbm_acc</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Setting 1</strong></p>

<pre><code>n_epochs_rbm=1</code></pre>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[74]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">epoch_rbm_acc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">deep_belief_net</span><span class="p">(</span><span class="n">n_epochs_rbm</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[START] Pre-training step:
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 58.882492
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 278.842926
[END] Pre-training step
[START] Fine tuning step:
&gt;&gt; Epoch 0 finished 	ANN training loss 2.223761
&gt;&gt; Epoch 1 finished 	ANN training loss 2.037116
&gt;&gt; Epoch 2 finished 	ANN training loss 1.761922
&gt;&gt; Epoch 3 finished 	ANN training loss 1.454467
&gt;&gt; Epoch 4 finished 	ANN training loss 1.263631
&gt;&gt; Epoch 5 finished 	ANN training loss 1.113083
&gt;&gt; Epoch 6 finished 	ANN training loss 0.964771
&gt;&gt; Epoch 7 finished 	ANN training loss 0.840998
&gt;&gt; Epoch 8 finished 	ANN training loss 0.768864
&gt;&gt; Epoch 9 finished 	ANN training loss 0.729350
&gt;&gt; Epoch 10 finished 	ANN training loss 0.596092
&gt;&gt; Epoch 11 finished 	ANN training loss 0.531411
&gt;&gt; Epoch 12 finished 	ANN training loss 0.463428
&gt;&gt; Epoch 13 finished 	ANN training loss 0.412320
&gt;&gt; Epoch 14 finished 	ANN training loss 0.361198
&gt;&gt; Epoch 15 finished 	ANN training loss 0.324128
&gt;&gt; Epoch 16 finished 	ANN training loss 0.300418
&gt;&gt; Epoch 17 finished 	ANN training loss 0.270532
&gt;&gt; Epoch 18 finished 	ANN training loss 0.237967
&gt;&gt; Epoch 19 finished 	ANN training loss 0.242986
&gt;&gt; Epoch 20 finished 	ANN training loss 0.193844
&gt;&gt; Epoch 21 finished 	ANN training loss 0.189994
&gt;&gt; Epoch 22 finished 	ANN training loss 0.164269
&gt;&gt; Epoch 23 finished 	ANN training loss 0.170173
&gt;&gt; Epoch 24 finished 	ANN training loss 0.146996
&gt;&gt; Epoch 25 finished 	ANN training loss 0.128791
&gt;&gt; Epoch 26 finished 	ANN training loss 0.124010
&gt;&gt; Epoch 27 finished 	ANN training loss 0.115098
&gt;&gt; Epoch 28 finished 	ANN training loss 0.101233
&gt;&gt; Epoch 29 finished 	ANN training loss 0.084624
&gt;&gt; Epoch 30 finished 	ANN training loss 0.089629
&gt;&gt; Epoch 31 finished 	ANN training loss 0.079986
&gt;&gt; Epoch 32 finished 	ANN training loss 0.072556
&gt;&gt; Epoch 33 finished 	ANN training loss 0.071409
&gt;&gt; Epoch 34 finished 	ANN training loss 0.056656
&gt;&gt; Epoch 35 finished 	ANN training loss 0.064879
&gt;&gt; Epoch 36 finished 	ANN training loss 0.055338
&gt;&gt; Epoch 37 finished 	ANN training loss 0.053665
&gt;&gt; Epoch 38 finished 	ANN training loss 0.047763
&gt;&gt; Epoch 39 finished 	ANN training loss 0.039060
&gt;&gt; Epoch 40 finished 	ANN training loss 0.038810
&gt;&gt; Epoch 41 finished 	ANN training loss 0.031391
&gt;&gt; Epoch 42 finished 	ANN training loss 0.028825
&gt;&gt; Epoch 43 finished 	ANN training loss 0.026952
&gt;&gt; Epoch 44 finished 	ANN training loss 0.026540
&gt;&gt; Epoch 45 finished 	ANN training loss 0.023928
&gt;&gt; Epoch 46 finished 	ANN training loss 0.025771
&gt;&gt; Epoch 47 finished 	ANN training loss 0.018797
&gt;&gt; Epoch 48 finished 	ANN training loss 0.020072
&gt;&gt; Epoch 49 finished 	ANN training loss 0.017710
&gt;&gt; Epoch 50 finished 	ANN training loss 0.019654
&gt;&gt; Epoch 51 finished 	ANN training loss 0.022472
&gt;&gt; Epoch 52 finished 	ANN training loss 0.016233
&gt;&gt; Epoch 53 finished 	ANN training loss 0.017495
&gt;&gt; Epoch 54 finished 	ANN training loss 0.012503
&gt;&gt; Epoch 55 finished 	ANN training loss 0.011917
&gt;&gt; Epoch 56 finished 	ANN training loss 0.013414
&gt;&gt; Epoch 57 finished 	ANN training loss 0.011644
&gt;&gt; Epoch 58 finished 	ANN training loss 0.010111
&gt;&gt; Epoch 59 finished 	ANN training loss 0.010870
&gt;&gt; Epoch 60 finished 	ANN training loss 0.011353
&gt;&gt; Epoch 61 finished 	ANN training loss 0.008745
&gt;&gt; Epoch 62 finished 	ANN training loss 0.007789
&gt;&gt; Epoch 63 finished 	ANN training loss 0.008426
&gt;&gt; Epoch 64 finished 	ANN training loss 0.006816
&gt;&gt; Epoch 65 finished 	ANN training loss 0.008983
&gt;&gt; Epoch 66 finished 	ANN training loss 0.009432
&gt;&gt; Epoch 67 finished 	ANN training loss 0.008194
&gt;&gt; Epoch 68 finished 	ANN training loss 0.007353
&gt;&gt; Epoch 69 finished 	ANN training loss 0.005795
&gt;&gt; Epoch 70 finished 	ANN training loss 0.006238
&gt;&gt; Epoch 71 finished 	ANN training loss 0.005558
&gt;&gt; Epoch 72 finished 	ANN training loss 0.005462
&gt;&gt; Epoch 73 finished 	ANN training loss 0.005598
&gt;&gt; Epoch 74 finished 	ANN training loss 0.006482
&gt;&gt; Epoch 75 finished 	ANN training loss 0.004952
&gt;&gt; Epoch 76 finished 	ANN training loss 0.006256
&gt;&gt; Epoch 77 finished 	ANN training loss 0.007180
&gt;&gt; Epoch 78 finished 	ANN training loss 0.005956
&gt;&gt; Epoch 79 finished 	ANN training loss 0.005255
&gt;&gt; Epoch 80 finished 	ANN training loss 0.005433
&gt;&gt; Epoch 81 finished 	ANN training loss 0.004257
&gt;&gt; Epoch 82 finished 	ANN training loss 0.004993
&gt;&gt; Epoch 83 finished 	ANN training loss 0.006366
&gt;&gt; Epoch 84 finished 	ANN training loss 0.003958
&gt;&gt; Epoch 85 finished 	ANN training loss 0.004470
&gt;&gt; Epoch 86 finished 	ANN training loss 0.003970
&gt;&gt; Epoch 87 finished 	ANN training loss 0.003097
&gt;&gt; Epoch 88 finished 	ANN training loss 0.003155
&gt;&gt; Epoch 89 finished 	ANN training loss 0.003054
&gt;&gt; Epoch 90 finished 	ANN training loss 0.003835
&gt;&gt; Epoch 91 finished 	ANN training loss 0.003231
&gt;&gt; Epoch 92 finished 	ANN training loss 0.004185
&gt;&gt; Epoch 93 finished 	ANN training loss 0.002709
&gt;&gt; Epoch 94 finished 	ANN training loss 0.002217
&gt;&gt; Epoch 95 finished 	ANN training loss 0.002616
&gt;&gt; Epoch 96 finished 	ANN training loss 0.002371
&gt;&gt; Epoch 97 finished 	ANN training loss 0.002233
&gt;&gt; Epoch 98 finished 	ANN training loss 0.002299
&gt;&gt; Epoch 99 finished 	ANN training loss 0.002585
[END] Fine tuning step
Done.
Accuracy: 0.880000
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[75]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ACCURACY: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">epoch_rbm_acc</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>ACCURACY: 0.88
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Setting 2</strong></p>

<pre><code>n_epochs_rbm=2</code></pre>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[76]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">epoch_rbm_acc</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">deep_belief_net</span><span class="p">(</span><span class="n">n_epochs_rbm</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[START] Pre-training step:
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 64.689949
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 59.904064
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 411.289185
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 575.223877
[END] Pre-training step
[START] Fine tuning step:
&gt;&gt; Epoch 0 finished 	ANN training loss 1.419303
&gt;&gt; Epoch 1 finished 	ANN training loss 1.063062
&gt;&gt; Epoch 2 finished 	ANN training loss 0.856164
&gt;&gt; Epoch 3 finished 	ANN training loss 0.754857
&gt;&gt; Epoch 4 finished 	ANN training loss 0.681133
&gt;&gt; Epoch 5 finished 	ANN training loss 0.563045
&gt;&gt; Epoch 6 finished 	ANN training loss 0.497078
&gt;&gt; Epoch 7 finished 	ANN training loss 0.439034
&gt;&gt; Epoch 8 finished 	ANN training loss 0.410330
&gt;&gt; Epoch 9 finished 	ANN training loss 0.372165
&gt;&gt; Epoch 10 finished 	ANN training loss 0.331818
&gt;&gt; Epoch 11 finished 	ANN training loss 0.277258
&gt;&gt; Epoch 12 finished 	ANN training loss 0.247271
&gt;&gt; Epoch 13 finished 	ANN training loss 0.233883
&gt;&gt; Epoch 14 finished 	ANN training loss 0.213489
&gt;&gt; Epoch 15 finished 	ANN training loss 0.200255
&gt;&gt; Epoch 16 finished 	ANN training loss 0.187413
&gt;&gt; Epoch 17 finished 	ANN training loss 0.151062
&gt;&gt; Epoch 18 finished 	ANN training loss 0.147466
&gt;&gt; Epoch 19 finished 	ANN training loss 0.132926
&gt;&gt; Epoch 20 finished 	ANN training loss 0.111680
&gt;&gt; Epoch 21 finished 	ANN training loss 0.102049
&gt;&gt; Epoch 22 finished 	ANN training loss 0.107214
&gt;&gt; Epoch 23 finished 	ANN training loss 0.087160
&gt;&gt; Epoch 24 finished 	ANN training loss 0.079395
&gt;&gt; Epoch 25 finished 	ANN training loss 0.067591
&gt;&gt; Epoch 26 finished 	ANN training loss 0.060134
&gt;&gt; Epoch 27 finished 	ANN training loss 0.060750
&gt;&gt; Epoch 28 finished 	ANN training loss 0.048710
&gt;&gt; Epoch 29 finished 	ANN training loss 0.043150
&gt;&gt; Epoch 30 finished 	ANN training loss 0.047188
&gt;&gt; Epoch 31 finished 	ANN training loss 0.040832
&gt;&gt; Epoch 32 finished 	ANN training loss 0.036856
&gt;&gt; Epoch 33 finished 	ANN training loss 0.029638
&gt;&gt; Epoch 34 finished 	ANN training loss 0.025295
&gt;&gt; Epoch 35 finished 	ANN training loss 0.030186
&gt;&gt; Epoch 36 finished 	ANN training loss 0.029155
&gt;&gt; Epoch 37 finished 	ANN training loss 0.032035
&gt;&gt; Epoch 38 finished 	ANN training loss 0.022894
&gt;&gt; Epoch 39 finished 	ANN training loss 0.021914
&gt;&gt; Epoch 40 finished 	ANN training loss 0.019929
&gt;&gt; Epoch 41 finished 	ANN training loss 0.016096
&gt;&gt; Epoch 42 finished 	ANN training loss 0.016274
&gt;&gt; Epoch 43 finished 	ANN training loss 0.013958
&gt;&gt; Epoch 44 finished 	ANN training loss 0.015070
&gt;&gt; Epoch 45 finished 	ANN training loss 0.013994
&gt;&gt; Epoch 46 finished 	ANN training loss 0.013595
&gt;&gt; Epoch 47 finished 	ANN training loss 0.010287
&gt;&gt; Epoch 48 finished 	ANN training loss 0.012040
&gt;&gt; Epoch 49 finished 	ANN training loss 0.010471
&gt;&gt; Epoch 50 finished 	ANN training loss 0.010486
&gt;&gt; Epoch 51 finished 	ANN training loss 0.009014
&gt;&gt; Epoch 52 finished 	ANN training loss 0.008870
&gt;&gt; Epoch 53 finished 	ANN training loss 0.015122
&gt;&gt; Epoch 54 finished 	ANN training loss 0.007489
&gt;&gt; Epoch 55 finished 	ANN training loss 0.007910
&gt;&gt; Epoch 56 finished 	ANN training loss 0.006729
&gt;&gt; Epoch 57 finished 	ANN training loss 0.007217
&gt;&gt; Epoch 58 finished 	ANN training loss 0.006205
&gt;&gt; Epoch 59 finished 	ANN training loss 0.005724
&gt;&gt; Epoch 60 finished 	ANN training loss 0.005335
&gt;&gt; Epoch 61 finished 	ANN training loss 0.004808
&gt;&gt; Epoch 62 finished 	ANN training loss 0.005119
&gt;&gt; Epoch 63 finished 	ANN training loss 0.005627
&gt;&gt; Epoch 64 finished 	ANN training loss 0.004749
&gt;&gt; Epoch 65 finished 	ANN training loss 0.005440
&gt;&gt; Epoch 66 finished 	ANN training loss 0.004403
&gt;&gt; Epoch 67 finished 	ANN training loss 0.004492
&gt;&gt; Epoch 68 finished 	ANN training loss 0.004223
&gt;&gt; Epoch 69 finished 	ANN training loss 0.004254
&gt;&gt; Epoch 70 finished 	ANN training loss 0.003173
&gt;&gt; Epoch 71 finished 	ANN training loss 0.003260
&gt;&gt; Epoch 72 finished 	ANN training loss 0.003513
&gt;&gt; Epoch 73 finished 	ANN training loss 0.003192
&gt;&gt; Epoch 74 finished 	ANN training loss 0.003297
&gt;&gt; Epoch 75 finished 	ANN training loss 0.002554
&gt;&gt; Epoch 76 finished 	ANN training loss 0.002418
&gt;&gt; Epoch 77 finished 	ANN training loss 0.002834
&gt;&gt; Epoch 78 finished 	ANN training loss 0.002748
&gt;&gt; Epoch 79 finished 	ANN training loss 0.004030
&gt;&gt; Epoch 80 finished 	ANN training loss 0.003492
&gt;&gt; Epoch 81 finished 	ANN training loss 0.002854
&gt;&gt; Epoch 82 finished 	ANN training loss 0.002339
&gt;&gt; Epoch 83 finished 	ANN training loss 0.002591
&gt;&gt; Epoch 84 finished 	ANN training loss 0.002238
&gt;&gt; Epoch 85 finished 	ANN training loss 0.002122
&gt;&gt; Epoch 86 finished 	ANN training loss 0.002470
&gt;&gt; Epoch 87 finished 	ANN training loss 0.002199
&gt;&gt; Epoch 88 finished 	ANN training loss 0.002292
&gt;&gt; Epoch 89 finished 	ANN training loss 0.002280
&gt;&gt; Epoch 90 finished 	ANN training loss 0.001784
&gt;&gt; Epoch 91 finished 	ANN training loss 0.001722
&gt;&gt; Epoch 92 finished 	ANN training loss 0.002249
&gt;&gt; Epoch 93 finished 	ANN training loss 0.001568
&gt;&gt; Epoch 94 finished 	ANN training loss 0.001680
&gt;&gt; Epoch 95 finished 	ANN training loss 0.001634
&gt;&gt; Epoch 96 finished 	ANN training loss 0.001589
&gt;&gt; Epoch 97 finished 	ANN training loss 0.002590
&gt;&gt; Epoch 98 finished 	ANN training loss 0.001645
&gt;&gt; Epoch 99 finished 	ANN training loss 0.001494
[END] Fine tuning step
Done.
Accuracy: 0.880000
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[77]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ACCURACY: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">epoch_rbm_acc</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>ACCURACY: 0.88
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Setting 3</strong></p>

<pre><code>n_epochs_rbm=5</code></pre>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[78]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">epoch_rbm_acc</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">deep_belief_net</span><span class="p">(</span><span class="n">n_epochs_rbm</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[START] Pre-training step:
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 56.931183
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 77.546394
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 65.859505
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 51.727715
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 30.440014
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 59.351768
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 79.193237
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 44.656254
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 90.029076
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 60.892635
[END] Pre-training step
[START] Fine tuning step:
&gt;&gt; Epoch 0 finished 	ANN training loss 0.755383
&gt;&gt; Epoch 1 finished 	ANN training loss 0.544380
&gt;&gt; Epoch 2 finished 	ANN training loss 0.465932
&gt;&gt; Epoch 3 finished 	ANN training loss 0.375281
&gt;&gt; Epoch 4 finished 	ANN training loss 0.321060
&gt;&gt; Epoch 5 finished 	ANN training loss 0.283924
&gt;&gt; Epoch 6 finished 	ANN training loss 0.263247
&gt;&gt; Epoch 7 finished 	ANN training loss 0.250553
&gt;&gt; Epoch 8 finished 	ANN training loss 0.237075
&gt;&gt; Epoch 9 finished 	ANN training loss 0.180646
&gt;&gt; Epoch 10 finished 	ANN training loss 0.170824
&gt;&gt; Epoch 11 finished 	ANN training loss 0.146446
&gt;&gt; Epoch 12 finished 	ANN training loss 0.148486
&gt;&gt; Epoch 13 finished 	ANN training loss 0.121909
&gt;&gt; Epoch 14 finished 	ANN training loss 0.130320
&gt;&gt; Epoch 15 finished 	ANN training loss 0.101270
&gt;&gt; Epoch 16 finished 	ANN training loss 0.095869
&gt;&gt; Epoch 17 finished 	ANN training loss 0.081758
&gt;&gt; Epoch 18 finished 	ANN training loss 0.072612
&gt;&gt; Epoch 19 finished 	ANN training loss 0.066798
&gt;&gt; Epoch 20 finished 	ANN training loss 0.060896
&gt;&gt; Epoch 21 finished 	ANN training loss 0.052189
&gt;&gt; Epoch 22 finished 	ANN training loss 0.049018
&gt;&gt; Epoch 23 finished 	ANN training loss 0.047216
&gt;&gt; Epoch 24 finished 	ANN training loss 0.043193
&gt;&gt; Epoch 25 finished 	ANN training loss 0.037723
&gt;&gt; Epoch 26 finished 	ANN training loss 0.035975
&gt;&gt; Epoch 27 finished 	ANN training loss 0.031218
&gt;&gt; Epoch 28 finished 	ANN training loss 0.031397
&gt;&gt; Epoch 29 finished 	ANN training loss 0.031476
&gt;&gt; Epoch 30 finished 	ANN training loss 0.024891
&gt;&gt; Epoch 31 finished 	ANN training loss 0.022046
&gt;&gt; Epoch 32 finished 	ANN training loss 0.020564
&gt;&gt; Epoch 33 finished 	ANN training loss 0.018182
&gt;&gt; Epoch 34 finished 	ANN training loss 0.016064
&gt;&gt; Epoch 35 finished 	ANN training loss 0.016191
&gt;&gt; Epoch 36 finished 	ANN training loss 0.015226
&gt;&gt; Epoch 37 finished 	ANN training loss 0.015814
&gt;&gt; Epoch 38 finished 	ANN training loss 0.014541
&gt;&gt; Epoch 39 finished 	ANN training loss 0.012586
&gt;&gt; Epoch 40 finished 	ANN training loss 0.012461
&gt;&gt; Epoch 41 finished 	ANN training loss 0.011501
&gt;&gt; Epoch 42 finished 	ANN training loss 0.011941
&gt;&gt; Epoch 43 finished 	ANN training loss 0.009156
&gt;&gt; Epoch 44 finished 	ANN training loss 0.010101
&gt;&gt; Epoch 45 finished 	ANN training loss 0.010293
&gt;&gt; Epoch 46 finished 	ANN training loss 0.008471
&gt;&gt; Epoch 47 finished 	ANN training loss 0.008303
&gt;&gt; Epoch 48 finished 	ANN training loss 0.007711
&gt;&gt; Epoch 49 finished 	ANN training loss 0.008316
&gt;&gt; Epoch 50 finished 	ANN training loss 0.006659
&gt;&gt; Epoch 51 finished 	ANN training loss 0.007318
&gt;&gt; Epoch 52 finished 	ANN training loss 0.006796
&gt;&gt; Epoch 53 finished 	ANN training loss 0.005895
&gt;&gt; Epoch 54 finished 	ANN training loss 0.006493
&gt;&gt; Epoch 55 finished 	ANN training loss 0.005107
&gt;&gt; Epoch 56 finished 	ANN training loss 0.005322
&gt;&gt; Epoch 57 finished 	ANN training loss 0.004942
&gt;&gt; Epoch 58 finished 	ANN training loss 0.004013
&gt;&gt; Epoch 59 finished 	ANN training loss 0.003807
&gt;&gt; Epoch 60 finished 	ANN training loss 0.004168
&gt;&gt; Epoch 61 finished 	ANN training loss 0.004650
&gt;&gt; Epoch 62 finished 	ANN training loss 0.004459
&gt;&gt; Epoch 63 finished 	ANN training loss 0.003537
&gt;&gt; Epoch 64 finished 	ANN training loss 0.003326
&gt;&gt; Epoch 65 finished 	ANN training loss 0.003472
&gt;&gt; Epoch 66 finished 	ANN training loss 0.004851
&gt;&gt; Epoch 67 finished 	ANN training loss 0.003296
&gt;&gt; Epoch 68 finished 	ANN training loss 0.002763
&gt;&gt; Epoch 69 finished 	ANN training loss 0.003118
&gt;&gt; Epoch 70 finished 	ANN training loss 0.002633
&gt;&gt; Epoch 71 finished 	ANN training loss 0.002410
&gt;&gt; Epoch 72 finished 	ANN training loss 0.002526
&gt;&gt; Epoch 73 finished 	ANN training loss 0.002244
&gt;&gt; Epoch 74 finished 	ANN training loss 0.002375
&gt;&gt; Epoch 75 finished 	ANN training loss 0.002440
&gt;&gt; Epoch 76 finished 	ANN training loss 0.002048
&gt;&gt; Epoch 77 finished 	ANN training loss 0.001956
&gt;&gt; Epoch 78 finished 	ANN training loss 0.001893
&gt;&gt; Epoch 79 finished 	ANN training loss 0.001904
&gt;&gt; Epoch 80 finished 	ANN training loss 0.002147
&gt;&gt; Epoch 81 finished 	ANN training loss 0.001924
&gt;&gt; Epoch 82 finished 	ANN training loss 0.001826
&gt;&gt; Epoch 83 finished 	ANN training loss 0.001678
&gt;&gt; Epoch 84 finished 	ANN training loss 0.001584
&gt;&gt; Epoch 85 finished 	ANN training loss 0.001587
&gt;&gt; Epoch 86 finished 	ANN training loss 0.001834
&gt;&gt; Epoch 87 finished 	ANN training loss 0.001483
&gt;&gt; Epoch 88 finished 	ANN training loss 0.001813
&gt;&gt; Epoch 89 finished 	ANN training loss 0.001701
&gt;&gt; Epoch 90 finished 	ANN training loss 0.001782
&gt;&gt; Epoch 91 finished 	ANN training loss 0.001362
&gt;&gt; Epoch 92 finished 	ANN training loss 0.001288
&gt;&gt; Epoch 93 finished 	ANN training loss 0.001483
&gt;&gt; Epoch 94 finished 	ANN training loss 0.001155
&gt;&gt; Epoch 95 finished 	ANN training loss 0.001198
&gt;&gt; Epoch 96 finished 	ANN training loss 0.001290
&gt;&gt; Epoch 97 finished 	ANN training loss 0.001352
&gt;&gt; Epoch 98 finished 	ANN training loss 0.001136
&gt;&gt; Epoch 99 finished 	ANN training loss 0.001038
[END] Fine tuning step
Done.
Accuracy: 0.910000
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[79]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ACCURACY: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">epoch_rbm_acc</span><span class="p">[</span><span class="mi">2</span><span class="p">]))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>ACCURACY: 0.91
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Setting 4 (Default)</strong></p>

<pre><code>n_epochs_rbm=10</code></pre>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[80]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">epoch_rbm_acc</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">=</span> <span class="n">default_acc</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ACCURACY: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">default_acc</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>ACCURACY: 0.91
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Setting 5</strong></p>

<pre><code>n_epochs_rbm=20</code></pre>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[81]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">epoch_rbm_acc</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span> <span class="o">=</span> <span class="n">deep_belief_net</span><span class="p">(</span><span class="n">n_epochs_rbm</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[START] Pre-training step:
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 56.427452
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 71.606277
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 55.203838
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 49.950108
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 43.959949
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 35.955669
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 31.652964
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 46.792759
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 32.297512
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 37.488388
&gt;&gt; Epoch 11 finished 	RBM Reconstruction error 35.783787
&gt;&gt; Epoch 12 finished 	RBM Reconstruction error 33.071846
&gt;&gt; Epoch 13 finished 	RBM Reconstruction error 42.293262
&gt;&gt; Epoch 14 finished 	RBM Reconstruction error 42.861328
&gt;&gt; Epoch 15 finished 	RBM Reconstruction error 44.246651
&gt;&gt; Epoch 16 finished 	RBM Reconstruction error 42.367062
&gt;&gt; Epoch 17 finished 	RBM Reconstruction error 45.918037
&gt;&gt; Epoch 18 finished 	RBM Reconstruction error 36.767139
&gt;&gt; Epoch 19 finished 	RBM Reconstruction error 39.103394
&gt;&gt; Epoch 20 finished 	RBM Reconstruction error 42.151669
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 187.944672
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 178.430161
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 161.261093
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 86.875801
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 153.360779
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 205.123718
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 179.342361
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 149.012955
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 172.777756
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 163.147797
&gt;&gt; Epoch 11 finished 	RBM Reconstruction error 161.883987
&gt;&gt; Epoch 12 finished 	RBM Reconstruction error 159.376404
&gt;&gt; Epoch 13 finished 	RBM Reconstruction error 194.027191
&gt;&gt; Epoch 14 finished 	RBM Reconstruction error 163.270905
&gt;&gt; Epoch 15 finished 	RBM Reconstruction error 144.504623
&gt;&gt; Epoch 16 finished 	RBM Reconstruction error 158.026321
&gt;&gt; Epoch 17 finished 	RBM Reconstruction error 147.669937
&gt;&gt; Epoch 18 finished 	RBM Reconstruction error 140.680222
&gt;&gt; Epoch 19 finished 	RBM Reconstruction error 137.220871
&gt;&gt; Epoch 20 finished 	RBM Reconstruction error 192.522034
[END] Pre-training step
[START] Fine tuning step:
&gt;&gt; Epoch 0 finished 	ANN training loss 0.597145
&gt;&gt; Epoch 1 finished 	ANN training loss 0.475535
&gt;&gt; Epoch 2 finished 	ANN training loss 0.324435
&gt;&gt; Epoch 3 finished 	ANN training loss 0.298316
&gt;&gt; Epoch 4 finished 	ANN training loss 0.248335
&gt;&gt; Epoch 5 finished 	ANN training loss 0.209339
&gt;&gt; Epoch 6 finished 	ANN training loss 0.184778
&gt;&gt; Epoch 7 finished 	ANN training loss 0.158373
&gt;&gt; Epoch 8 finished 	ANN training loss 0.139129
&gt;&gt; Epoch 9 finished 	ANN training loss 0.114644
&gt;&gt; Epoch 10 finished 	ANN training loss 0.111366
&gt;&gt; Epoch 11 finished 	ANN training loss 0.098022
&gt;&gt; Epoch 12 finished 	ANN training loss 0.083005
&gt;&gt; Epoch 13 finished 	ANN training loss 0.068859
&gt;&gt; Epoch 14 finished 	ANN training loss 0.075899
&gt;&gt; Epoch 15 finished 	ANN training loss 0.069448
&gt;&gt; Epoch 16 finished 	ANN training loss 0.053975
&gt;&gt; Epoch 17 finished 	ANN training loss 0.050634
&gt;&gt; Epoch 18 finished 	ANN training loss 0.041964
&gt;&gt; Epoch 19 finished 	ANN training loss 0.035965
&gt;&gt; Epoch 20 finished 	ANN training loss 0.039626
&gt;&gt; Epoch 21 finished 	ANN training loss 0.029006
&gt;&gt; Epoch 22 finished 	ANN training loss 0.027374
&gt;&gt; Epoch 23 finished 	ANN training loss 0.026816
&gt;&gt; Epoch 24 finished 	ANN training loss 0.025267
&gt;&gt; Epoch 25 finished 	ANN training loss 0.024297
&gt;&gt; Epoch 26 finished 	ANN training loss 0.018325
&gt;&gt; Epoch 27 finished 	ANN training loss 0.019077
&gt;&gt; Epoch 28 finished 	ANN training loss 0.017407
&gt;&gt; Epoch 29 finished 	ANN training loss 0.014804
&gt;&gt; Epoch 30 finished 	ANN training loss 0.013759
&gt;&gt; Epoch 31 finished 	ANN training loss 0.012384
&gt;&gt; Epoch 32 finished 	ANN training loss 0.013008
&gt;&gt; Epoch 33 finished 	ANN training loss 0.012786
&gt;&gt; Epoch 34 finished 	ANN training loss 0.014741
&gt;&gt; Epoch 35 finished 	ANN training loss 0.009634
&gt;&gt; Epoch 36 finished 	ANN training loss 0.009522
&gt;&gt; Epoch 37 finished 	ANN training loss 0.008735
&gt;&gt; Epoch 38 finished 	ANN training loss 0.007373
&gt;&gt; Epoch 39 finished 	ANN training loss 0.007362
&gt;&gt; Epoch 40 finished 	ANN training loss 0.006693
&gt;&gt; Epoch 41 finished 	ANN training loss 0.007376
&gt;&gt; Epoch 42 finished 	ANN training loss 0.006166
&gt;&gt; Epoch 43 finished 	ANN training loss 0.006046
&gt;&gt; Epoch 44 finished 	ANN training loss 0.005588
&gt;&gt; Epoch 45 finished 	ANN training loss 0.004943
&gt;&gt; Epoch 46 finished 	ANN training loss 0.005082
&gt;&gt; Epoch 47 finished 	ANN training loss 0.004644
&gt;&gt; Epoch 48 finished 	ANN training loss 0.004235
&gt;&gt; Epoch 49 finished 	ANN training loss 0.003759
&gt;&gt; Epoch 50 finished 	ANN training loss 0.003394
&gt;&gt; Epoch 51 finished 	ANN training loss 0.003706
&gt;&gt; Epoch 52 finished 	ANN training loss 0.003375
&gt;&gt; Epoch 53 finished 	ANN training loss 0.003192
&gt;&gt; Epoch 54 finished 	ANN training loss 0.003678
&gt;&gt; Epoch 55 finished 	ANN training loss 0.003450
&gt;&gt; Epoch 56 finished 	ANN training loss 0.002996
&gt;&gt; Epoch 57 finished 	ANN training loss 0.003453
&gt;&gt; Epoch 58 finished 	ANN training loss 0.003489
&gt;&gt; Epoch 59 finished 	ANN training loss 0.002746
&gt;&gt; Epoch 60 finished 	ANN training loss 0.002690
&gt;&gt; Epoch 61 finished 	ANN training loss 0.002511
&gt;&gt; Epoch 62 finished 	ANN training loss 0.002690
&gt;&gt; Epoch 63 finished 	ANN training loss 0.002292
&gt;&gt; Epoch 64 finished 	ANN training loss 0.002188
&gt;&gt; Epoch 65 finished 	ANN training loss 0.001994
&gt;&gt; Epoch 66 finished 	ANN training loss 0.002216
&gt;&gt; Epoch 67 finished 	ANN training loss 0.001872
&gt;&gt; Epoch 68 finished 	ANN training loss 0.001980
&gt;&gt; Epoch 69 finished 	ANN training loss 0.001816
&gt;&gt; Epoch 70 finished 	ANN training loss 0.001969
&gt;&gt; Epoch 71 finished 	ANN training loss 0.002076
&gt;&gt; Epoch 72 finished 	ANN training loss 0.002132
&gt;&gt; Epoch 73 finished 	ANN training loss 0.001786
&gt;&gt; Epoch 74 finished 	ANN training loss 0.001732
&gt;&gt; Epoch 75 finished 	ANN training loss 0.001871
&gt;&gt; Epoch 76 finished 	ANN training loss 0.001601
&gt;&gt; Epoch 77 finished 	ANN training loss 0.001554
&gt;&gt; Epoch 78 finished 	ANN training loss 0.001788
&gt;&gt; Epoch 79 finished 	ANN training loss 0.001425
&gt;&gt; Epoch 80 finished 	ANN training loss 0.001425
&gt;&gt; Epoch 81 finished 	ANN training loss 0.001311
&gt;&gt; Epoch 82 finished 	ANN training loss 0.001223
&gt;&gt; Epoch 83 finished 	ANN training loss 0.001107
&gt;&gt; Epoch 84 finished 	ANN training loss 0.001179
&gt;&gt; Epoch 85 finished 	ANN training loss 0.001530
&gt;&gt; Epoch 86 finished 	ANN training loss 0.001150
&gt;&gt; Epoch 87 finished 	ANN training loss 0.001061
&gt;&gt; Epoch 88 finished 	ANN training loss 0.001028
&gt;&gt; Epoch 89 finished 	ANN training loss 0.001123
&gt;&gt; Epoch 90 finished 	ANN training loss 0.001113
&gt;&gt; Epoch 91 finished 	ANN training loss 0.001308
&gt;&gt; Epoch 92 finished 	ANN training loss 0.001027
&gt;&gt; Epoch 93 finished 	ANN training loss 0.000953
&gt;&gt; Epoch 94 finished 	ANN training loss 0.000967
&gt;&gt; Epoch 95 finished 	ANN training loss 0.000763
&gt;&gt; Epoch 96 finished 	ANN training loss 0.000894
&gt;&gt; Epoch 97 finished 	ANN training loss 0.001017
&gt;&gt; Epoch 98 finished 	ANN training loss 0.000718
&gt;&gt; Epoch 99 finished 	ANN training loss 0.000792
[END] Fine tuning step
Done.
Accuracy: 0.905000
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[82]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ACCURACY: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">epoch_rbm_acc</span><span class="p">[</span><span class="mi">4</span><span class="p">]))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>ACCURACY: 0.905
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Setting 6</strong></p>

<pre><code>n_epochs_rbm=30</code></pre>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[83]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">epoch_rbm_acc</span><span class="p">[</span><span class="mi">5</span><span class="p">]</span> <span class="o">=</span> <span class="n">deep_belief_net</span><span class="p">(</span><span class="n">n_epochs_rbm</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[START] Pre-training step:
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 92.470978
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 85.301331
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 42.295254
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 49.635422
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 35.004749
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 54.408066
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 28.305761
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 34.892990
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 29.667559
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 45.455029
&gt;&gt; Epoch 11 finished 	RBM Reconstruction error 37.146450
&gt;&gt; Epoch 12 finished 	RBM Reconstruction error 38.424442
&gt;&gt; Epoch 13 finished 	RBM Reconstruction error 23.905962
&gt;&gt; Epoch 14 finished 	RBM Reconstruction error 27.231716
&gt;&gt; Epoch 15 finished 	RBM Reconstruction error 36.267136
&gt;&gt; Epoch 16 finished 	RBM Reconstruction error 36.911022
&gt;&gt; Epoch 17 finished 	RBM Reconstruction error 29.398628
&gt;&gt; Epoch 18 finished 	RBM Reconstruction error 29.262503
&gt;&gt; Epoch 19 finished 	RBM Reconstruction error 60.107067
&gt;&gt; Epoch 20 finished 	RBM Reconstruction error 31.638620
&gt;&gt; Epoch 21 finished 	RBM Reconstruction error 47.515308
&gt;&gt; Epoch 22 finished 	RBM Reconstruction error 26.997276
&gt;&gt; Epoch 23 finished 	RBM Reconstruction error 41.345928
&gt;&gt; Epoch 24 finished 	RBM Reconstruction error 31.000488
&gt;&gt; Epoch 25 finished 	RBM Reconstruction error 28.862558
&gt;&gt; Epoch 26 finished 	RBM Reconstruction error 48.974628
&gt;&gt; Epoch 27 finished 	RBM Reconstruction error 26.598774
&gt;&gt; Epoch 28 finished 	RBM Reconstruction error 38.259293
&gt;&gt; Epoch 29 finished 	RBM Reconstruction error 32.167492
&gt;&gt; Epoch 30 finished 	RBM Reconstruction error 38.163063
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 156.527618
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 170.634262
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 131.709961
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 84.261086
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 110.089180
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 133.341827
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 149.274155
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 169.515671
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 162.134476
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 134.458298
&gt;&gt; Epoch 11 finished 	RBM Reconstruction error 167.629059
&gt;&gt; Epoch 12 finished 	RBM Reconstruction error 76.496315
&gt;&gt; Epoch 13 finished 	RBM Reconstruction error 136.987457
&gt;&gt; Epoch 14 finished 	RBM Reconstruction error 94.472458
&gt;&gt; Epoch 15 finished 	RBM Reconstruction error 118.150604
&gt;&gt; Epoch 16 finished 	RBM Reconstruction error 105.890312
&gt;&gt; Epoch 17 finished 	RBM Reconstruction error 100.782692
&gt;&gt; Epoch 18 finished 	RBM Reconstruction error 117.506126
&gt;&gt; Epoch 19 finished 	RBM Reconstruction error 142.523926
&gt;&gt; Epoch 20 finished 	RBM Reconstruction error 130.437424
&gt;&gt; Epoch 21 finished 	RBM Reconstruction error 139.270248
&gt;&gt; Epoch 22 finished 	RBM Reconstruction error 129.217331
&gt;&gt; Epoch 23 finished 	RBM Reconstruction error 167.897339
&gt;&gt; Epoch 24 finished 	RBM Reconstruction error 129.140259
&gt;&gt; Epoch 25 finished 	RBM Reconstruction error 141.708908
&gt;&gt; Epoch 26 finished 	RBM Reconstruction error 101.961205
&gt;&gt; Epoch 27 finished 	RBM Reconstruction error 114.496872
&gt;&gt; Epoch 28 finished 	RBM Reconstruction error 160.465378
&gt;&gt; Epoch 29 finished 	RBM Reconstruction error 110.425629
&gt;&gt; Epoch 30 finished 	RBM Reconstruction error 123.323578
[END] Pre-training step
[START] Fine tuning step:
&gt;&gt; Epoch 0 finished 	ANN training loss 0.572528
&gt;&gt; Epoch 1 finished 	ANN training loss 0.395362
&gt;&gt; Epoch 2 finished 	ANN training loss 0.310572
&gt;&gt; Epoch 3 finished 	ANN training loss 0.239717
&gt;&gt; Epoch 4 finished 	ANN training loss 0.241273
&gt;&gt; Epoch 5 finished 	ANN training loss 0.175358
&gt;&gt; Epoch 6 finished 	ANN training loss 0.148248
&gt;&gt; Epoch 7 finished 	ANN training loss 0.142117
&gt;&gt; Epoch 8 finished 	ANN training loss 0.113329
&gt;&gt; Epoch 9 finished 	ANN training loss 0.106426
&gt;&gt; Epoch 10 finished 	ANN training loss 0.092405
&gt;&gt; Epoch 11 finished 	ANN training loss 0.072843
&gt;&gt; Epoch 12 finished 	ANN training loss 0.077364
&gt;&gt; Epoch 13 finished 	ANN training loss 0.056641
&gt;&gt; Epoch 14 finished 	ANN training loss 0.053819
&gt;&gt; Epoch 15 finished 	ANN training loss 0.050993
&gt;&gt; Epoch 16 finished 	ANN training loss 0.044622
&gt;&gt; Epoch 17 finished 	ANN training loss 0.037784
&gt;&gt; Epoch 18 finished 	ANN training loss 0.035975
&gt;&gt; Epoch 19 finished 	ANN training loss 0.030879
&gt;&gt; Epoch 20 finished 	ANN training loss 0.028483
&gt;&gt; Epoch 21 finished 	ANN training loss 0.023566
&gt;&gt; Epoch 22 finished 	ANN training loss 0.023037
&gt;&gt; Epoch 23 finished 	ANN training loss 0.018066
&gt;&gt; Epoch 24 finished 	ANN training loss 0.016938
&gt;&gt; Epoch 25 finished 	ANN training loss 0.016175
&gt;&gt; Epoch 26 finished 	ANN training loss 0.016632
&gt;&gt; Epoch 27 finished 	ANN training loss 0.016116
&gt;&gt; Epoch 28 finished 	ANN training loss 0.013732
&gt;&gt; Epoch 29 finished 	ANN training loss 0.012201
&gt;&gt; Epoch 30 finished 	ANN training loss 0.012063
&gt;&gt; Epoch 31 finished 	ANN training loss 0.009918
&gt;&gt; Epoch 32 finished 	ANN training loss 0.009608
&gt;&gt; Epoch 33 finished 	ANN training loss 0.008646
&gt;&gt; Epoch 34 finished 	ANN training loss 0.008744
&gt;&gt; Epoch 35 finished 	ANN training loss 0.007667
&gt;&gt; Epoch 36 finished 	ANN training loss 0.011065
&gt;&gt; Epoch 37 finished 	ANN training loss 0.006568
&gt;&gt; Epoch 38 finished 	ANN training loss 0.006130
&gt;&gt; Epoch 39 finished 	ANN training loss 0.006220
&gt;&gt; Epoch 40 finished 	ANN training loss 0.005824
&gt;&gt; Epoch 41 finished 	ANN training loss 0.004509
&gt;&gt; Epoch 42 finished 	ANN training loss 0.004289
&gt;&gt; Epoch 43 finished 	ANN training loss 0.004463
&gt;&gt; Epoch 44 finished 	ANN training loss 0.003908
&gt;&gt; Epoch 45 finished 	ANN training loss 0.003970
&gt;&gt; Epoch 46 finished 	ANN training loss 0.003736
&gt;&gt; Epoch 47 finished 	ANN training loss 0.004104
&gt;&gt; Epoch 48 finished 	ANN training loss 0.004193
&gt;&gt; Epoch 49 finished 	ANN training loss 0.003379
&gt;&gt; Epoch 50 finished 	ANN training loss 0.003105
&gt;&gt; Epoch 51 finished 	ANN training loss 0.003201
&gt;&gt; Epoch 52 finished 	ANN training loss 0.002963
&gt;&gt; Epoch 53 finished 	ANN training loss 0.003433
&gt;&gt; Epoch 54 finished 	ANN training loss 0.002954
&gt;&gt; Epoch 55 finished 	ANN training loss 0.003347
&gt;&gt; Epoch 56 finished 	ANN training loss 0.003261
&gt;&gt; Epoch 57 finished 	ANN training loss 0.002726
&gt;&gt; Epoch 58 finished 	ANN training loss 0.002441
&gt;&gt; Epoch 59 finished 	ANN training loss 0.004095
&gt;&gt; Epoch 60 finished 	ANN training loss 0.002078
&gt;&gt; Epoch 61 finished 	ANN training loss 0.002155
&gt;&gt; Epoch 62 finished 	ANN training loss 0.001965
&gt;&gt; Epoch 63 finished 	ANN training loss 0.002055
&gt;&gt; Epoch 64 finished 	ANN training loss 0.002164
&gt;&gt; Epoch 65 finished 	ANN training loss 0.002029
&gt;&gt; Epoch 66 finished 	ANN training loss 0.001824
&gt;&gt; Epoch 67 finished 	ANN training loss 0.001766
&gt;&gt; Epoch 68 finished 	ANN training loss 0.001924
&gt;&gt; Epoch 69 finished 	ANN training loss 0.001645
&gt;&gt; Epoch 70 finished 	ANN training loss 0.001501
&gt;&gt; Epoch 71 finished 	ANN training loss 0.001504
&gt;&gt; Epoch 72 finished 	ANN training loss 0.001443
&gt;&gt; Epoch 73 finished 	ANN training loss 0.001615
&gt;&gt; Epoch 74 finished 	ANN training loss 0.001390
&gt;&gt; Epoch 75 finished 	ANN training loss 0.001322
&gt;&gt; Epoch 76 finished 	ANN training loss 0.001251
&gt;&gt; Epoch 77 finished 	ANN training loss 0.001251
&gt;&gt; Epoch 78 finished 	ANN training loss 0.001105
&gt;&gt; Epoch 79 finished 	ANN training loss 0.001350
&gt;&gt; Epoch 80 finished 	ANN training loss 0.001378
&gt;&gt; Epoch 81 finished 	ANN training loss 0.001187
&gt;&gt; Epoch 82 finished 	ANN training loss 0.001070
&gt;&gt; Epoch 83 finished 	ANN training loss 0.001095
&gt;&gt; Epoch 84 finished 	ANN training loss 0.001141
&gt;&gt; Epoch 85 finished 	ANN training loss 0.001168
&gt;&gt; Epoch 86 finished 	ANN training loss 0.001146
&gt;&gt; Epoch 87 finished 	ANN training loss 0.001034
&gt;&gt; Epoch 88 finished 	ANN training loss 0.001084
&gt;&gt; Epoch 89 finished 	ANN training loss 0.001132
&gt;&gt; Epoch 90 finished 	ANN training loss 0.001036
&gt;&gt; Epoch 91 finished 	ANN training loss 0.000937
&gt;&gt; Epoch 92 finished 	ANN training loss 0.000908
&gt;&gt; Epoch 93 finished 	ANN training loss 0.000818
&gt;&gt; Epoch 94 finished 	ANN training loss 0.000884
&gt;&gt; Epoch 95 finished 	ANN training loss 0.000947
&gt;&gt; Epoch 96 finished 	ANN training loss 0.000796
&gt;&gt; Epoch 97 finished 	ANN training loss 0.000713
&gt;&gt; Epoch 98 finished 	ANN training loss 0.001003
&gt;&gt; Epoch 99 finished 	ANN training loss 0.000714
[END] Fine tuning step
Done.
Accuracy: 0.920000
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[84]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ACCURACY: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">epoch_rbm_acc</span><span class="p">[</span><span class="mi">5</span><span class="p">]))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>ACCURACY: 0.92
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Setting 7</strong></p>

<pre><code>n_epochs_rbm=40</code></pre>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[85]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">epoch_rbm_acc</span><span class="p">[</span><span class="mi">6</span><span class="p">]</span> <span class="o">=</span> <span class="n">deep_belief_net</span><span class="p">(</span><span class="n">n_epochs_rbm</span><span class="o">=</span><span class="mi">40</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[START] Pre-training step:
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 43.358288
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 56.363575
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 53.744125
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 51.788158
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 38.050350
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 33.176849
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 34.224003
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 21.834377
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 45.354679
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 22.315962
&gt;&gt; Epoch 11 finished 	RBM Reconstruction error 35.863102
&gt;&gt; Epoch 12 finished 	RBM Reconstruction error 29.260483
&gt;&gt; Epoch 13 finished 	RBM Reconstruction error 27.823875
&gt;&gt; Epoch 14 finished 	RBM Reconstruction error 29.280943
&gt;&gt; Epoch 15 finished 	RBM Reconstruction error 21.086147
&gt;&gt; Epoch 16 finished 	RBM Reconstruction error 20.398621
&gt;&gt; Epoch 17 finished 	RBM Reconstruction error 48.363113
&gt;&gt; Epoch 18 finished 	RBM Reconstruction error 36.256248
&gt;&gt; Epoch 19 finished 	RBM Reconstruction error 23.029125
&gt;&gt; Epoch 20 finished 	RBM Reconstruction error 36.814152
&gt;&gt; Epoch 21 finished 	RBM Reconstruction error 28.656294
&gt;&gt; Epoch 22 finished 	RBM Reconstruction error 33.848091
&gt;&gt; Epoch 23 finished 	RBM Reconstruction error 23.485928
&gt;&gt; Epoch 24 finished 	RBM Reconstruction error 36.647797
&gt;&gt; Epoch 25 finished 	RBM Reconstruction error 40.407646
&gt;&gt; Epoch 26 finished 	RBM Reconstruction error 34.541111
&gt;&gt; Epoch 27 finished 	RBM Reconstruction error 41.327587
&gt;&gt; Epoch 28 finished 	RBM Reconstruction error 40.208481
&gt;&gt; Epoch 29 finished 	RBM Reconstruction error 28.690596
&gt;&gt; Epoch 30 finished 	RBM Reconstruction error 40.044468
&gt;&gt; Epoch 31 finished 	RBM Reconstruction error 34.405594
&gt;&gt; Epoch 32 finished 	RBM Reconstruction error 30.950840
&gt;&gt; Epoch 33 finished 	RBM Reconstruction error 33.167236
&gt;&gt; Epoch 34 finished 	RBM Reconstruction error 38.003414
&gt;&gt; Epoch 35 finished 	RBM Reconstruction error 42.338654
&gt;&gt; Epoch 36 finished 	RBM Reconstruction error 40.864254
&gt;&gt; Epoch 37 finished 	RBM Reconstruction error 41.628902
&gt;&gt; Epoch 38 finished 	RBM Reconstruction error 27.510088
&gt;&gt; Epoch 39 finished 	RBM Reconstruction error 30.467539
&gt;&gt; Epoch 40 finished 	RBM Reconstruction error 36.698532
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 96.253242
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 183.684174
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 78.674377
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 91.714569
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 110.993431
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 138.788040
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 89.061523
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 136.134369
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 114.479111
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 108.112259
&gt;&gt; Epoch 11 finished 	RBM Reconstruction error 98.016914
&gt;&gt; Epoch 12 finished 	RBM Reconstruction error 104.318283
&gt;&gt; Epoch 13 finished 	RBM Reconstruction error 100.402695
&gt;&gt; Epoch 14 finished 	RBM Reconstruction error 139.686798
&gt;&gt; Epoch 15 finished 	RBM Reconstruction error 115.318695
&gt;&gt; Epoch 16 finished 	RBM Reconstruction error 102.192581
&gt;&gt; Epoch 17 finished 	RBM Reconstruction error 122.168610
&gt;&gt; Epoch 18 finished 	RBM Reconstruction error 124.092590
&gt;&gt; Epoch 19 finished 	RBM Reconstruction error 88.544075
&gt;&gt; Epoch 20 finished 	RBM Reconstruction error 120.426048
&gt;&gt; Epoch 21 finished 	RBM Reconstruction error 129.410858
&gt;&gt; Epoch 22 finished 	RBM Reconstruction error 98.306053
&gt;&gt; Epoch 23 finished 	RBM Reconstruction error 136.514603
&gt;&gt; Epoch 24 finished 	RBM Reconstruction error 98.119904
&gt;&gt; Epoch 25 finished 	RBM Reconstruction error 100.555511
&gt;&gt; Epoch 26 finished 	RBM Reconstruction error 143.315979
&gt;&gt; Epoch 27 finished 	RBM Reconstruction error 142.413483
&gt;&gt; Epoch 28 finished 	RBM Reconstruction error 119.005058
&gt;&gt; Epoch 29 finished 	RBM Reconstruction error 95.662399
&gt;&gt; Epoch 30 finished 	RBM Reconstruction error 132.270020
&gt;&gt; Epoch 31 finished 	RBM Reconstruction error 118.468826
&gt;&gt; Epoch 32 finished 	RBM Reconstruction error 136.883179
&gt;&gt; Epoch 33 finished 	RBM Reconstruction error 123.653419
&gt;&gt; Epoch 34 finished 	RBM Reconstruction error 153.304733
&gt;&gt; Epoch 35 finished 	RBM Reconstruction error 115.269630
&gt;&gt; Epoch 36 finished 	RBM Reconstruction error 136.281601
&gt;&gt; Epoch 37 finished 	RBM Reconstruction error 122.730011
&gt;&gt; Epoch 38 finished 	RBM Reconstruction error 119.815926
&gt;&gt; Epoch 39 finished 	RBM Reconstruction error 147.712051
&gt;&gt; Epoch 40 finished 	RBM Reconstruction error 97.136719
[END] Pre-training step
[START] Fine tuning step:
&gt;&gt; Epoch 0 finished 	ANN training loss 0.575559
&gt;&gt; Epoch 1 finished 	ANN training loss 0.405406
&gt;&gt; Epoch 2 finished 	ANN training loss 0.308998
&gt;&gt; Epoch 3 finished 	ANN training loss 0.247995
&gt;&gt; Epoch 4 finished 	ANN training loss 0.228701
&gt;&gt; Epoch 5 finished 	ANN training loss 0.175525
&gt;&gt; Epoch 6 finished 	ANN training loss 0.154795
&gt;&gt; Epoch 7 finished 	ANN training loss 0.200008
&gt;&gt; Epoch 8 finished 	ANN training loss 0.124494
&gt;&gt; Epoch 9 finished 	ANN training loss 0.096474
&gt;&gt; Epoch 10 finished 	ANN training loss 0.087040
&gt;&gt; Epoch 11 finished 	ANN training loss 0.073994
&gt;&gt; Epoch 12 finished 	ANN training loss 0.068757
&gt;&gt; Epoch 13 finished 	ANN training loss 0.060631
&gt;&gt; Epoch 14 finished 	ANN training loss 0.054501
&gt;&gt; Epoch 15 finished 	ANN training loss 0.047281
&gt;&gt; Epoch 16 finished 	ANN training loss 0.052018
&gt;&gt; Epoch 17 finished 	ANN training loss 0.041238
&gt;&gt; Epoch 18 finished 	ANN training loss 0.032027
&gt;&gt; Epoch 19 finished 	ANN training loss 0.029175
&gt;&gt; Epoch 20 finished 	ANN training loss 0.030474
&gt;&gt; Epoch 21 finished 	ANN training loss 0.026703
&gt;&gt; Epoch 22 finished 	ANN training loss 0.023318
&gt;&gt; Epoch 23 finished 	ANN training loss 0.022907
&gt;&gt; Epoch 24 finished 	ANN training loss 0.018816
&gt;&gt; Epoch 25 finished 	ANN training loss 0.017781
&gt;&gt; Epoch 26 finished 	ANN training loss 0.015022
&gt;&gt; Epoch 27 finished 	ANN training loss 0.014593
&gt;&gt; Epoch 28 finished 	ANN training loss 0.012941
&gt;&gt; Epoch 29 finished 	ANN training loss 0.012645
&gt;&gt; Epoch 30 finished 	ANN training loss 0.010471
&gt;&gt; Epoch 31 finished 	ANN training loss 0.010958
&gt;&gt; Epoch 32 finished 	ANN training loss 0.009815
&gt;&gt; Epoch 33 finished 	ANN training loss 0.009746
&gt;&gt; Epoch 34 finished 	ANN training loss 0.007565
&gt;&gt; Epoch 35 finished 	ANN training loss 0.008848
&gt;&gt; Epoch 36 finished 	ANN training loss 0.007828
&gt;&gt; Epoch 37 finished 	ANN training loss 0.006719
&gt;&gt; Epoch 38 finished 	ANN training loss 0.006489
&gt;&gt; Epoch 39 finished 	ANN training loss 0.006735
&gt;&gt; Epoch 40 finished 	ANN training loss 0.005985
&gt;&gt; Epoch 41 finished 	ANN training loss 0.005306
&gt;&gt; Epoch 42 finished 	ANN training loss 0.005140
&gt;&gt; Epoch 43 finished 	ANN training loss 0.005064
&gt;&gt; Epoch 44 finished 	ANN training loss 0.005068
&gt;&gt; Epoch 45 finished 	ANN training loss 0.004523
&gt;&gt; Epoch 46 finished 	ANN training loss 0.004456
&gt;&gt; Epoch 47 finished 	ANN training loss 0.004155
&gt;&gt; Epoch 48 finished 	ANN training loss 0.003685
&gt;&gt; Epoch 49 finished 	ANN training loss 0.003845
&gt;&gt; Epoch 50 finished 	ANN training loss 0.003182
&gt;&gt; Epoch 51 finished 	ANN training loss 0.003157
&gt;&gt; Epoch 52 finished 	ANN training loss 0.002802
&gt;&gt; Epoch 53 finished 	ANN training loss 0.002670
&gt;&gt; Epoch 54 finished 	ANN training loss 0.002610
&gt;&gt; Epoch 55 finished 	ANN training loss 0.002858
&gt;&gt; Epoch 56 finished 	ANN training loss 0.002601
&gt;&gt; Epoch 57 finished 	ANN training loss 0.002551
&gt;&gt; Epoch 58 finished 	ANN training loss 0.002157
&gt;&gt; Epoch 59 finished 	ANN training loss 0.002258
&gt;&gt; Epoch 60 finished 	ANN training loss 0.001994
&gt;&gt; Epoch 61 finished 	ANN training loss 0.002260
&gt;&gt; Epoch 62 finished 	ANN training loss 0.001872
&gt;&gt; Epoch 63 finished 	ANN training loss 0.001822
&gt;&gt; Epoch 64 finished 	ANN training loss 0.001723
&gt;&gt; Epoch 65 finished 	ANN training loss 0.001777
&gt;&gt; Epoch 66 finished 	ANN training loss 0.001591
&gt;&gt; Epoch 67 finished 	ANN training loss 0.001657
&gt;&gt; Epoch 68 finished 	ANN training loss 0.001491
&gt;&gt; Epoch 69 finished 	ANN training loss 0.001864
&gt;&gt; Epoch 70 finished 	ANN training loss 0.001798
&gt;&gt; Epoch 71 finished 	ANN training loss 0.001346
&gt;&gt; Epoch 72 finished 	ANN training loss 0.001330
&gt;&gt; Epoch 73 finished 	ANN training loss 0.001384
&gt;&gt; Epoch 74 finished 	ANN training loss 0.001189
&gt;&gt; Epoch 75 finished 	ANN training loss 0.001220
&gt;&gt; Epoch 76 finished 	ANN training loss 0.001221
&gt;&gt; Epoch 77 finished 	ANN training loss 0.001270
&gt;&gt; Epoch 78 finished 	ANN training loss 0.001121
&gt;&gt; Epoch 79 finished 	ANN training loss 0.001050
&gt;&gt; Epoch 80 finished 	ANN training loss 0.000943
&gt;&gt; Epoch 81 finished 	ANN training loss 0.001007
&gt;&gt; Epoch 82 finished 	ANN training loss 0.001005
&gt;&gt; Epoch 83 finished 	ANN training loss 0.000976
&gt;&gt; Epoch 84 finished 	ANN training loss 0.000965
&gt;&gt; Epoch 85 finished 	ANN training loss 0.001021
&gt;&gt; Epoch 86 finished 	ANN training loss 0.000942
&gt;&gt; Epoch 87 finished 	ANN training loss 0.000949
&gt;&gt; Epoch 88 finished 	ANN training loss 0.000796
&gt;&gt; Epoch 89 finished 	ANN training loss 0.000785
&gt;&gt; Epoch 90 finished 	ANN training loss 0.000735
&gt;&gt; Epoch 91 finished 	ANN training loss 0.000766
&gt;&gt; Epoch 92 finished 	ANN training loss 0.000729
&gt;&gt; Epoch 93 finished 	ANN training loss 0.000678
&gt;&gt; Epoch 94 finished 	ANN training loss 0.000698
&gt;&gt; Epoch 95 finished 	ANN training loss 0.000730
&gt;&gt; Epoch 96 finished 	ANN training loss 0.000700
&gt;&gt; Epoch 97 finished 	ANN training loss 0.000633
&gt;&gt; Epoch 98 finished 	ANN training loss 0.000574
&gt;&gt; Epoch 99 finished 	ANN training loss 0.000587
[END] Fine tuning step
Done.
Accuracy: 0.930000
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[86]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ACCURACY: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">epoch_rbm_acc</span><span class="p">[</span><span class="mi">6</span><span class="p">]))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>ACCURACY: 0.93
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Setting 8</strong></p>

<pre><code>n_epochs_rbm=50</code></pre>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[87]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">epoch_rbm_acc</span><span class="p">[</span><span class="mi">7</span><span class="p">]</span> <span class="o">=</span> <span class="n">deep_belief_net</span><span class="p">(</span><span class="n">n_epochs_rbm</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[START] Pre-training step:
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 54.985214
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 62.096172
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 45.251816
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 43.081493
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 29.928345
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 28.466885
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 36.912857
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 45.177746
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 49.283730
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 46.618320
&gt;&gt; Epoch 11 finished 	RBM Reconstruction error 35.642426
&gt;&gt; Epoch 12 finished 	RBM Reconstruction error 30.457457
&gt;&gt; Epoch 13 finished 	RBM Reconstruction error 31.268808
&gt;&gt; Epoch 14 finished 	RBM Reconstruction error 45.809601
&gt;&gt; Epoch 15 finished 	RBM Reconstruction error 44.016468
&gt;&gt; Epoch 16 finished 	RBM Reconstruction error 31.719570
&gt;&gt; Epoch 17 finished 	RBM Reconstruction error 33.370472
&gt;&gt; Epoch 18 finished 	RBM Reconstruction error 40.761951
&gt;&gt; Epoch 19 finished 	RBM Reconstruction error 31.600405
&gt;&gt; Epoch 20 finished 	RBM Reconstruction error 32.468315
&gt;&gt; Epoch 21 finished 	RBM Reconstruction error 31.679224
&gt;&gt; Epoch 22 finished 	RBM Reconstruction error 32.492111
&gt;&gt; Epoch 23 finished 	RBM Reconstruction error 35.449799
&gt;&gt; Epoch 24 finished 	RBM Reconstruction error 43.675613
&gt;&gt; Epoch 25 finished 	RBM Reconstruction error 33.906666
&gt;&gt; Epoch 26 finished 	RBM Reconstruction error 40.161636
&gt;&gt; Epoch 27 finished 	RBM Reconstruction error 28.779116
&gt;&gt; Epoch 28 finished 	RBM Reconstruction error 34.979935
&gt;&gt; Epoch 29 finished 	RBM Reconstruction error 28.730215
&gt;&gt; Epoch 30 finished 	RBM Reconstruction error 31.814503
&gt;&gt; Epoch 31 finished 	RBM Reconstruction error 42.189648
&gt;&gt; Epoch 32 finished 	RBM Reconstruction error 35.736607
&gt;&gt; Epoch 33 finished 	RBM Reconstruction error 36.740726
&gt;&gt; Epoch 34 finished 	RBM Reconstruction error 30.774963
&gt;&gt; Epoch 35 finished 	RBM Reconstruction error 38.731300
&gt;&gt; Epoch 36 finished 	RBM Reconstruction error 36.828842
&gt;&gt; Epoch 37 finished 	RBM Reconstruction error 40.284721
&gt;&gt; Epoch 38 finished 	RBM Reconstruction error 33.431606
&gt;&gt; Epoch 39 finished 	RBM Reconstruction error 53.262314
&gt;&gt; Epoch 40 finished 	RBM Reconstruction error 33.641586
&gt;&gt; Epoch 41 finished 	RBM Reconstruction error 45.688915
&gt;&gt; Epoch 42 finished 	RBM Reconstruction error 37.920570
&gt;&gt; Epoch 43 finished 	RBM Reconstruction error 34.334785
&gt;&gt; Epoch 44 finished 	RBM Reconstruction error 29.965361
&gt;&gt; Epoch 45 finished 	RBM Reconstruction error 35.227638
&gt;&gt; Epoch 46 finished 	RBM Reconstruction error 32.324764
&gt;&gt; Epoch 47 finished 	RBM Reconstruction error 36.219288
&gt;&gt; Epoch 48 finished 	RBM Reconstruction error 38.799026
&gt;&gt; Epoch 49 finished 	RBM Reconstruction error 39.128159
&gt;&gt; Epoch 50 finished 	RBM Reconstruction error 32.884838
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 125.804100
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 77.486778
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 93.401787
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 99.944412
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 78.066780
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 99.720978
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 43.306313
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 82.794495
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 98.328117
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 86.998634
&gt;&gt; Epoch 11 finished 	RBM Reconstruction error 86.324905
&gt;&gt; Epoch 12 finished 	RBM Reconstruction error 76.527878
&gt;&gt; Epoch 13 finished 	RBM Reconstruction error 59.903404
&gt;&gt; Epoch 14 finished 	RBM Reconstruction error 88.480637
&gt;&gt; Epoch 15 finished 	RBM Reconstruction error 86.590782
&gt;&gt; Epoch 16 finished 	RBM Reconstruction error 102.271576
&gt;&gt; Epoch 17 finished 	RBM Reconstruction error 87.106445
&gt;&gt; Epoch 18 finished 	RBM Reconstruction error 99.319748
&gt;&gt; Epoch 19 finished 	RBM Reconstruction error 111.583473
&gt;&gt; Epoch 20 finished 	RBM Reconstruction error 83.346527
&gt;&gt; Epoch 21 finished 	RBM Reconstruction error 86.381500
&gt;&gt; Epoch 22 finished 	RBM Reconstruction error 106.694183
&gt;&gt; Epoch 23 finished 	RBM Reconstruction error 102.709122
&gt;&gt; Epoch 24 finished 	RBM Reconstruction error 124.166740
&gt;&gt; Epoch 25 finished 	RBM Reconstruction error 85.671097
&gt;&gt; Epoch 26 finished 	RBM Reconstruction error 93.935783
&gt;&gt; Epoch 27 finished 	RBM Reconstruction error 81.691200
&gt;&gt; Epoch 28 finished 	RBM Reconstruction error 85.807098
&gt;&gt; Epoch 29 finished 	RBM Reconstruction error 77.376297
&gt;&gt; Epoch 30 finished 	RBM Reconstruction error 93.682793
&gt;&gt; Epoch 31 finished 	RBM Reconstruction error 159.651398
&gt;&gt; Epoch 32 finished 	RBM Reconstruction error 90.419952
&gt;&gt; Epoch 33 finished 	RBM Reconstruction error 94.738319
&gt;&gt; Epoch 34 finished 	RBM Reconstruction error 67.020050
&gt;&gt; Epoch 35 finished 	RBM Reconstruction error 84.792435
&gt;&gt; Epoch 36 finished 	RBM Reconstruction error 96.659531
&gt;&gt; Epoch 37 finished 	RBM Reconstruction error 105.481972
&gt;&gt; Epoch 38 finished 	RBM Reconstruction error 114.097282
&gt;&gt; Epoch 39 finished 	RBM Reconstruction error 76.890915
&gt;&gt; Epoch 40 finished 	RBM Reconstruction error 81.217575
&gt;&gt; Epoch 41 finished 	RBM Reconstruction error 97.449081
&gt;&gt; Epoch 42 finished 	RBM Reconstruction error 75.984642
&gt;&gt; Epoch 43 finished 	RBM Reconstruction error 79.706650
&gt;&gt; Epoch 44 finished 	RBM Reconstruction error 96.392365
&gt;&gt; Epoch 45 finished 	RBM Reconstruction error 114.727180
&gt;&gt; Epoch 46 finished 	RBM Reconstruction error 104.383186
&gt;&gt; Epoch 47 finished 	RBM Reconstruction error 95.007362
&gt;&gt; Epoch 48 finished 	RBM Reconstruction error 91.924377
&gt;&gt; Epoch 49 finished 	RBM Reconstruction error 92.933937
&gt;&gt; Epoch 50 finished 	RBM Reconstruction error 87.460838
[END] Pre-training step
[START] Fine tuning step:
&gt;&gt; Epoch 0 finished 	ANN training loss 0.610499
&gt;&gt; Epoch 1 finished 	ANN training loss 0.395352
&gt;&gt; Epoch 2 finished 	ANN training loss 0.286621
&gt;&gt; Epoch 3 finished 	ANN training loss 0.246337
&gt;&gt; Epoch 4 finished 	ANN training loss 0.223768
&gt;&gt; Epoch 5 finished 	ANN training loss 0.197915
&gt;&gt; Epoch 6 finished 	ANN training loss 0.145112
&gt;&gt; Epoch 7 finished 	ANN training loss 0.133209
&gt;&gt; Epoch 8 finished 	ANN training loss 0.112177
&gt;&gt; Epoch 9 finished 	ANN training loss 0.086660
&gt;&gt; Epoch 10 finished 	ANN training loss 0.084649
&gt;&gt; Epoch 11 finished 	ANN training loss 0.068575
&gt;&gt; Epoch 12 finished 	ANN training loss 0.077028
&gt;&gt; Epoch 13 finished 	ANN training loss 0.061931
&gt;&gt; Epoch 14 finished 	ANN training loss 0.056546
&gt;&gt; Epoch 15 finished 	ANN training loss 0.042343
&gt;&gt; Epoch 16 finished 	ANN training loss 0.040199
&gt;&gt; Epoch 17 finished 	ANN training loss 0.037044
&gt;&gt; Epoch 18 finished 	ANN training loss 0.030045
&gt;&gt; Epoch 19 finished 	ANN training loss 0.031748
&gt;&gt; Epoch 20 finished 	ANN training loss 0.028787
&gt;&gt; Epoch 21 finished 	ANN training loss 0.023705
&gt;&gt; Epoch 22 finished 	ANN training loss 0.021906
&gt;&gt; Epoch 23 finished 	ANN training loss 0.020605
&gt;&gt; Epoch 24 finished 	ANN training loss 0.017037
&gt;&gt; Epoch 25 finished 	ANN training loss 0.018187
&gt;&gt; Epoch 26 finished 	ANN training loss 0.015041
&gt;&gt; Epoch 27 finished 	ANN training loss 0.015201
&gt;&gt; Epoch 28 finished 	ANN training loss 0.012150
&gt;&gt; Epoch 29 finished 	ANN training loss 0.011889
&gt;&gt; Epoch 30 finished 	ANN training loss 0.010933
&gt;&gt; Epoch 31 finished 	ANN training loss 0.010373
&gt;&gt; Epoch 32 finished 	ANN training loss 0.009542
&gt;&gt; Epoch 33 finished 	ANN training loss 0.009623
&gt;&gt; Epoch 34 finished 	ANN training loss 0.009694
&gt;&gt; Epoch 35 finished 	ANN training loss 0.008224
&gt;&gt; Epoch 36 finished 	ANN training loss 0.007116
&gt;&gt; Epoch 37 finished 	ANN training loss 0.007418
&gt;&gt; Epoch 38 finished 	ANN training loss 0.005990
&gt;&gt; Epoch 39 finished 	ANN training loss 0.006221
&gt;&gt; Epoch 40 finished 	ANN training loss 0.005930
&gt;&gt; Epoch 41 finished 	ANN training loss 0.005008
&gt;&gt; Epoch 42 finished 	ANN training loss 0.004821
&gt;&gt; Epoch 43 finished 	ANN training loss 0.004608
&gt;&gt; Epoch 44 finished 	ANN training loss 0.004338
&gt;&gt; Epoch 45 finished 	ANN training loss 0.004340
&gt;&gt; Epoch 46 finished 	ANN training loss 0.004184
&gt;&gt; Epoch 47 finished 	ANN training loss 0.004392
&gt;&gt; Epoch 48 finished 	ANN training loss 0.003624
&gt;&gt; Epoch 49 finished 	ANN training loss 0.003609
&gt;&gt; Epoch 50 finished 	ANN training loss 0.003329
&gt;&gt; Epoch 51 finished 	ANN training loss 0.003367
&gt;&gt; Epoch 52 finished 	ANN training loss 0.003713
&gt;&gt; Epoch 53 finished 	ANN training loss 0.003610
&gt;&gt; Epoch 54 finished 	ANN training loss 0.003883
&gt;&gt; Epoch 55 finished 	ANN training loss 0.002767
&gt;&gt; Epoch 56 finished 	ANN training loss 0.003932
&gt;&gt; Epoch 57 finished 	ANN training loss 0.002878
&gt;&gt; Epoch 58 finished 	ANN training loss 0.002623
&gt;&gt; Epoch 59 finished 	ANN training loss 0.002451
&gt;&gt; Epoch 60 finished 	ANN training loss 0.002198
&gt;&gt; Epoch 61 finished 	ANN training loss 0.002257
&gt;&gt; Epoch 62 finished 	ANN training loss 0.002150
&gt;&gt; Epoch 63 finished 	ANN training loss 0.001997
&gt;&gt; Epoch 64 finished 	ANN training loss 0.002560
&gt;&gt; Epoch 65 finished 	ANN training loss 0.002030
&gt;&gt; Epoch 66 finished 	ANN training loss 0.001902
&gt;&gt; Epoch 67 finished 	ANN training loss 0.001634
&gt;&gt; Epoch 68 finished 	ANN training loss 0.001685
&gt;&gt; Epoch 69 finished 	ANN training loss 0.001626
&gt;&gt; Epoch 70 finished 	ANN training loss 0.001636
&gt;&gt; Epoch 71 finished 	ANN training loss 0.001502
&gt;&gt; Epoch 72 finished 	ANN training loss 0.001697
&gt;&gt; Epoch 73 finished 	ANN training loss 0.001512
&gt;&gt; Epoch 74 finished 	ANN training loss 0.001432
&gt;&gt; Epoch 75 finished 	ANN training loss 0.001206
&gt;&gt; Epoch 76 finished 	ANN training loss 0.001236
&gt;&gt; Epoch 77 finished 	ANN training loss 0.001196
&gt;&gt; Epoch 78 finished 	ANN training loss 0.001236
&gt;&gt; Epoch 79 finished 	ANN training loss 0.001169
&gt;&gt; Epoch 80 finished 	ANN training loss 0.001196
&gt;&gt; Epoch 81 finished 	ANN training loss 0.001166
&gt;&gt; Epoch 82 finished 	ANN training loss 0.001105
&gt;&gt; Epoch 83 finished 	ANN training loss 0.001154
&gt;&gt; Epoch 84 finished 	ANN training loss 0.001090
&gt;&gt; Epoch 85 finished 	ANN training loss 0.001076
&gt;&gt; Epoch 86 finished 	ANN training loss 0.001028
&gt;&gt; Epoch 87 finished 	ANN training loss 0.000973
&gt;&gt; Epoch 88 finished 	ANN training loss 0.000949
&gt;&gt; Epoch 89 finished 	ANN training loss 0.000939
&gt;&gt; Epoch 90 finished 	ANN training loss 0.000887
&gt;&gt; Epoch 91 finished 	ANN training loss 0.000964
&gt;&gt; Epoch 92 finished 	ANN training loss 0.000898
&gt;&gt; Epoch 93 finished 	ANN training loss 0.000889
&gt;&gt; Epoch 94 finished 	ANN training loss 0.000803
&gt;&gt; Epoch 95 finished 	ANN training loss 0.000858
&gt;&gt; Epoch 96 finished 	ANN training loss 0.000817
&gt;&gt; Epoch 97 finished 	ANN training loss 0.000752
&gt;&gt; Epoch 98 finished 	ANN training loss 0.000755
&gt;&gt; Epoch 99 finished 	ANN training loss 0.000734
[END] Fine tuning step
Done.
Accuracy: 0.900000
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[88]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ACCURACY: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">epoch_rbm_acc</span><span class="p">[</span><span class="mi">7</span><span class="p">]))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>ACCURACY: 0.9
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Setting 9</strong></p>

<pre><code>n_epochs_rbm=100</code></pre>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[89]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">epoch_rbm_acc</span><span class="p">[</span><span class="mi">8</span><span class="p">]</span> <span class="o">=</span> <span class="n">deep_belief_net</span><span class="p">(</span><span class="n">n_epochs_rbm</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[START] Pre-training step:
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 102.369392
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 39.580040
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 58.195435
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 42.953144
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 30.074026
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 25.546534
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 32.715004
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 28.658079
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 27.248291
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 21.905323
&gt;&gt; Epoch 11 finished 	RBM Reconstruction error 20.972374
&gt;&gt; Epoch 12 finished 	RBM Reconstruction error 30.766115
&gt;&gt; Epoch 13 finished 	RBM Reconstruction error 43.735683
&gt;&gt; Epoch 14 finished 	RBM Reconstruction error 27.307983
&gt;&gt; Epoch 15 finished 	RBM Reconstruction error 33.528534
&gt;&gt; Epoch 16 finished 	RBM Reconstruction error 18.731007
&gt;&gt; Epoch 17 finished 	RBM Reconstruction error 27.331324
&gt;&gt; Epoch 18 finished 	RBM Reconstruction error 38.402706
&gt;&gt; Epoch 19 finished 	RBM Reconstruction error 29.862865
&gt;&gt; Epoch 20 finished 	RBM Reconstruction error 40.268932
&gt;&gt; Epoch 21 finished 	RBM Reconstruction error 32.184887
&gt;&gt; Epoch 22 finished 	RBM Reconstruction error 39.825115
&gt;&gt; Epoch 23 finished 	RBM Reconstruction error 26.806177
&gt;&gt; Epoch 24 finished 	RBM Reconstruction error 29.430204
&gt;&gt; Epoch 25 finished 	RBM Reconstruction error 23.967192
&gt;&gt; Epoch 26 finished 	RBM Reconstruction error 23.805079
&gt;&gt; Epoch 27 finished 	RBM Reconstruction error 34.138329
&gt;&gt; Epoch 28 finished 	RBM Reconstruction error 36.367519
&gt;&gt; Epoch 29 finished 	RBM Reconstruction error 32.880257
&gt;&gt; Epoch 30 finished 	RBM Reconstruction error 43.483456
&gt;&gt; Epoch 31 finished 	RBM Reconstruction error 27.266542
&gt;&gt; Epoch 32 finished 	RBM Reconstruction error 29.320518
&gt;&gt; Epoch 33 finished 	RBM Reconstruction error 37.196865
&gt;&gt; Epoch 34 finished 	RBM Reconstruction error 27.303625
&gt;&gt; Epoch 35 finished 	RBM Reconstruction error 26.077913
&gt;&gt; Epoch 36 finished 	RBM Reconstruction error 50.016006
&gt;&gt; Epoch 37 finished 	RBM Reconstruction error 27.148087
&gt;&gt; Epoch 38 finished 	RBM Reconstruction error 23.290499
&gt;&gt; Epoch 39 finished 	RBM Reconstruction error 32.554523
&gt;&gt; Epoch 40 finished 	RBM Reconstruction error 30.474489
&gt;&gt; Epoch 41 finished 	RBM Reconstruction error 30.306696
&gt;&gt; Epoch 42 finished 	RBM Reconstruction error 36.048634
&gt;&gt; Epoch 43 finished 	RBM Reconstruction error 29.288630
&gt;&gt; Epoch 44 finished 	RBM Reconstruction error 40.861832
&gt;&gt; Epoch 45 finished 	RBM Reconstruction error 30.047388
&gt;&gt; Epoch 46 finished 	RBM Reconstruction error 26.523979
&gt;&gt; Epoch 47 finished 	RBM Reconstruction error 33.720680
&gt;&gt; Epoch 48 finished 	RBM Reconstruction error 40.593685
&gt;&gt; Epoch 49 finished 	RBM Reconstruction error 37.461300
&gt;&gt; Epoch 50 finished 	RBM Reconstruction error 35.021915
&gt;&gt; Epoch 51 finished 	RBM Reconstruction error 33.216915
&gt;&gt; Epoch 52 finished 	RBM Reconstruction error 31.770569
&gt;&gt; Epoch 53 finished 	RBM Reconstruction error 42.008743
&gt;&gt; Epoch 54 finished 	RBM Reconstruction error 34.392147
&gt;&gt; Epoch 55 finished 	RBM Reconstruction error 35.436268
&gt;&gt; Epoch 56 finished 	RBM Reconstruction error 39.889271
&gt;&gt; Epoch 57 finished 	RBM Reconstruction error 36.003551
&gt;&gt; Epoch 58 finished 	RBM Reconstruction error 40.840450
&gt;&gt; Epoch 59 finished 	RBM Reconstruction error 25.438360
&gt;&gt; Epoch 60 finished 	RBM Reconstruction error 38.855698
&gt;&gt; Epoch 61 finished 	RBM Reconstruction error 34.215500
&gt;&gt; Epoch 62 finished 	RBM Reconstruction error 33.483765
&gt;&gt; Epoch 63 finished 	RBM Reconstruction error 56.107422
&gt;&gt; Epoch 64 finished 	RBM Reconstruction error 41.467388
&gt;&gt; Epoch 65 finished 	RBM Reconstruction error 30.269831
&gt;&gt; Epoch 66 finished 	RBM Reconstruction error 38.067558
&gt;&gt; Epoch 67 finished 	RBM Reconstruction error 37.634613
&gt;&gt; Epoch 68 finished 	RBM Reconstruction error 48.928848
&gt;&gt; Epoch 69 finished 	RBM Reconstruction error 48.783077
&gt;&gt; Epoch 70 finished 	RBM Reconstruction error 42.592354
&gt;&gt; Epoch 71 finished 	RBM Reconstruction error 33.132034
&gt;&gt; Epoch 72 finished 	RBM Reconstruction error 36.890575
&gt;&gt; Epoch 73 finished 	RBM Reconstruction error 33.157074
&gt;&gt; Epoch 74 finished 	RBM Reconstruction error 33.242275
&gt;&gt; Epoch 75 finished 	RBM Reconstruction error 43.174435
&gt;&gt; Epoch 76 finished 	RBM Reconstruction error 27.996876
&gt;&gt; Epoch 77 finished 	RBM Reconstruction error 36.515167
&gt;&gt; Epoch 78 finished 	RBM Reconstruction error 39.309006
&gt;&gt; Epoch 79 finished 	RBM Reconstruction error 33.881725
&gt;&gt; Epoch 80 finished 	RBM Reconstruction error 37.876244
&gt;&gt; Epoch 81 finished 	RBM Reconstruction error 36.871010
&gt;&gt; Epoch 82 finished 	RBM Reconstruction error 27.419258
&gt;&gt; Epoch 83 finished 	RBM Reconstruction error 38.832897
&gt;&gt; Epoch 84 finished 	RBM Reconstruction error 36.318336
&gt;&gt; Epoch 85 finished 	RBM Reconstruction error 36.781708
&gt;&gt; Epoch 86 finished 	RBM Reconstruction error 46.488720
&gt;&gt; Epoch 87 finished 	RBM Reconstruction error 26.829906
&gt;&gt; Epoch 88 finished 	RBM Reconstruction error 31.571228
&gt;&gt; Epoch 89 finished 	RBM Reconstruction error 47.068878
&gt;&gt; Epoch 90 finished 	RBM Reconstruction error 48.483788
&gt;&gt; Epoch 91 finished 	RBM Reconstruction error 25.893618
&gt;&gt; Epoch 92 finished 	RBM Reconstruction error 28.653183
&gt;&gt; Epoch 93 finished 	RBM Reconstruction error 35.576008
&gt;&gt; Epoch 94 finished 	RBM Reconstruction error 30.349709
&gt;&gt; Epoch 95 finished 	RBM Reconstruction error 36.958496
&gt;&gt; Epoch 96 finished 	RBM Reconstruction error 40.241615
&gt;&gt; Epoch 97 finished 	RBM Reconstruction error 46.928314
&gt;&gt; Epoch 98 finished 	RBM Reconstruction error 41.136864
&gt;&gt; Epoch 99 finished 	RBM Reconstruction error 36.787724
&gt;&gt; Epoch 100 finished 	RBM Reconstruction error 37.594799
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 111.787949
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 113.721779
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 59.413731
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 114.905830
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 105.282150
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 142.728775
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 106.057503
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 86.573914
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 111.232780
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 88.242256
&gt;&gt; Epoch 11 finished 	RBM Reconstruction error 83.717697
&gt;&gt; Epoch 12 finished 	RBM Reconstruction error 87.396034
&gt;&gt; Epoch 13 finished 	RBM Reconstruction error 68.227180
&gt;&gt; Epoch 14 finished 	RBM Reconstruction error 58.917084
&gt;&gt; Epoch 15 finished 	RBM Reconstruction error 94.475250
&gt;&gt; Epoch 16 finished 	RBM Reconstruction error 77.703255
&gt;&gt; Epoch 17 finished 	RBM Reconstruction error 64.360077
&gt;&gt; Epoch 18 finished 	RBM Reconstruction error 82.129433
&gt;&gt; Epoch 19 finished 	RBM Reconstruction error 79.948997
&gt;&gt; Epoch 20 finished 	RBM Reconstruction error 119.310173
&gt;&gt; Epoch 21 finished 	RBM Reconstruction error 85.296211
&gt;&gt; Epoch 22 finished 	RBM Reconstruction error 100.491905
&gt;&gt; Epoch 23 finished 	RBM Reconstruction error 79.802795
&gt;&gt; Epoch 24 finished 	RBM Reconstruction error 98.110596
&gt;&gt; Epoch 25 finished 	RBM Reconstruction error 83.812149
&gt;&gt; Epoch 26 finished 	RBM Reconstruction error 93.394333
&gt;&gt; Epoch 27 finished 	RBM Reconstruction error 83.264099
&gt;&gt; Epoch 28 finished 	RBM Reconstruction error 114.985497
&gt;&gt; Epoch 29 finished 	RBM Reconstruction error 110.320137
&gt;&gt; Epoch 30 finished 	RBM Reconstruction error 122.557419
&gt;&gt; Epoch 31 finished 	RBM Reconstruction error 101.644287
&gt;&gt; Epoch 32 finished 	RBM Reconstruction error 106.259033
&gt;&gt; Epoch 33 finished 	RBM Reconstruction error 115.398964
&gt;&gt; Epoch 34 finished 	RBM Reconstruction error 92.343513
&gt;&gt; Epoch 35 finished 	RBM Reconstruction error 94.540451
&gt;&gt; Epoch 36 finished 	RBM Reconstruction error 104.574768
&gt;&gt; Epoch 37 finished 	RBM Reconstruction error 125.920013
&gt;&gt; Epoch 38 finished 	RBM Reconstruction error 103.285080
&gt;&gt; Epoch 39 finished 	RBM Reconstruction error 120.245918
&gt;&gt; Epoch 40 finished 	RBM Reconstruction error 118.585564
&gt;&gt; Epoch 41 finished 	RBM Reconstruction error 81.128998
&gt;&gt; Epoch 42 finished 	RBM Reconstruction error 129.073593
&gt;&gt; Epoch 43 finished 	RBM Reconstruction error 97.606560
&gt;&gt; Epoch 44 finished 	RBM Reconstruction error 121.162376
&gt;&gt; Epoch 45 finished 	RBM Reconstruction error 105.466980
&gt;&gt; Epoch 46 finished 	RBM Reconstruction error 101.058105
&gt;&gt; Epoch 47 finished 	RBM Reconstruction error 86.785782
&gt;&gt; Epoch 48 finished 	RBM Reconstruction error 93.663162
&gt;&gt; Epoch 49 finished 	RBM Reconstruction error 106.378914
&gt;&gt; Epoch 50 finished 	RBM Reconstruction error 103.022987
&gt;&gt; Epoch 51 finished 	RBM Reconstruction error 102.834541
&gt;&gt; Epoch 52 finished 	RBM Reconstruction error 107.451698
&gt;&gt; Epoch 53 finished 	RBM Reconstruction error 100.687538
&gt;&gt; Epoch 54 finished 	RBM Reconstruction error 97.238434
&gt;&gt; Epoch 55 finished 	RBM Reconstruction error 98.880295
&gt;&gt; Epoch 56 finished 	RBM Reconstruction error 111.265289
&gt;&gt; Epoch 57 finished 	RBM Reconstruction error 97.847542
&gt;&gt; Epoch 58 finished 	RBM Reconstruction error 107.616867
&gt;&gt; Epoch 59 finished 	RBM Reconstruction error 103.735153
&gt;&gt; Epoch 60 finished 	RBM Reconstruction error 83.978668
&gt;&gt; Epoch 61 finished 	RBM Reconstruction error 127.891464
&gt;&gt; Epoch 62 finished 	RBM Reconstruction error 103.064163
&gt;&gt; Epoch 63 finished 	RBM Reconstruction error 106.983223
&gt;&gt; Epoch 64 finished 	RBM Reconstruction error 90.207481
&gt;&gt; Epoch 65 finished 	RBM Reconstruction error 103.057190
&gt;&gt; Epoch 66 finished 	RBM Reconstruction error 157.250000
&gt;&gt; Epoch 67 finished 	RBM Reconstruction error 118.494110
&gt;&gt; Epoch 68 finished 	RBM Reconstruction error 90.348381
&gt;&gt; Epoch 69 finished 	RBM Reconstruction error 94.758858
&gt;&gt; Epoch 70 finished 	RBM Reconstruction error 98.701248
&gt;&gt; Epoch 71 finished 	RBM Reconstruction error 86.486549
&gt;&gt; Epoch 72 finished 	RBM Reconstruction error 90.717346
&gt;&gt; Epoch 73 finished 	RBM Reconstruction error 112.103401
&gt;&gt; Epoch 74 finished 	RBM Reconstruction error 107.603416
&gt;&gt; Epoch 75 finished 	RBM Reconstruction error 122.361267
&gt;&gt; Epoch 76 finished 	RBM Reconstruction error 114.831375
&gt;&gt; Epoch 77 finished 	RBM Reconstruction error 98.925232
&gt;&gt; Epoch 78 finished 	RBM Reconstruction error 102.833397
&gt;&gt; Epoch 79 finished 	RBM Reconstruction error 108.348137
&gt;&gt; Epoch 80 finished 	RBM Reconstruction error 93.257637
&gt;&gt; Epoch 81 finished 	RBM Reconstruction error 101.417900
&gt;&gt; Epoch 82 finished 	RBM Reconstruction error 104.676659
&gt;&gt; Epoch 83 finished 	RBM Reconstruction error 94.100647
&gt;&gt; Epoch 84 finished 	RBM Reconstruction error 112.348595
&gt;&gt; Epoch 85 finished 	RBM Reconstruction error 89.634171
&gt;&gt; Epoch 86 finished 	RBM Reconstruction error 101.568123
&gt;&gt; Epoch 87 finished 	RBM Reconstruction error 115.196754
&gt;&gt; Epoch 88 finished 	RBM Reconstruction error 107.449051
&gt;&gt; Epoch 89 finished 	RBM Reconstruction error 94.930321
&gt;&gt; Epoch 90 finished 	RBM Reconstruction error 105.782578
&gt;&gt; Epoch 91 finished 	RBM Reconstruction error 139.425262
&gt;&gt; Epoch 92 finished 	RBM Reconstruction error 85.520081
&gt;&gt; Epoch 93 finished 	RBM Reconstruction error 131.636383
&gt;&gt; Epoch 94 finished 	RBM Reconstruction error 122.221222
&gt;&gt; Epoch 95 finished 	RBM Reconstruction error 114.124588
&gt;&gt; Epoch 96 finished 	RBM Reconstruction error 104.876503
&gt;&gt; Epoch 97 finished 	RBM Reconstruction error 99.367996
&gt;&gt; Epoch 98 finished 	RBM Reconstruction error 98.790466
&gt;&gt; Epoch 99 finished 	RBM Reconstruction error 83.223122
&gt;&gt; Epoch 100 finished 	RBM Reconstruction error 103.494896
[END] Pre-training step
[START] Fine tuning step:
&gt;&gt; Epoch 0 finished 	ANN training loss 0.559153
&gt;&gt; Epoch 1 finished 	ANN training loss 0.423848
&gt;&gt; Epoch 2 finished 	ANN training loss 0.286882
&gt;&gt; Epoch 3 finished 	ANN training loss 0.247302
&gt;&gt; Epoch 4 finished 	ANN training loss 0.203537
&gt;&gt; Epoch 5 finished 	ANN training loss 0.164076
&gt;&gt; Epoch 6 finished 	ANN training loss 0.146482
&gt;&gt; Epoch 7 finished 	ANN training loss 0.126781
&gt;&gt; Epoch 8 finished 	ANN training loss 0.096451
&gt;&gt; Epoch 9 finished 	ANN training loss 0.084875
&gt;&gt; Epoch 10 finished 	ANN training loss 0.075282
&gt;&gt; Epoch 11 finished 	ANN training loss 0.066013
&gt;&gt; Epoch 12 finished 	ANN training loss 0.054026
&gt;&gt; Epoch 13 finished 	ANN training loss 0.056047
&gt;&gt; Epoch 14 finished 	ANN training loss 0.043809
&gt;&gt; Epoch 15 finished 	ANN training loss 0.036771
&gt;&gt; Epoch 16 finished 	ANN training loss 0.034257
&gt;&gt; Epoch 17 finished 	ANN training loss 0.029328
&gt;&gt; Epoch 18 finished 	ANN training loss 0.024533
&gt;&gt; Epoch 19 finished 	ANN training loss 0.023145
&gt;&gt; Epoch 20 finished 	ANN training loss 0.021318
&gt;&gt; Epoch 21 finished 	ANN training loss 0.021223
&gt;&gt; Epoch 22 finished 	ANN training loss 0.020408
&gt;&gt; Epoch 23 finished 	ANN training loss 0.015606
&gt;&gt; Epoch 24 finished 	ANN training loss 0.014383
&gt;&gt; Epoch 25 finished 	ANN training loss 0.012754
&gt;&gt; Epoch 26 finished 	ANN training loss 0.014770
&gt;&gt; Epoch 27 finished 	ANN training loss 0.012297
&gt;&gt; Epoch 28 finished 	ANN training loss 0.010822
&gt;&gt; Epoch 29 finished 	ANN training loss 0.010341
&gt;&gt; Epoch 30 finished 	ANN training loss 0.009971
&gt;&gt; Epoch 31 finished 	ANN training loss 0.008069
&gt;&gt; Epoch 32 finished 	ANN training loss 0.008010
&gt;&gt; Epoch 33 finished 	ANN training loss 0.006893
&gt;&gt; Epoch 34 finished 	ANN training loss 0.007492
&gt;&gt; Epoch 35 finished 	ANN training loss 0.005975
&gt;&gt; Epoch 36 finished 	ANN training loss 0.006351
&gt;&gt; Epoch 37 finished 	ANN training loss 0.005977
&gt;&gt; Epoch 38 finished 	ANN training loss 0.005839
&gt;&gt; Epoch 39 finished 	ANN training loss 0.004995
&gt;&gt; Epoch 40 finished 	ANN training loss 0.004933
&gt;&gt; Epoch 41 finished 	ANN training loss 0.005292
&gt;&gt; Epoch 42 finished 	ANN training loss 0.003973
&gt;&gt; Epoch 43 finished 	ANN training loss 0.003726
&gt;&gt; Epoch 44 finished 	ANN training loss 0.004571
&gt;&gt; Epoch 45 finished 	ANN training loss 0.004490
&gt;&gt; Epoch 46 finished 	ANN training loss 0.003382
&gt;&gt; Epoch 47 finished 	ANN training loss 0.003802
&gt;&gt; Epoch 48 finished 	ANN training loss 0.003105
&gt;&gt; Epoch 49 finished 	ANN training loss 0.002749
&gt;&gt; Epoch 50 finished 	ANN training loss 0.003888
&gt;&gt; Epoch 51 finished 	ANN training loss 0.002902
&gt;&gt; Epoch 52 finished 	ANN training loss 0.002542
&gt;&gt; Epoch 53 finished 	ANN training loss 0.002921
&gt;&gt; Epoch 54 finished 	ANN training loss 0.002682
&gt;&gt; Epoch 55 finished 	ANN training loss 0.003040
&gt;&gt; Epoch 56 finished 	ANN training loss 0.002450
&gt;&gt; Epoch 57 finished 	ANN training loss 0.002197
&gt;&gt; Epoch 58 finished 	ANN training loss 0.002013
&gt;&gt; Epoch 59 finished 	ANN training loss 0.002560
&gt;&gt; Epoch 60 finished 	ANN training loss 0.002253
&gt;&gt; Epoch 61 finished 	ANN training loss 0.002195
&gt;&gt; Epoch 62 finished 	ANN training loss 0.001861
&gt;&gt; Epoch 63 finished 	ANN training loss 0.001955
&gt;&gt; Epoch 64 finished 	ANN training loss 0.001645
&gt;&gt; Epoch 65 finished 	ANN training loss 0.001881
&gt;&gt; Epoch 66 finished 	ANN training loss 0.001828
&gt;&gt; Epoch 67 finished 	ANN training loss 0.001551
&gt;&gt; Epoch 68 finished 	ANN training loss 0.001548
&gt;&gt; Epoch 69 finished 	ANN training loss 0.002024
&gt;&gt; Epoch 70 finished 	ANN training loss 0.001331
&gt;&gt; Epoch 71 finished 	ANN training loss 0.001269
&gt;&gt; Epoch 72 finished 	ANN training loss 0.001172
&gt;&gt; Epoch 73 finished 	ANN training loss 0.001160
&gt;&gt; Epoch 74 finished 	ANN training loss 0.001090
&gt;&gt; Epoch 75 finished 	ANN training loss 0.001022
&gt;&gt; Epoch 76 finished 	ANN training loss 0.000925
&gt;&gt; Epoch 77 finished 	ANN training loss 0.000902
&gt;&gt; Epoch 78 finished 	ANN training loss 0.000879
&gt;&gt; Epoch 79 finished 	ANN training loss 0.000904
&gt;&gt; Epoch 80 finished 	ANN training loss 0.000900
&gt;&gt; Epoch 81 finished 	ANN training loss 0.001140
&gt;&gt; Epoch 82 finished 	ANN training loss 0.000874
&gt;&gt; Epoch 83 finished 	ANN training loss 0.000823
&gt;&gt; Epoch 84 finished 	ANN training loss 0.000886
&gt;&gt; Epoch 85 finished 	ANN training loss 0.000766
&gt;&gt; Epoch 86 finished 	ANN training loss 0.000763
&gt;&gt; Epoch 87 finished 	ANN training loss 0.000728
&gt;&gt; Epoch 88 finished 	ANN training loss 0.000686
&gt;&gt; Epoch 89 finished 	ANN training loss 0.000714
&gt;&gt; Epoch 90 finished 	ANN training loss 0.000649
&gt;&gt; Epoch 91 finished 	ANN training loss 0.000681
&gt;&gt; Epoch 92 finished 	ANN training loss 0.000651
&gt;&gt; Epoch 93 finished 	ANN training loss 0.000573
&gt;&gt; Epoch 94 finished 	ANN training loss 0.000613
&gt;&gt; Epoch 95 finished 	ANN training loss 0.000744
&gt;&gt; Epoch 96 finished 	ANN training loss 0.000732
&gt;&gt; Epoch 97 finished 	ANN training loss 0.000748
&gt;&gt; Epoch 98 finished 	ANN training loss 0.000601
&gt;&gt; Epoch 99 finished 	ANN training loss 0.000600
[END] Fine tuning step
Done.
Accuracy: 0.900000
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[90]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ACCURACY: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">epoch_rbm_acc</span><span class="p">[</span><span class="mi">8</span><span class="p">]))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>ACCURACY: 0.9
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Setting 10</strong></p>

<pre><code>n_epochs_rbm=200</code></pre>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[91]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">epoch_rbm_acc</span><span class="p">[</span><span class="mi">9</span><span class="p">]</span> <span class="o">=</span> <span class="n">deep_belief_net</span><span class="p">(</span><span class="n">n_epochs_rbm</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[START] Pre-training step:
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 57.116241
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 58.623379
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 47.995167
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 46.244217
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 58.939911
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 44.408817
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 42.027470
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 43.308262
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 32.640736
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 24.688789
&gt;&gt; Epoch 11 finished 	RBM Reconstruction error 36.358410
&gt;&gt; Epoch 12 finished 	RBM Reconstruction error 29.182411
&gt;&gt; Epoch 13 finished 	RBM Reconstruction error 33.299675
&gt;&gt; Epoch 14 finished 	RBM Reconstruction error 27.233667
&gt;&gt; Epoch 15 finished 	RBM Reconstruction error 53.459248
&gt;&gt; Epoch 16 finished 	RBM Reconstruction error 37.537029
&gt;&gt; Epoch 17 finished 	RBM Reconstruction error 31.129135
&gt;&gt; Epoch 18 finished 	RBM Reconstruction error 30.803146
&gt;&gt; Epoch 19 finished 	RBM Reconstruction error 38.337326
&gt;&gt; Epoch 20 finished 	RBM Reconstruction error 38.912827
&gt;&gt; Epoch 21 finished 	RBM Reconstruction error 31.926487
&gt;&gt; Epoch 22 finished 	RBM Reconstruction error 45.075577
&gt;&gt; Epoch 23 finished 	RBM Reconstruction error 36.450039
&gt;&gt; Epoch 24 finished 	RBM Reconstruction error 45.967323
&gt;&gt; Epoch 25 finished 	RBM Reconstruction error 40.121899
&gt;&gt; Epoch 26 finished 	RBM Reconstruction error 45.008419
&gt;&gt; Epoch 27 finished 	RBM Reconstruction error 44.445240
&gt;&gt; Epoch 28 finished 	RBM Reconstruction error 41.197735
&gt;&gt; Epoch 29 finished 	RBM Reconstruction error 38.312874
&gt;&gt; Epoch 30 finished 	RBM Reconstruction error 32.829533
&gt;&gt; Epoch 31 finished 	RBM Reconstruction error 30.037983
&gt;&gt; Epoch 32 finished 	RBM Reconstruction error 37.973209
&gt;&gt; Epoch 33 finished 	RBM Reconstruction error 40.791405
&gt;&gt; Epoch 34 finished 	RBM Reconstruction error 35.718197
&gt;&gt; Epoch 35 finished 	RBM Reconstruction error 25.079138
&gt;&gt; Epoch 36 finished 	RBM Reconstruction error 50.273823
&gt;&gt; Epoch 37 finished 	RBM Reconstruction error 33.513794
&gt;&gt; Epoch 38 finished 	RBM Reconstruction error 37.381702
&gt;&gt; Epoch 39 finished 	RBM Reconstruction error 39.995655
&gt;&gt; Epoch 40 finished 	RBM Reconstruction error 42.303234
&gt;&gt; Epoch 41 finished 	RBM Reconstruction error 39.147671
&gt;&gt; Epoch 42 finished 	RBM Reconstruction error 34.810463
&gt;&gt; Epoch 43 finished 	RBM Reconstruction error 54.893890
&gt;&gt; Epoch 44 finished 	RBM Reconstruction error 56.195595
&gt;&gt; Epoch 45 finished 	RBM Reconstruction error 37.099545
&gt;&gt; Epoch 46 finished 	RBM Reconstruction error 43.047619
&gt;&gt; Epoch 47 finished 	RBM Reconstruction error 47.525684
&gt;&gt; Epoch 48 finished 	RBM Reconstruction error 39.647533
&gt;&gt; Epoch 49 finished 	RBM Reconstruction error 38.821484
&gt;&gt; Epoch 50 finished 	RBM Reconstruction error 39.319244
&gt;&gt; Epoch 51 finished 	RBM Reconstruction error 54.708984
&gt;&gt; Epoch 52 finished 	RBM Reconstruction error 35.621536
&gt;&gt; Epoch 53 finished 	RBM Reconstruction error 40.572105
&gt;&gt; Epoch 54 finished 	RBM Reconstruction error 31.320713
&gt;&gt; Epoch 55 finished 	RBM Reconstruction error 38.724178
&gt;&gt; Epoch 56 finished 	RBM Reconstruction error 55.324631
&gt;&gt; Epoch 57 finished 	RBM Reconstruction error 41.388615
&gt;&gt; Epoch 58 finished 	RBM Reconstruction error 45.927841
&gt;&gt; Epoch 59 finished 	RBM Reconstruction error 40.084740
&gt;&gt; Epoch 60 finished 	RBM Reconstruction error 31.872160
&gt;&gt; Epoch 61 finished 	RBM Reconstruction error 46.405949
&gt;&gt; Epoch 62 finished 	RBM Reconstruction error 32.117935
&gt;&gt; Epoch 63 finished 	RBM Reconstruction error 48.683487
&gt;&gt; Epoch 64 finished 	RBM Reconstruction error 42.904560
&gt;&gt; Epoch 65 finished 	RBM Reconstruction error 38.039425
&gt;&gt; Epoch 66 finished 	RBM Reconstruction error 30.659721
&gt;&gt; Epoch 67 finished 	RBM Reconstruction error 52.985546
&gt;&gt; Epoch 68 finished 	RBM Reconstruction error 39.618866
&gt;&gt; Epoch 69 finished 	RBM Reconstruction error 43.825546
&gt;&gt; Epoch 70 finished 	RBM Reconstruction error 37.541351
&gt;&gt; Epoch 71 finished 	RBM Reconstruction error 46.164940
&gt;&gt; Epoch 72 finished 	RBM Reconstruction error 43.311016
&gt;&gt; Epoch 73 finished 	RBM Reconstruction error 51.480202
&gt;&gt; Epoch 74 finished 	RBM Reconstruction error 49.289940
&gt;&gt; Epoch 75 finished 	RBM Reconstruction error 52.675526
&gt;&gt; Epoch 76 finished 	RBM Reconstruction error 51.576584
&gt;&gt; Epoch 77 finished 	RBM Reconstruction error 44.223202
&gt;&gt; Epoch 78 finished 	RBM Reconstruction error 41.007568
&gt;&gt; Epoch 79 finished 	RBM Reconstruction error 45.420475
&gt;&gt; Epoch 80 finished 	RBM Reconstruction error 48.857426
&gt;&gt; Epoch 81 finished 	RBM Reconstruction error 52.293354
&gt;&gt; Epoch 82 finished 	RBM Reconstruction error 37.909679
&gt;&gt; Epoch 83 finished 	RBM Reconstruction error 40.521431
&gt;&gt; Epoch 84 finished 	RBM Reconstruction error 48.091846
&gt;&gt; Epoch 85 finished 	RBM Reconstruction error 50.331036
&gt;&gt; Epoch 86 finished 	RBM Reconstruction error 39.579212
&gt;&gt; Epoch 87 finished 	RBM Reconstruction error 40.900753
&gt;&gt; Epoch 88 finished 	RBM Reconstruction error 47.013878
&gt;&gt; Epoch 89 finished 	RBM Reconstruction error 35.184589
&gt;&gt; Epoch 90 finished 	RBM Reconstruction error 44.269997
&gt;&gt; Epoch 91 finished 	RBM Reconstruction error 49.158096
&gt;&gt; Epoch 92 finished 	RBM Reconstruction error 43.611385
&gt;&gt; Epoch 93 finished 	RBM Reconstruction error 60.741196
&gt;&gt; Epoch 94 finished 	RBM Reconstruction error 47.691978
&gt;&gt; Epoch 95 finished 	RBM Reconstruction error 44.343307
&gt;&gt; Epoch 96 finished 	RBM Reconstruction error 53.239323
&gt;&gt; Epoch 97 finished 	RBM Reconstruction error 41.688107
&gt;&gt; Epoch 98 finished 	RBM Reconstruction error 48.550625
&gt;&gt; Epoch 99 finished 	RBM Reconstruction error 47.987186
&gt;&gt; Epoch 100 finished 	RBM Reconstruction error 34.288380
&gt;&gt; Epoch 101 finished 	RBM Reconstruction error 32.221981
&gt;&gt; Epoch 102 finished 	RBM Reconstruction error 45.224442
&gt;&gt; Epoch 103 finished 	RBM Reconstruction error 43.469223
&gt;&gt; Epoch 104 finished 	RBM Reconstruction error 48.746647
&gt;&gt; Epoch 105 finished 	RBM Reconstruction error 61.833721
&gt;&gt; Epoch 106 finished 	RBM Reconstruction error 41.287601
&gt;&gt; Epoch 107 finished 	RBM Reconstruction error 59.132675
&gt;&gt; Epoch 108 finished 	RBM Reconstruction error 43.732594
&gt;&gt; Epoch 109 finished 	RBM Reconstruction error 49.017735
&gt;&gt; Epoch 110 finished 	RBM Reconstruction error 43.722050
&gt;&gt; Epoch 111 finished 	RBM Reconstruction error 39.138744
&gt;&gt; Epoch 112 finished 	RBM Reconstruction error 45.585972
&gt;&gt; Epoch 113 finished 	RBM Reconstruction error 48.090126
&gt;&gt; Epoch 114 finished 	RBM Reconstruction error 57.641083
&gt;&gt; Epoch 115 finished 	RBM Reconstruction error 44.586620
&gt;&gt; Epoch 116 finished 	RBM Reconstruction error 59.575802
&gt;&gt; Epoch 117 finished 	RBM Reconstruction error 42.615536
&gt;&gt; Epoch 118 finished 	RBM Reconstruction error 51.957764
&gt;&gt; Epoch 119 finished 	RBM Reconstruction error 44.619247
&gt;&gt; Epoch 120 finished 	RBM Reconstruction error 51.005386
&gt;&gt; Epoch 121 finished 	RBM Reconstruction error 42.891911
&gt;&gt; Epoch 122 finished 	RBM Reconstruction error 42.635792
&gt;&gt; Epoch 123 finished 	RBM Reconstruction error 57.663155
&gt;&gt; Epoch 124 finished 	RBM Reconstruction error 44.235672
&gt;&gt; Epoch 125 finished 	RBM Reconstruction error 48.467464
&gt;&gt; Epoch 126 finished 	RBM Reconstruction error 57.959911
&gt;&gt; Epoch 127 finished 	RBM Reconstruction error 47.443966
&gt;&gt; Epoch 128 finished 	RBM Reconstruction error 43.067379
&gt;&gt; Epoch 129 finished 	RBM Reconstruction error 47.289131
&gt;&gt; Epoch 130 finished 	RBM Reconstruction error 47.855812
&gt;&gt; Epoch 131 finished 	RBM Reconstruction error 49.009102
&gt;&gt; Epoch 132 finished 	RBM Reconstruction error 58.451550
&gt;&gt; Epoch 133 finished 	RBM Reconstruction error 52.637852
&gt;&gt; Epoch 134 finished 	RBM Reconstruction error 55.585186
&gt;&gt; Epoch 135 finished 	RBM Reconstruction error 59.746181
&gt;&gt; Epoch 136 finished 	RBM Reconstruction error 45.799423
&gt;&gt; Epoch 137 finished 	RBM Reconstruction error 58.681973
&gt;&gt; Epoch 138 finished 	RBM Reconstruction error 49.083973
&gt;&gt; Epoch 139 finished 	RBM Reconstruction error 44.902637
&gt;&gt; Epoch 140 finished 	RBM Reconstruction error 45.707970
&gt;&gt; Epoch 141 finished 	RBM Reconstruction error 48.067883
&gt;&gt; Epoch 142 finished 	RBM Reconstruction error 54.794247
&gt;&gt; Epoch 143 finished 	RBM Reconstruction error 50.475479
&gt;&gt; Epoch 144 finished 	RBM Reconstruction error 61.509777
&gt;&gt; Epoch 145 finished 	RBM Reconstruction error 45.346893
&gt;&gt; Epoch 146 finished 	RBM Reconstruction error 52.008034
&gt;&gt; Epoch 147 finished 	RBM Reconstruction error 53.157570
&gt;&gt; Epoch 148 finished 	RBM Reconstruction error 49.277363
&gt;&gt; Epoch 149 finished 	RBM Reconstruction error 65.432045
&gt;&gt; Epoch 150 finished 	RBM Reconstruction error 44.341007
&gt;&gt; Epoch 151 finished 	RBM Reconstruction error 53.542480
&gt;&gt; Epoch 152 finished 	RBM Reconstruction error 50.519344
&gt;&gt; Epoch 153 finished 	RBM Reconstruction error 54.931206
&gt;&gt; Epoch 154 finished 	RBM Reconstruction error 51.126076
&gt;&gt; Epoch 155 finished 	RBM Reconstruction error 56.965164
&gt;&gt; Epoch 156 finished 	RBM Reconstruction error 51.243221
&gt;&gt; Epoch 157 finished 	RBM Reconstruction error 51.636642
&gt;&gt; Epoch 158 finished 	RBM Reconstruction error 46.180283
&gt;&gt; Epoch 159 finished 	RBM Reconstruction error 45.546783
&gt;&gt; Epoch 160 finished 	RBM Reconstruction error 55.428909
&gt;&gt; Epoch 161 finished 	RBM Reconstruction error 45.517998
&gt;&gt; Epoch 162 finished 	RBM Reconstruction error 67.060349
&gt;&gt; Epoch 163 finished 	RBM Reconstruction error 52.193935
&gt;&gt; Epoch 164 finished 	RBM Reconstruction error 51.672436
&gt;&gt; Epoch 165 finished 	RBM Reconstruction error 48.624073
&gt;&gt; Epoch 166 finished 	RBM Reconstruction error 56.212635
&gt;&gt; Epoch 167 finished 	RBM Reconstruction error 46.617111
&gt;&gt; Epoch 168 finished 	RBM Reconstruction error 56.426338
&gt;&gt; Epoch 169 finished 	RBM Reconstruction error 50.296825
&gt;&gt; Epoch 170 finished 	RBM Reconstruction error 60.637421
&gt;&gt; Epoch 171 finished 	RBM Reconstruction error 49.461910
&gt;&gt; Epoch 172 finished 	RBM Reconstruction error 55.824753
&gt;&gt; Epoch 173 finished 	RBM Reconstruction error 45.668785
&gt;&gt; Epoch 174 finished 	RBM Reconstruction error 54.185070
&gt;&gt; Epoch 175 finished 	RBM Reconstruction error 56.532116
&gt;&gt; Epoch 176 finished 	RBM Reconstruction error 55.155918
&gt;&gt; Epoch 177 finished 	RBM Reconstruction error 64.086472
&gt;&gt; Epoch 178 finished 	RBM Reconstruction error 43.452862
&gt;&gt; Epoch 179 finished 	RBM Reconstruction error 49.613544
&gt;&gt; Epoch 180 finished 	RBM Reconstruction error 65.419159
&gt;&gt; Epoch 181 finished 	RBM Reconstruction error 49.341522
&gt;&gt; Epoch 182 finished 	RBM Reconstruction error 45.563507
&gt;&gt; Epoch 183 finished 	RBM Reconstruction error 56.294167
&gt;&gt; Epoch 184 finished 	RBM Reconstruction error 56.491844
&gt;&gt; Epoch 185 finished 	RBM Reconstruction error 56.194443
&gt;&gt; Epoch 186 finished 	RBM Reconstruction error 55.997520
&gt;&gt; Epoch 187 finished 	RBM Reconstruction error 55.332954
&gt;&gt; Epoch 188 finished 	RBM Reconstruction error 53.397930
&gt;&gt; Epoch 189 finished 	RBM Reconstruction error 54.762638
&gt;&gt; Epoch 190 finished 	RBM Reconstruction error 48.943295
&gt;&gt; Epoch 191 finished 	RBM Reconstruction error 57.285973
&gt;&gt; Epoch 192 finished 	RBM Reconstruction error 47.129757
&gt;&gt; Epoch 193 finished 	RBM Reconstruction error 54.448589
&gt;&gt; Epoch 194 finished 	RBM Reconstruction error 58.092270
&gt;&gt; Epoch 195 finished 	RBM Reconstruction error 61.148083
&gt;&gt; Epoch 196 finished 	RBM Reconstruction error 51.700756
&gt;&gt; Epoch 197 finished 	RBM Reconstruction error 46.888985
&gt;&gt; Epoch 198 finished 	RBM Reconstruction error 56.939980
&gt;&gt; Epoch 199 finished 	RBM Reconstruction error 58.377651
&gt;&gt; Epoch 200 finished 	RBM Reconstruction error 53.385487
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 200.181366
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 242.774338
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 275.711334
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 201.150269
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 186.731674
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 168.622955
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 175.088379
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 223.309570
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 197.670670
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 160.714081
&gt;&gt; Epoch 11 finished 	RBM Reconstruction error 162.731384
&gt;&gt; Epoch 12 finished 	RBM Reconstruction error 177.109116
&gt;&gt; Epoch 13 finished 	RBM Reconstruction error 280.858490
&gt;&gt; Epoch 14 finished 	RBM Reconstruction error 185.056870
&gt;&gt; Epoch 15 finished 	RBM Reconstruction error 149.487122
&gt;&gt; Epoch 16 finished 	RBM Reconstruction error 131.544662
&gt;&gt; Epoch 17 finished 	RBM Reconstruction error 183.991882
&gt;&gt; Epoch 18 finished 	RBM Reconstruction error 255.154068
&gt;&gt; Epoch 19 finished 	RBM Reconstruction error 177.688324
&gt;&gt; Epoch 20 finished 	RBM Reconstruction error 174.188171
&gt;&gt; Epoch 21 finished 	RBM Reconstruction error 118.692497
&gt;&gt; Epoch 22 finished 	RBM Reconstruction error 189.338089
&gt;&gt; Epoch 23 finished 	RBM Reconstruction error 149.635941
&gt;&gt; Epoch 24 finished 	RBM Reconstruction error 147.106979
&gt;&gt; Epoch 25 finished 	RBM Reconstruction error 185.047897
&gt;&gt; Epoch 26 finished 	RBM Reconstruction error 166.772186
&gt;&gt; Epoch 27 finished 	RBM Reconstruction error 210.826218
&gt;&gt; Epoch 28 finished 	RBM Reconstruction error 226.418152
&gt;&gt; Epoch 29 finished 	RBM Reconstruction error 235.436722
&gt;&gt; Epoch 30 finished 	RBM Reconstruction error 189.990982
&gt;&gt; Epoch 31 finished 	RBM Reconstruction error 193.610413
&gt;&gt; Epoch 32 finished 	RBM Reconstruction error 213.243942
&gt;&gt; Epoch 33 finished 	RBM Reconstruction error 169.294922
&gt;&gt; Epoch 34 finished 	RBM Reconstruction error 213.200058
&gt;&gt; Epoch 35 finished 	RBM Reconstruction error 192.671524
&gt;&gt; Epoch 36 finished 	RBM Reconstruction error 191.311646
&gt;&gt; Epoch 37 finished 	RBM Reconstruction error 180.954803
&gt;&gt; Epoch 38 finished 	RBM Reconstruction error 203.571152
&gt;&gt; Epoch 39 finished 	RBM Reconstruction error 235.308517
&gt;&gt; Epoch 40 finished 	RBM Reconstruction error 205.586380
&gt;&gt; Epoch 41 finished 	RBM Reconstruction error 201.645111
&gt;&gt; Epoch 42 finished 	RBM Reconstruction error 224.614685
&gt;&gt; Epoch 43 finished 	RBM Reconstruction error 217.219162
&gt;&gt; Epoch 44 finished 	RBM Reconstruction error 188.733200
&gt;&gt; Epoch 45 finished 	RBM Reconstruction error 232.208588
&gt;&gt; Epoch 46 finished 	RBM Reconstruction error 189.405548
&gt;&gt; Epoch 47 finished 	RBM Reconstruction error 195.155945
&gt;&gt; Epoch 48 finished 	RBM Reconstruction error 186.081284
&gt;&gt; Epoch 49 finished 	RBM Reconstruction error 224.220718
&gt;&gt; Epoch 50 finished 	RBM Reconstruction error 200.341934
&gt;&gt; Epoch 51 finished 	RBM Reconstruction error 214.661972
&gt;&gt; Epoch 52 finished 	RBM Reconstruction error 195.970551
&gt;&gt; Epoch 53 finished 	RBM Reconstruction error 207.705765
&gt;&gt; Epoch 54 finished 	RBM Reconstruction error 208.093246
&gt;&gt; Epoch 55 finished 	RBM Reconstruction error 208.947311
&gt;&gt; Epoch 56 finished 	RBM Reconstruction error 196.998749
&gt;&gt; Epoch 57 finished 	RBM Reconstruction error 220.380508
&gt;&gt; Epoch 58 finished 	RBM Reconstruction error 217.909576
&gt;&gt; Epoch 59 finished 	RBM Reconstruction error 202.082886
&gt;&gt; Epoch 60 finished 	RBM Reconstruction error 196.337616
&gt;&gt; Epoch 61 finished 	RBM Reconstruction error 221.512329
&gt;&gt; Epoch 62 finished 	RBM Reconstruction error 188.130463
&gt;&gt; Epoch 63 finished 	RBM Reconstruction error 198.272324
&gt;&gt; Epoch 64 finished 	RBM Reconstruction error 195.477737
&gt;&gt; Epoch 65 finished 	RBM Reconstruction error 240.258347
&gt;&gt; Epoch 66 finished 	RBM Reconstruction error 198.034927
&gt;&gt; Epoch 67 finished 	RBM Reconstruction error 196.602890
&gt;&gt; Epoch 68 finished 	RBM Reconstruction error 195.838470
&gt;&gt; Epoch 69 finished 	RBM Reconstruction error 224.448196
&gt;&gt; Epoch 70 finished 	RBM Reconstruction error 184.104370
&gt;&gt; Epoch 71 finished 	RBM Reconstruction error 218.700699
&gt;&gt; Epoch 72 finished 	RBM Reconstruction error 206.159103
&gt;&gt; Epoch 73 finished 	RBM Reconstruction error 176.194336
&gt;&gt; Epoch 74 finished 	RBM Reconstruction error 237.567230
&gt;&gt; Epoch 75 finished 	RBM Reconstruction error 213.303970
&gt;&gt; Epoch 76 finished 	RBM Reconstruction error 225.758438
&gt;&gt; Epoch 77 finished 	RBM Reconstruction error 155.049759
&gt;&gt; Epoch 78 finished 	RBM Reconstruction error 212.505310
&gt;&gt; Epoch 79 finished 	RBM Reconstruction error 241.326096
&gt;&gt; Epoch 80 finished 	RBM Reconstruction error 206.550858
&gt;&gt; Epoch 81 finished 	RBM Reconstruction error 194.035782
&gt;&gt; Epoch 82 finished 	RBM Reconstruction error 167.370239
&gt;&gt; Epoch 83 finished 	RBM Reconstruction error 205.280258
&gt;&gt; Epoch 84 finished 	RBM Reconstruction error 206.364807
&gt;&gt; Epoch 85 finished 	RBM Reconstruction error 196.309525
&gt;&gt; Epoch 86 finished 	RBM Reconstruction error 196.848480
&gt;&gt; Epoch 87 finished 	RBM Reconstruction error 197.108459
&gt;&gt; Epoch 88 finished 	RBM Reconstruction error 214.650269
&gt;&gt; Epoch 89 finished 	RBM Reconstruction error 250.796478
&gt;&gt; Epoch 90 finished 	RBM Reconstruction error 145.884583
&gt;&gt; Epoch 91 finished 	RBM Reconstruction error 247.266068
&gt;&gt; Epoch 92 finished 	RBM Reconstruction error 202.665390
&gt;&gt; Epoch 93 finished 	RBM Reconstruction error 218.380386
&gt;&gt; Epoch 94 finished 	RBM Reconstruction error 220.665466
&gt;&gt; Epoch 95 finished 	RBM Reconstruction error 221.336609
&gt;&gt; Epoch 96 finished 	RBM Reconstruction error 267.542877
&gt;&gt; Epoch 97 finished 	RBM Reconstruction error 224.327896
&gt;&gt; Epoch 98 finished 	RBM Reconstruction error 198.308685
&gt;&gt; Epoch 99 finished 	RBM Reconstruction error 233.491577
&gt;&gt; Epoch 100 finished 	RBM Reconstruction error 204.297836
&gt;&gt; Epoch 101 finished 	RBM Reconstruction error 174.189606
&gt;&gt; Epoch 102 finished 	RBM Reconstruction error 223.431763
&gt;&gt; Epoch 103 finished 	RBM Reconstruction error 252.579056
&gt;&gt; Epoch 104 finished 	RBM Reconstruction error 213.763168
&gt;&gt; Epoch 105 finished 	RBM Reconstruction error 228.726059
&gt;&gt; Epoch 106 finished 	RBM Reconstruction error 225.025574
&gt;&gt; Epoch 107 finished 	RBM Reconstruction error 211.435074
&gt;&gt; Epoch 108 finished 	RBM Reconstruction error 226.241760
&gt;&gt; Epoch 109 finished 	RBM Reconstruction error 193.766479
&gt;&gt; Epoch 110 finished 	RBM Reconstruction error 213.593704
&gt;&gt; Epoch 111 finished 	RBM Reconstruction error 216.383499
&gt;&gt; Epoch 112 finished 	RBM Reconstruction error 207.028732
&gt;&gt; Epoch 113 finished 	RBM Reconstruction error 214.052521
&gt;&gt; Epoch 114 finished 	RBM Reconstruction error 221.775192
&gt;&gt; Epoch 115 finished 	RBM Reconstruction error 231.043106
&gt;&gt; Epoch 116 finished 	RBM Reconstruction error 219.929962
&gt;&gt; Epoch 117 finished 	RBM Reconstruction error 204.093277
&gt;&gt; Epoch 118 finished 	RBM Reconstruction error 214.711868
&gt;&gt; Epoch 119 finished 	RBM Reconstruction error 219.799301
&gt;&gt; Epoch 120 finished 	RBM Reconstruction error 243.114960
&gt;&gt; Epoch 121 finished 	RBM Reconstruction error 206.204163
&gt;&gt; Epoch 122 finished 	RBM Reconstruction error 229.379578
&gt;&gt; Epoch 123 finished 	RBM Reconstruction error 217.622833
&gt;&gt; Epoch 124 finished 	RBM Reconstruction error 197.841522
&gt;&gt; Epoch 125 finished 	RBM Reconstruction error 226.795425
&gt;&gt; Epoch 126 finished 	RBM Reconstruction error 223.074844
&gt;&gt; Epoch 127 finished 	RBM Reconstruction error 248.984253
&gt;&gt; Epoch 128 finished 	RBM Reconstruction error 225.798874
&gt;&gt; Epoch 129 finished 	RBM Reconstruction error 227.191635
&gt;&gt; Epoch 130 finished 	RBM Reconstruction error 244.432755
&gt;&gt; Epoch 131 finished 	RBM Reconstruction error 231.049026
&gt;&gt; Epoch 132 finished 	RBM Reconstruction error 239.001099
&gt;&gt; Epoch 133 finished 	RBM Reconstruction error 217.556198
&gt;&gt; Epoch 134 finished 	RBM Reconstruction error 239.072388
&gt;&gt; Epoch 135 finished 	RBM Reconstruction error 214.383209
&gt;&gt; Epoch 136 finished 	RBM Reconstruction error 213.258438
&gt;&gt; Epoch 137 finished 	RBM Reconstruction error 221.597946
&gt;&gt; Epoch 138 finished 	RBM Reconstruction error 213.698151
&gt;&gt; Epoch 139 finished 	RBM Reconstruction error 219.739609
&gt;&gt; Epoch 140 finished 	RBM Reconstruction error 219.074921
&gt;&gt; Epoch 141 finished 	RBM Reconstruction error 207.256119
&gt;&gt; Epoch 142 finished 	RBM Reconstruction error 206.058792
&gt;&gt; Epoch 143 finished 	RBM Reconstruction error 192.350708
&gt;&gt; Epoch 144 finished 	RBM Reconstruction error 240.975357
&gt;&gt; Epoch 145 finished 	RBM Reconstruction error 210.719955
&gt;&gt; Epoch 146 finished 	RBM Reconstruction error 203.691910
&gt;&gt; Epoch 147 finished 	RBM Reconstruction error 209.966293
&gt;&gt; Epoch 148 finished 	RBM Reconstruction error 204.440033
&gt;&gt; Epoch 149 finished 	RBM Reconstruction error 226.639145
&gt;&gt; Epoch 150 finished 	RBM Reconstruction error 240.488159
&gt;&gt; Epoch 151 finished 	RBM Reconstruction error 230.223984
&gt;&gt; Epoch 152 finished 	RBM Reconstruction error 235.704926
&gt;&gt; Epoch 153 finished 	RBM Reconstruction error 233.968353
&gt;&gt; Epoch 154 finished 	RBM Reconstruction error 220.618591
&gt;&gt; Epoch 155 finished 	RBM Reconstruction error 205.623123
&gt;&gt; Epoch 156 finished 	RBM Reconstruction error 264.416382
&gt;&gt; Epoch 157 finished 	RBM Reconstruction error 205.860489
&gt;&gt; Epoch 158 finished 	RBM Reconstruction error 220.431763
&gt;&gt; Epoch 159 finished 	RBM Reconstruction error 215.606873
&gt;&gt; Epoch 160 finished 	RBM Reconstruction error 208.144028
&gt;&gt; Epoch 161 finished 	RBM Reconstruction error 223.293655
&gt;&gt; Epoch 162 finished 	RBM Reconstruction error 218.477493
&gt;&gt; Epoch 163 finished 	RBM Reconstruction error 229.473755
&gt;&gt; Epoch 164 finished 	RBM Reconstruction error 240.140137
&gt;&gt; Epoch 165 finished 	RBM Reconstruction error 207.617264
&gt;&gt; Epoch 166 finished 	RBM Reconstruction error 218.916916
&gt;&gt; Epoch 167 finished 	RBM Reconstruction error 229.642319
&gt;&gt; Epoch 168 finished 	RBM Reconstruction error 227.399216
&gt;&gt; Epoch 169 finished 	RBM Reconstruction error 199.985855
&gt;&gt; Epoch 170 finished 	RBM Reconstruction error 233.785904
&gt;&gt; Epoch 171 finished 	RBM Reconstruction error 220.627808
&gt;&gt; Epoch 172 finished 	RBM Reconstruction error 222.667191
&gt;&gt; Epoch 173 finished 	RBM Reconstruction error 221.127594
&gt;&gt; Epoch 174 finished 	RBM Reconstruction error 212.889420
&gt;&gt; Epoch 175 finished 	RBM Reconstruction error 254.490738
&gt;&gt; Epoch 176 finished 	RBM Reconstruction error 234.652771
&gt;&gt; Epoch 177 finished 	RBM Reconstruction error 219.652542
&gt;&gt; Epoch 178 finished 	RBM Reconstruction error 242.585602
&gt;&gt; Epoch 179 finished 	RBM Reconstruction error 255.395081
&gt;&gt; Epoch 180 finished 	RBM Reconstruction error 229.205032
&gt;&gt; Epoch 181 finished 	RBM Reconstruction error 252.319427
&gt;&gt; Epoch 182 finished 	RBM Reconstruction error 243.640900
&gt;&gt; Epoch 183 finished 	RBM Reconstruction error 206.429260
&gt;&gt; Epoch 184 finished 	RBM Reconstruction error 222.625778
&gt;&gt; Epoch 185 finished 	RBM Reconstruction error 246.307343
&gt;&gt; Epoch 186 finished 	RBM Reconstruction error 220.067245
&gt;&gt; Epoch 187 finished 	RBM Reconstruction error 226.229187
&gt;&gt; Epoch 188 finished 	RBM Reconstruction error 208.193420
&gt;&gt; Epoch 189 finished 	RBM Reconstruction error 231.715118
&gt;&gt; Epoch 190 finished 	RBM Reconstruction error 229.132385
&gt;&gt; Epoch 191 finished 	RBM Reconstruction error 227.938568
&gt;&gt; Epoch 192 finished 	RBM Reconstruction error 227.725891
&gt;&gt; Epoch 193 finished 	RBM Reconstruction error 220.878632
&gt;&gt; Epoch 194 finished 	RBM Reconstruction error 222.349106
&gt;&gt; Epoch 195 finished 	RBM Reconstruction error 241.142532
&gt;&gt; Epoch 196 finished 	RBM Reconstruction error 222.700241
&gt;&gt; Epoch 197 finished 	RBM Reconstruction error 211.630432
&gt;&gt; Epoch 198 finished 	RBM Reconstruction error 229.865845
&gt;&gt; Epoch 199 finished 	RBM Reconstruction error 214.611023
&gt;&gt; Epoch 200 finished 	RBM Reconstruction error 252.983765
[END] Pre-training step
[START] Fine tuning step:
&gt;&gt; Epoch 0 finished 	ANN training loss 0.580909
&gt;&gt; Epoch 1 finished 	ANN training loss 0.348968
&gt;&gt; Epoch 2 finished 	ANN training loss 0.293230
&gt;&gt; Epoch 3 finished 	ANN training loss 0.229737
&gt;&gt; Epoch 4 finished 	ANN training loss 0.211241
&gt;&gt; Epoch 5 finished 	ANN training loss 0.163032
&gt;&gt; Epoch 6 finished 	ANN training loss 0.135197
&gt;&gt; Epoch 7 finished 	ANN training loss 0.107732
&gt;&gt; Epoch 8 finished 	ANN training loss 0.097198
&gt;&gt; Epoch 9 finished 	ANN training loss 0.084452
&gt;&gt; Epoch 10 finished 	ANN training loss 0.079183
&gt;&gt; Epoch 11 finished 	ANN training loss 0.062415
&gt;&gt; Epoch 12 finished 	ANN training loss 0.056549
&gt;&gt; Epoch 13 finished 	ANN training loss 0.054099
&gt;&gt; Epoch 14 finished 	ANN training loss 0.042692
&gt;&gt; Epoch 15 finished 	ANN training loss 0.037218
&gt;&gt; Epoch 16 finished 	ANN training loss 0.034735
&gt;&gt; Epoch 17 finished 	ANN training loss 0.030426
&gt;&gt; Epoch 18 finished 	ANN training loss 0.029074
&gt;&gt; Epoch 19 finished 	ANN training loss 0.025898
&gt;&gt; Epoch 20 finished 	ANN training loss 0.023864
&gt;&gt; Epoch 21 finished 	ANN training loss 0.022095
&gt;&gt; Epoch 22 finished 	ANN training loss 0.017579
&gt;&gt; Epoch 23 finished 	ANN training loss 0.015936
&gt;&gt; Epoch 24 finished 	ANN training loss 0.017030
&gt;&gt; Epoch 25 finished 	ANN training loss 0.014391
&gt;&gt; Epoch 26 finished 	ANN training loss 0.013288
&gt;&gt; Epoch 27 finished 	ANN training loss 0.012022
&gt;&gt; Epoch 28 finished 	ANN training loss 0.012725
&gt;&gt; Epoch 29 finished 	ANN training loss 0.011279
&gt;&gt; Epoch 30 finished 	ANN training loss 0.010585
&gt;&gt; Epoch 31 finished 	ANN training loss 0.008383
&gt;&gt; Epoch 32 finished 	ANN training loss 0.008156
&gt;&gt; Epoch 33 finished 	ANN training loss 0.007844
&gt;&gt; Epoch 34 finished 	ANN training loss 0.007210
&gt;&gt; Epoch 35 finished 	ANN training loss 0.007595
&gt;&gt; Epoch 36 finished 	ANN training loss 0.006130
&gt;&gt; Epoch 37 finished 	ANN training loss 0.006037
&gt;&gt; Epoch 38 finished 	ANN training loss 0.005578
&gt;&gt; Epoch 39 finished 	ANN training loss 0.004982
&gt;&gt; Epoch 40 finished 	ANN training loss 0.004639
&gt;&gt; Epoch 41 finished 	ANN training loss 0.004240
&gt;&gt; Epoch 42 finished 	ANN training loss 0.004408
&gt;&gt; Epoch 43 finished 	ANN training loss 0.004036
&gt;&gt; Epoch 44 finished 	ANN training loss 0.003707
&gt;&gt; Epoch 45 finished 	ANN training loss 0.004197
&gt;&gt; Epoch 46 finished 	ANN training loss 0.003487
&gt;&gt; Epoch 47 finished 	ANN training loss 0.003265
&gt;&gt; Epoch 48 finished 	ANN training loss 0.002998
&gt;&gt; Epoch 49 finished 	ANN training loss 0.003865
&gt;&gt; Epoch 50 finished 	ANN training loss 0.003190
&gt;&gt; Epoch 51 finished 	ANN training loss 0.002724
&gt;&gt; Epoch 52 finished 	ANN training loss 0.002447
&gt;&gt; Epoch 53 finished 	ANN training loss 0.002349
&gt;&gt; Epoch 54 finished 	ANN training loss 0.002605
&gt;&gt; Epoch 55 finished 	ANN training loss 0.002337
&gt;&gt; Epoch 56 finished 	ANN training loss 0.002328
&gt;&gt; Epoch 57 finished 	ANN training loss 0.002467
&gt;&gt; Epoch 58 finished 	ANN training loss 0.001995
&gt;&gt; Epoch 59 finished 	ANN training loss 0.001728
&gt;&gt; Epoch 60 finished 	ANN training loss 0.002199
&gt;&gt; Epoch 61 finished 	ANN training loss 0.001832
&gt;&gt; Epoch 62 finished 	ANN training loss 0.002042
&gt;&gt; Epoch 63 finished 	ANN training loss 0.001837
&gt;&gt; Epoch 64 finished 	ANN training loss 0.001811
&gt;&gt; Epoch 65 finished 	ANN training loss 0.001750
&gt;&gt; Epoch 66 finished 	ANN training loss 0.001519
&gt;&gt; Epoch 67 finished 	ANN training loss 0.001650
&gt;&gt; Epoch 68 finished 	ANN training loss 0.001392
&gt;&gt; Epoch 69 finished 	ANN training loss 0.001390
&gt;&gt; Epoch 70 finished 	ANN training loss 0.001337
&gt;&gt; Epoch 71 finished 	ANN training loss 0.001254
&gt;&gt; Epoch 72 finished 	ANN training loss 0.001549
&gt;&gt; Epoch 73 finished 	ANN training loss 0.001348
&gt;&gt; Epoch 74 finished 	ANN training loss 0.001286
&gt;&gt; Epoch 75 finished 	ANN training loss 0.001154
&gt;&gt; Epoch 76 finished 	ANN training loss 0.001271
&gt;&gt; Epoch 77 finished 	ANN training loss 0.001495
&gt;&gt; Epoch 78 finished 	ANN training loss 0.001031
&gt;&gt; Epoch 79 finished 	ANN training loss 0.001087
&gt;&gt; Epoch 80 finished 	ANN training loss 0.001100
&gt;&gt; Epoch 81 finished 	ANN training loss 0.000989
&gt;&gt; Epoch 82 finished 	ANN training loss 0.000921
&gt;&gt; Epoch 83 finished 	ANN training loss 0.000923
&gt;&gt; Epoch 84 finished 	ANN training loss 0.001123
&gt;&gt; Epoch 85 finished 	ANN training loss 0.001008
&gt;&gt; Epoch 86 finished 	ANN training loss 0.001041
&gt;&gt; Epoch 87 finished 	ANN training loss 0.000961
&gt;&gt; Epoch 88 finished 	ANN training loss 0.001001
&gt;&gt; Epoch 89 finished 	ANN training loss 0.000905
&gt;&gt; Epoch 90 finished 	ANN training loss 0.000852
&gt;&gt; Epoch 91 finished 	ANN training loss 0.000830
&gt;&gt; Epoch 92 finished 	ANN training loss 0.000839
&gt;&gt; Epoch 93 finished 	ANN training loss 0.000802
&gt;&gt; Epoch 94 finished 	ANN training loss 0.000783
&gt;&gt; Epoch 95 finished 	ANN training loss 0.000803
&gt;&gt; Epoch 96 finished 	ANN training loss 0.000765
&gt;&gt; Epoch 97 finished 	ANN training loss 0.000748
&gt;&gt; Epoch 98 finished 	ANN training loss 0.000769
&gt;&gt; Epoch 99 finished 	ANN training loss 0.000660
[END] Fine tuning step
Done.
Accuracy: 0.890000
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[92]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ACCURACY: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">epoch_rbm_acc</span><span class="p">[</span><span class="mi">9</span><span class="p">]))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>ACCURACY: 0.89
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[93]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Collated results</span>
<span class="nb">print</span><span class="p">(</span><span class="n">epoch_rbm_acc</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Most accurate n_epochs_rbm setting is Setting &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">epoch_rbm_acc</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="n">epoch_rbm_acc</span><span class="p">))</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[0.88, 0.88, 0.91000000000000003, 0.91000000000000003, 0.90500000000000003, 0.92000000000000004, 0.93000000000000005, 0.90000000000000002, 0.90000000000000002, 0.89000000000000001]
Most accurate n_epochs_rbm setting is Setting 7
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[60]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Create bar graph</span>
<span class="n">plt</span><span class="o">.</span><span class="n">clf</span><span class="p">()</span>
<span class="n">objects</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;1&#39;</span><span class="p">,</span> <span class="s1">&#39;2&#39;</span><span class="p">,</span> <span class="s1">&#39;5&#39;</span><span class="p">,</span> <span class="s1">&#39;10&#39;</span><span class="p">,</span> <span class="s1">&#39;20&#39;</span><span class="p">,</span> <span class="s1">&#39;30&#39;</span><span class="p">,</span> <span class="s1">&#39;40&#39;</span><span class="p">,</span> <span class="s1">&#39;50&#39;</span><span class="p">,</span> <span class="s1">&#39;100&#39;</span><span class="p">,</span> <span class="s1">&#39;200&#39;</span><span class="p">)</span>
<span class="n">y_pos</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">objects</span><span class="p">))</span>
<span class="n">performance</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.88</span><span class="p">,</span> <span class="mf">0.88</span><span class="p">,</span> <span class="mf">0.91000000000000003</span><span class="p">,</span> <span class="mf">0.91000000000000003</span><span class="p">,</span> <span class="mf">0.90500000000000003</span><span class="p">,</span> <span class="mf">0.92000000000000004</span><span class="p">,</span> <span class="mf">0.93000000000000005</span><span class="p">,</span> <span class="mf">0.90000000000000002</span><span class="p">,</span> <span class="mf">0.90000000000000002</span><span class="p">,</span> <span class="mf">0.89000000000000001</span><span class="p">]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">y_pos</span><span class="p">,</span> <span class="n">performance</span><span class="p">,</span> <span class="n">align</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">y_pos</span><span class="p">,</span> <span class="n">objects</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Settings&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Accuracy&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;n_epochs_rbm Settings&#39;</span><span class="p">)</span>
 
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">clf</span><span class="p">()</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>




<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAYUAAAEeCAYAAABlggnIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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"
>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Different-Fine-tuning-/-Backprop-Lengths:">Different Fine-tuning / Backprop Lengths:<a class="anchor-link" href="#Different-Fine-tuning-/-Backprop-Lengths:">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[94]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Store accuracies</span>
<span class="n">backprop_acc</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Setting 1</strong></p>

<pre><code>n_iter_backprop=1</code></pre>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[95]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">backprop_acc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">deep_belief_net</span><span class="p">(</span><span class="n">n_iter_backprop</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[START] Pre-training step:
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 55.817745
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 69.508820
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 48.128082
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 46.322353
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 39.850433
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 27.097710
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 33.053356
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 35.346004
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 35.167938
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 33.056343
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 145.429611
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 134.780823
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 96.752975
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 97.752327
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 124.652870
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 62.641727
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 132.078110
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 139.843246
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 149.523788
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 71.515213
[END] Pre-training step
[START] Fine tuning step:
&gt;&gt; Epoch 0 finished 	ANN training loss 0.670907
[END] Fine tuning step
Done.
Accuracy: 0.785000
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[96]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ACCURACY: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">backprop_acc</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>ACCURACY: 0.785
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Setting 2</strong></p>

<pre><code>n_iter_backprop=5</code></pre>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[97]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">backprop_acc</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">deep_belief_net</span><span class="p">(</span><span class="n">n_iter_backprop</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[START] Pre-training step:
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 56.174522
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 37.896420
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 32.486263
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 55.078617
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 31.739643
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 52.946396
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 27.569790
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 30.449902
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 37.940350
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 36.021034
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 165.765762
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 148.581039
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 89.941345
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 126.390038
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 106.427254
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 142.782364
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 102.480690
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 145.183884
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 92.604729
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 141.417679
[END] Pre-training step
[START] Fine tuning step:
&gt;&gt; Epoch 0 finished 	ANN training loss 0.702097
&gt;&gt; Epoch 1 finished 	ANN training loss 0.569819
&gt;&gt; Epoch 2 finished 	ANN training loss 0.361315
&gt;&gt; Epoch 3 finished 	ANN training loss 0.324648
&gt;&gt; Epoch 4 finished 	ANN training loss 0.272950
[END] Fine tuning step
Done.
Accuracy: 0.845000
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[98]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ACCURACY: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">backprop_acc</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>ACCURACY: 0.845
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Setting 3</strong></p>

<pre><code>n_iter_backprop=10</code></pre>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[99]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">backprop_acc</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">deep_belief_net</span><span class="p">(</span><span class="n">n_iter_backprop</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[START] Pre-training step:
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 65.974220
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 53.847179
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 51.343510
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 65.502045
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 50.545322
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 37.524105
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 39.445473
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 43.069767
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 35.095516
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 60.808556
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 86.329010
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 210.858826
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 214.150864
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 200.452820
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 238.501770
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 236.603210
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 200.314056
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 373.685852
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 199.616257
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 157.811234
[END] Pre-training step
[START] Fine tuning step:
&gt;&gt; Epoch 0 finished 	ANN training loss 0.648157
&gt;&gt; Epoch 1 finished 	ANN training loss 0.503972
&gt;&gt; Epoch 2 finished 	ANN training loss 0.361671
&gt;&gt; Epoch 3 finished 	ANN training loss 0.321565
&gt;&gt; Epoch 4 finished 	ANN training loss 0.304847
&gt;&gt; Epoch 5 finished 	ANN training loss 0.242860
&gt;&gt; Epoch 6 finished 	ANN training loss 0.212873
&gt;&gt; Epoch 7 finished 	ANN training loss 0.192529
&gt;&gt; Epoch 8 finished 	ANN training loss 0.169078
&gt;&gt; Epoch 9 finished 	ANN training loss 0.159680
[END] Fine tuning step
Done.
Accuracy: 0.870000
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[100]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ACCURACY: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">backprop_acc</span><span class="p">[</span><span class="mi">2</span><span class="p">]))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>ACCURACY: 0.87
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Setting 4</strong></p>

<pre><code>n_iter_backprop=20</code></pre>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[101]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">backprop_acc</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">=</span> <span class="n">deep_belief_net</span><span class="p">(</span><span class="n">n_iter_backprop</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[START] Pre-training step:
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 79.951347
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 64.819191
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 60.852627
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 63.349876
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 34.447636
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 43.758175
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 39.598602
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 24.591450
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 48.674816
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 24.759068
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 77.523529
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 91.118660
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 84.581970
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 61.938057
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 86.447746
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 92.810883
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 89.089783
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 89.824196
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 96.242249
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 90.174896
[END] Pre-training step
[START] Fine tuning step:
&gt;&gt; Epoch 0 finished 	ANN training loss 0.670005
&gt;&gt; Epoch 1 finished 	ANN training loss 0.455668
&gt;&gt; Epoch 2 finished 	ANN training loss 0.385733
&gt;&gt; Epoch 3 finished 	ANN training loss 0.316249
&gt;&gt; Epoch 4 finished 	ANN training loss 0.298953
&gt;&gt; Epoch 5 finished 	ANN training loss 0.232435
&gt;&gt; Epoch 6 finished 	ANN training loss 0.205809
&gt;&gt; Epoch 7 finished 	ANN training loss 0.185147
&gt;&gt; Epoch 8 finished 	ANN training loss 0.172254
&gt;&gt; Epoch 9 finished 	ANN training loss 0.174326
&gt;&gt; Epoch 10 finished 	ANN training loss 0.118040
&gt;&gt; Epoch 11 finished 	ANN training loss 0.121350
&gt;&gt; Epoch 12 finished 	ANN training loss 0.097296
&gt;&gt; Epoch 13 finished 	ANN training loss 0.098613
&gt;&gt; Epoch 14 finished 	ANN training loss 0.080863
&gt;&gt; Epoch 15 finished 	ANN training loss 0.079201
&gt;&gt; Epoch 16 finished 	ANN training loss 0.065509
&gt;&gt; Epoch 17 finished 	ANN training loss 0.060506
&gt;&gt; Epoch 18 finished 	ANN training loss 0.050646
&gt;&gt; Epoch 19 finished 	ANN training loss 0.044502
[END] Fine tuning step
Done.
Accuracy: 0.890000
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[102]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ACCURACY: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">backprop_acc</span><span class="p">[</span><span class="mi">3</span><span class="p">]))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>ACCURACY: 0.89
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Setting 5</strong></p>

<pre><code>n_iter_backprop=50</code></pre>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[103]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">backprop_acc</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span> <span class="o">=</span> <span class="n">deep_belief_net</span><span class="p">(</span><span class="n">n_iter_backprop</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[START] Pre-training step:
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 47.966709
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 45.884468
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 41.949032
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 51.684757
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 54.949287
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 35.029099
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 47.786827
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 47.673374
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 51.909813
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 39.520073
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 224.726135
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 144.034760
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 202.503006
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 146.634048
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 132.441116
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 153.882095
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 146.382202
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 122.807571
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 128.477036
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 153.221161
[END] Pre-training step
[START] Fine tuning step:
&gt;&gt; Epoch 0 finished 	ANN training loss 0.656392
&gt;&gt; Epoch 1 finished 	ANN training loss 0.511056
&gt;&gt; Epoch 2 finished 	ANN training loss 0.419523
&gt;&gt; Epoch 3 finished 	ANN training loss 0.345274
&gt;&gt; Epoch 4 finished 	ANN training loss 0.291805
&gt;&gt; Epoch 5 finished 	ANN training loss 0.234449
&gt;&gt; Epoch 6 finished 	ANN training loss 0.202438
&gt;&gt; Epoch 7 finished 	ANN training loss 0.189213
&gt;&gt; Epoch 8 finished 	ANN training loss 0.161982
&gt;&gt; Epoch 9 finished 	ANN training loss 0.142671
&gt;&gt; Epoch 10 finished 	ANN training loss 0.125817
&gt;&gt; Epoch 11 finished 	ANN training loss 0.116951
&gt;&gt; Epoch 12 finished 	ANN training loss 0.118129
&gt;&gt; Epoch 13 finished 	ANN training loss 0.094424
&gt;&gt; Epoch 14 finished 	ANN training loss 0.100267
&gt;&gt; Epoch 15 finished 	ANN training loss 0.081743
&gt;&gt; Epoch 16 finished 	ANN training loss 0.087958
&gt;&gt; Epoch 17 finished 	ANN training loss 0.059302
&gt;&gt; Epoch 18 finished 	ANN training loss 0.053444
&gt;&gt; Epoch 19 finished 	ANN training loss 0.048428
&gt;&gt; Epoch 20 finished 	ANN training loss 0.042279
&gt;&gt; Epoch 21 finished 	ANN training loss 0.040057
&gt;&gt; Epoch 22 finished 	ANN training loss 0.036358
&gt;&gt; Epoch 23 finished 	ANN training loss 0.038913
&gt;&gt; Epoch 24 finished 	ANN training loss 0.032932
&gt;&gt; Epoch 25 finished 	ANN training loss 0.029065
&gt;&gt; Epoch 26 finished 	ANN training loss 0.026651
&gt;&gt; Epoch 27 finished 	ANN training loss 0.026513
&gt;&gt; Epoch 28 finished 	ANN training loss 0.023277
&gt;&gt; Epoch 29 finished 	ANN training loss 0.020492
&gt;&gt; Epoch 30 finished 	ANN training loss 0.023263
&gt;&gt; Epoch 31 finished 	ANN training loss 0.017441
&gt;&gt; Epoch 32 finished 	ANN training loss 0.019259
&gt;&gt; Epoch 33 finished 	ANN training loss 0.014802
&gt;&gt; Epoch 34 finished 	ANN training loss 0.015365
&gt;&gt; Epoch 35 finished 	ANN training loss 0.012633
&gt;&gt; Epoch 36 finished 	ANN training loss 0.012282
&gt;&gt; Epoch 37 finished 	ANN training loss 0.013217
&gt;&gt; Epoch 38 finished 	ANN training loss 0.011150
&gt;&gt; Epoch 39 finished 	ANN training loss 0.009588
&gt;&gt; Epoch 40 finished 	ANN training loss 0.010154
&gt;&gt; Epoch 41 finished 	ANN training loss 0.008799
&gt;&gt; Epoch 42 finished 	ANN training loss 0.008715
&gt;&gt; Epoch 43 finished 	ANN training loss 0.008538
&gt;&gt; Epoch 44 finished 	ANN training loss 0.007858
&gt;&gt; Epoch 45 finished 	ANN training loss 0.007424
&gt;&gt; Epoch 46 finished 	ANN training loss 0.006622
&gt;&gt; Epoch 47 finished 	ANN training loss 0.006787
&gt;&gt; Epoch 48 finished 	ANN training loss 0.006269
&gt;&gt; Epoch 49 finished 	ANN training loss 0.006362
[END] Fine tuning step
Done.
Accuracy: 0.920000
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[104]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ACCURACY: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">backprop_acc</span><span class="p">[</span><span class="mi">4</span><span class="p">]))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>ACCURACY: 0.92
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Setting 6 (Default)</strong></p>

<pre><code>n_iter_backprop=100</code></pre>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[105]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">backprop_acc</span><span class="p">[</span><span class="mi">5</span><span class="p">]</span> <span class="o">=</span> <span class="n">default_acc</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[106]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ACCURACY: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">backprop_acc</span><span class="p">[</span><span class="mi">5</span><span class="p">]))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>ACCURACY: 0.91
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Setting 7</strong></p>

<pre><code>n_iter_backprop=200</code></pre>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[107]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">backprop_acc</span><span class="p">[</span><span class="mi">6</span><span class="p">]</span> <span class="o">=</span> <span class="n">deep_belief_net</span><span class="p">(</span><span class="n">n_iter_backprop</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[START] Pre-training step:
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 55.013329
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 48.502865
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 59.771660
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 53.611660
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 41.888935
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 33.063171
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 39.538239
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 65.689186
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 32.433292
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 32.080685
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 82.105980
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 51.835796
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 61.509434
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 72.298897
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 93.334030
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 126.274864
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 145.872345
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 99.858513
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 124.187401
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 147.684265
[END] Pre-training step
[START] Fine tuning step:
&gt;&gt; Epoch 0 finished 	ANN training loss 0.664079
&gt;&gt; Epoch 1 finished 	ANN training loss 0.479317
&gt;&gt; Epoch 2 finished 	ANN training loss 0.408934
&gt;&gt; Epoch 3 finished 	ANN training loss 0.317145
&gt;&gt; Epoch 4 finished 	ANN training loss 0.277289
&gt;&gt; Epoch 5 finished 	ANN training loss 0.272827
&gt;&gt; Epoch 6 finished 	ANN training loss 0.227836
&gt;&gt; Epoch 7 finished 	ANN training loss 0.199976
&gt;&gt; Epoch 8 finished 	ANN training loss 0.185234
&gt;&gt; Epoch 9 finished 	ANN training loss 0.158111
&gt;&gt; Epoch 10 finished 	ANN training loss 0.134525
&gt;&gt; Epoch 11 finished 	ANN training loss 0.122839
&gt;&gt; Epoch 12 finished 	ANN training loss 0.117652
&gt;&gt; Epoch 13 finished 	ANN training loss 0.104176
&gt;&gt; Epoch 14 finished 	ANN training loss 0.091423
&gt;&gt; Epoch 15 finished 	ANN training loss 0.095539
&gt;&gt; Epoch 16 finished 	ANN training loss 0.072739
&gt;&gt; Epoch 17 finished 	ANN training loss 0.066299
&gt;&gt; Epoch 18 finished 	ANN training loss 0.072635
&gt;&gt; Epoch 19 finished 	ANN training loss 0.058822
&gt;&gt; Epoch 20 finished 	ANN training loss 0.049980
&gt;&gt; Epoch 21 finished 	ANN training loss 0.042655
&gt;&gt; Epoch 22 finished 	ANN training loss 0.043048
&gt;&gt; Epoch 23 finished 	ANN training loss 0.040025
&gt;&gt; Epoch 24 finished 	ANN training loss 0.037563
&gt;&gt; Epoch 25 finished 	ANN training loss 0.032806
&gt;&gt; Epoch 26 finished 	ANN training loss 0.028028
&gt;&gt; Epoch 27 finished 	ANN training loss 0.025567
&gt;&gt; Epoch 28 finished 	ANN training loss 0.023030
&gt;&gt; Epoch 29 finished 	ANN training loss 0.022123
&gt;&gt; Epoch 30 finished 	ANN training loss 0.020134
&gt;&gt; Epoch 31 finished 	ANN training loss 0.019087
&gt;&gt; Epoch 32 finished 	ANN training loss 0.020946
&gt;&gt; Epoch 33 finished 	ANN training loss 0.016696
&gt;&gt; Epoch 34 finished 	ANN training loss 0.017109
&gt;&gt; Epoch 35 finished 	ANN training loss 0.014412
&gt;&gt; Epoch 36 finished 	ANN training loss 0.012136
&gt;&gt; Epoch 37 finished 	ANN training loss 0.012011
&gt;&gt; Epoch 38 finished 	ANN training loss 0.010189
&gt;&gt; Epoch 39 finished 	ANN training loss 0.010192
&gt;&gt; Epoch 40 finished 	ANN training loss 0.011124
&gt;&gt; Epoch 41 finished 	ANN training loss 0.008018
&gt;&gt; Epoch 42 finished 	ANN training loss 0.009929
&gt;&gt; Epoch 43 finished 	ANN training loss 0.008672
&gt;&gt; Epoch 44 finished 	ANN training loss 0.007237
&gt;&gt; Epoch 45 finished 	ANN training loss 0.006594
&gt;&gt; Epoch 46 finished 	ANN training loss 0.006595
&gt;&gt; Epoch 47 finished 	ANN training loss 0.005762
&gt;&gt; Epoch 48 finished 	ANN training loss 0.005818
&gt;&gt; Epoch 49 finished 	ANN training loss 0.005897
&gt;&gt; Epoch 50 finished 	ANN training loss 0.005935
&gt;&gt; Epoch 51 finished 	ANN training loss 0.005573
&gt;&gt; Epoch 52 finished 	ANN training loss 0.005448
&gt;&gt; Epoch 53 finished 	ANN training loss 0.005461
&gt;&gt; Epoch 54 finished 	ANN training loss 0.004520
&gt;&gt; Epoch 55 finished 	ANN training loss 0.004873
&gt;&gt; Epoch 56 finished 	ANN training loss 0.004795
&gt;&gt; Epoch 57 finished 	ANN training loss 0.004075
&gt;&gt; Epoch 58 finished 	ANN training loss 0.004024
&gt;&gt; Epoch 59 finished 	ANN training loss 0.003248
&gt;&gt; Epoch 60 finished 	ANN training loss 0.003506
&gt;&gt; Epoch 61 finished 	ANN training loss 0.004726
&gt;&gt; Epoch 62 finished 	ANN training loss 0.003061
&gt;&gt; Epoch 63 finished 	ANN training loss 0.002853
&gt;&gt; Epoch 64 finished 	ANN training loss 0.002889
&gt;&gt; Epoch 65 finished 	ANN training loss 0.003064
&gt;&gt; Epoch 66 finished 	ANN training loss 0.002212
&gt;&gt; Epoch 67 finished 	ANN training loss 0.002568
&gt;&gt; Epoch 68 finished 	ANN training loss 0.002115
&gt;&gt; Epoch 69 finished 	ANN training loss 0.002289
&gt;&gt; Epoch 70 finished 	ANN training loss 0.002122
&gt;&gt; Epoch 71 finished 	ANN training loss 0.002099
&gt;&gt; Epoch 72 finished 	ANN training loss 0.001884
&gt;&gt; Epoch 73 finished 	ANN training loss 0.001751
&gt;&gt; Epoch 74 finished 	ANN training loss 0.001685
&gt;&gt; Epoch 75 finished 	ANN training loss 0.001586
&gt;&gt; Epoch 76 finished 	ANN training loss 0.001635
&gt;&gt; Epoch 77 finished 	ANN training loss 0.001536
&gt;&gt; Epoch 78 finished 	ANN training loss 0.001605
&gt;&gt; Epoch 79 finished 	ANN training loss 0.001707
&gt;&gt; Epoch 80 finished 	ANN training loss 0.001363
&gt;&gt; Epoch 81 finished 	ANN training loss 0.001408
&gt;&gt; Epoch 82 finished 	ANN training loss 0.001385
&gt;&gt; Epoch 83 finished 	ANN training loss 0.001387
&gt;&gt; Epoch 84 finished 	ANN training loss 0.001440
&gt;&gt; Epoch 85 finished 	ANN training loss 0.001480
&gt;&gt; Epoch 86 finished 	ANN training loss 0.001481
&gt;&gt; Epoch 87 finished 	ANN training loss 0.001435
&gt;&gt; Epoch 88 finished 	ANN training loss 0.001389
&gt;&gt; Epoch 89 finished 	ANN training loss 0.001259
&gt;&gt; Epoch 90 finished 	ANN training loss 0.001328
&gt;&gt; Epoch 91 finished 	ANN training loss 0.001192
&gt;&gt; Epoch 92 finished 	ANN training loss 0.001289
&gt;&gt; Epoch 93 finished 	ANN training loss 0.001206
&gt;&gt; Epoch 94 finished 	ANN training loss 0.001175
&gt;&gt; Epoch 95 finished 	ANN training loss 0.001065
&gt;&gt; Epoch 96 finished 	ANN training loss 0.001057
&gt;&gt; Epoch 97 finished 	ANN training loss 0.000936
&gt;&gt; Epoch 98 finished 	ANN training loss 0.000948
&gt;&gt; Epoch 99 finished 	ANN training loss 0.000945
&gt;&gt; Epoch 100 finished 	ANN training loss 0.000825
&gt;&gt; Epoch 101 finished 	ANN training loss 0.001418
&gt;&gt; Epoch 102 finished 	ANN training loss 0.000817
&gt;&gt; Epoch 103 finished 	ANN training loss 0.000747
&gt;&gt; Epoch 104 finished 	ANN training loss 0.000851
&gt;&gt; Epoch 105 finished 	ANN training loss 0.000741
&gt;&gt; Epoch 106 finished 	ANN training loss 0.000805
&gt;&gt; Epoch 107 finished 	ANN training loss 0.000930
&gt;&gt; Epoch 108 finished 	ANN training loss 0.000816
&gt;&gt; Epoch 109 finished 	ANN training loss 0.000909
&gt;&gt; Epoch 110 finished 	ANN training loss 0.001038
&gt;&gt; Epoch 111 finished 	ANN training loss 0.000716
&gt;&gt; Epoch 112 finished 	ANN training loss 0.000699
&gt;&gt; Epoch 113 finished 	ANN training loss 0.000722
&gt;&gt; Epoch 114 finished 	ANN training loss 0.000797
&gt;&gt; Epoch 115 finished 	ANN training loss 0.000785
&gt;&gt; Epoch 116 finished 	ANN training loss 0.000767
&gt;&gt; Epoch 117 finished 	ANN training loss 0.000665
&gt;&gt; Epoch 118 finished 	ANN training loss 0.000673
&gt;&gt; Epoch 119 finished 	ANN training loss 0.000680
&gt;&gt; Epoch 120 finished 	ANN training loss 0.000577
&gt;&gt; Epoch 121 finished 	ANN training loss 0.000502
&gt;&gt; Epoch 122 finished 	ANN training loss 0.000570
&gt;&gt; Epoch 123 finished 	ANN training loss 0.000556
&gt;&gt; Epoch 124 finished 	ANN training loss 0.000497
&gt;&gt; Epoch 125 finished 	ANN training loss 0.000563
&gt;&gt; Epoch 126 finished 	ANN training loss 0.000554
&gt;&gt; Epoch 127 finished 	ANN training loss 0.000470
&gt;&gt; Epoch 128 finished 	ANN training loss 0.000462
&gt;&gt; Epoch 129 finished 	ANN training loss 0.000384
&gt;&gt; Epoch 130 finished 	ANN training loss 0.000390
&gt;&gt; Epoch 131 finished 	ANN training loss 0.000342
&gt;&gt; Epoch 132 finished 	ANN training loss 0.000722
&gt;&gt; Epoch 133 finished 	ANN training loss 0.000378
&gt;&gt; Epoch 134 finished 	ANN training loss 0.000430
&gt;&gt; Epoch 135 finished 	ANN training loss 0.000490
&gt;&gt; Epoch 136 finished 	ANN training loss 0.000416
&gt;&gt; Epoch 137 finished 	ANN training loss 0.000397
&gt;&gt; Epoch 138 finished 	ANN training loss 0.000467
&gt;&gt; Epoch 139 finished 	ANN training loss 0.000413
&gt;&gt; Epoch 140 finished 	ANN training loss 0.000400
&gt;&gt; Epoch 141 finished 	ANN training loss 0.000416
&gt;&gt; Epoch 142 finished 	ANN training loss 0.000476
&gt;&gt; Epoch 143 finished 	ANN training loss 0.000475
&gt;&gt; Epoch 144 finished 	ANN training loss 0.000576
&gt;&gt; Epoch 145 finished 	ANN training loss 0.000468
&gt;&gt; Epoch 146 finished 	ANN training loss 0.000416
&gt;&gt; Epoch 147 finished 	ANN training loss 0.000362
&gt;&gt; Epoch 148 finished 	ANN training loss 0.000440
&gt;&gt; Epoch 149 finished 	ANN training loss 0.000433
&gt;&gt; Epoch 150 finished 	ANN training loss 0.000334
&gt;&gt; Epoch 151 finished 	ANN training loss 0.000325
&gt;&gt; Epoch 152 finished 	ANN training loss 0.000354
&gt;&gt; Epoch 153 finished 	ANN training loss 0.000328
&gt;&gt; Epoch 154 finished 	ANN training loss 0.000353
&gt;&gt; Epoch 155 finished 	ANN training loss 0.000335
&gt;&gt; Epoch 156 finished 	ANN training loss 0.000308
&gt;&gt; Epoch 157 finished 	ANN training loss 0.000300
&gt;&gt; Epoch 158 finished 	ANN training loss 0.000375
&gt;&gt; Epoch 159 finished 	ANN training loss 0.000329
&gt;&gt; Epoch 160 finished 	ANN training loss 0.000290
&gt;&gt; Epoch 161 finished 	ANN training loss 0.000288
&gt;&gt; Epoch 162 finished 	ANN training loss 0.000283
&gt;&gt; Epoch 163 finished 	ANN training loss 0.000279
&gt;&gt; Epoch 164 finished 	ANN training loss 0.000279
&gt;&gt; Epoch 165 finished 	ANN training loss 0.000261
&gt;&gt; Epoch 166 finished 	ANN training loss 0.000226
&gt;&gt; Epoch 167 finished 	ANN training loss 0.000221
&gt;&gt; Epoch 168 finished 	ANN training loss 0.000218
&gt;&gt; Epoch 169 finished 	ANN training loss 0.000215
&gt;&gt; Epoch 170 finished 	ANN training loss 0.000274
&gt;&gt; Epoch 171 finished 	ANN training loss 0.000204
&gt;&gt; Epoch 172 finished 	ANN training loss 0.000258
&gt;&gt; Epoch 173 finished 	ANN training loss 0.000208
&gt;&gt; Epoch 174 finished 	ANN training loss 0.000188
&gt;&gt; Epoch 175 finished 	ANN training loss 0.000196
&gt;&gt; Epoch 176 finished 	ANN training loss 0.000186
&gt;&gt; Epoch 177 finished 	ANN training loss 0.000187
&gt;&gt; Epoch 178 finished 	ANN training loss 0.000217
&gt;&gt; Epoch 179 finished 	ANN training loss 0.000236
&gt;&gt; Epoch 180 finished 	ANN training loss 0.000253
&gt;&gt; Epoch 181 finished 	ANN training loss 0.000222
&gt;&gt; Epoch 182 finished 	ANN training loss 0.000223
&gt;&gt; Epoch 183 finished 	ANN training loss 0.000217
&gt;&gt; Epoch 184 finished 	ANN training loss 0.000199
&gt;&gt; Epoch 185 finished 	ANN training loss 0.000241
&gt;&gt; Epoch 186 finished 	ANN training loss 0.000200
&gt;&gt; Epoch 187 finished 	ANN training loss 0.000198
&gt;&gt; Epoch 188 finished 	ANN training loss 0.000182
&gt;&gt; Epoch 189 finished 	ANN training loss 0.000161
&gt;&gt; Epoch 190 finished 	ANN training loss 0.000156
&gt;&gt; Epoch 191 finished 	ANN training loss 0.000153
&gt;&gt; Epoch 192 finished 	ANN training loss 0.000147
&gt;&gt; Epoch 193 finished 	ANN training loss 0.000182
&gt;&gt; Epoch 194 finished 	ANN training loss 0.000158
&gt;&gt; Epoch 195 finished 	ANN training loss 0.000146
&gt;&gt; Epoch 196 finished 	ANN training loss 0.000180
&gt;&gt; Epoch 197 finished 	ANN training loss 0.000152
&gt;&gt; Epoch 198 finished 	ANN training loss 0.000146
&gt;&gt; Epoch 199 finished 	ANN training loss 0.000159
[END] Fine tuning step
Done.
Accuracy: 0.930000
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[108]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ACCURACY: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">backprop_acc</span><span class="p">[</span><span class="mi">6</span><span class="p">]))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>ACCURACY: 0.93
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[109]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Collated results</span>
<span class="nb">print</span><span class="p">(</span><span class="n">backprop_acc</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Most accurate backprop iteration setting is Setting &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">backprop_acc</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="n">backprop_acc</span><span class="p">))</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[0.78500000000000003, 0.84499999999999997, 0.87, 0.89000000000000001, 0.92000000000000004, 0.91000000000000003, 0.93000000000000005]
Most accurate backprop iteration setting is Setting 7
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[62]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Create bar graph</span>
<span class="n">plt</span><span class="o">.</span><span class="n">clf</span><span class="p">()</span>
<span class="n">objects</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;1&#39;</span><span class="p">,</span> <span class="s1">&#39;5&#39;</span><span class="p">,</span> <span class="s1">&#39;10&#39;</span><span class="p">,</span> <span class="s1">&#39;20&#39;</span><span class="p">,</span> <span class="s1">&#39;50&#39;</span><span class="p">,</span> <span class="s1">&#39;100&#39;</span><span class="p">,</span> <span class="s1">&#39;200&#39;</span><span class="p">)</span>
<span class="n">y_pos</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">objects</span><span class="p">))</span>
<span class="n">performance</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.78500000000000003</span><span class="p">,</span> <span class="mf">0.84499999999999997</span><span class="p">,</span> <span class="mf">0.87</span><span class="p">,</span> <span class="mf">0.89000000000000001</span><span class="p">,</span> <span class="mf">0.92000000000000004</span><span class="p">,</span> <span class="mf">0.91000000000000003</span><span class="p">,</span> <span class="mf">0.93000000000000005</span><span class="p">]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">y_pos</span><span class="p">,</span> <span class="n">performance</span><span class="p">,</span> <span class="n">align</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">y_pos</span><span class="p">,</span> <span class="n">objects</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Settings&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Accuracy&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;n_iter_backprop Settings&#39;</span><span class="p">)</span>
 
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">clf</span><span class="p">()</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>




<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAYUAAAEeCAYAAABlggnIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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"
>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Benchmark-1:">Benchmark 1:<a class="anchor-link" href="#Benchmark-1:">&#182;</a></h3><p>Take the best setting for each n-layer setup and use the best performing parameters.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[111]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># 1-layer</span>
<span class="n">acc1</span> <span class="o">=</span> <span class="n">deep_belief_net</span><span class="p">(</span><span class="n">hidden_layers_structure</span><span class="o">=</span><span class="p">[</span><span class="mi">200</span><span class="p">],</span> 
                       <span class="n">learning_rate_rbm</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span>
                       <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                       <span class="n">n_epochs_rbm</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span>
                       <span class="n">n_iter_backprop</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
                       <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
                       <span class="n">activation_function</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span>
                       <span class="n">dropout_p</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[START] Pre-training step:
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 48.168800
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 60.400215
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 63.935898
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 39.533291
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 32.563824
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 37.166458
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 26.511868
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 39.058151
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 37.210552
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 43.879555
&gt;&gt; Epoch 11 finished 	RBM Reconstruction error 55.275127
&gt;&gt; Epoch 12 finished 	RBM Reconstruction error 37.254604
&gt;&gt; Epoch 13 finished 	RBM Reconstruction error 37.860001
&gt;&gt; Epoch 14 finished 	RBM Reconstruction error 37.345356
&gt;&gt; Epoch 15 finished 	RBM Reconstruction error 34.719990
&gt;&gt; Epoch 16 finished 	RBM Reconstruction error 36.081120
&gt;&gt; Epoch 17 finished 	RBM Reconstruction error 32.172657
&gt;&gt; Epoch 18 finished 	RBM Reconstruction error 33.625538
&gt;&gt; Epoch 19 finished 	RBM Reconstruction error 34.060772
&gt;&gt; Epoch 20 finished 	RBM Reconstruction error 33.108856
&gt;&gt; Epoch 21 finished 	RBM Reconstruction error 35.387943
&gt;&gt; Epoch 22 finished 	RBM Reconstruction error 36.899574
&gt;&gt; Epoch 23 finished 	RBM Reconstruction error 43.065262
&gt;&gt; Epoch 24 finished 	RBM Reconstruction error 49.713409
&gt;&gt; Epoch 25 finished 	RBM Reconstruction error 30.912466
&gt;&gt; Epoch 26 finished 	RBM Reconstruction error 39.174515
&gt;&gt; Epoch 27 finished 	RBM Reconstruction error 37.267910
&gt;&gt; Epoch 28 finished 	RBM Reconstruction error 39.332657
&gt;&gt; Epoch 29 finished 	RBM Reconstruction error 36.845791
&gt;&gt; Epoch 30 finished 	RBM Reconstruction error 38.480324
&gt;&gt; Epoch 31 finished 	RBM Reconstruction error 29.171761
&gt;&gt; Epoch 32 finished 	RBM Reconstruction error 40.481262
&gt;&gt; Epoch 33 finished 	RBM Reconstruction error 51.669361
&gt;&gt; Epoch 34 finished 	RBM Reconstruction error 58.144600
&gt;&gt; Epoch 35 finished 	RBM Reconstruction error 43.789810
&gt;&gt; Epoch 36 finished 	RBM Reconstruction error 37.336426
&gt;&gt; Epoch 37 finished 	RBM Reconstruction error 37.533741
&gt;&gt; Epoch 38 finished 	RBM Reconstruction error 50.071625
&gt;&gt; Epoch 39 finished 	RBM Reconstruction error 52.193783
&gt;&gt; Epoch 40 finished 	RBM Reconstruction error 43.204815
[END] Pre-training step
[START] Fine tuning step:
&gt;&gt; Epoch 0 finished 	ANN training loss 0.746216
&gt;&gt; Epoch 1 finished 	ANN training loss 0.512527
&gt;&gt; Epoch 2 finished 	ANN training loss 0.407565
&gt;&gt; Epoch 3 finished 	ANN training loss 0.344573
&gt;&gt; Epoch 4 finished 	ANN training loss 0.307931
&gt;&gt; Epoch 5 finished 	ANN training loss 0.261170
&gt;&gt; Epoch 6 finished 	ANN training loss 0.232531
&gt;&gt; Epoch 7 finished 	ANN training loss 0.213916
&gt;&gt; Epoch 8 finished 	ANN training loss 0.189582
&gt;&gt; Epoch 9 finished 	ANN training loss 0.187780
&gt;&gt; Epoch 10 finished 	ANN training loss 0.159324
&gt;&gt; Epoch 11 finished 	ANN training loss 0.149528
&gt;&gt; Epoch 12 finished 	ANN training loss 0.137469
&gt;&gt; Epoch 13 finished 	ANN training loss 0.126154
&gt;&gt; Epoch 14 finished 	ANN training loss 0.109239
&gt;&gt; Epoch 15 finished 	ANN training loss 0.111346
&gt;&gt; Epoch 16 finished 	ANN training loss 0.097815
&gt;&gt; Epoch 17 finished 	ANN training loss 0.095576
&gt;&gt; Epoch 18 finished 	ANN training loss 0.080653
&gt;&gt; Epoch 19 finished 	ANN training loss 0.075779
&gt;&gt; Epoch 20 finished 	ANN training loss 0.067027
&gt;&gt; Epoch 21 finished 	ANN training loss 0.062688
&gt;&gt; Epoch 22 finished 	ANN training loss 0.060348
&gt;&gt; Epoch 23 finished 	ANN training loss 0.065758
&gt;&gt; Epoch 24 finished 	ANN training loss 0.057402
&gt;&gt; Epoch 25 finished 	ANN training loss 0.047824
&gt;&gt; Epoch 26 finished 	ANN training loss 0.043683
&gt;&gt; Epoch 27 finished 	ANN training loss 0.042293
&gt;&gt; Epoch 28 finished 	ANN training loss 0.039879
&gt;&gt; Epoch 29 finished 	ANN training loss 0.036098
&gt;&gt; Epoch 30 finished 	ANN training loss 0.034039
&gt;&gt; Epoch 31 finished 	ANN training loss 0.033823
&gt;&gt; Epoch 32 finished 	ANN training loss 0.029938
&gt;&gt; Epoch 33 finished 	ANN training loss 0.033278
&gt;&gt; Epoch 34 finished 	ANN training loss 0.027886
&gt;&gt; Epoch 35 finished 	ANN training loss 0.025632
&gt;&gt; Epoch 36 finished 	ANN training loss 0.024100
&gt;&gt; Epoch 37 finished 	ANN training loss 0.023274
&gt;&gt; Epoch 38 finished 	ANN training loss 0.022080
&gt;&gt; Epoch 39 finished 	ANN training loss 0.021919
&gt;&gt; Epoch 40 finished 	ANN training loss 0.022422
&gt;&gt; Epoch 41 finished 	ANN training loss 0.019018
&gt;&gt; Epoch 42 finished 	ANN training loss 0.018405
&gt;&gt; Epoch 43 finished 	ANN training loss 0.016863
&gt;&gt; Epoch 44 finished 	ANN training loss 0.017013
&gt;&gt; Epoch 45 finished 	ANN training loss 0.015901
&gt;&gt; Epoch 46 finished 	ANN training loss 0.015003
&gt;&gt; Epoch 47 finished 	ANN training loss 0.015038
&gt;&gt; Epoch 48 finished 	ANN training loss 0.013627
&gt;&gt; Epoch 49 finished 	ANN training loss 0.013029
&gt;&gt; Epoch 50 finished 	ANN training loss 0.012874
&gt;&gt; Epoch 51 finished 	ANN training loss 0.012056
&gt;&gt; Epoch 52 finished 	ANN training loss 0.011890
&gt;&gt; Epoch 53 finished 	ANN training loss 0.010734
&gt;&gt; Epoch 54 finished 	ANN training loss 0.011105
&gt;&gt; Epoch 55 finished 	ANN training loss 0.010316
&gt;&gt; Epoch 56 finished 	ANN training loss 0.009811
&gt;&gt; Epoch 57 finished 	ANN training loss 0.009628
&gt;&gt; Epoch 58 finished 	ANN training loss 0.009383
&gt;&gt; Epoch 59 finished 	ANN training loss 0.008875
&gt;&gt; Epoch 60 finished 	ANN training loss 0.008443
&gt;&gt; Epoch 61 finished 	ANN training loss 0.009048
&gt;&gt; Epoch 62 finished 	ANN training loss 0.008089
&gt;&gt; Epoch 63 finished 	ANN training loss 0.007561
&gt;&gt; Epoch 64 finished 	ANN training loss 0.007407
&gt;&gt; Epoch 65 finished 	ANN training loss 0.007198
&gt;&gt; Epoch 66 finished 	ANN training loss 0.007208
&gt;&gt; Epoch 67 finished 	ANN training loss 0.006687
&gt;&gt; Epoch 68 finished 	ANN training loss 0.006617
&gt;&gt; Epoch 69 finished 	ANN training loss 0.006374
&gt;&gt; Epoch 70 finished 	ANN training loss 0.006170
&gt;&gt; Epoch 71 finished 	ANN training loss 0.006176
&gt;&gt; Epoch 72 finished 	ANN training loss 0.005998
&gt;&gt; Epoch 73 finished 	ANN training loss 0.005869
&gt;&gt; Epoch 74 finished 	ANN training loss 0.005771
&gt;&gt; Epoch 75 finished 	ANN training loss 0.005277
&gt;&gt; Epoch 76 finished 	ANN training loss 0.005309
&gt;&gt; Epoch 77 finished 	ANN training loss 0.005109
&gt;&gt; Epoch 78 finished 	ANN training loss 0.005513
&gt;&gt; Epoch 79 finished 	ANN training loss 0.004751
&gt;&gt; Epoch 80 finished 	ANN training loss 0.004898
&gt;&gt; Epoch 81 finished 	ANN training loss 0.005011
&gt;&gt; Epoch 82 finished 	ANN training loss 0.004672
&gt;&gt; Epoch 83 finished 	ANN training loss 0.004685
&gt;&gt; Epoch 84 finished 	ANN training loss 0.004802
&gt;&gt; Epoch 85 finished 	ANN training loss 0.004400
&gt;&gt; Epoch 86 finished 	ANN training loss 0.004481
&gt;&gt; Epoch 87 finished 	ANN training loss 0.004204
&gt;&gt; Epoch 88 finished 	ANN training loss 0.004285
&gt;&gt; Epoch 89 finished 	ANN training loss 0.004075
&gt;&gt; Epoch 90 finished 	ANN training loss 0.004067
&gt;&gt; Epoch 91 finished 	ANN training loss 0.003937
&gt;&gt; Epoch 92 finished 	ANN training loss 0.003868
&gt;&gt; Epoch 93 finished 	ANN training loss 0.003730
&gt;&gt; Epoch 94 finished 	ANN training loss 0.003618
&gt;&gt; Epoch 95 finished 	ANN training loss 0.003564
&gt;&gt; Epoch 96 finished 	ANN training loss 0.003621
&gt;&gt; Epoch 97 finished 	ANN training loss 0.003333
&gt;&gt; Epoch 98 finished 	ANN training loss 0.003281
&gt;&gt; Epoch 99 finished 	ANN training loss 0.003266
&gt;&gt; Epoch 100 finished 	ANN training loss 0.003322
&gt;&gt; Epoch 101 finished 	ANN training loss 0.003187
&gt;&gt; Epoch 102 finished 	ANN training loss 0.003153
&gt;&gt; Epoch 103 finished 	ANN training loss 0.003042
&gt;&gt; Epoch 104 finished 	ANN training loss 0.002924
&gt;&gt; Epoch 105 finished 	ANN training loss 0.003084
&gt;&gt; Epoch 106 finished 	ANN training loss 0.002815
&gt;&gt; Epoch 107 finished 	ANN training loss 0.002869
&gt;&gt; Epoch 108 finished 	ANN training loss 0.002703
&gt;&gt; Epoch 109 finished 	ANN training loss 0.002769
&gt;&gt; Epoch 110 finished 	ANN training loss 0.002646
&gt;&gt; Epoch 111 finished 	ANN training loss 0.002603
&gt;&gt; Epoch 112 finished 	ANN training loss 0.002593
&gt;&gt; Epoch 113 finished 	ANN training loss 0.002497
&gt;&gt; Epoch 114 finished 	ANN training loss 0.002500
&gt;&gt; Epoch 115 finished 	ANN training loss 0.002625
&gt;&gt; Epoch 116 finished 	ANN training loss 0.002350
&gt;&gt; Epoch 117 finished 	ANN training loss 0.002422
&gt;&gt; Epoch 118 finished 	ANN training loss 0.002280
&gt;&gt; Epoch 119 finished 	ANN training loss 0.002365
&gt;&gt; Epoch 120 finished 	ANN training loss 0.002161
&gt;&gt; Epoch 121 finished 	ANN training loss 0.002177
&gt;&gt; Epoch 122 finished 	ANN training loss 0.002173
&gt;&gt; Epoch 123 finished 	ANN training loss 0.002193
&gt;&gt; Epoch 124 finished 	ANN training loss 0.002075
&gt;&gt; Epoch 125 finished 	ANN training loss 0.002106
&gt;&gt; Epoch 126 finished 	ANN training loss 0.002071
&gt;&gt; Epoch 127 finished 	ANN training loss 0.001971
&gt;&gt; Epoch 128 finished 	ANN training loss 0.002027
&gt;&gt; Epoch 129 finished 	ANN training loss 0.001945
&gt;&gt; Epoch 130 finished 	ANN training loss 0.001889
&gt;&gt; Epoch 131 finished 	ANN training loss 0.001941
&gt;&gt; Epoch 132 finished 	ANN training loss 0.001796
&gt;&gt; Epoch 133 finished 	ANN training loss 0.001808
&gt;&gt; Epoch 134 finished 	ANN training loss 0.001759
&gt;&gt; Epoch 135 finished 	ANN training loss 0.001947
&gt;&gt; Epoch 136 finished 	ANN training loss 0.001682
&gt;&gt; Epoch 137 finished 	ANN training loss 0.001930
&gt;&gt; Epoch 138 finished 	ANN training loss 0.001909
&gt;&gt; Epoch 139 finished 	ANN training loss 0.001750
&gt;&gt; Epoch 140 finished 	ANN training loss 0.001728
&gt;&gt; Epoch 141 finished 	ANN training loss 0.001677
&gt;&gt; Epoch 142 finished 	ANN training loss 0.001666
&gt;&gt; Epoch 143 finished 	ANN training loss 0.001662
&gt;&gt; Epoch 144 finished 	ANN training loss 0.001768
&gt;&gt; Epoch 145 finished 	ANN training loss 0.001849
&gt;&gt; Epoch 146 finished 	ANN training loss 0.001542
&gt;&gt; Epoch 147 finished 	ANN training loss 0.001520
&gt;&gt; Epoch 148 finished 	ANN training loss 0.001529
&gt;&gt; Epoch 149 finished 	ANN training loss 0.001437
&gt;&gt; Epoch 150 finished 	ANN training loss 0.001457
&gt;&gt; Epoch 151 finished 	ANN training loss 0.001440
&gt;&gt; Epoch 152 finished 	ANN training loss 0.001432
&gt;&gt; Epoch 153 finished 	ANN training loss 0.001411
&gt;&gt; Epoch 154 finished 	ANN training loss 0.001426
&gt;&gt; Epoch 155 finished 	ANN training loss 0.001417
&gt;&gt; Epoch 156 finished 	ANN training loss 0.001393
&gt;&gt; Epoch 157 finished 	ANN training loss 0.001388
&gt;&gt; Epoch 158 finished 	ANN training loss 0.001304
&gt;&gt; Epoch 159 finished 	ANN training loss 0.001268
&gt;&gt; Epoch 160 finished 	ANN training loss 0.001241
&gt;&gt; Epoch 161 finished 	ANN training loss 0.001285
&gt;&gt; Epoch 162 finished 	ANN training loss 0.001192
&gt;&gt; Epoch 163 finished 	ANN training loss 0.001189
&gt;&gt; Epoch 164 finished 	ANN training loss 0.001181
&gt;&gt; Epoch 165 finished 	ANN training loss 0.001169
&gt;&gt; Epoch 166 finished 	ANN training loss 0.001175
&gt;&gt; Epoch 167 finished 	ANN training loss 0.001183
&gt;&gt; Epoch 168 finished 	ANN training loss 0.001137
&gt;&gt; Epoch 169 finished 	ANN training loss 0.001075
&gt;&gt; Epoch 170 finished 	ANN training loss 0.001086
&gt;&gt; Epoch 171 finished 	ANN training loss 0.001091
&gt;&gt; Epoch 172 finished 	ANN training loss 0.001077
&gt;&gt; Epoch 173 finished 	ANN training loss 0.001035
&gt;&gt; Epoch 174 finished 	ANN training loss 0.001144
&gt;&gt; Epoch 175 finished 	ANN training loss 0.001037
&gt;&gt; Epoch 176 finished 	ANN training loss 0.001068
&gt;&gt; Epoch 177 finished 	ANN training loss 0.001070
&gt;&gt; Epoch 178 finished 	ANN training loss 0.001022
&gt;&gt; Epoch 179 finished 	ANN training loss 0.001009
&gt;&gt; Epoch 180 finished 	ANN training loss 0.001062
&gt;&gt; Epoch 181 finished 	ANN training loss 0.000995
&gt;&gt; Epoch 182 finished 	ANN training loss 0.001005
&gt;&gt; Epoch 183 finished 	ANN training loss 0.001026
&gt;&gt; Epoch 184 finished 	ANN training loss 0.000988
&gt;&gt; Epoch 185 finished 	ANN training loss 0.001043
&gt;&gt; Epoch 186 finished 	ANN training loss 0.000989
&gt;&gt; Epoch 187 finished 	ANN training loss 0.000963
&gt;&gt; Epoch 188 finished 	ANN training loss 0.001189
&gt;&gt; Epoch 189 finished 	ANN training loss 0.001081
&gt;&gt; Epoch 190 finished 	ANN training loss 0.001003
&gt;&gt; Epoch 191 finished 	ANN training loss 0.001001
&gt;&gt; Epoch 192 finished 	ANN training loss 0.000973
&gt;&gt; Epoch 193 finished 	ANN training loss 0.000970
&gt;&gt; Epoch 194 finished 	ANN training loss 0.000913
&gt;&gt; Epoch 195 finished 	ANN training loss 0.000908
&gt;&gt; Epoch 196 finished 	ANN training loss 0.000873
&gt;&gt; Epoch 197 finished 	ANN training loss 0.000865
&gt;&gt; Epoch 198 finished 	ANN training loss 0.000858
&gt;&gt; Epoch 199 finished 	ANN training loss 0.000947
[END] Fine tuning step
Done.
Accuracy: 0.900000
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[112]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ACCURACY: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">acc1</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>ACCURACY: 0.9
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[113]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># 1-layer (2nd one tied)</span>
<span class="n">acc2</span> <span class="o">=</span> <span class="n">deep_belief_net</span><span class="p">(</span><span class="n">hidden_layers_structure</span><span class="o">=</span><span class="p">[</span><span class="mi">300</span><span class="p">],</span> 
                       <span class="n">learning_rate_rbm</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span>
                       <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                       <span class="n">n_epochs_rbm</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span>
                       <span class="n">n_iter_backprop</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
                       <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
                       <span class="n">activation_function</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span>
                       <span class="n">dropout_p</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[START] Pre-training step:
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 52.362724
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 67.402321
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 55.501465
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 55.674171
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 51.376099
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 39.820408
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 49.372471
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 42.299225
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 43.152420
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 44.879902
&gt;&gt; Epoch 11 finished 	RBM Reconstruction error 44.187225
&gt;&gt; Epoch 12 finished 	RBM Reconstruction error 38.721596
&gt;&gt; Epoch 13 finished 	RBM Reconstruction error 44.987022
&gt;&gt; Epoch 14 finished 	RBM Reconstruction error 34.595253
&gt;&gt; Epoch 15 finished 	RBM Reconstruction error 38.884151
&gt;&gt; Epoch 16 finished 	RBM Reconstruction error 45.426708
&gt;&gt; Epoch 17 finished 	RBM Reconstruction error 42.453545
&gt;&gt; Epoch 18 finished 	RBM Reconstruction error 39.000511
&gt;&gt; Epoch 19 finished 	RBM Reconstruction error 39.822796
&gt;&gt; Epoch 20 finished 	RBM Reconstruction error 44.699356
&gt;&gt; Epoch 21 finished 	RBM Reconstruction error 42.462193
&gt;&gt; Epoch 22 finished 	RBM Reconstruction error 51.749691
&gt;&gt; Epoch 23 finished 	RBM Reconstruction error 54.024845
&gt;&gt; Epoch 24 finished 	RBM Reconstruction error 36.917061
&gt;&gt; Epoch 25 finished 	RBM Reconstruction error 35.519672
&gt;&gt; Epoch 26 finished 	RBM Reconstruction error 57.290401
&gt;&gt; Epoch 27 finished 	RBM Reconstruction error 47.573944
&gt;&gt; Epoch 28 finished 	RBM Reconstruction error 46.187237
&gt;&gt; Epoch 29 finished 	RBM Reconstruction error 37.448875
&gt;&gt; Epoch 30 finished 	RBM Reconstruction error 43.712196
&gt;&gt; Epoch 31 finished 	RBM Reconstruction error 35.429165
&gt;&gt; Epoch 32 finished 	RBM Reconstruction error 59.610336
&gt;&gt; Epoch 33 finished 	RBM Reconstruction error 40.510052
&gt;&gt; Epoch 34 finished 	RBM Reconstruction error 41.287785
&gt;&gt; Epoch 35 finished 	RBM Reconstruction error 39.892479
&gt;&gt; Epoch 36 finished 	RBM Reconstruction error 44.331387
&gt;&gt; Epoch 37 finished 	RBM Reconstruction error 41.451927
&gt;&gt; Epoch 38 finished 	RBM Reconstruction error 33.561676
&gt;&gt; Epoch 39 finished 	RBM Reconstruction error 51.854393
&gt;&gt; Epoch 40 finished 	RBM Reconstruction error 48.544121
[END] Pre-training step
[START] Fine tuning step:
&gt;&gt; Epoch 0 finished 	ANN training loss 0.737389
&gt;&gt; Epoch 1 finished 	ANN training loss 0.510684
&gt;&gt; Epoch 2 finished 	ANN training loss 0.416880
&gt;&gt; Epoch 3 finished 	ANN training loss 0.363944
&gt;&gt; Epoch 4 finished 	ANN training loss 0.300254
&gt;&gt; Epoch 5 finished 	ANN training loss 0.278735
&gt;&gt; Epoch 6 finished 	ANN training loss 0.240704
&gt;&gt; Epoch 7 finished 	ANN training loss 0.215216
&gt;&gt; Epoch 8 finished 	ANN training loss 0.193875
&gt;&gt; Epoch 9 finished 	ANN training loss 0.200080
&gt;&gt; Epoch 10 finished 	ANN training loss 0.164040
&gt;&gt; Epoch 11 finished 	ANN training loss 0.151093
&gt;&gt; Epoch 12 finished 	ANN training loss 0.135506
&gt;&gt; Epoch 13 finished 	ANN training loss 0.129488
&gt;&gt; Epoch 14 finished 	ANN training loss 0.118267
&gt;&gt; Epoch 15 finished 	ANN training loss 0.108225
&gt;&gt; Epoch 16 finished 	ANN training loss 0.097833
&gt;&gt; Epoch 17 finished 	ANN training loss 0.088887
&gt;&gt; Epoch 18 finished 	ANN training loss 0.086095
&gt;&gt; Epoch 19 finished 	ANN training loss 0.076195
&gt;&gt; Epoch 20 finished 	ANN training loss 0.069642
&gt;&gt; Epoch 21 finished 	ANN training loss 0.063800
&gt;&gt; Epoch 22 finished 	ANN training loss 0.060976
&gt;&gt; Epoch 23 finished 	ANN training loss 0.055572
&gt;&gt; Epoch 24 finished 	ANN training loss 0.050424
&gt;&gt; Epoch 25 finished 	ANN training loss 0.054087
&gt;&gt; Epoch 26 finished 	ANN training loss 0.050476
&gt;&gt; Epoch 27 finished 	ANN training loss 0.042417
&gt;&gt; Epoch 28 finished 	ANN training loss 0.041815
&gt;&gt; Epoch 29 finished 	ANN training loss 0.037267
&gt;&gt; Epoch 30 finished 	ANN training loss 0.035003
&gt;&gt; Epoch 31 finished 	ANN training loss 0.035003
&gt;&gt; Epoch 32 finished 	ANN training loss 0.032139
&gt;&gt; Epoch 33 finished 	ANN training loss 0.030261
&gt;&gt; Epoch 34 finished 	ANN training loss 0.028318
&gt;&gt; Epoch 35 finished 	ANN training loss 0.028338
&gt;&gt; Epoch 36 finished 	ANN training loss 0.027152
&gt;&gt; Epoch 37 finished 	ANN training loss 0.026517
&gt;&gt; Epoch 38 finished 	ANN training loss 0.022998
&gt;&gt; Epoch 39 finished 	ANN training loss 0.022701
&gt;&gt; Epoch 40 finished 	ANN training loss 0.020563
&gt;&gt; Epoch 41 finished 	ANN training loss 0.018844
&gt;&gt; Epoch 42 finished 	ANN training loss 0.018133
&gt;&gt; Epoch 43 finished 	ANN training loss 0.018060
&gt;&gt; Epoch 44 finished 	ANN training loss 0.017219
&gt;&gt; Epoch 45 finished 	ANN training loss 0.015878
&gt;&gt; Epoch 46 finished 	ANN training loss 0.015737
&gt;&gt; Epoch 47 finished 	ANN training loss 0.014439
&gt;&gt; Epoch 48 finished 	ANN training loss 0.015131
&gt;&gt; Epoch 49 finished 	ANN training loss 0.013918
&gt;&gt; Epoch 50 finished 	ANN training loss 0.013113
&gt;&gt; Epoch 51 finished 	ANN training loss 0.013314
&gt;&gt; Epoch 52 finished 	ANN training loss 0.012216
&gt;&gt; Epoch 53 finished 	ANN training loss 0.011207
&gt;&gt; Epoch 54 finished 	ANN training loss 0.010975
&gt;&gt; Epoch 55 finished 	ANN training loss 0.010746
&gt;&gt; Epoch 56 finished 	ANN training loss 0.010390
&gt;&gt; Epoch 57 finished 	ANN training loss 0.010228
&gt;&gt; Epoch 58 finished 	ANN training loss 0.010118
&gt;&gt; Epoch 59 finished 	ANN training loss 0.009898
&gt;&gt; Epoch 60 finished 	ANN training loss 0.009475
&gt;&gt; Epoch 61 finished 	ANN training loss 0.009077
&gt;&gt; Epoch 62 finished 	ANN training loss 0.008506
&gt;&gt; Epoch 63 finished 	ANN training loss 0.008389
&gt;&gt; Epoch 64 finished 	ANN training loss 0.008302
&gt;&gt; Epoch 65 finished 	ANN training loss 0.007664
&gt;&gt; Epoch 66 finished 	ANN training loss 0.007477
&gt;&gt; Epoch 67 finished 	ANN training loss 0.007072
&gt;&gt; Epoch 68 finished 	ANN training loss 0.007408
&gt;&gt; Epoch 69 finished 	ANN training loss 0.006817
&gt;&gt; Epoch 70 finished 	ANN training loss 0.006450
&gt;&gt; Epoch 71 finished 	ANN training loss 0.006885
&gt;&gt; Epoch 72 finished 	ANN training loss 0.006061
&gt;&gt; Epoch 73 finished 	ANN training loss 0.005946
&gt;&gt; Epoch 74 finished 	ANN training loss 0.005856
&gt;&gt; Epoch 75 finished 	ANN training loss 0.005532
&gt;&gt; Epoch 76 finished 	ANN training loss 0.005609
&gt;&gt; Epoch 77 finished 	ANN training loss 0.005455
&gt;&gt; Epoch 78 finished 	ANN training loss 0.005257
&gt;&gt; Epoch 79 finished 	ANN training loss 0.004813
&gt;&gt; Epoch 80 finished 	ANN training loss 0.004732
&gt;&gt; Epoch 81 finished 	ANN training loss 0.004738
&gt;&gt; Epoch 82 finished 	ANN training loss 0.004537
&gt;&gt; Epoch 83 finished 	ANN training loss 0.005078
&gt;&gt; Epoch 84 finished 	ANN training loss 0.004463
&gt;&gt; Epoch 85 finished 	ANN training loss 0.004523
&gt;&gt; Epoch 86 finished 	ANN training loss 0.004206
&gt;&gt; Epoch 87 finished 	ANN training loss 0.004330
&gt;&gt; Epoch 88 finished 	ANN training loss 0.004132
&gt;&gt; Epoch 89 finished 	ANN training loss 0.003999
&gt;&gt; Epoch 90 finished 	ANN training loss 0.003782
&gt;&gt; Epoch 91 finished 	ANN training loss 0.003765
&gt;&gt; Epoch 92 finished 	ANN training loss 0.003818
&gt;&gt; Epoch 93 finished 	ANN training loss 0.003652
&gt;&gt; Epoch 94 finished 	ANN training loss 0.003607
&gt;&gt; Epoch 95 finished 	ANN training loss 0.003499
&gt;&gt; Epoch 96 finished 	ANN training loss 0.003454
&gt;&gt; Epoch 97 finished 	ANN training loss 0.003335
&gt;&gt; Epoch 98 finished 	ANN training loss 0.003360
&gt;&gt; Epoch 99 finished 	ANN training loss 0.003448
&gt;&gt; Epoch 100 finished 	ANN training loss 0.003380
&gt;&gt; Epoch 101 finished 	ANN training loss 0.003367
&gt;&gt; Epoch 102 finished 	ANN training loss 0.003077
&gt;&gt; Epoch 103 finished 	ANN training loss 0.003057
&gt;&gt; Epoch 104 finished 	ANN training loss 0.002935
&gt;&gt; Epoch 105 finished 	ANN training loss 0.003174
&gt;&gt; Epoch 106 finished 	ANN training loss 0.002816
&gt;&gt; Epoch 107 finished 	ANN training loss 0.002890
&gt;&gt; Epoch 108 finished 	ANN training loss 0.002659
&gt;&gt; Epoch 109 finished 	ANN training loss 0.002688
&gt;&gt; Epoch 110 finished 	ANN training loss 0.002526
&gt;&gt; Epoch 111 finished 	ANN training loss 0.002529
&gt;&gt; Epoch 112 finished 	ANN training loss 0.002553
&gt;&gt; Epoch 113 finished 	ANN training loss 0.002484
&gt;&gt; Epoch 114 finished 	ANN training loss 0.002312
&gt;&gt; Epoch 115 finished 	ANN training loss 0.002354
&gt;&gt; Epoch 116 finished 	ANN training loss 0.002415
&gt;&gt; Epoch 117 finished 	ANN training loss 0.002437
&gt;&gt; Epoch 118 finished 	ANN training loss 0.002383
&gt;&gt; Epoch 119 finished 	ANN training loss 0.002175
&gt;&gt; Epoch 120 finished 	ANN training loss 0.002120
&gt;&gt; Epoch 121 finished 	ANN training loss 0.002113
&gt;&gt; Epoch 122 finished 	ANN training loss 0.002175
&gt;&gt; Epoch 123 finished 	ANN training loss 0.002189
&gt;&gt; Epoch 124 finished 	ANN training loss 0.002079
&gt;&gt; Epoch 125 finished 	ANN training loss 0.002005
&gt;&gt; Epoch 126 finished 	ANN training loss 0.001969
&gt;&gt; Epoch 127 finished 	ANN training loss 0.002142
&gt;&gt; Epoch 128 finished 	ANN training loss 0.001984
&gt;&gt; Epoch 129 finished 	ANN training loss 0.001972
&gt;&gt; Epoch 130 finished 	ANN training loss 0.001884
&gt;&gt; Epoch 131 finished 	ANN training loss 0.001838
&gt;&gt; Epoch 132 finished 	ANN training loss 0.001891
&gt;&gt; Epoch 133 finished 	ANN training loss 0.001746
&gt;&gt; Epoch 134 finished 	ANN training loss 0.001782
&gt;&gt; Epoch 135 finished 	ANN training loss 0.001758
&gt;&gt; Epoch 136 finished 	ANN training loss 0.001703
&gt;&gt; Epoch 137 finished 	ANN training loss 0.001698
&gt;&gt; Epoch 138 finished 	ANN training loss 0.001643
&gt;&gt; Epoch 139 finished 	ANN training loss 0.001669
&gt;&gt; Epoch 140 finished 	ANN training loss 0.001719
&gt;&gt; Epoch 141 finished 	ANN training loss 0.001643
&gt;&gt; Epoch 142 finished 	ANN training loss 0.001615
&gt;&gt; Epoch 143 finished 	ANN training loss 0.001607
&gt;&gt; Epoch 144 finished 	ANN training loss 0.001587
&gt;&gt; Epoch 145 finished 	ANN training loss 0.001580
&gt;&gt; Epoch 146 finished 	ANN training loss 0.001538
&gt;&gt; Epoch 147 finished 	ANN training loss 0.001552
&gt;&gt; Epoch 148 finished 	ANN training loss 0.001547
&gt;&gt; Epoch 149 finished 	ANN training loss 0.001549
&gt;&gt; Epoch 150 finished 	ANN training loss 0.001478
&gt;&gt; Epoch 151 finished 	ANN training loss 0.001405
&gt;&gt; Epoch 152 finished 	ANN training loss 0.001398
&gt;&gt; Epoch 153 finished 	ANN training loss 0.001515
&gt;&gt; Epoch 154 finished 	ANN training loss 0.001330
&gt;&gt; Epoch 155 finished 	ANN training loss 0.001339
&gt;&gt; Epoch 156 finished 	ANN training loss 0.001325
&gt;&gt; Epoch 157 finished 	ANN training loss 0.001312
&gt;&gt; Epoch 158 finished 	ANN training loss 0.001292
&gt;&gt; Epoch 159 finished 	ANN training loss 0.001291
&gt;&gt; Epoch 160 finished 	ANN training loss 0.001314
&gt;&gt; Epoch 161 finished 	ANN training loss 0.001263
&gt;&gt; Epoch 162 finished 	ANN training loss 0.001240
&gt;&gt; Epoch 163 finished 	ANN training loss 0.001305
&gt;&gt; Epoch 164 finished 	ANN training loss 0.001269
&gt;&gt; Epoch 165 finished 	ANN training loss 0.001226
&gt;&gt; Epoch 166 finished 	ANN training loss 0.001308
&gt;&gt; Epoch 167 finished 	ANN training loss 0.001164
&gt;&gt; Epoch 168 finished 	ANN training loss 0.001201
&gt;&gt; Epoch 169 finished 	ANN training loss 0.001224
&gt;&gt; Epoch 170 finished 	ANN training loss 0.001134
&gt;&gt; Epoch 171 finished 	ANN training loss 0.001156
&gt;&gt; Epoch 172 finished 	ANN training loss 0.001219
&gt;&gt; Epoch 173 finished 	ANN training loss 0.001138
&gt;&gt; Epoch 174 finished 	ANN training loss 0.001191
&gt;&gt; Epoch 175 finished 	ANN training loss 0.001072
&gt;&gt; Epoch 176 finished 	ANN training loss 0.001021
&gt;&gt; Epoch 177 finished 	ANN training loss 0.001048
&gt;&gt; Epoch 178 finished 	ANN training loss 0.001036
&gt;&gt; Epoch 179 finished 	ANN training loss 0.001074
&gt;&gt; Epoch 180 finished 	ANN training loss 0.001007
&gt;&gt; Epoch 181 finished 	ANN training loss 0.001034
&gt;&gt; Epoch 182 finished 	ANN training loss 0.001050
&gt;&gt; Epoch 183 finished 	ANN training loss 0.000974
&gt;&gt; Epoch 184 finished 	ANN training loss 0.000935
&gt;&gt; Epoch 185 finished 	ANN training loss 0.000912
&gt;&gt; Epoch 186 finished 	ANN training loss 0.000919
&gt;&gt; Epoch 187 finished 	ANN training loss 0.000944
&gt;&gt; Epoch 188 finished 	ANN training loss 0.000946
&gt;&gt; Epoch 189 finished 	ANN training loss 0.000926
&gt;&gt; Epoch 190 finished 	ANN training loss 0.000878
&gt;&gt; Epoch 191 finished 	ANN training loss 0.000943
&gt;&gt; Epoch 192 finished 	ANN training loss 0.000918
&gt;&gt; Epoch 193 finished 	ANN training loss 0.000912
&gt;&gt; Epoch 194 finished 	ANN training loss 0.000890
&gt;&gt; Epoch 195 finished 	ANN training loss 0.000913
&gt;&gt; Epoch 196 finished 	ANN training loss 0.000890
&gt;&gt; Epoch 197 finished 	ANN training loss 0.000882
&gt;&gt; Epoch 198 finished 	ANN training loss 0.000913
&gt;&gt; Epoch 199 finished 	ANN training loss 0.000929
[END] Fine tuning step
Done.
Accuracy: 0.920000
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[114]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ACCURACY: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">acc2</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>ACCURACY: 0.92
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[115]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># 2-layer</span>
<span class="n">acc3</span> <span class="o">=</span> <span class="n">deep_belief_net</span><span class="p">(</span><span class="n">hidden_layers_structure</span><span class="o">=</span><span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">500</span><span class="p">],</span> 
                       <span class="n">learning_rate_rbm</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span>
                       <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                       <span class="n">n_epochs_rbm</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span>
                       <span class="n">n_iter_backprop</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
                       <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
                       <span class="n">activation_function</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span>
                       <span class="n">dropout_p</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[START] Pre-training step:
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 61.901642
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 60.275616
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 87.619865
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 56.597225
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 45.425381
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 46.222080
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 69.010742
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 45.242207
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 50.292126
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 57.479366
&gt;&gt; Epoch 11 finished 	RBM Reconstruction error 73.998779
&gt;&gt; Epoch 12 finished 	RBM Reconstruction error 68.098679
&gt;&gt; Epoch 13 finished 	RBM Reconstruction error 83.567871
&gt;&gt; Epoch 14 finished 	RBM Reconstruction error 75.080116
&gt;&gt; Epoch 15 finished 	RBM Reconstruction error 100.359024
&gt;&gt; Epoch 16 finished 	RBM Reconstruction error 72.527649
&gt;&gt; Epoch 17 finished 	RBM Reconstruction error 80.413300
&gt;&gt; Epoch 18 finished 	RBM Reconstruction error 85.674294
&gt;&gt; Epoch 19 finished 	RBM Reconstruction error 79.484909
&gt;&gt; Epoch 20 finished 	RBM Reconstruction error 107.319412
&gt;&gt; Epoch 21 finished 	RBM Reconstruction error 85.888161
&gt;&gt; Epoch 22 finished 	RBM Reconstruction error 84.266258
&gt;&gt; Epoch 23 finished 	RBM Reconstruction error 78.896393
&gt;&gt; Epoch 24 finished 	RBM Reconstruction error 78.359489
&gt;&gt; Epoch 25 finished 	RBM Reconstruction error 114.280861
&gt;&gt; Epoch 26 finished 	RBM Reconstruction error 104.662018
&gt;&gt; Epoch 27 finished 	RBM Reconstruction error 96.259026
&gt;&gt; Epoch 28 finished 	RBM Reconstruction error 88.241425
&gt;&gt; Epoch 29 finished 	RBM Reconstruction error 92.975906
&gt;&gt; Epoch 30 finished 	RBM Reconstruction error 99.582123
&gt;&gt; Epoch 31 finished 	RBM Reconstruction error 92.787491
&gt;&gt; Epoch 32 finished 	RBM Reconstruction error 99.423019
&gt;&gt; Epoch 33 finished 	RBM Reconstruction error 105.046349
&gt;&gt; Epoch 34 finished 	RBM Reconstruction error 104.780960
&gt;&gt; Epoch 35 finished 	RBM Reconstruction error 99.292625
&gt;&gt; Epoch 36 finished 	RBM Reconstruction error 115.861115
&gt;&gt; Epoch 37 finished 	RBM Reconstruction error 105.354256
&gt;&gt; Epoch 38 finished 	RBM Reconstruction error 112.433105
&gt;&gt; Epoch 39 finished 	RBM Reconstruction error 118.529953
&gt;&gt; Epoch 40 finished 	RBM Reconstruction error 98.022736
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 343.910980
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 387.981964
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 312.443115
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 352.085144
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 320.552429
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 405.200012
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 371.407196
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 343.438293
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 371.127533
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 397.807495
&gt;&gt; Epoch 11 finished 	RBM Reconstruction error 362.312927
&gt;&gt; Epoch 12 finished 	RBM Reconstruction error 397.225220
&gt;&gt; Epoch 13 finished 	RBM Reconstruction error 384.043701
&gt;&gt; Epoch 14 finished 	RBM Reconstruction error 467.161560
&gt;&gt; Epoch 15 finished 	RBM Reconstruction error 469.198364
&gt;&gt; Epoch 16 finished 	RBM Reconstruction error 419.519287
&gt;&gt; Epoch 17 finished 	RBM Reconstruction error 494.007996
&gt;&gt; Epoch 18 finished 	RBM Reconstruction error 452.421082
&gt;&gt; Epoch 19 finished 	RBM Reconstruction error 477.412628
&gt;&gt; Epoch 20 finished 	RBM Reconstruction error 430.911041
&gt;&gt; Epoch 21 finished 	RBM Reconstruction error 365.288452
&gt;&gt; Epoch 22 finished 	RBM Reconstruction error 557.352783
&gt;&gt; Epoch 23 finished 	RBM Reconstruction error 478.143463
&gt;&gt; Epoch 24 finished 	RBM Reconstruction error 381.100677
&gt;&gt; Epoch 25 finished 	RBM Reconstruction error 415.115631
&gt;&gt; Epoch 26 finished 	RBM Reconstruction error 400.681610
&gt;&gt; Epoch 27 finished 	RBM Reconstruction error 484.287567
&gt;&gt; Epoch 28 finished 	RBM Reconstruction error 417.944702
&gt;&gt; Epoch 29 finished 	RBM Reconstruction error 394.090271
&gt;&gt; Epoch 30 finished 	RBM Reconstruction error 499.719910
&gt;&gt; Epoch 31 finished 	RBM Reconstruction error 389.697815
&gt;&gt; Epoch 32 finished 	RBM Reconstruction error 521.718140
&gt;&gt; Epoch 33 finished 	RBM Reconstruction error 527.772644
&gt;&gt; Epoch 34 finished 	RBM Reconstruction error 402.980469
&gt;&gt; Epoch 35 finished 	RBM Reconstruction error 465.946320
&gt;&gt; Epoch 36 finished 	RBM Reconstruction error 477.205780
&gt;&gt; Epoch 37 finished 	RBM Reconstruction error 451.609131
&gt;&gt; Epoch 38 finished 	RBM Reconstruction error 404.333740
&gt;&gt; Epoch 39 finished 	RBM Reconstruction error 411.326874
&gt;&gt; Epoch 40 finished 	RBM Reconstruction error 417.262665
[END] Pre-training step
[START] Fine tuning step:
&gt;&gt; Epoch 0 finished 	ANN training loss 0.568238
&gt;&gt; Epoch 1 finished 	ANN training loss 0.408312
&gt;&gt; Epoch 2 finished 	ANN training loss 0.342579
&gt;&gt; Epoch 3 finished 	ANN training loss 0.283675
&gt;&gt; Epoch 4 finished 	ANN training loss 0.287408
&gt;&gt; Epoch 5 finished 	ANN training loss 0.211970
&gt;&gt; Epoch 6 finished 	ANN training loss 0.191535
&gt;&gt; Epoch 7 finished 	ANN training loss 0.162043
&gt;&gt; Epoch 8 finished 	ANN training loss 0.161872
&gt;&gt; Epoch 9 finished 	ANN training loss 0.120234
&gt;&gt; Epoch 10 finished 	ANN training loss 0.112354
&gt;&gt; Epoch 11 finished 	ANN training loss 0.099087
&gt;&gt; Epoch 12 finished 	ANN training loss 0.088026
&gt;&gt; Epoch 13 finished 	ANN training loss 0.088477
&gt;&gt; Epoch 14 finished 	ANN training loss 0.073664
&gt;&gt; Epoch 15 finished 	ANN training loss 0.072788
&gt;&gt; Epoch 16 finished 	ANN training loss 0.052852
&gt;&gt; Epoch 17 finished 	ANN training loss 0.055829
&gt;&gt; Epoch 18 finished 	ANN training loss 0.044760
&gt;&gt; Epoch 19 finished 	ANN training loss 0.042253
&gt;&gt; Epoch 20 finished 	ANN training loss 0.037475
&gt;&gt; Epoch 21 finished 	ANN training loss 0.034571
&gt;&gt; Epoch 22 finished 	ANN training loss 0.037231
&gt;&gt; Epoch 23 finished 	ANN training loss 0.032096
&gt;&gt; Epoch 24 finished 	ANN training loss 0.027734
&gt;&gt; Epoch 25 finished 	ANN training loss 0.027753
&gt;&gt; Epoch 26 finished 	ANN training loss 0.024580
&gt;&gt; Epoch 27 finished 	ANN training loss 0.023303
&gt;&gt; Epoch 28 finished 	ANN training loss 0.021136
&gt;&gt; Epoch 29 finished 	ANN training loss 0.015942
&gt;&gt; Epoch 30 finished 	ANN training loss 0.016998
&gt;&gt; Epoch 31 finished 	ANN training loss 0.014881
&gt;&gt; Epoch 32 finished 	ANN training loss 0.015414
&gt;&gt; Epoch 33 finished 	ANN training loss 0.012773
&gt;&gt; Epoch 34 finished 	ANN training loss 0.014762
&gt;&gt; Epoch 35 finished 	ANN training loss 0.011117
&gt;&gt; Epoch 36 finished 	ANN training loss 0.012771
&gt;&gt; Epoch 37 finished 	ANN training loss 0.011061
&gt;&gt; Epoch 38 finished 	ANN training loss 0.010126
&gt;&gt; Epoch 39 finished 	ANN training loss 0.009290
&gt;&gt; Epoch 40 finished 	ANN training loss 0.007884
&gt;&gt; Epoch 41 finished 	ANN training loss 0.008647
&gt;&gt; Epoch 42 finished 	ANN training loss 0.007533
&gt;&gt; Epoch 43 finished 	ANN training loss 0.007249
&gt;&gt; Epoch 44 finished 	ANN training loss 0.006757
&gt;&gt; Epoch 45 finished 	ANN training loss 0.006556
&gt;&gt; Epoch 46 finished 	ANN training loss 0.006021
&gt;&gt; Epoch 47 finished 	ANN training loss 0.005268
&gt;&gt; Epoch 48 finished 	ANN training loss 0.006024
&gt;&gt; Epoch 49 finished 	ANN training loss 0.005204
&gt;&gt; Epoch 50 finished 	ANN training loss 0.006755
&gt;&gt; Epoch 51 finished 	ANN training loss 0.005459
&gt;&gt; Epoch 52 finished 	ANN training loss 0.004151
&gt;&gt; Epoch 53 finished 	ANN training loss 0.004102
&gt;&gt; Epoch 54 finished 	ANN training loss 0.004380
&gt;&gt; Epoch 55 finished 	ANN training loss 0.004356
&gt;&gt; Epoch 56 finished 	ANN training loss 0.003965
&gt;&gt; Epoch 57 finished 	ANN training loss 0.003204
&gt;&gt; Epoch 58 finished 	ANN training loss 0.004867
&gt;&gt; Epoch 59 finished 	ANN training loss 0.003666
&gt;&gt; Epoch 60 finished 	ANN training loss 0.003274
&gt;&gt; Epoch 61 finished 	ANN training loss 0.003063
&gt;&gt; Epoch 62 finished 	ANN training loss 0.002788
&gt;&gt; Epoch 63 finished 	ANN training loss 0.002697
&gt;&gt; Epoch 64 finished 	ANN training loss 0.002677
&gt;&gt; Epoch 65 finished 	ANN training loss 0.002473
&gt;&gt; Epoch 66 finished 	ANN training loss 0.002594
&gt;&gt; Epoch 67 finished 	ANN training loss 0.002305
&gt;&gt; Epoch 68 finished 	ANN training loss 0.002568
&gt;&gt; Epoch 69 finished 	ANN training loss 0.002258
&gt;&gt; Epoch 70 finished 	ANN training loss 0.002216
&gt;&gt; Epoch 71 finished 	ANN training loss 0.002410
&gt;&gt; Epoch 72 finished 	ANN training loss 0.002367
&gt;&gt; Epoch 73 finished 	ANN training loss 0.001925
&gt;&gt; Epoch 74 finished 	ANN training loss 0.002563
&gt;&gt; Epoch 75 finished 	ANN training loss 0.003048
&gt;&gt; Epoch 76 finished 	ANN training loss 0.001842
&gt;&gt; Epoch 77 finished 	ANN training loss 0.001902
&gt;&gt; Epoch 78 finished 	ANN training loss 0.001958
&gt;&gt; Epoch 79 finished 	ANN training loss 0.001821
&gt;&gt; Epoch 80 finished 	ANN training loss 0.002197
&gt;&gt; Epoch 81 finished 	ANN training loss 0.002116
&gt;&gt; Epoch 82 finished 	ANN training loss 0.001709
&gt;&gt; Epoch 83 finished 	ANN training loss 0.001723
&gt;&gt; Epoch 84 finished 	ANN training loss 0.001719
&gt;&gt; Epoch 85 finished 	ANN training loss 0.001590
&gt;&gt; Epoch 86 finished 	ANN training loss 0.001469
&gt;&gt; Epoch 87 finished 	ANN training loss 0.001514
&gt;&gt; Epoch 88 finished 	ANN training loss 0.001274
&gt;&gt; Epoch 89 finished 	ANN training loss 0.001641
&gt;&gt; Epoch 90 finished 	ANN training loss 0.001597
&gt;&gt; Epoch 91 finished 	ANN training loss 0.001392
&gt;&gt; Epoch 92 finished 	ANN training loss 0.001239
&gt;&gt; Epoch 93 finished 	ANN training loss 0.001163
&gt;&gt; Epoch 94 finished 	ANN training loss 0.001130
&gt;&gt; Epoch 95 finished 	ANN training loss 0.001050
&gt;&gt; Epoch 96 finished 	ANN training loss 0.001007
&gt;&gt; Epoch 97 finished 	ANN training loss 0.000900
&gt;&gt; Epoch 98 finished 	ANN training loss 0.000936
&gt;&gt; Epoch 99 finished 	ANN training loss 0.000859
&gt;&gt; Epoch 100 finished 	ANN training loss 0.000805
&gt;&gt; Epoch 101 finished 	ANN training loss 0.000932
&gt;&gt; Epoch 102 finished 	ANN training loss 0.000821
&gt;&gt; Epoch 103 finished 	ANN training loss 0.000816
&gt;&gt; Epoch 104 finished 	ANN training loss 0.000858
&gt;&gt; Epoch 105 finished 	ANN training loss 0.000936
&gt;&gt; Epoch 106 finished 	ANN training loss 0.000810
&gt;&gt; Epoch 107 finished 	ANN training loss 0.000759
&gt;&gt; Epoch 108 finished 	ANN training loss 0.000790
&gt;&gt; Epoch 109 finished 	ANN training loss 0.000723
&gt;&gt; Epoch 110 finished 	ANN training loss 0.000656
&gt;&gt; Epoch 111 finished 	ANN training loss 0.000597
&gt;&gt; Epoch 112 finished 	ANN training loss 0.000620
&gt;&gt; Epoch 113 finished 	ANN training loss 0.000619
&gt;&gt; Epoch 114 finished 	ANN training loss 0.000569
&gt;&gt; Epoch 115 finished 	ANN training loss 0.000587
&gt;&gt; Epoch 116 finished 	ANN training loss 0.000567
&gt;&gt; Epoch 117 finished 	ANN training loss 0.000659
&gt;&gt; Epoch 118 finished 	ANN training loss 0.000684
&gt;&gt; Epoch 119 finished 	ANN training loss 0.000604
&gt;&gt; Epoch 120 finished 	ANN training loss 0.000655
&gt;&gt; Epoch 121 finished 	ANN training loss 0.000630
&gt;&gt; Epoch 122 finished 	ANN training loss 0.000547
&gt;&gt; Epoch 123 finished 	ANN training loss 0.000639
&gt;&gt; Epoch 124 finished 	ANN training loss 0.000573
&gt;&gt; Epoch 125 finished 	ANN training loss 0.000506
&gt;&gt; Epoch 126 finished 	ANN training loss 0.000500
&gt;&gt; Epoch 127 finished 	ANN training loss 0.000538
&gt;&gt; Epoch 128 finished 	ANN training loss 0.000463
&gt;&gt; Epoch 129 finished 	ANN training loss 0.000441
&gt;&gt; Epoch 130 finished 	ANN training loss 0.000470
&gt;&gt; Epoch 131 finished 	ANN training loss 0.000741
&gt;&gt; Epoch 132 finished 	ANN training loss 0.000440
&gt;&gt; Epoch 133 finished 	ANN training loss 0.000426
&gt;&gt; Epoch 134 finished 	ANN training loss 0.000410
&gt;&gt; Epoch 135 finished 	ANN training loss 0.000448
&gt;&gt; Epoch 136 finished 	ANN training loss 0.000442
&gt;&gt; Epoch 137 finished 	ANN training loss 0.000487
&gt;&gt; Epoch 138 finished 	ANN training loss 0.000508
&gt;&gt; Epoch 139 finished 	ANN training loss 0.000427
&gt;&gt; Epoch 140 finished 	ANN training loss 0.000454
&gt;&gt; Epoch 141 finished 	ANN training loss 0.000385
&gt;&gt; Epoch 142 finished 	ANN training loss 0.000360
&gt;&gt; Epoch 143 finished 	ANN training loss 0.000393
&gt;&gt; Epoch 144 finished 	ANN training loss 0.000398
&gt;&gt; Epoch 145 finished 	ANN training loss 0.000385
&gt;&gt; Epoch 146 finished 	ANN training loss 0.000384
&gt;&gt; Epoch 147 finished 	ANN training loss 0.000336
&gt;&gt; Epoch 148 finished 	ANN training loss 0.000328
&gt;&gt; Epoch 149 finished 	ANN training loss 0.000326
&gt;&gt; Epoch 150 finished 	ANN training loss 0.000377
&gt;&gt; Epoch 151 finished 	ANN training loss 0.000335
&gt;&gt; Epoch 152 finished 	ANN training loss 0.000338
&gt;&gt; Epoch 153 finished 	ANN training loss 0.000292
&gt;&gt; Epoch 154 finished 	ANN training loss 0.000377
&gt;&gt; Epoch 155 finished 	ANN training loss 0.000363
&gt;&gt; Epoch 156 finished 	ANN training loss 0.000306
&gt;&gt; Epoch 157 finished 	ANN training loss 0.000299
&gt;&gt; Epoch 158 finished 	ANN training loss 0.000316
&gt;&gt; Epoch 159 finished 	ANN training loss 0.000353
&gt;&gt; Epoch 160 finished 	ANN training loss 0.000344
&gt;&gt; Epoch 161 finished 	ANN training loss 0.000374
&gt;&gt; Epoch 162 finished 	ANN training loss 0.000384
&gt;&gt; Epoch 163 finished 	ANN training loss 0.000331
&gt;&gt; Epoch 164 finished 	ANN training loss 0.000278
&gt;&gt; Epoch 165 finished 	ANN training loss 0.000302
&gt;&gt; Epoch 166 finished 	ANN training loss 0.000273
&gt;&gt; Epoch 167 finished 	ANN training loss 0.000305
&gt;&gt; Epoch 168 finished 	ANN training loss 0.000289
&gt;&gt; Epoch 169 finished 	ANN training loss 0.000311
&gt;&gt; Epoch 170 finished 	ANN training loss 0.000329
&gt;&gt; Epoch 171 finished 	ANN training loss 0.000300
&gt;&gt; Epoch 172 finished 	ANN training loss 0.000392
&gt;&gt; Epoch 173 finished 	ANN training loss 0.000311
&gt;&gt; Epoch 174 finished 	ANN training loss 0.000458
&gt;&gt; Epoch 175 finished 	ANN training loss 0.000305
&gt;&gt; Epoch 176 finished 	ANN training loss 0.000290
&gt;&gt; Epoch 177 finished 	ANN training loss 0.000531
&gt;&gt; Epoch 178 finished 	ANN training loss 0.000354
&gt;&gt; Epoch 179 finished 	ANN training loss 0.000298
&gt;&gt; Epoch 180 finished 	ANN training loss 0.000301
&gt;&gt; Epoch 181 finished 	ANN training loss 0.000269
&gt;&gt; Epoch 182 finished 	ANN training loss 0.000252
&gt;&gt; Epoch 183 finished 	ANN training loss 0.000263
&gt;&gt; Epoch 184 finished 	ANN training loss 0.000263
&gt;&gt; Epoch 185 finished 	ANN training loss 0.000303
&gt;&gt; Epoch 186 finished 	ANN training loss 0.000354
&gt;&gt; Epoch 187 finished 	ANN training loss 0.000286
&gt;&gt; Epoch 188 finished 	ANN training loss 0.000282
&gt;&gt; Epoch 189 finished 	ANN training loss 0.000311
&gt;&gt; Epoch 190 finished 	ANN training loss 0.000273
&gt;&gt; Epoch 191 finished 	ANN training loss 0.000215
&gt;&gt; Epoch 192 finished 	ANN training loss 0.000232
&gt;&gt; Epoch 193 finished 	ANN training loss 0.000232
&gt;&gt; Epoch 194 finished 	ANN training loss 0.000214
&gt;&gt; Epoch 195 finished 	ANN training loss 0.000209
&gt;&gt; Epoch 196 finished 	ANN training loss 0.000210
&gt;&gt; Epoch 197 finished 	ANN training loss 0.000193
&gt;&gt; Epoch 198 finished 	ANN training loss 0.000199
&gt;&gt; Epoch 199 finished 	ANN training loss 0.000174
[END] Fine tuning step
Done.
Accuracy: 0.905000
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[116]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ACCURACY: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">acc3</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>ACCURACY: 0.905
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[117]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># 3-layer</span>
<span class="n">acc4</span> <span class="o">=</span> <span class="n">deep_belief_net</span><span class="p">(</span><span class="n">hidden_layers_structure</span><span class="o">=</span><span class="p">[</span><span class="mi">500</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">300</span><span class="p">],</span> 
                       <span class="n">learning_rate_rbm</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span>
                       <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                       <span class="n">n_epochs_rbm</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span>
                       <span class="n">n_iter_backprop</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
                       <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
                       <span class="n">activation_function</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span>
                       <span class="n">dropout_p</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[START] Pre-training step:
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 44.434200
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 36.059303
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 27.978998
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 43.884365
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 27.703917
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 40.638386
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 22.463474
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 20.800531
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 28.822454
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 32.591835
&gt;&gt; Epoch 11 finished 	RBM Reconstruction error 41.460468
&gt;&gt; Epoch 12 finished 	RBM Reconstruction error 29.742527
&gt;&gt; Epoch 13 finished 	RBM Reconstruction error 37.216793
&gt;&gt; Epoch 14 finished 	RBM Reconstruction error 27.116709
&gt;&gt; Epoch 15 finished 	RBM Reconstruction error 34.042049
&gt;&gt; Epoch 16 finished 	RBM Reconstruction error 17.215670
&gt;&gt; Epoch 17 finished 	RBM Reconstruction error 29.130920
&gt;&gt; Epoch 18 finished 	RBM Reconstruction error 28.987253
&gt;&gt; Epoch 19 finished 	RBM Reconstruction error 40.967419
&gt;&gt; Epoch 20 finished 	RBM Reconstruction error 19.373569
&gt;&gt; Epoch 21 finished 	RBM Reconstruction error 39.018585
&gt;&gt; Epoch 22 finished 	RBM Reconstruction error 21.660276
&gt;&gt; Epoch 23 finished 	RBM Reconstruction error 26.941730
&gt;&gt; Epoch 24 finished 	RBM Reconstruction error 25.511240
&gt;&gt; Epoch 25 finished 	RBM Reconstruction error 25.745319
&gt;&gt; Epoch 26 finished 	RBM Reconstruction error 22.503906
&gt;&gt; Epoch 27 finished 	RBM Reconstruction error 26.412804
&gt;&gt; Epoch 28 finished 	RBM Reconstruction error 22.252998
&gt;&gt; Epoch 29 finished 	RBM Reconstruction error 28.351004
&gt;&gt; Epoch 30 finished 	RBM Reconstruction error 31.632248
&gt;&gt; Epoch 31 finished 	RBM Reconstruction error 25.962896
&gt;&gt; Epoch 32 finished 	RBM Reconstruction error 17.551035
&gt;&gt; Epoch 33 finished 	RBM Reconstruction error 26.432123
&gt;&gt; Epoch 34 finished 	RBM Reconstruction error 26.573092
&gt;&gt; Epoch 35 finished 	RBM Reconstruction error 22.466236
&gt;&gt; Epoch 36 finished 	RBM Reconstruction error 25.236826
&gt;&gt; Epoch 37 finished 	RBM Reconstruction error 24.970171
&gt;&gt; Epoch 38 finished 	RBM Reconstruction error 29.371695
&gt;&gt; Epoch 39 finished 	RBM Reconstruction error 25.026304
&gt;&gt; Epoch 40 finished 	RBM Reconstruction error 46.429874
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 190.199234
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 273.150726
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 317.882355
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 187.512558
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 203.559860
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 259.353577
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 247.309570
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 204.795410
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 172.767212
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 200.218307
&gt;&gt; Epoch 11 finished 	RBM Reconstruction error 382.932251
&gt;&gt; Epoch 12 finished 	RBM Reconstruction error 298.597290
&gt;&gt; Epoch 13 finished 	RBM Reconstruction error 287.038605
&gt;&gt; Epoch 14 finished 	RBM Reconstruction error 232.804962
&gt;&gt; Epoch 15 finished 	RBM Reconstruction error 291.033691
&gt;&gt; Epoch 16 finished 	RBM Reconstruction error 272.512756
&gt;&gt; Epoch 17 finished 	RBM Reconstruction error 242.022385
&gt;&gt; Epoch 18 finished 	RBM Reconstruction error 262.276886
&gt;&gt; Epoch 19 finished 	RBM Reconstruction error 303.549805
&gt;&gt; Epoch 20 finished 	RBM Reconstruction error 284.918823
&gt;&gt; Epoch 21 finished 	RBM Reconstruction error 260.383209
&gt;&gt; Epoch 22 finished 	RBM Reconstruction error 302.546722
&gt;&gt; Epoch 23 finished 	RBM Reconstruction error 246.841385
&gt;&gt; Epoch 24 finished 	RBM Reconstruction error 273.669617
&gt;&gt; Epoch 25 finished 	RBM Reconstruction error 295.891388
&gt;&gt; Epoch 26 finished 	RBM Reconstruction error 310.213867
&gt;&gt; Epoch 27 finished 	RBM Reconstruction error 257.509644
&gt;&gt; Epoch 28 finished 	RBM Reconstruction error 270.249023
&gt;&gt; Epoch 29 finished 	RBM Reconstruction error 310.550232
&gt;&gt; Epoch 30 finished 	RBM Reconstruction error 316.861298
&gt;&gt; Epoch 31 finished 	RBM Reconstruction error 272.223724
&gt;&gt; Epoch 32 finished 	RBM Reconstruction error 297.711884
&gt;&gt; Epoch 33 finished 	RBM Reconstruction error 316.275909
&gt;&gt; Epoch 34 finished 	RBM Reconstruction error 308.092712
&gt;&gt; Epoch 35 finished 	RBM Reconstruction error 289.956726
&gt;&gt; Epoch 36 finished 	RBM Reconstruction error 301.680237
&gt;&gt; Epoch 37 finished 	RBM Reconstruction error 276.194153
&gt;&gt; Epoch 38 finished 	RBM Reconstruction error 296.185822
&gt;&gt; Epoch 39 finished 	RBM Reconstruction error 286.745422
&gt;&gt; Epoch 40 finished 	RBM Reconstruction error 273.325165
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 2320.280273
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 2860.946777
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 4900.419922
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 5983.877930
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 6004.254883
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 5797.275391
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 7914.038086
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 8993.140625
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 7948.834961
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 8781.235352
&gt;&gt; Epoch 11 finished 	RBM Reconstruction error 9260.076172
&gt;&gt; Epoch 12 finished 	RBM Reconstruction error 10654.962891
&gt;&gt; Epoch 13 finished 	RBM Reconstruction error 10094.716797
&gt;&gt; Epoch 14 finished 	RBM Reconstruction error 11494.802734
&gt;&gt; Epoch 15 finished 	RBM Reconstruction error 12828.037109
&gt;&gt; Epoch 16 finished 	RBM Reconstruction error 13806.967773
&gt;&gt; Epoch 17 finished 	RBM Reconstruction error 13699.952148
&gt;&gt; Epoch 18 finished 	RBM Reconstruction error 14856.209961
&gt;&gt; Epoch 19 finished 	RBM Reconstruction error 15005.352539
&gt;&gt; Epoch 20 finished 	RBM Reconstruction error 15271.562500
&gt;&gt; Epoch 21 finished 	RBM Reconstruction error 15914.365234
&gt;&gt; Epoch 22 finished 	RBM Reconstruction error 16437.126953
&gt;&gt; Epoch 23 finished 	RBM Reconstruction error 14874.750977
&gt;&gt; Epoch 24 finished 	RBM Reconstruction error 19135.523438
&gt;&gt; Epoch 25 finished 	RBM Reconstruction error 18322.642578
&gt;&gt; Epoch 26 finished 	RBM Reconstruction error 16946.261719
&gt;&gt; Epoch 27 finished 	RBM Reconstruction error 18924.007812
&gt;&gt; Epoch 28 finished 	RBM Reconstruction error 18296.394531
&gt;&gt; Epoch 29 finished 	RBM Reconstruction error 18818.375000
&gt;&gt; Epoch 30 finished 	RBM Reconstruction error 18824.070312
&gt;&gt; Epoch 31 finished 	RBM Reconstruction error 17882.273438
&gt;&gt; Epoch 32 finished 	RBM Reconstruction error 20025.429688
&gt;&gt; Epoch 33 finished 	RBM Reconstruction error 19703.408203
&gt;&gt; Epoch 34 finished 	RBM Reconstruction error 18131.257812
&gt;&gt; Epoch 35 finished 	RBM Reconstruction error 19559.080078
&gt;&gt; Epoch 36 finished 	RBM Reconstruction error 17912.009766
&gt;&gt; Epoch 37 finished 	RBM Reconstruction error 18638.230469
&gt;&gt; Epoch 38 finished 	RBM Reconstruction error 20198.675781
&gt;&gt; Epoch 39 finished 	RBM Reconstruction error 20073.988281
&gt;&gt; Epoch 40 finished 	RBM Reconstruction error 19722.828125
[END] Pre-training step
[START] Fine tuning step:
&gt;&gt; Epoch 0 finished 	ANN training loss 0.781871
&gt;&gt; Epoch 1 finished 	ANN training loss 0.473890
&gt;&gt; Epoch 2 finished 	ANN training loss 0.332426
&gt;&gt; Epoch 3 finished 	ANN training loss 0.272003
&gt;&gt; Epoch 4 finished 	ANN training loss 0.211460
&gt;&gt; Epoch 5 finished 	ANN training loss 0.172745
&gt;&gt; Epoch 6 finished 	ANN training loss 0.159768
&gt;&gt; Epoch 7 finished 	ANN training loss 0.114711
&gt;&gt; Epoch 8 finished 	ANN training loss 0.096943
&gt;&gt; Epoch 9 finished 	ANN training loss 0.083168
&gt;&gt; Epoch 10 finished 	ANN training loss 0.082297
&gt;&gt; Epoch 11 finished 	ANN training loss 0.064451
&gt;&gt; Epoch 12 finished 	ANN training loss 0.053686
&gt;&gt; Epoch 13 finished 	ANN training loss 0.056996
&gt;&gt; Epoch 14 finished 	ANN training loss 0.046761
&gt;&gt; Epoch 15 finished 	ANN training loss 0.035447
&gt;&gt; Epoch 16 finished 	ANN training loss 0.025796
&gt;&gt; Epoch 17 finished 	ANN training loss 0.026041
&gt;&gt; Epoch 18 finished 	ANN training loss 0.032909
&gt;&gt; Epoch 19 finished 	ANN training loss 0.025882
&gt;&gt; Epoch 20 finished 	ANN training loss 0.019785
&gt;&gt; Epoch 21 finished 	ANN training loss 0.017260
&gt;&gt; Epoch 22 finished 	ANN training loss 0.015189
&gt;&gt; Epoch 23 finished 	ANN training loss 0.011439
&gt;&gt; Epoch 24 finished 	ANN training loss 0.010435
&gt;&gt; Epoch 25 finished 	ANN training loss 0.009148
&gt;&gt; Epoch 26 finished 	ANN training loss 0.009593
&gt;&gt; Epoch 27 finished 	ANN training loss 0.007510
&gt;&gt; Epoch 28 finished 	ANN training loss 0.006532
&gt;&gt; Epoch 29 finished 	ANN training loss 0.006323
&gt;&gt; Epoch 30 finished 	ANN training loss 0.006246
&gt;&gt; Epoch 31 finished 	ANN training loss 0.005392
&gt;&gt; Epoch 32 finished 	ANN training loss 0.007369
&gt;&gt; Epoch 33 finished 	ANN training loss 0.004952
&gt;&gt; Epoch 34 finished 	ANN training loss 0.004019
&gt;&gt; Epoch 35 finished 	ANN training loss 0.003908
&gt;&gt; Epoch 36 finished 	ANN training loss 0.002986
&gt;&gt; Epoch 37 finished 	ANN training loss 0.002723
&gt;&gt; Epoch 38 finished 	ANN training loss 0.002689
&gt;&gt; Epoch 39 finished 	ANN training loss 0.003579
&gt;&gt; Epoch 40 finished 	ANN training loss 0.003775
&gt;&gt; Epoch 41 finished 	ANN training loss 0.002453
&gt;&gt; Epoch 42 finished 	ANN training loss 0.002384
&gt;&gt; Epoch 43 finished 	ANN training loss 0.001628
&gt;&gt; Epoch 44 finished 	ANN training loss 0.001892
&gt;&gt; Epoch 45 finished 	ANN training loss 0.002331
&gt;&gt; Epoch 46 finished 	ANN training loss 0.001743
&gt;&gt; Epoch 47 finished 	ANN training loss 0.001330
&gt;&gt; Epoch 48 finished 	ANN training loss 0.001611
&gt;&gt; Epoch 49 finished 	ANN training loss 0.001564
&gt;&gt; Epoch 50 finished 	ANN training loss 0.001503
&gt;&gt; Epoch 51 finished 	ANN training loss 0.001696
&gt;&gt; Epoch 52 finished 	ANN training loss 0.001428
&gt;&gt; Epoch 53 finished 	ANN training loss 0.001068
&gt;&gt; Epoch 54 finished 	ANN training loss 0.001225
&gt;&gt; Epoch 55 finished 	ANN training loss 0.001250
&gt;&gt; Epoch 56 finished 	ANN training loss 0.000799
&gt;&gt; Epoch 57 finished 	ANN training loss 0.001030
&gt;&gt; Epoch 58 finished 	ANN training loss 0.000706
&gt;&gt; Epoch 59 finished 	ANN training loss 0.001596
&gt;&gt; Epoch 60 finished 	ANN training loss 0.000798
&gt;&gt; Epoch 61 finished 	ANN training loss 0.000904
&gt;&gt; Epoch 62 finished 	ANN training loss 0.000607
&gt;&gt; Epoch 63 finished 	ANN training loss 0.000501
&gt;&gt; Epoch 64 finished 	ANN training loss 0.000571
&gt;&gt; Epoch 65 finished 	ANN training loss 0.000558
&gt;&gt; Epoch 66 finished 	ANN training loss 0.000803
&gt;&gt; Epoch 67 finished 	ANN training loss 0.000618
&gt;&gt; Epoch 68 finished 	ANN training loss 0.000715
&gt;&gt; Epoch 69 finished 	ANN training loss 0.000610
&gt;&gt; Epoch 70 finished 	ANN training loss 0.000558
&gt;&gt; Epoch 71 finished 	ANN training loss 0.004027
&gt;&gt; Epoch 72 finished 	ANN training loss 0.000548
&gt;&gt; Epoch 73 finished 	ANN training loss 0.000539
&gt;&gt; Epoch 74 finished 	ANN training loss 0.000364
&gt;&gt; Epoch 75 finished 	ANN training loss 0.000523
&gt;&gt; Epoch 76 finished 	ANN training loss 0.000355
&gt;&gt; Epoch 77 finished 	ANN training loss 0.000377
&gt;&gt; Epoch 78 finished 	ANN training loss 0.000585
&gt;&gt; Epoch 79 finished 	ANN training loss 0.000468
&gt;&gt; Epoch 80 finished 	ANN training loss 0.000459
&gt;&gt; Epoch 81 finished 	ANN training loss 0.000390
&gt;&gt; Epoch 82 finished 	ANN training loss 0.000309
&gt;&gt; Epoch 83 finished 	ANN training loss 0.000235
&gt;&gt; Epoch 84 finished 	ANN training loss 0.000185
&gt;&gt; Epoch 85 finished 	ANN training loss 0.000270
&gt;&gt; Epoch 86 finished 	ANN training loss 0.000414
&gt;&gt; Epoch 87 finished 	ANN training loss 0.000283
&gt;&gt; Epoch 88 finished 	ANN training loss 0.000221
&gt;&gt; Epoch 89 finished 	ANN training loss 0.000968
&gt;&gt; Epoch 90 finished 	ANN training loss 0.000152
&gt;&gt; Epoch 91 finished 	ANN training loss 0.000132
&gt;&gt; Epoch 92 finished 	ANN training loss 0.000157
&gt;&gt; Epoch 93 finished 	ANN training loss 0.000203
&gt;&gt; Epoch 94 finished 	ANN training loss 0.000193
&gt;&gt; Epoch 95 finished 	ANN training loss 0.000223
&gt;&gt; Epoch 96 finished 	ANN training loss 0.000148
&gt;&gt; Epoch 97 finished 	ANN training loss 0.000173
&gt;&gt; Epoch 98 finished 	ANN training loss 0.000326
&gt;&gt; Epoch 99 finished 	ANN training loss 0.000149
&gt;&gt; Epoch 100 finished 	ANN training loss 0.000151
&gt;&gt; Epoch 101 finished 	ANN training loss 0.000207
&gt;&gt; Epoch 102 finished 	ANN training loss 0.000122
&gt;&gt; Epoch 103 finished 	ANN training loss 0.000200
&gt;&gt; Epoch 104 finished 	ANN training loss 0.000382
&gt;&gt; Epoch 105 finished 	ANN training loss 0.000209
&gt;&gt; Epoch 106 finished 	ANN training loss 0.000221
&gt;&gt; Epoch 107 finished 	ANN training loss 0.000218
&gt;&gt; Epoch 108 finished 	ANN training loss 0.000260
&gt;&gt; Epoch 109 finished 	ANN training loss 0.000125
&gt;&gt; Epoch 110 finished 	ANN training loss 0.000343
&gt;&gt; Epoch 111 finished 	ANN training loss 0.000623
&gt;&gt; Epoch 112 finished 	ANN training loss 0.000095
&gt;&gt; Epoch 113 finished 	ANN training loss 0.000105
&gt;&gt; Epoch 114 finished 	ANN training loss 0.000071
&gt;&gt; Epoch 115 finished 	ANN training loss 0.000062
&gt;&gt; Epoch 116 finished 	ANN training loss 0.000169
&gt;&gt; Epoch 117 finished 	ANN training loss 0.000145
&gt;&gt; Epoch 118 finished 	ANN training loss 0.000192
&gt;&gt; Epoch 119 finished 	ANN training loss 0.000117
&gt;&gt; Epoch 120 finished 	ANN training loss 0.000171
&gt;&gt; Epoch 121 finished 	ANN training loss 0.000191
&gt;&gt; Epoch 122 finished 	ANN training loss 0.000118
&gt;&gt; Epoch 123 finished 	ANN training loss 0.000094
&gt;&gt; Epoch 124 finished 	ANN training loss 0.000074
&gt;&gt; Epoch 125 finished 	ANN training loss 0.000070
&gt;&gt; Epoch 126 finished 	ANN training loss 0.000067
&gt;&gt; Epoch 127 finished 	ANN training loss 0.000064
&gt;&gt; Epoch 128 finished 	ANN training loss 0.000055
&gt;&gt; Epoch 129 finished 	ANN training loss 0.000061
&gt;&gt; Epoch 130 finished 	ANN training loss 0.000671
&gt;&gt; Epoch 131 finished 	ANN training loss 0.000077
&gt;&gt; Epoch 132 finished 	ANN training loss 0.000054
&gt;&gt; Epoch 133 finished 	ANN training loss 0.000041
&gt;&gt; Epoch 134 finished 	ANN training loss 0.000039
&gt;&gt; Epoch 135 finished 	ANN training loss 0.000039
&gt;&gt; Epoch 136 finished 	ANN training loss 0.000039
&gt;&gt; Epoch 137 finished 	ANN training loss 0.000039
&gt;&gt; Epoch 138 finished 	ANN training loss 0.000046
&gt;&gt; Epoch 139 finished 	ANN training loss 0.000062
&gt;&gt; Epoch 140 finished 	ANN training loss 0.000059
&gt;&gt; Epoch 141 finished 	ANN training loss 0.000047
&gt;&gt; Epoch 142 finished 	ANN training loss 0.000041
&gt;&gt; Epoch 143 finished 	ANN training loss 0.000043
&gt;&gt; Epoch 144 finished 	ANN training loss 0.000034
&gt;&gt; Epoch 145 finished 	ANN training loss 0.000048
&gt;&gt; Epoch 146 finished 	ANN training loss 0.000047
&gt;&gt; Epoch 147 finished 	ANN training loss 0.000040
&gt;&gt; Epoch 148 finished 	ANN training loss 0.000031
&gt;&gt; Epoch 149 finished 	ANN training loss 0.000038
&gt;&gt; Epoch 150 finished 	ANN training loss 0.000040
&gt;&gt; Epoch 151 finished 	ANN training loss 0.000058
&gt;&gt; Epoch 152 finished 	ANN training loss 0.000082
&gt;&gt; Epoch 153 finished 	ANN training loss 0.000039
&gt;&gt; Epoch 154 finished 	ANN training loss 0.000050
&gt;&gt; Epoch 155 finished 	ANN training loss 0.000029
&gt;&gt; Epoch 156 finished 	ANN training loss 0.000069
&gt;&gt; Epoch 157 finished 	ANN training loss 0.000037
&gt;&gt; Epoch 158 finished 	ANN training loss 0.000034
&gt;&gt; Epoch 159 finished 	ANN training loss 0.000037
&gt;&gt; Epoch 160 finished 	ANN training loss 0.000052
&gt;&gt; Epoch 161 finished 	ANN training loss 0.000047
&gt;&gt; Epoch 162 finished 	ANN training loss 0.000044
&gt;&gt; Epoch 163 finished 	ANN training loss 0.000115
&gt;&gt; Epoch 164 finished 	ANN training loss 0.000112
&gt;&gt; Epoch 165 finished 	ANN training loss 0.000042
&gt;&gt; Epoch 166 finished 	ANN training loss 0.000036
&gt;&gt; Epoch 167 finished 	ANN training loss 0.000032
&gt;&gt; Epoch 168 finished 	ANN training loss 0.000038
&gt;&gt; Epoch 169 finished 	ANN training loss 0.000037
&gt;&gt; Epoch 170 finished 	ANN training loss 0.000031
&gt;&gt; Epoch 171 finished 	ANN training loss 0.000026
&gt;&gt; Epoch 172 finished 	ANN training loss 0.000032
&gt;&gt; Epoch 173 finished 	ANN training loss 0.000026
&gt;&gt; Epoch 174 finished 	ANN training loss 0.000021
&gt;&gt; Epoch 175 finished 	ANN training loss 0.000030
&gt;&gt; Epoch 176 finished 	ANN training loss 0.000026
&gt;&gt; Epoch 177 finished 	ANN training loss 0.000041
&gt;&gt; Epoch 178 finished 	ANN training loss 0.000048
&gt;&gt; Epoch 179 finished 	ANN training loss 0.000026
&gt;&gt; Epoch 180 finished 	ANN training loss 0.000031
&gt;&gt; Epoch 181 finished 	ANN training loss 0.000032
&gt;&gt; Epoch 182 finished 	ANN training loss 0.000028
&gt;&gt; Epoch 183 finished 	ANN training loss 0.000043
&gt;&gt; Epoch 184 finished 	ANN training loss 0.000038
&gt;&gt; Epoch 185 finished 	ANN training loss 0.000035
&gt;&gt; Epoch 186 finished 	ANN training loss 0.000058
&gt;&gt; Epoch 187 finished 	ANN training loss 0.000059
&gt;&gt; Epoch 188 finished 	ANN training loss 0.000053
&gt;&gt; Epoch 189 finished 	ANN training loss 0.000039
&gt;&gt; Epoch 190 finished 	ANN training loss 0.000032
&gt;&gt; Epoch 191 finished 	ANN training loss 0.000029
&gt;&gt; Epoch 192 finished 	ANN training loss 0.000027
&gt;&gt; Epoch 193 finished 	ANN training loss 0.000025
&gt;&gt; Epoch 194 finished 	ANN training loss 0.000025
&gt;&gt; Epoch 195 finished 	ANN training loss 0.000021
&gt;&gt; Epoch 196 finished 	ANN training loss 0.000019
&gt;&gt; Epoch 197 finished 	ANN training loss 0.000022
&gt;&gt; Epoch 198 finished 	ANN training loss 0.000020
&gt;&gt; Epoch 199 finished 	ANN training loss 0.000027
[END] Fine tuning step
Done.
Accuracy: 0.905000
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[118]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ACCURACY: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">acc4</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>ACCURACY: 0.905
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[63]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Create bar graph</span>
<span class="n">plt</span><span class="o">.</span><span class="n">clf</span><span class="p">()</span>
<span class="n">objects</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;[200]&#39;</span><span class="p">,</span> <span class="s1">&#39;[300]&#39;</span><span class="p">,</span> <span class="s1">&#39;[100, 500]&#39;</span><span class="p">,</span> <span class="s1">&#39;[500, 200, 300]&#39;</span><span class="p">)</span>
<span class="n">y_pos</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">objects</span><span class="p">))</span>
<span class="n">performance</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.92</span><span class="p">,</span> <span class="mf">0.905</span><span class="p">,</span> <span class="mf">0.905</span><span class="p">]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">y_pos</span><span class="p">,</span> <span class="n">performance</span><span class="p">,</span> <span class="n">align</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">y_pos</span><span class="p">,</span> <span class="n">objects</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Settings&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Accuracy&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Benchmark 1 Settings&#39;</span><span class="p">)</span>
 
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">clf</span><span class="p">()</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>




<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAYUAAAE7CAYAAAAy451NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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"
>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Benchmark-2:">Benchmark 2:<a class="anchor-link" href="#Benchmark-2:">&#182;</a></h3><p>Take the best performing one from benchmark 1 and test with the sigmoidal activation function and various dropouts.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[19]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Sigmoidal activation, dropout = 0</span>
<span class="n">acc1</span> <span class="o">=</span> <span class="n">deep_belief_net</span><span class="p">(</span><span class="n">hidden_layers_structure</span><span class="o">=</span><span class="p">[</span><span class="mi">300</span><span class="p">],</span> 
                       <span class="n">learning_rate_rbm</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span>
                       <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                       <span class="n">n_epochs_rbm</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span>
                       <span class="n">n_iter_backprop</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
                       <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
                       <span class="n">activation_function</span><span class="o">=</span><span class="s1">&#39;sigmoid&#39;</span><span class="p">,</span>
                       <span class="n">dropout_p</span><span class="o">=</span><span class="mf">0.</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[START] Pre-training step:
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 44.978886
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 35.064987
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 29.802525
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 26.875952
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 24.820986
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 23.147341
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 21.914331
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 20.743584
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 19.739283
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 18.733677
&gt;&gt; Epoch 11 finished 	RBM Reconstruction error 18.087568
&gt;&gt; Epoch 12 finished 	RBM Reconstruction error 17.153889
&gt;&gt; Epoch 13 finished 	RBM Reconstruction error 16.515764
&gt;&gt; Epoch 14 finished 	RBM Reconstruction error 15.871891
&gt;&gt; Epoch 15 finished 	RBM Reconstruction error 15.334622
&gt;&gt; Epoch 16 finished 	RBM Reconstruction error 14.836199
&gt;&gt; Epoch 17 finished 	RBM Reconstruction error 14.399968
&gt;&gt; Epoch 18 finished 	RBM Reconstruction error 13.953169
&gt;&gt; Epoch 19 finished 	RBM Reconstruction error 13.539706
&gt;&gt; Epoch 20 finished 	RBM Reconstruction error 13.197330
&gt;&gt; Epoch 21 finished 	RBM Reconstruction error 12.844109
&gt;&gt; Epoch 22 finished 	RBM Reconstruction error 12.559888
&gt;&gt; Epoch 23 finished 	RBM Reconstruction error 12.235383
&gt;&gt; Epoch 24 finished 	RBM Reconstruction error 11.954041
&gt;&gt; Epoch 25 finished 	RBM Reconstruction error 11.678492
&gt;&gt; Epoch 26 finished 	RBM Reconstruction error 11.447932
&gt;&gt; Epoch 27 finished 	RBM Reconstruction error 11.173759
&gt;&gt; Epoch 28 finished 	RBM Reconstruction error 10.916043
&gt;&gt; Epoch 29 finished 	RBM Reconstruction error 10.677363
&gt;&gt; Epoch 30 finished 	RBM Reconstruction error 10.474979
&gt;&gt; Epoch 31 finished 	RBM Reconstruction error 10.296218
&gt;&gt; Epoch 32 finished 	RBM Reconstruction error 10.132094
&gt;&gt; Epoch 33 finished 	RBM Reconstruction error 9.909482
&gt;&gt; Epoch 34 finished 	RBM Reconstruction error 9.733149
&gt;&gt; Epoch 35 finished 	RBM Reconstruction error 9.567850
&gt;&gt; Epoch 36 finished 	RBM Reconstruction error 9.448261
&gt;&gt; Epoch 37 finished 	RBM Reconstruction error 9.236425
&gt;&gt; Epoch 38 finished 	RBM Reconstruction error 9.135662
&gt;&gt; Epoch 39 finished 	RBM Reconstruction error 8.978279
&gt;&gt; Epoch 40 finished 	RBM Reconstruction error 8.830650
[END] Pre-training step
[START] Fine tuning step:
&gt;&gt; Epoch 0 finished 	ANN training loss 1.059366
&gt;&gt; Epoch 1 finished 	ANN training loss 0.720585
&gt;&gt; Epoch 2 finished 	ANN training loss 0.576323
&gt;&gt; Epoch 3 finished 	ANN training loss 0.508379
&gt;&gt; Epoch 4 finished 	ANN training loss 0.452819
&gt;&gt; Epoch 5 finished 	ANN training loss 0.419483
&gt;&gt; Epoch 6 finished 	ANN training loss 0.389207
&gt;&gt; Epoch 7 finished 	ANN training loss 0.368384
&gt;&gt; Epoch 8 finished 	ANN training loss 0.353782
&gt;&gt; Epoch 9 finished 	ANN training loss 0.328215
&gt;&gt; Epoch 10 finished 	ANN training loss 0.317579
&gt;&gt; Epoch 11 finished 	ANN training loss 0.299478
&gt;&gt; Epoch 12 finished 	ANN training loss 0.286153
&gt;&gt; Epoch 13 finished 	ANN training loss 0.277577
&gt;&gt; Epoch 14 finished 	ANN training loss 0.269549
&gt;&gt; Epoch 15 finished 	ANN training loss 0.258548
&gt;&gt; Epoch 16 finished 	ANN training loss 0.252736
&gt;&gt; Epoch 17 finished 	ANN training loss 0.240714
&gt;&gt; Epoch 18 finished 	ANN training loss 0.235240
&gt;&gt; Epoch 19 finished 	ANN training loss 0.228007
&gt;&gt; Epoch 20 finished 	ANN training loss 0.221333
&gt;&gt; Epoch 21 finished 	ANN training loss 0.215079
&gt;&gt; Epoch 22 finished 	ANN training loss 0.208948
&gt;&gt; Epoch 23 finished 	ANN training loss 0.204645
&gt;&gt; Epoch 24 finished 	ANN training loss 0.195886
&gt;&gt; Epoch 25 finished 	ANN training loss 0.191143
&gt;&gt; Epoch 26 finished 	ANN training loss 0.186023
&gt;&gt; Epoch 27 finished 	ANN training loss 0.180871
&gt;&gt; Epoch 28 finished 	ANN training loss 0.177058
&gt;&gt; Epoch 29 finished 	ANN training loss 0.173154
&gt;&gt; Epoch 30 finished 	ANN training loss 0.168348
&gt;&gt; Epoch 31 finished 	ANN training loss 0.163330
&gt;&gt; Epoch 32 finished 	ANN training loss 0.159497
&gt;&gt; Epoch 33 finished 	ANN training loss 0.156670
&gt;&gt; Epoch 34 finished 	ANN training loss 0.152804
&gt;&gt; Epoch 35 finished 	ANN training loss 0.149355
&gt;&gt; Epoch 36 finished 	ANN training loss 0.144886
&gt;&gt; Epoch 37 finished 	ANN training loss 0.141691
&gt;&gt; Epoch 38 finished 	ANN training loss 0.138142
&gt;&gt; Epoch 39 finished 	ANN training loss 0.135929
&gt;&gt; Epoch 40 finished 	ANN training loss 0.132970
&gt;&gt; Epoch 41 finished 	ANN training loss 0.130139
&gt;&gt; Epoch 42 finished 	ANN training loss 0.126875
&gt;&gt; Epoch 43 finished 	ANN training loss 0.124062
&gt;&gt; Epoch 44 finished 	ANN training loss 0.121516
&gt;&gt; Epoch 45 finished 	ANN training loss 0.119083
&gt;&gt; Epoch 46 finished 	ANN training loss 0.115736
&gt;&gt; Epoch 47 finished 	ANN training loss 0.113316
&gt;&gt; Epoch 48 finished 	ANN training loss 0.111461
&gt;&gt; Epoch 49 finished 	ANN training loss 0.108518
&gt;&gt; Epoch 50 finished 	ANN training loss 0.107530
&gt;&gt; Epoch 51 finished 	ANN training loss 0.104256
&gt;&gt; Epoch 52 finished 	ANN training loss 0.101806
&gt;&gt; Epoch 53 finished 	ANN training loss 0.100084
&gt;&gt; Epoch 54 finished 	ANN training loss 0.097725
&gt;&gt; Epoch 55 finished 	ANN training loss 0.096299
&gt;&gt; Epoch 56 finished 	ANN training loss 0.095102
&gt;&gt; Epoch 57 finished 	ANN training loss 0.091962
&gt;&gt; Epoch 58 finished 	ANN training loss 0.090533
&gt;&gt; Epoch 59 finished 	ANN training loss 0.088415
&gt;&gt; Epoch 60 finished 	ANN training loss 0.087197
&gt;&gt; Epoch 61 finished 	ANN training loss 0.085696
&gt;&gt; Epoch 62 finished 	ANN training loss 0.083831
&gt;&gt; Epoch 63 finished 	ANN training loss 0.082041
&gt;&gt; Epoch 64 finished 	ANN training loss 0.080171
&gt;&gt; Epoch 65 finished 	ANN training loss 0.078968
&gt;&gt; Epoch 66 finished 	ANN training loss 0.077286
&gt;&gt; Epoch 67 finished 	ANN training loss 0.076095
&gt;&gt; Epoch 68 finished 	ANN training loss 0.074650
&gt;&gt; Epoch 69 finished 	ANN training loss 0.073128
&gt;&gt; Epoch 70 finished 	ANN training loss 0.071851
&gt;&gt; Epoch 71 finished 	ANN training loss 0.070724
&gt;&gt; Epoch 72 finished 	ANN training loss 0.069507
&gt;&gt; Epoch 73 finished 	ANN training loss 0.068230
&gt;&gt; Epoch 74 finished 	ANN training loss 0.067374
&gt;&gt; Epoch 75 finished 	ANN training loss 0.065934
&gt;&gt; Epoch 76 finished 	ANN training loss 0.064883
&gt;&gt; Epoch 77 finished 	ANN training loss 0.063910
&gt;&gt; Epoch 78 finished 	ANN training loss 0.062483
&gt;&gt; Epoch 79 finished 	ANN training loss 0.061581
&gt;&gt; Epoch 80 finished 	ANN training loss 0.060599
&gt;&gt; Epoch 81 finished 	ANN training loss 0.059575
&gt;&gt; Epoch 82 finished 	ANN training loss 0.058504
&gt;&gt; Epoch 83 finished 	ANN training loss 0.057707
&gt;&gt; Epoch 84 finished 	ANN training loss 0.056563
&gt;&gt; Epoch 85 finished 	ANN training loss 0.055735
&gt;&gt; Epoch 86 finished 	ANN training loss 0.054942
&gt;&gt; Epoch 87 finished 	ANN training loss 0.054162
&gt;&gt; Epoch 88 finished 	ANN training loss 0.053100
&gt;&gt; Epoch 89 finished 	ANN training loss 0.052387
&gt;&gt; Epoch 90 finished 	ANN training loss 0.051655
&gt;&gt; Epoch 91 finished 	ANN training loss 0.051044
&gt;&gt; Epoch 92 finished 	ANN training loss 0.050097
&gt;&gt; Epoch 93 finished 	ANN training loss 0.049394
&gt;&gt; Epoch 94 finished 	ANN training loss 0.048670
&gt;&gt; Epoch 95 finished 	ANN training loss 0.048230
&gt;&gt; Epoch 96 finished 	ANN training loss 0.047345
&gt;&gt; Epoch 97 finished 	ANN training loss 0.046534
&gt;&gt; Epoch 98 finished 	ANN training loss 0.046075
&gt;&gt; Epoch 99 finished 	ANN training loss 0.045411
&gt;&gt; Epoch 100 finished 	ANN training loss 0.044641
&gt;&gt; Epoch 101 finished 	ANN training loss 0.043997
&gt;&gt; Epoch 102 finished 	ANN training loss 0.043410
&gt;&gt; Epoch 103 finished 	ANN training loss 0.042910
&gt;&gt; Epoch 104 finished 	ANN training loss 0.042227
&gt;&gt; Epoch 105 finished 	ANN training loss 0.041661
&gt;&gt; Epoch 106 finished 	ANN training loss 0.041168
&gt;&gt; Epoch 107 finished 	ANN training loss 0.040615
&gt;&gt; Epoch 108 finished 	ANN training loss 0.040181
&gt;&gt; Epoch 109 finished 	ANN training loss 0.039569
&gt;&gt; Epoch 110 finished 	ANN training loss 0.039080
&gt;&gt; Epoch 111 finished 	ANN training loss 0.038646
&gt;&gt; Epoch 112 finished 	ANN training loss 0.038152
&gt;&gt; Epoch 113 finished 	ANN training loss 0.037687
&gt;&gt; Epoch 114 finished 	ANN training loss 0.037301
&gt;&gt; Epoch 115 finished 	ANN training loss 0.036786
&gt;&gt; Epoch 116 finished 	ANN training loss 0.036402
&gt;&gt; Epoch 117 finished 	ANN training loss 0.035859
&gt;&gt; Epoch 118 finished 	ANN training loss 0.035426
&gt;&gt; Epoch 119 finished 	ANN training loss 0.034989
&gt;&gt; Epoch 120 finished 	ANN training loss 0.034605
&gt;&gt; Epoch 121 finished 	ANN training loss 0.034182
&gt;&gt; Epoch 122 finished 	ANN training loss 0.033898
&gt;&gt; Epoch 123 finished 	ANN training loss 0.033460
&gt;&gt; Epoch 124 finished 	ANN training loss 0.033117
&gt;&gt; Epoch 125 finished 	ANN training loss 0.032694
&gt;&gt; Epoch 126 finished 	ANN training loss 0.032297
&gt;&gt; Epoch 127 finished 	ANN training loss 0.031986
&gt;&gt; Epoch 128 finished 	ANN training loss 0.031597
&gt;&gt; Epoch 129 finished 	ANN training loss 0.031262
&gt;&gt; Epoch 130 finished 	ANN training loss 0.030999
&gt;&gt; Epoch 131 finished 	ANN training loss 0.030615
&gt;&gt; Epoch 132 finished 	ANN training loss 0.030283
&gt;&gt; Epoch 133 finished 	ANN training loss 0.029961
&gt;&gt; Epoch 134 finished 	ANN training loss 0.029701
&gt;&gt; Epoch 135 finished 	ANN training loss 0.029337
&gt;&gt; Epoch 136 finished 	ANN training loss 0.029031
&gt;&gt; Epoch 137 finished 	ANN training loss 0.028730
&gt;&gt; Epoch 138 finished 	ANN training loss 0.028474
&gt;&gt; Epoch 139 finished 	ANN training loss 0.028186
&gt;&gt; Epoch 140 finished 	ANN training loss 0.027884
&gt;&gt; Epoch 141 finished 	ANN training loss 0.027608
&gt;&gt; Epoch 142 finished 	ANN training loss 0.027324
&gt;&gt; Epoch 143 finished 	ANN training loss 0.027062
&gt;&gt; Epoch 144 finished 	ANN training loss 0.026830
&gt;&gt; Epoch 145 finished 	ANN training loss 0.026540
&gt;&gt; Epoch 146 finished 	ANN training loss 0.026304
&gt;&gt; Epoch 147 finished 	ANN training loss 0.026049
&gt;&gt; Epoch 148 finished 	ANN training loss 0.025802
&gt;&gt; Epoch 149 finished 	ANN training loss 0.025571
&gt;&gt; Epoch 150 finished 	ANN training loss 0.025335
&gt;&gt; Epoch 151 finished 	ANN training loss 0.025099
&gt;&gt; Epoch 152 finished 	ANN training loss 0.024860
&gt;&gt; Epoch 153 finished 	ANN training loss 0.024638
&gt;&gt; Epoch 154 finished 	ANN training loss 0.024423
&gt;&gt; Epoch 155 finished 	ANN training loss 0.024196
&gt;&gt; Epoch 156 finished 	ANN training loss 0.023975
&gt;&gt; Epoch 157 finished 	ANN training loss 0.023751
&gt;&gt; Epoch 158 finished 	ANN training loss 0.023558
&gt;&gt; Epoch 159 finished 	ANN training loss 0.023334
&gt;&gt; Epoch 160 finished 	ANN training loss 0.023134
&gt;&gt; Epoch 161 finished 	ANN training loss 0.022939
&gt;&gt; Epoch 162 finished 	ANN training loss 0.022745
&gt;&gt; Epoch 163 finished 	ANN training loss 0.022560
&gt;&gt; Epoch 164 finished 	ANN training loss 0.022366
&gt;&gt; Epoch 165 finished 	ANN training loss 0.022168
&gt;&gt; Epoch 166 finished 	ANN training loss 0.021991
&gt;&gt; Epoch 167 finished 	ANN training loss 0.021808
&gt;&gt; Epoch 168 finished 	ANN training loss 0.021625
&gt;&gt; Epoch 169 finished 	ANN training loss 0.021464
&gt;&gt; Epoch 170 finished 	ANN training loss 0.021280
&gt;&gt; Epoch 171 finished 	ANN training loss 0.021098
&gt;&gt; Epoch 172 finished 	ANN training loss 0.020937
&gt;&gt; Epoch 173 finished 	ANN training loss 0.020760
&gt;&gt; Epoch 174 finished 	ANN training loss 0.020600
&gt;&gt; Epoch 175 finished 	ANN training loss 0.020432
&gt;&gt; Epoch 176 finished 	ANN training loss 0.020273
&gt;&gt; Epoch 177 finished 	ANN training loss 0.020119
&gt;&gt; Epoch 178 finished 	ANN training loss 0.019959
&gt;&gt; Epoch 179 finished 	ANN training loss 0.019806
&gt;&gt; Epoch 180 finished 	ANN training loss 0.019655
&gt;&gt; Epoch 181 finished 	ANN training loss 0.019507
&gt;&gt; Epoch 182 finished 	ANN training loss 0.019356
&gt;&gt; Epoch 183 finished 	ANN training loss 0.019211
&gt;&gt; Epoch 184 finished 	ANN training loss 0.019066
&gt;&gt; Epoch 185 finished 	ANN training loss 0.018923
&gt;&gt; Epoch 186 finished 	ANN training loss 0.018785
&gt;&gt; Epoch 187 finished 	ANN training loss 0.018649
&gt;&gt; Epoch 188 finished 	ANN training loss 0.018511
&gt;&gt; Epoch 189 finished 	ANN training loss 0.018385
&gt;&gt; Epoch 190 finished 	ANN training loss 0.018245
&gt;&gt; Epoch 191 finished 	ANN training loss 0.018110
&gt;&gt; Epoch 192 finished 	ANN training loss 0.017983
&gt;&gt; Epoch 193 finished 	ANN training loss 0.017857
&gt;&gt; Epoch 194 finished 	ANN training loss 0.017730
&gt;&gt; Epoch 195 finished 	ANN training loss 0.017604
&gt;&gt; Epoch 196 finished 	ANN training loss 0.017482
&gt;&gt; Epoch 197 finished 	ANN training loss 0.017361
&gt;&gt; Epoch 198 finished 	ANN training loss 0.017240
&gt;&gt; Epoch 199 finished 	ANN training loss 0.017121
[END] Fine tuning step
Done.
Accuracy: 0.885000
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[20]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ACCURACY: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">acc1</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>ACCURACY: 0.885
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[21]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Sigmoidal activation, dropout = 0.1</span>
<span class="n">acc2</span> <span class="o">=</span> <span class="n">deep_belief_net</span><span class="p">(</span><span class="n">hidden_layers_structure</span><span class="o">=</span><span class="p">[</span><span class="mi">300</span><span class="p">],</span> 
                       <span class="n">learning_rate_rbm</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span>
                       <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                       <span class="n">n_epochs_rbm</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span>
                       <span class="n">n_iter_backprop</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
                       <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
                       <span class="n">activation_function</span><span class="o">=</span><span class="s1">&#39;sigmoid&#39;</span><span class="p">,</span>
                       <span class="n">dropout_p</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[START] Pre-training step:
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 44.260117
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 35.192554
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 29.961706
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 26.800102
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 24.817398
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 23.394810
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 22.047796
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 20.963203
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 19.795736
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 18.846254
&gt;&gt; Epoch 11 finished 	RBM Reconstruction error 17.976402
&gt;&gt; Epoch 12 finished 	RBM Reconstruction error 17.134426
&gt;&gt; Epoch 13 finished 	RBM Reconstruction error 16.498646
&gt;&gt; Epoch 14 finished 	RBM Reconstruction error 15.851318
&gt;&gt; Epoch 15 finished 	RBM Reconstruction error 15.397351
&gt;&gt; Epoch 16 finished 	RBM Reconstruction error 14.870476
&gt;&gt; Epoch 17 finished 	RBM Reconstruction error 14.406140
&gt;&gt; Epoch 18 finished 	RBM Reconstruction error 13.980465
&gt;&gt; Epoch 19 finished 	RBM Reconstruction error 13.518435
&gt;&gt; Epoch 20 finished 	RBM Reconstruction error 13.194644
&gt;&gt; Epoch 21 finished 	RBM Reconstruction error 12.848903
&gt;&gt; Epoch 22 finished 	RBM Reconstruction error 12.519985
&gt;&gt; Epoch 23 finished 	RBM Reconstruction error 12.247717
&gt;&gt; Epoch 24 finished 	RBM Reconstruction error 11.962986
&gt;&gt; Epoch 25 finished 	RBM Reconstruction error 11.697039
&gt;&gt; Epoch 26 finished 	RBM Reconstruction error 11.470684
&gt;&gt; Epoch 27 finished 	RBM Reconstruction error 11.205069
&gt;&gt; Epoch 28 finished 	RBM Reconstruction error 10.994288
&gt;&gt; Epoch 29 finished 	RBM Reconstruction error 10.739779
&gt;&gt; Epoch 30 finished 	RBM Reconstruction error 10.553040
&gt;&gt; Epoch 31 finished 	RBM Reconstruction error 10.393013
&gt;&gt; Epoch 32 finished 	RBM Reconstruction error 10.134296
&gt;&gt; Epoch 33 finished 	RBM Reconstruction error 9.982868
&gt;&gt; Epoch 34 finished 	RBM Reconstruction error 9.832887
&gt;&gt; Epoch 35 finished 	RBM Reconstruction error 9.677349
&gt;&gt; Epoch 36 finished 	RBM Reconstruction error 9.445460
&gt;&gt; Epoch 37 finished 	RBM Reconstruction error 9.336734
&gt;&gt; Epoch 38 finished 	RBM Reconstruction error 9.187797
&gt;&gt; Epoch 39 finished 	RBM Reconstruction error 9.029409
&gt;&gt; Epoch 40 finished 	RBM Reconstruction error 8.905331
[END] Pre-training step
[START] Fine tuning step:
&gt;&gt; Epoch 0 finished 	ANN training loss 1.007746
&gt;&gt; Epoch 1 finished 	ANN training loss 0.713660
&gt;&gt; Epoch 2 finished 	ANN training loss 0.589967
&gt;&gt; Epoch 3 finished 	ANN training loss 0.507092
&gt;&gt; Epoch 4 finished 	ANN training loss 0.456064
&gt;&gt; Epoch 5 finished 	ANN training loss 0.423914
&gt;&gt; Epoch 6 finished 	ANN training loss 0.399210
&gt;&gt; Epoch 7 finished 	ANN training loss 0.371243
&gt;&gt; Epoch 8 finished 	ANN training loss 0.354643
&gt;&gt; Epoch 9 finished 	ANN training loss 0.335047
&gt;&gt; Epoch 10 finished 	ANN training loss 0.316869
&gt;&gt; Epoch 11 finished 	ANN training loss 0.303064
&gt;&gt; Epoch 12 finished 	ANN training loss 0.294619
&gt;&gt; Epoch 13 finished 	ANN training loss 0.287201
&gt;&gt; Epoch 14 finished 	ANN training loss 0.270468
&gt;&gt; Epoch 15 finished 	ANN training loss 0.261257
&gt;&gt; Epoch 16 finished 	ANN training loss 0.252111
&gt;&gt; Epoch 17 finished 	ANN training loss 0.254067
&gt;&gt; Epoch 18 finished 	ANN training loss 0.239964
&gt;&gt; Epoch 19 finished 	ANN training loss 0.233298
&gt;&gt; Epoch 20 finished 	ANN training loss 0.224330
&gt;&gt; Epoch 21 finished 	ANN training loss 0.218712
&gt;&gt; Epoch 22 finished 	ANN training loss 0.212346
&gt;&gt; Epoch 23 finished 	ANN training loss 0.205572
&gt;&gt; Epoch 24 finished 	ANN training loss 0.201879
&gt;&gt; Epoch 25 finished 	ANN training loss 0.195010
&gt;&gt; Epoch 26 finished 	ANN training loss 0.190675
&gt;&gt; Epoch 27 finished 	ANN training loss 0.185076
&gt;&gt; Epoch 28 finished 	ANN training loss 0.180749
&gt;&gt; Epoch 29 finished 	ANN training loss 0.175511
&gt;&gt; Epoch 30 finished 	ANN training loss 0.170807
&gt;&gt; Epoch 31 finished 	ANN training loss 0.167809
&gt;&gt; Epoch 32 finished 	ANN training loss 0.163036
&gt;&gt; Epoch 33 finished 	ANN training loss 0.159895
&gt;&gt; Epoch 34 finished 	ANN training loss 0.156336
&gt;&gt; Epoch 35 finished 	ANN training loss 0.152482
&gt;&gt; Epoch 36 finished 	ANN training loss 0.149658
&gt;&gt; Epoch 37 finished 	ANN training loss 0.146000
&gt;&gt; Epoch 38 finished 	ANN training loss 0.141716
&gt;&gt; Epoch 39 finished 	ANN training loss 0.141648
&gt;&gt; Epoch 40 finished 	ANN training loss 0.135041
&gt;&gt; Epoch 41 finished 	ANN training loss 0.131795
&gt;&gt; Epoch 42 finished 	ANN training loss 0.129641
&gt;&gt; Epoch 43 finished 	ANN training loss 0.127404
&gt;&gt; Epoch 44 finished 	ANN training loss 0.125876
&gt;&gt; Epoch 45 finished 	ANN training loss 0.121129
&gt;&gt; Epoch 46 finished 	ANN training loss 0.119839
&gt;&gt; Epoch 47 finished 	ANN training loss 0.117801
&gt;&gt; Epoch 48 finished 	ANN training loss 0.115641
&gt;&gt; Epoch 49 finished 	ANN training loss 0.111761
&gt;&gt; Epoch 50 finished 	ANN training loss 0.109673
&gt;&gt; Epoch 51 finished 	ANN training loss 0.106887
&gt;&gt; Epoch 52 finished 	ANN training loss 0.104325
&gt;&gt; Epoch 53 finished 	ANN training loss 0.103820
&gt;&gt; Epoch 54 finished 	ANN training loss 0.102717
&gt;&gt; Epoch 55 finished 	ANN training loss 0.097918
&gt;&gt; Epoch 56 finished 	ANN training loss 0.096818
&gt;&gt; Epoch 57 finished 	ANN training loss 0.094471
&gt;&gt; Epoch 58 finished 	ANN training loss 0.092703
&gt;&gt; Epoch 59 finished 	ANN training loss 0.091341
&gt;&gt; Epoch 60 finished 	ANN training loss 0.088968
&gt;&gt; Epoch 61 finished 	ANN training loss 0.086384
&gt;&gt; Epoch 62 finished 	ANN training loss 0.085044
&gt;&gt; Epoch 63 finished 	ANN training loss 0.084166
&gt;&gt; Epoch 64 finished 	ANN training loss 0.082316
&gt;&gt; Epoch 65 finished 	ANN training loss 0.080008
&gt;&gt; Epoch 66 finished 	ANN training loss 0.078601
&gt;&gt; Epoch 67 finished 	ANN training loss 0.077741
&gt;&gt; Epoch 68 finished 	ANN training loss 0.075995
&gt;&gt; Epoch 69 finished 	ANN training loss 0.073865
&gt;&gt; Epoch 70 finished 	ANN training loss 0.072436
&gt;&gt; Epoch 71 finished 	ANN training loss 0.071137
&gt;&gt; Epoch 72 finished 	ANN training loss 0.070434
&gt;&gt; Epoch 73 finished 	ANN training loss 0.069817
&gt;&gt; Epoch 74 finished 	ANN training loss 0.067969
&gt;&gt; Epoch 75 finished 	ANN training loss 0.066689
&gt;&gt; Epoch 76 finished 	ANN training loss 0.065018
&gt;&gt; Epoch 77 finished 	ANN training loss 0.063822
&gt;&gt; Epoch 78 finished 	ANN training loss 0.062932
&gt;&gt; Epoch 79 finished 	ANN training loss 0.061710
&gt;&gt; Epoch 80 finished 	ANN training loss 0.060961
&gt;&gt; Epoch 81 finished 	ANN training loss 0.059906
&gt;&gt; Epoch 82 finished 	ANN training loss 0.058800
&gt;&gt; Epoch 83 finished 	ANN training loss 0.057310
&gt;&gt; Epoch 84 finished 	ANN training loss 0.056255
&gt;&gt; Epoch 85 finished 	ANN training loss 0.058016
&gt;&gt; Epoch 86 finished 	ANN training loss 0.054740
&gt;&gt; Epoch 87 finished 	ANN training loss 0.053552
&gt;&gt; Epoch 88 finished 	ANN training loss 0.052823
&gt;&gt; Epoch 89 finished 	ANN training loss 0.051639
&gt;&gt; Epoch 90 finished 	ANN training loss 0.051049
&gt;&gt; Epoch 91 finished 	ANN training loss 0.050649
&gt;&gt; Epoch 92 finished 	ANN training loss 0.049994
&gt;&gt; Epoch 93 finished 	ANN training loss 0.048658
&gt;&gt; Epoch 94 finished 	ANN training loss 0.047739
&gt;&gt; Epoch 95 finished 	ANN training loss 0.047055
&gt;&gt; Epoch 96 finished 	ANN training loss 0.046979
&gt;&gt; Epoch 97 finished 	ANN training loss 0.045918
&gt;&gt; Epoch 98 finished 	ANN training loss 0.044987
&gt;&gt; Epoch 99 finished 	ANN training loss 0.045042
&gt;&gt; Epoch 100 finished 	ANN training loss 0.043815
&gt;&gt; Epoch 101 finished 	ANN training loss 0.043272
&gt;&gt; Epoch 102 finished 	ANN training loss 0.042332
&gt;&gt; Epoch 103 finished 	ANN training loss 0.041709
&gt;&gt; Epoch 104 finished 	ANN training loss 0.041419
&gt;&gt; Epoch 105 finished 	ANN training loss 0.040349
&gt;&gt; Epoch 106 finished 	ANN training loss 0.040287
&gt;&gt; Epoch 107 finished 	ANN training loss 0.039639
&gt;&gt; Epoch 108 finished 	ANN training loss 0.038552
&gt;&gt; Epoch 109 finished 	ANN training loss 0.038260
&gt;&gt; Epoch 110 finished 	ANN training loss 0.037713
&gt;&gt; Epoch 111 finished 	ANN training loss 0.037437
&gt;&gt; Epoch 112 finished 	ANN training loss 0.036814
&gt;&gt; Epoch 113 finished 	ANN training loss 0.036073
&gt;&gt; Epoch 114 finished 	ANN training loss 0.035448
&gt;&gt; Epoch 115 finished 	ANN training loss 0.035394
&gt;&gt; Epoch 116 finished 	ANN training loss 0.034451
&gt;&gt; Epoch 117 finished 	ANN training loss 0.033980
&gt;&gt; Epoch 118 finished 	ANN training loss 0.033906
&gt;&gt; Epoch 119 finished 	ANN training loss 0.033660
&gt;&gt; Epoch 120 finished 	ANN training loss 0.032951
&gt;&gt; Epoch 121 finished 	ANN training loss 0.032556
&gt;&gt; Epoch 122 finished 	ANN training loss 0.032018
&gt;&gt; Epoch 123 finished 	ANN training loss 0.031477
&gt;&gt; Epoch 124 finished 	ANN training loss 0.030894
&gt;&gt; Epoch 125 finished 	ANN training loss 0.030610
&gt;&gt; Epoch 126 finished 	ANN training loss 0.030331
&gt;&gt; Epoch 127 finished 	ANN training loss 0.029928
&gt;&gt; Epoch 128 finished 	ANN training loss 0.029626
&gt;&gt; Epoch 129 finished 	ANN training loss 0.029394
&gt;&gt; Epoch 130 finished 	ANN training loss 0.028863
&gt;&gt; Epoch 131 finished 	ANN training loss 0.028396
&gt;&gt; Epoch 132 finished 	ANN training loss 0.028059
&gt;&gt; Epoch 133 finished 	ANN training loss 0.027879
&gt;&gt; Epoch 134 finished 	ANN training loss 0.027217
&gt;&gt; Epoch 135 finished 	ANN training loss 0.026838
&gt;&gt; Epoch 136 finished 	ANN training loss 0.026872
&gt;&gt; Epoch 137 finished 	ANN training loss 0.026302
&gt;&gt; Epoch 138 finished 	ANN training loss 0.025856
&gt;&gt; Epoch 139 finished 	ANN training loss 0.025802
&gt;&gt; Epoch 140 finished 	ANN training loss 0.025997
&gt;&gt; Epoch 141 finished 	ANN training loss 0.025253
&gt;&gt; Epoch 142 finished 	ANN training loss 0.025097
&gt;&gt; Epoch 143 finished 	ANN training loss 0.024676
&gt;&gt; Epoch 144 finished 	ANN training loss 0.024377
&gt;&gt; Epoch 145 finished 	ANN training loss 0.023999
&gt;&gt; Epoch 146 finished 	ANN training loss 0.023892
&gt;&gt; Epoch 147 finished 	ANN training loss 0.023500
&gt;&gt; Epoch 148 finished 	ANN training loss 0.023084
&gt;&gt; Epoch 149 finished 	ANN training loss 0.022898
&gt;&gt; Epoch 150 finished 	ANN training loss 0.022741
&gt;&gt; Epoch 151 finished 	ANN training loss 0.022486
&gt;&gt; Epoch 152 finished 	ANN training loss 0.022017
&gt;&gt; Epoch 153 finished 	ANN training loss 0.021829
&gt;&gt; Epoch 154 finished 	ANN training loss 0.021505
&gt;&gt; Epoch 155 finished 	ANN training loss 0.021365
&gt;&gt; Epoch 156 finished 	ANN training loss 0.021389
&gt;&gt; Epoch 157 finished 	ANN training loss 0.021049
&gt;&gt; Epoch 158 finished 	ANN training loss 0.020777
&gt;&gt; Epoch 159 finished 	ANN training loss 0.020614
&gt;&gt; Epoch 160 finished 	ANN training loss 0.020214
&gt;&gt; Epoch 161 finished 	ANN training loss 0.020221
&gt;&gt; Epoch 162 finished 	ANN training loss 0.019769
&gt;&gt; Epoch 163 finished 	ANN training loss 0.019639
&gt;&gt; Epoch 164 finished 	ANN training loss 0.019366
&gt;&gt; Epoch 165 finished 	ANN training loss 0.019223
&gt;&gt; Epoch 166 finished 	ANN training loss 0.019122
&gt;&gt; Epoch 167 finished 	ANN training loss 0.018791
&gt;&gt; Epoch 168 finished 	ANN training loss 0.018652
&gt;&gt; Epoch 169 finished 	ANN training loss 0.018470
&gt;&gt; Epoch 170 finished 	ANN training loss 0.018272
&gt;&gt; Epoch 171 finished 	ANN training loss 0.018165
&gt;&gt; Epoch 172 finished 	ANN training loss 0.017990
&gt;&gt; Epoch 173 finished 	ANN training loss 0.018009
&gt;&gt; Epoch 174 finished 	ANN training loss 0.017530
&gt;&gt; Epoch 175 finished 	ANN training loss 0.017341
&gt;&gt; Epoch 176 finished 	ANN training loss 0.017174
&gt;&gt; Epoch 177 finished 	ANN training loss 0.017167
&gt;&gt; Epoch 178 finished 	ANN training loss 0.016956
&gt;&gt; Epoch 179 finished 	ANN training loss 0.016857
&gt;&gt; Epoch 180 finished 	ANN training loss 0.016559
&gt;&gt; Epoch 181 finished 	ANN training loss 0.016506
&gt;&gt; Epoch 182 finished 	ANN training loss 0.016211
&gt;&gt; Epoch 183 finished 	ANN training loss 0.016135
&gt;&gt; Epoch 184 finished 	ANN training loss 0.015914
&gt;&gt; Epoch 185 finished 	ANN training loss 0.015859
&gt;&gt; Epoch 186 finished 	ANN training loss 0.015627
&gt;&gt; Epoch 187 finished 	ANN training loss 0.015765
&gt;&gt; Epoch 188 finished 	ANN training loss 0.015330
&gt;&gt; Epoch 189 finished 	ANN training loss 0.015262
&gt;&gt; Epoch 190 finished 	ANN training loss 0.015225
&gt;&gt; Epoch 191 finished 	ANN training loss 0.015007
&gt;&gt; Epoch 192 finished 	ANN training loss 0.014917
&gt;&gt; Epoch 193 finished 	ANN training loss 0.014645
&gt;&gt; Epoch 194 finished 	ANN training loss 0.014746
&gt;&gt; Epoch 195 finished 	ANN training loss 0.014429
&gt;&gt; Epoch 196 finished 	ANN training loss 0.014312
&gt;&gt; Epoch 197 finished 	ANN training loss 0.014340
&gt;&gt; Epoch 198 finished 	ANN training loss 0.014098
&gt;&gt; Epoch 199 finished 	ANN training loss 0.014554
[END] Fine tuning step
Done.
Accuracy: 0.920000
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[22]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ACCURACY: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">acc2</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>ACCURACY: 0.92
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[15]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Sigmoidal activation, dropout = 0.2</span>
<span class="n">acc3</span> <span class="o">=</span> <span class="n">deep_belief_net</span><span class="p">(</span><span class="n">hidden_layers_structure</span><span class="o">=</span><span class="p">[</span><span class="mi">300</span><span class="p">],</span> 
                       <span class="n">learning_rate_rbm</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span>
                       <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                       <span class="n">n_epochs_rbm</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span>
                       <span class="n">n_iter_backprop</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
                       <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
                       <span class="n">activation_function</span><span class="o">=</span><span class="s1">&#39;sigmoid&#39;</span><span class="p">,</span>
                       <span class="n">dropout_p</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[START] Pre-training step:
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 43.777512
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 34.819828
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 29.929005
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 26.783167
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 24.906950
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 23.168297
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 21.832165
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 20.717415
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 19.738537
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 18.720924
&gt;&gt; Epoch 11 finished 	RBM Reconstruction error 17.846592
&gt;&gt; Epoch 12 finished 	RBM Reconstruction error 17.117878
&gt;&gt; Epoch 13 finished 	RBM Reconstruction error 16.533508
&gt;&gt; Epoch 14 finished 	RBM Reconstruction error 15.922471
&gt;&gt; Epoch 15 finished 	RBM Reconstruction error 15.373255
&gt;&gt; Epoch 16 finished 	RBM Reconstruction error 14.934807
&gt;&gt; Epoch 17 finished 	RBM Reconstruction error 14.433249
&gt;&gt; Epoch 18 finished 	RBM Reconstruction error 13.970982
&gt;&gt; Epoch 19 finished 	RBM Reconstruction error 13.633357
&gt;&gt; Epoch 20 finished 	RBM Reconstruction error 13.290371
&gt;&gt; Epoch 21 finished 	RBM Reconstruction error 12.880840
&gt;&gt; Epoch 22 finished 	RBM Reconstruction error 12.614532
&gt;&gt; Epoch 23 finished 	RBM Reconstruction error 12.266057
&gt;&gt; Epoch 24 finished 	RBM Reconstruction error 11.948356
&gt;&gt; Epoch 25 finished 	RBM Reconstruction error 11.696796
&gt;&gt; Epoch 26 finished 	RBM Reconstruction error 11.445100
&gt;&gt; Epoch 27 finished 	RBM Reconstruction error 11.176467
&gt;&gt; Epoch 28 finished 	RBM Reconstruction error 11.006009
&gt;&gt; Epoch 29 finished 	RBM Reconstruction error 10.782668
&gt;&gt; Epoch 30 finished 	RBM Reconstruction error 10.539347
&gt;&gt; Epoch 31 finished 	RBM Reconstruction error 10.344507
&gt;&gt; Epoch 32 finished 	RBM Reconstruction error 10.168985
&gt;&gt; Epoch 33 finished 	RBM Reconstruction error 9.972208
&gt;&gt; Epoch 34 finished 	RBM Reconstruction error 9.822224
&gt;&gt; Epoch 35 finished 	RBM Reconstruction error 9.626230
&gt;&gt; Epoch 36 finished 	RBM Reconstruction error 9.475251
&gt;&gt; Epoch 37 finished 	RBM Reconstruction error 9.329857
&gt;&gt; Epoch 38 finished 	RBM Reconstruction error 9.136558
&gt;&gt; Epoch 39 finished 	RBM Reconstruction error 9.002902
&gt;&gt; Epoch 40 finished 	RBM Reconstruction error 8.850791
[END] Pre-training step
[START] Fine tuning step:
&gt;&gt; Epoch 0 finished 	ANN training loss 1.065697
&gt;&gt; Epoch 1 finished 	ANN training loss 0.717583
&gt;&gt; Epoch 2 finished 	ANN training loss 0.600794
&gt;&gt; Epoch 3 finished 	ANN training loss 0.527482
&gt;&gt; Epoch 4 finished 	ANN training loss 0.467975
&gt;&gt; Epoch 5 finished 	ANN training loss 0.450443
&gt;&gt; Epoch 6 finished 	ANN training loss 0.405933
&gt;&gt; Epoch 7 finished 	ANN training loss 0.383285
&gt;&gt; Epoch 8 finished 	ANN training loss 0.361676
&gt;&gt; Epoch 9 finished 	ANN training loss 0.348030
&gt;&gt; Epoch 10 finished 	ANN training loss 0.329410
&gt;&gt; Epoch 11 finished 	ANN training loss 0.318654
&gt;&gt; Epoch 12 finished 	ANN training loss 0.301345
&gt;&gt; Epoch 13 finished 	ANN training loss 0.296071
&gt;&gt; Epoch 14 finished 	ANN training loss 0.281912
&gt;&gt; Epoch 15 finished 	ANN training loss 0.277744
&gt;&gt; Epoch 16 finished 	ANN training loss 0.262597
&gt;&gt; Epoch 17 finished 	ANN training loss 0.260922
&gt;&gt; Epoch 18 finished 	ANN training loss 0.247801
&gt;&gt; Epoch 19 finished 	ANN training loss 0.241431
&gt;&gt; Epoch 20 finished 	ANN training loss 0.232367
&gt;&gt; Epoch 21 finished 	ANN training loss 0.229119
&gt;&gt; Epoch 22 finished 	ANN training loss 0.224992
&gt;&gt; Epoch 23 finished 	ANN training loss 0.221587
&gt;&gt; Epoch 24 finished 	ANN training loss 0.210380
&gt;&gt; Epoch 25 finished 	ANN training loss 0.206023
&gt;&gt; Epoch 26 finished 	ANN training loss 0.201842
&gt;&gt; Epoch 27 finished 	ANN training loss 0.197195
&gt;&gt; Epoch 28 finished 	ANN training loss 0.193826
&gt;&gt; Epoch 29 finished 	ANN training loss 0.191098
&gt;&gt; Epoch 30 finished 	ANN training loss 0.180707
&gt;&gt; Epoch 31 finished 	ANN training loss 0.178848
&gt;&gt; Epoch 32 finished 	ANN training loss 0.175316
&gt;&gt; Epoch 33 finished 	ANN training loss 0.169990
&gt;&gt; Epoch 34 finished 	ANN training loss 0.168140
&gt;&gt; Epoch 35 finished 	ANN training loss 0.162141
&gt;&gt; Epoch 36 finished 	ANN training loss 0.158436
&gt;&gt; Epoch 37 finished 	ANN training loss 0.159946
&gt;&gt; Epoch 38 finished 	ANN training loss 0.152144
&gt;&gt; Epoch 39 finished 	ANN training loss 0.151135
&gt;&gt; Epoch 40 finished 	ANN training loss 0.146563
&gt;&gt; Epoch 41 finished 	ANN training loss 0.143737
&gt;&gt; Epoch 42 finished 	ANN training loss 0.140931
&gt;&gt; Epoch 43 finished 	ANN training loss 0.136631
&gt;&gt; Epoch 44 finished 	ANN training loss 0.133715
&gt;&gt; Epoch 45 finished 	ANN training loss 0.132241
&gt;&gt; Epoch 46 finished 	ANN training loss 0.129652
&gt;&gt; Epoch 47 finished 	ANN training loss 0.127489
&gt;&gt; Epoch 48 finished 	ANN training loss 0.125692
&gt;&gt; Epoch 49 finished 	ANN training loss 0.120991
&gt;&gt; Epoch 50 finished 	ANN training loss 0.119548
&gt;&gt; Epoch 51 finished 	ANN training loss 0.117144
&gt;&gt; Epoch 52 finished 	ANN training loss 0.115995
&gt;&gt; Epoch 53 finished 	ANN training loss 0.116354
&gt;&gt; Epoch 54 finished 	ANN training loss 0.109835
&gt;&gt; Epoch 55 finished 	ANN training loss 0.109501
&gt;&gt; Epoch 56 finished 	ANN training loss 0.104453
&gt;&gt; Epoch 57 finished 	ANN training loss 0.102877
&gt;&gt; Epoch 58 finished 	ANN training loss 0.101962
&gt;&gt; Epoch 59 finished 	ANN training loss 0.098809
&gt;&gt; Epoch 60 finished 	ANN training loss 0.098519
&gt;&gt; Epoch 61 finished 	ANN training loss 0.094914
&gt;&gt; Epoch 62 finished 	ANN training loss 0.093527
&gt;&gt; Epoch 63 finished 	ANN training loss 0.091179
&gt;&gt; Epoch 64 finished 	ANN training loss 0.089851
&gt;&gt; Epoch 65 finished 	ANN training loss 0.088348
&gt;&gt; Epoch 66 finished 	ANN training loss 0.086305
&gt;&gt; Epoch 67 finished 	ANN training loss 0.085820
&gt;&gt; Epoch 68 finished 	ANN training loss 0.083887
&gt;&gt; Epoch 69 finished 	ANN training loss 0.083223
&gt;&gt; Epoch 70 finished 	ANN training loss 0.081161
&gt;&gt; Epoch 71 finished 	ANN training loss 0.080457
&gt;&gt; Epoch 72 finished 	ANN training loss 0.080812
&gt;&gt; Epoch 73 finished 	ANN training loss 0.077779
&gt;&gt; Epoch 74 finished 	ANN training loss 0.076037
&gt;&gt; Epoch 75 finished 	ANN training loss 0.073930
&gt;&gt; Epoch 76 finished 	ANN training loss 0.073253
&gt;&gt; Epoch 77 finished 	ANN training loss 0.071949
&gt;&gt; Epoch 78 finished 	ANN training loss 0.071994
&gt;&gt; Epoch 79 finished 	ANN training loss 0.071813
&gt;&gt; Epoch 80 finished 	ANN training loss 0.068681
&gt;&gt; Epoch 81 finished 	ANN training loss 0.066972
&gt;&gt; Epoch 82 finished 	ANN training loss 0.066640
&gt;&gt; Epoch 83 finished 	ANN training loss 0.065313
&gt;&gt; Epoch 84 finished 	ANN training loss 0.064598
&gt;&gt; Epoch 85 finished 	ANN training loss 0.062437
&gt;&gt; Epoch 86 finished 	ANN training loss 0.062538
&gt;&gt; Epoch 87 finished 	ANN training loss 0.061102
&gt;&gt; Epoch 88 finished 	ANN training loss 0.059184
&gt;&gt; Epoch 89 finished 	ANN training loss 0.058145
&gt;&gt; Epoch 90 finished 	ANN training loss 0.058763
&gt;&gt; Epoch 91 finished 	ANN training loss 0.056060
&gt;&gt; Epoch 92 finished 	ANN training loss 0.055468
&gt;&gt; Epoch 93 finished 	ANN training loss 0.054332
&gt;&gt; Epoch 94 finished 	ANN training loss 0.054812
&gt;&gt; Epoch 95 finished 	ANN training loss 0.052975
&gt;&gt; Epoch 96 finished 	ANN training loss 0.051502
&gt;&gt; Epoch 97 finished 	ANN training loss 0.050517
&gt;&gt; Epoch 98 finished 	ANN training loss 0.049684
&gt;&gt; Epoch 99 finished 	ANN training loss 0.049138
&gt;&gt; Epoch 100 finished 	ANN training loss 0.049206
&gt;&gt; Epoch 101 finished 	ANN training loss 0.047021
&gt;&gt; Epoch 102 finished 	ANN training loss 0.046450
&gt;&gt; Epoch 103 finished 	ANN training loss 0.045885
&gt;&gt; Epoch 104 finished 	ANN training loss 0.045520
&gt;&gt; Epoch 105 finished 	ANN training loss 0.044633
&gt;&gt; Epoch 106 finished 	ANN training loss 0.044794
&gt;&gt; Epoch 107 finished 	ANN training loss 0.043565
&gt;&gt; Epoch 108 finished 	ANN training loss 0.042984
&gt;&gt; Epoch 109 finished 	ANN training loss 0.043377
&gt;&gt; Epoch 110 finished 	ANN training loss 0.041503
&gt;&gt; Epoch 111 finished 	ANN training loss 0.041402
&gt;&gt; Epoch 112 finished 	ANN training loss 0.040439
&gt;&gt; Epoch 113 finished 	ANN training loss 0.040754
&gt;&gt; Epoch 114 finished 	ANN training loss 0.039418
&gt;&gt; Epoch 115 finished 	ANN training loss 0.038984
&gt;&gt; Epoch 116 finished 	ANN training loss 0.038889
&gt;&gt; Epoch 117 finished 	ANN training loss 0.038442
&gt;&gt; Epoch 118 finished 	ANN training loss 0.037643
&gt;&gt; Epoch 119 finished 	ANN training loss 0.036852
&gt;&gt; Epoch 120 finished 	ANN training loss 0.036258
&gt;&gt; Epoch 121 finished 	ANN training loss 0.035302
&gt;&gt; Epoch 122 finished 	ANN training loss 0.034379
&gt;&gt; Epoch 123 finished 	ANN training loss 0.034103
&gt;&gt; Epoch 124 finished 	ANN training loss 0.034058
&gt;&gt; Epoch 125 finished 	ANN training loss 0.033410
&gt;&gt; Epoch 126 finished 	ANN training loss 0.033458
&gt;&gt; Epoch 127 finished 	ANN training loss 0.033981
&gt;&gt; Epoch 128 finished 	ANN training loss 0.032542
&gt;&gt; Epoch 129 finished 	ANN training loss 0.032299
&gt;&gt; Epoch 130 finished 	ANN training loss 0.031160
&gt;&gt; Epoch 131 finished 	ANN training loss 0.030333
&gt;&gt; Epoch 132 finished 	ANN training loss 0.030050
&gt;&gt; Epoch 133 finished 	ANN training loss 0.030630
&gt;&gt; Epoch 134 finished 	ANN training loss 0.029221
&gt;&gt; Epoch 135 finished 	ANN training loss 0.029662
&gt;&gt; Epoch 136 finished 	ANN training loss 0.028692
&gt;&gt; Epoch 137 finished 	ANN training loss 0.028272
&gt;&gt; Epoch 138 finished 	ANN training loss 0.028279
&gt;&gt; Epoch 139 finished 	ANN training loss 0.028901
&gt;&gt; Epoch 140 finished 	ANN training loss 0.027454
&gt;&gt; Epoch 141 finished 	ANN training loss 0.026754
&gt;&gt; Epoch 142 finished 	ANN training loss 0.027001
&gt;&gt; Epoch 143 finished 	ANN training loss 0.026237
&gt;&gt; Epoch 144 finished 	ANN training loss 0.025722
&gt;&gt; Epoch 145 finished 	ANN training loss 0.025650
&gt;&gt; Epoch 146 finished 	ANN training loss 0.025234
&gt;&gt; Epoch 147 finished 	ANN training loss 0.025666
&gt;&gt; Epoch 148 finished 	ANN training loss 0.024611
&gt;&gt; Epoch 149 finished 	ANN training loss 0.025244
&gt;&gt; Epoch 150 finished 	ANN training loss 0.024567
&gt;&gt; Epoch 151 finished 	ANN training loss 0.023598
&gt;&gt; Epoch 152 finished 	ANN training loss 0.023260
&gt;&gt; Epoch 153 finished 	ANN training loss 0.023109
&gt;&gt; Epoch 154 finished 	ANN training loss 0.022550
&gt;&gt; Epoch 155 finished 	ANN training loss 0.023260
&gt;&gt; Epoch 156 finished 	ANN training loss 0.022206
&gt;&gt; Epoch 157 finished 	ANN training loss 0.021841
&gt;&gt; Epoch 158 finished 	ANN training loss 0.021922
&gt;&gt; Epoch 159 finished 	ANN training loss 0.021165
&gt;&gt; Epoch 160 finished 	ANN training loss 0.021195
&gt;&gt; Epoch 161 finished 	ANN training loss 0.020878
&gt;&gt; Epoch 162 finished 	ANN training loss 0.020912
&gt;&gt; Epoch 163 finished 	ANN training loss 0.020570
&gt;&gt; Epoch 164 finished 	ANN training loss 0.020186
&gt;&gt; Epoch 165 finished 	ANN training loss 0.020545
&gt;&gt; Epoch 166 finished 	ANN training loss 0.020354
&gt;&gt; Epoch 167 finished 	ANN training loss 0.019553
&gt;&gt; Epoch 168 finished 	ANN training loss 0.019099
&gt;&gt; Epoch 169 finished 	ANN training loss 0.019211
&gt;&gt; Epoch 170 finished 	ANN training loss 0.019486
&gt;&gt; Epoch 171 finished 	ANN training loss 0.018621
&gt;&gt; Epoch 172 finished 	ANN training loss 0.018918
&gt;&gt; Epoch 173 finished 	ANN training loss 0.018962
&gt;&gt; Epoch 174 finished 	ANN training loss 0.017976
&gt;&gt; Epoch 175 finished 	ANN training loss 0.018078
&gt;&gt; Epoch 176 finished 	ANN training loss 0.017955
&gt;&gt; Epoch 177 finished 	ANN training loss 0.017690
&gt;&gt; Epoch 178 finished 	ANN training loss 0.017473
&gt;&gt; Epoch 179 finished 	ANN training loss 0.017312
&gt;&gt; Epoch 180 finished 	ANN training loss 0.017008
&gt;&gt; Epoch 181 finished 	ANN training loss 0.016748
&gt;&gt; Epoch 182 finished 	ANN training loss 0.016467
&gt;&gt; Epoch 183 finished 	ANN training loss 0.016673
&gt;&gt; Epoch 184 finished 	ANN training loss 0.016249
&gt;&gt; Epoch 185 finished 	ANN training loss 0.016325
&gt;&gt; Epoch 186 finished 	ANN training loss 0.016157
&gt;&gt; Epoch 187 finished 	ANN training loss 0.015667
&gt;&gt; Epoch 188 finished 	ANN training loss 0.015258
&gt;&gt; Epoch 189 finished 	ANN training loss 0.015330
&gt;&gt; Epoch 190 finished 	ANN training loss 0.014973
&gt;&gt; Epoch 191 finished 	ANN training loss 0.014862
&gt;&gt; Epoch 192 finished 	ANN training loss 0.014803
&gt;&gt; Epoch 193 finished 	ANN training loss 0.014689
&gt;&gt; Epoch 194 finished 	ANN training loss 0.014628
&gt;&gt; Epoch 195 finished 	ANN training loss 0.014397
&gt;&gt; Epoch 196 finished 	ANN training loss 0.014091
&gt;&gt; Epoch 197 finished 	ANN training loss 0.013958
&gt;&gt; Epoch 198 finished 	ANN training loss 0.014125
&gt;&gt; Epoch 199 finished 	ANN training loss 0.014154
[END] Fine tuning step
Done.
Accuracy: 0.895000
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[16]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ACCURACY: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">acc3</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>ACCURACY: 0.895
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[23]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Sigmoidal activation, dropout=0.3</span>
<span class="n">acc4</span> <span class="o">=</span> <span class="n">deep_belief_net</span><span class="p">(</span><span class="n">hidden_layers_structure</span><span class="o">=</span><span class="p">[</span><span class="mi">300</span><span class="p">],</span> 
                       <span class="n">learning_rate_rbm</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span>
                       <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                       <span class="n">n_epochs_rbm</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span>
                       <span class="n">n_iter_backprop</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
                       <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
                       <span class="n">activation_function</span><span class="o">=</span><span class="s1">&#39;sigmoid&#39;</span><span class="p">,</span>
                       <span class="n">dropout_p</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[START] Pre-training step:
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 44.225895
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 34.718281
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 29.788921
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 26.949614
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 24.911928
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 23.260574
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 21.936123
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 20.846138
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 19.758648
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 18.762615
&gt;&gt; Epoch 11 finished 	RBM Reconstruction error 17.996824
&gt;&gt; Epoch 12 finished 	RBM Reconstruction error 17.189198
&gt;&gt; Epoch 13 finished 	RBM Reconstruction error 16.503515
&gt;&gt; Epoch 14 finished 	RBM Reconstruction error 15.971636
&gt;&gt; Epoch 15 finished 	RBM Reconstruction error 15.345450
&gt;&gt; Epoch 16 finished 	RBM Reconstruction error 14.842959
&gt;&gt; Epoch 17 finished 	RBM Reconstruction error 14.420224
&gt;&gt; Epoch 18 finished 	RBM Reconstruction error 14.055085
&gt;&gt; Epoch 19 finished 	RBM Reconstruction error 13.568018
&gt;&gt; Epoch 20 finished 	RBM Reconstruction error 13.243212
&gt;&gt; Epoch 21 finished 	RBM Reconstruction error 12.880724
&gt;&gt; Epoch 22 finished 	RBM Reconstruction error 12.603806
&gt;&gt; Epoch 23 finished 	RBM Reconstruction error 12.256539
&gt;&gt; Epoch 24 finished 	RBM Reconstruction error 12.033975
&gt;&gt; Epoch 25 finished 	RBM Reconstruction error 11.768235
&gt;&gt; Epoch 26 finished 	RBM Reconstruction error 11.515949
&gt;&gt; Epoch 27 finished 	RBM Reconstruction error 11.250635
&gt;&gt; Epoch 28 finished 	RBM Reconstruction error 11.042171
&gt;&gt; Epoch 29 finished 	RBM Reconstruction error 10.783718
&gt;&gt; Epoch 30 finished 	RBM Reconstruction error 10.576234
&gt;&gt; Epoch 31 finished 	RBM Reconstruction error 10.382644
&gt;&gt; Epoch 32 finished 	RBM Reconstruction error 10.254204
&gt;&gt; Epoch 33 finished 	RBM Reconstruction error 10.007308
&gt;&gt; Epoch 34 finished 	RBM Reconstruction error 9.881972
&gt;&gt; Epoch 35 finished 	RBM Reconstruction error 9.700727
&gt;&gt; Epoch 36 finished 	RBM Reconstruction error 9.511007
&gt;&gt; Epoch 37 finished 	RBM Reconstruction error 9.342621
&gt;&gt; Epoch 38 finished 	RBM Reconstruction error 9.188943
&gt;&gt; Epoch 39 finished 	RBM Reconstruction error 9.156702
&gt;&gt; Epoch 40 finished 	RBM Reconstruction error 8.973461
[END] Pre-training step
[START] Fine tuning step:
&gt;&gt; Epoch 0 finished 	ANN training loss 1.062474
&gt;&gt; Epoch 1 finished 	ANN training loss 0.760325
&gt;&gt; Epoch 2 finished 	ANN training loss 0.615764
&gt;&gt; Epoch 3 finished 	ANN training loss 0.523059
&gt;&gt; Epoch 4 finished 	ANN training loss 0.485734
&gt;&gt; Epoch 5 finished 	ANN training loss 0.436973
&gt;&gt; Epoch 6 finished 	ANN training loss 0.420036
&gt;&gt; Epoch 7 finished 	ANN training loss 0.385624
&gt;&gt; Epoch 8 finished 	ANN training loss 0.373335
&gt;&gt; Epoch 9 finished 	ANN training loss 0.348785
&gt;&gt; Epoch 10 finished 	ANN training loss 0.339566
&gt;&gt; Epoch 11 finished 	ANN training loss 0.327256
&gt;&gt; Epoch 12 finished 	ANN training loss 0.311299
&gt;&gt; Epoch 13 finished 	ANN training loss 0.303708
&gt;&gt; Epoch 14 finished 	ANN training loss 0.292219
&gt;&gt; Epoch 15 finished 	ANN training loss 0.283522
&gt;&gt; Epoch 16 finished 	ANN training loss 0.277875
&gt;&gt; Epoch 17 finished 	ANN training loss 0.267163
&gt;&gt; Epoch 18 finished 	ANN training loss 0.260130
&gt;&gt; Epoch 19 finished 	ANN training loss 0.255586
&gt;&gt; Epoch 20 finished 	ANN training loss 0.248993
&gt;&gt; Epoch 21 finished 	ANN training loss 0.237659
&gt;&gt; Epoch 22 finished 	ANN training loss 0.235448
&gt;&gt; Epoch 23 finished 	ANN training loss 0.231541
&gt;&gt; Epoch 24 finished 	ANN training loss 0.222743
&gt;&gt; Epoch 25 finished 	ANN training loss 0.216500
&gt;&gt; Epoch 26 finished 	ANN training loss 0.214079
&gt;&gt; Epoch 27 finished 	ANN training loss 0.211491
&gt;&gt; Epoch 28 finished 	ANN training loss 0.211819
&gt;&gt; Epoch 29 finished 	ANN training loss 0.199755
&gt;&gt; Epoch 30 finished 	ANN training loss 0.194282
&gt;&gt; Epoch 31 finished 	ANN training loss 0.192168
&gt;&gt; Epoch 32 finished 	ANN training loss 0.189339
&gt;&gt; Epoch 33 finished 	ANN training loss 0.185098
&gt;&gt; Epoch 34 finished 	ANN training loss 0.182081
&gt;&gt; Epoch 35 finished 	ANN training loss 0.179471
&gt;&gt; Epoch 36 finished 	ANN training loss 0.173248
&gt;&gt; Epoch 37 finished 	ANN training loss 0.170629
&gt;&gt; Epoch 38 finished 	ANN training loss 0.166584
&gt;&gt; Epoch 39 finished 	ANN training loss 0.166431
&gt;&gt; Epoch 40 finished 	ANN training loss 0.160197
&gt;&gt; Epoch 41 finished 	ANN training loss 0.156991
&gt;&gt; Epoch 42 finished 	ANN training loss 0.153219
&gt;&gt; Epoch 43 finished 	ANN training loss 0.151327
&gt;&gt; Epoch 44 finished 	ANN training loss 0.150356
&gt;&gt; Epoch 45 finished 	ANN training loss 0.148230
&gt;&gt; Epoch 46 finished 	ANN training loss 0.151358
&gt;&gt; Epoch 47 finished 	ANN training loss 0.144002
&gt;&gt; Epoch 48 finished 	ANN training loss 0.139987
&gt;&gt; Epoch 49 finished 	ANN training loss 0.139422
&gt;&gt; Epoch 50 finished 	ANN training loss 0.134528
&gt;&gt; Epoch 51 finished 	ANN training loss 0.134255
&gt;&gt; Epoch 52 finished 	ANN training loss 0.132808
&gt;&gt; Epoch 53 finished 	ANN training loss 0.129911
&gt;&gt; Epoch 54 finished 	ANN training loss 0.126849
&gt;&gt; Epoch 55 finished 	ANN training loss 0.123737
&gt;&gt; Epoch 56 finished 	ANN training loss 0.123060
&gt;&gt; Epoch 57 finished 	ANN training loss 0.120189
&gt;&gt; Epoch 58 finished 	ANN training loss 0.121035
&gt;&gt; Epoch 59 finished 	ANN training loss 0.116809
&gt;&gt; Epoch 60 finished 	ANN training loss 0.115650
&gt;&gt; Epoch 61 finished 	ANN training loss 0.111809
&gt;&gt; Epoch 62 finished 	ANN training loss 0.114089
&gt;&gt; Epoch 63 finished 	ANN training loss 0.111344
&gt;&gt; Epoch 64 finished 	ANN training loss 0.107773
&gt;&gt; Epoch 65 finished 	ANN training loss 0.105873
&gt;&gt; Epoch 66 finished 	ANN training loss 0.106155
&gt;&gt; Epoch 67 finished 	ANN training loss 0.102239
&gt;&gt; Epoch 68 finished 	ANN training loss 0.104585
&gt;&gt; Epoch 69 finished 	ANN training loss 0.100008
&gt;&gt; Epoch 70 finished 	ANN training loss 0.099806
&gt;&gt; Epoch 71 finished 	ANN training loss 0.096420
&gt;&gt; Epoch 72 finished 	ANN training loss 0.095706
&gt;&gt; Epoch 73 finished 	ANN training loss 0.093789
&gt;&gt; Epoch 74 finished 	ANN training loss 0.092127
&gt;&gt; Epoch 75 finished 	ANN training loss 0.089948
&gt;&gt; Epoch 76 finished 	ANN training loss 0.088883
&gt;&gt; Epoch 77 finished 	ANN training loss 0.089548
&gt;&gt; Epoch 78 finished 	ANN training loss 0.086699
&gt;&gt; Epoch 79 finished 	ANN training loss 0.087140
&gt;&gt; Epoch 80 finished 	ANN training loss 0.083655
&gt;&gt; Epoch 81 finished 	ANN training loss 0.083052
&gt;&gt; Epoch 82 finished 	ANN training loss 0.082934
&gt;&gt; Epoch 83 finished 	ANN training loss 0.080912
&gt;&gt; Epoch 84 finished 	ANN training loss 0.078495
&gt;&gt; Epoch 85 finished 	ANN training loss 0.078432
&gt;&gt; Epoch 86 finished 	ANN training loss 0.076849
&gt;&gt; Epoch 87 finished 	ANN training loss 0.076220
&gt;&gt; Epoch 88 finished 	ANN training loss 0.074069
&gt;&gt; Epoch 89 finished 	ANN training loss 0.074916
&gt;&gt; Epoch 90 finished 	ANN training loss 0.071443
&gt;&gt; Epoch 91 finished 	ANN training loss 0.072242
&gt;&gt; Epoch 92 finished 	ANN training loss 0.069687
&gt;&gt; Epoch 93 finished 	ANN training loss 0.068627
&gt;&gt; Epoch 94 finished 	ANN training loss 0.068944
&gt;&gt; Epoch 95 finished 	ANN training loss 0.067667
&gt;&gt; Epoch 96 finished 	ANN training loss 0.067966
&gt;&gt; Epoch 97 finished 	ANN training loss 0.064765
&gt;&gt; Epoch 98 finished 	ANN training loss 0.064252
&gt;&gt; Epoch 99 finished 	ANN training loss 0.064404
&gt;&gt; Epoch 100 finished 	ANN training loss 0.063088
&gt;&gt; Epoch 101 finished 	ANN training loss 0.061834
&gt;&gt; Epoch 102 finished 	ANN training loss 0.063005
&gt;&gt; Epoch 103 finished 	ANN training loss 0.059850
&gt;&gt; Epoch 104 finished 	ANN training loss 0.059700
&gt;&gt; Epoch 105 finished 	ANN training loss 0.059272
&gt;&gt; Epoch 106 finished 	ANN training loss 0.057944
&gt;&gt; Epoch 107 finished 	ANN training loss 0.059798
&gt;&gt; Epoch 108 finished 	ANN training loss 0.057077
&gt;&gt; Epoch 109 finished 	ANN training loss 0.056709
&gt;&gt; Epoch 110 finished 	ANN training loss 0.057682
&gt;&gt; Epoch 111 finished 	ANN training loss 0.055551
&gt;&gt; Epoch 112 finished 	ANN training loss 0.054197
&gt;&gt; Epoch 113 finished 	ANN training loss 0.052079
&gt;&gt; Epoch 114 finished 	ANN training loss 0.051852
&gt;&gt; Epoch 115 finished 	ANN training loss 0.050854
&gt;&gt; Epoch 116 finished 	ANN training loss 0.051101
&gt;&gt; Epoch 117 finished 	ANN training loss 0.051081
&gt;&gt; Epoch 118 finished 	ANN training loss 0.049335
&gt;&gt; Epoch 119 finished 	ANN training loss 0.048444
&gt;&gt; Epoch 120 finished 	ANN training loss 0.046677
&gt;&gt; Epoch 121 finished 	ANN training loss 0.046563
&gt;&gt; Epoch 122 finished 	ANN training loss 0.046307
&gt;&gt; Epoch 123 finished 	ANN training loss 0.047078
&gt;&gt; Epoch 124 finished 	ANN training loss 0.046094
&gt;&gt; Epoch 125 finished 	ANN training loss 0.044045
&gt;&gt; Epoch 126 finished 	ANN training loss 0.043938
&gt;&gt; Epoch 127 finished 	ANN training loss 0.043707
&gt;&gt; Epoch 128 finished 	ANN training loss 0.043026
&gt;&gt; Epoch 129 finished 	ANN training loss 0.043500
&gt;&gt; Epoch 130 finished 	ANN training loss 0.041800
&gt;&gt; Epoch 131 finished 	ANN training loss 0.041400
&gt;&gt; Epoch 132 finished 	ANN training loss 0.040661
&gt;&gt; Epoch 133 finished 	ANN training loss 0.039805
&gt;&gt; Epoch 134 finished 	ANN training loss 0.040103
&gt;&gt; Epoch 135 finished 	ANN training loss 0.040058
&gt;&gt; Epoch 136 finished 	ANN training loss 0.039108
&gt;&gt; Epoch 137 finished 	ANN training loss 0.038925
&gt;&gt; Epoch 138 finished 	ANN training loss 0.037337
&gt;&gt; Epoch 139 finished 	ANN training loss 0.037386
&gt;&gt; Epoch 140 finished 	ANN training loss 0.037181
&gt;&gt; Epoch 141 finished 	ANN training loss 0.036612
&gt;&gt; Epoch 142 finished 	ANN training loss 0.036788
&gt;&gt; Epoch 143 finished 	ANN training loss 0.035576
&gt;&gt; Epoch 144 finished 	ANN training loss 0.035235
&gt;&gt; Epoch 145 finished 	ANN training loss 0.034494
&gt;&gt; Epoch 146 finished 	ANN training loss 0.034027
&gt;&gt; Epoch 147 finished 	ANN training loss 0.033946
&gt;&gt; Epoch 148 finished 	ANN training loss 0.033116
&gt;&gt; Epoch 149 finished 	ANN training loss 0.032932
&gt;&gt; Epoch 150 finished 	ANN training loss 0.032445
&gt;&gt; Epoch 151 finished 	ANN training loss 0.032538
&gt;&gt; Epoch 152 finished 	ANN training loss 0.031991
&gt;&gt; Epoch 153 finished 	ANN training loss 0.031763
&gt;&gt; Epoch 154 finished 	ANN training loss 0.030953
&gt;&gt; Epoch 155 finished 	ANN training loss 0.030711
&gt;&gt; Epoch 156 finished 	ANN training loss 0.030280
&gt;&gt; Epoch 157 finished 	ANN training loss 0.030527
&gt;&gt; Epoch 158 finished 	ANN training loss 0.029342
&gt;&gt; Epoch 159 finished 	ANN training loss 0.028982
&gt;&gt; Epoch 160 finished 	ANN training loss 0.028938
&gt;&gt; Epoch 161 finished 	ANN training loss 0.030103
&gt;&gt; Epoch 162 finished 	ANN training loss 0.028069
&gt;&gt; Epoch 163 finished 	ANN training loss 0.027824
&gt;&gt; Epoch 164 finished 	ANN training loss 0.028156
&gt;&gt; Epoch 165 finished 	ANN training loss 0.026654
&gt;&gt; Epoch 166 finished 	ANN training loss 0.027255
&gt;&gt; Epoch 167 finished 	ANN training loss 0.027186
&gt;&gt; Epoch 168 finished 	ANN training loss 0.026104
&gt;&gt; Epoch 169 finished 	ANN training loss 0.026762
&gt;&gt; Epoch 170 finished 	ANN training loss 0.026078
&gt;&gt; Epoch 171 finished 	ANN training loss 0.025248
&gt;&gt; Epoch 172 finished 	ANN training loss 0.025239
&gt;&gt; Epoch 173 finished 	ANN training loss 0.024748
&gt;&gt; Epoch 174 finished 	ANN training loss 0.024953
&gt;&gt; Epoch 175 finished 	ANN training loss 0.024957
&gt;&gt; Epoch 176 finished 	ANN training loss 0.024372
&gt;&gt; Epoch 177 finished 	ANN training loss 0.024070
&gt;&gt; Epoch 178 finished 	ANN training loss 0.023700
&gt;&gt; Epoch 179 finished 	ANN training loss 0.023419
&gt;&gt; Epoch 180 finished 	ANN training loss 0.023801
&gt;&gt; Epoch 181 finished 	ANN training loss 0.023324
&gt;&gt; Epoch 182 finished 	ANN training loss 0.022724
&gt;&gt; Epoch 183 finished 	ANN training loss 0.022894
&gt;&gt; Epoch 184 finished 	ANN training loss 0.023263
&gt;&gt; Epoch 185 finished 	ANN training loss 0.021969
&gt;&gt; Epoch 186 finished 	ANN training loss 0.021412
&gt;&gt; Epoch 187 finished 	ANN training loss 0.021252
&gt;&gt; Epoch 188 finished 	ANN training loss 0.021840
&gt;&gt; Epoch 189 finished 	ANN training loss 0.021442
&gt;&gt; Epoch 190 finished 	ANN training loss 0.021317
&gt;&gt; Epoch 191 finished 	ANN training loss 0.020864
&gt;&gt; Epoch 192 finished 	ANN training loss 0.021455
&gt;&gt; Epoch 193 finished 	ANN training loss 0.020897
&gt;&gt; Epoch 194 finished 	ANN training loss 0.020465
&gt;&gt; Epoch 195 finished 	ANN training loss 0.020478
&gt;&gt; Epoch 196 finished 	ANN training loss 0.020300
&gt;&gt; Epoch 197 finished 	ANN training loss 0.019588
&gt;&gt; Epoch 198 finished 	ANN training loss 0.020095
&gt;&gt; Epoch 199 finished 	ANN training loss 0.020331
[END] Fine tuning step
Done.
Accuracy: 0.885000
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[24]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ACCURACY: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">acc4</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>ACCURACY: 0.885
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[25]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Sigmoidal activation, dropout=0.4</span>
<span class="n">acc5</span> <span class="o">=</span> <span class="n">deep_belief_net</span><span class="p">(</span><span class="n">hidden_layers_structure</span><span class="o">=</span><span class="p">[</span><span class="mi">300</span><span class="p">],</span> 
                       <span class="n">learning_rate_rbm</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span>
                       <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                       <span class="n">n_epochs_rbm</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span>
                       <span class="n">n_iter_backprop</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
                       <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
                       <span class="n">activation_function</span><span class="o">=</span><span class="s1">&#39;sigmoid&#39;</span><span class="p">,</span>
                       <span class="n">dropout_p</span><span class="o">=</span><span class="mf">0.4</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[START] Pre-training step:
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 44.140255
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 34.516369
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 29.879543
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 26.920717
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 24.939825
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 23.316221
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 21.916279
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 20.817162
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 19.809341
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 18.751474
&gt;&gt; Epoch 11 finished 	RBM Reconstruction error 17.958969
&gt;&gt; Epoch 12 finished 	RBM Reconstruction error 17.174906
&gt;&gt; Epoch 13 finished 	RBM Reconstruction error 16.434946
&gt;&gt; Epoch 14 finished 	RBM Reconstruction error 15.872507
&gt;&gt; Epoch 15 finished 	RBM Reconstruction error 15.349248
&gt;&gt; Epoch 16 finished 	RBM Reconstruction error 14.827047
&gt;&gt; Epoch 17 finished 	RBM Reconstruction error 14.338945
&gt;&gt; Epoch 18 finished 	RBM Reconstruction error 13.959961
&gt;&gt; Epoch 19 finished 	RBM Reconstruction error 13.526467
&gt;&gt; Epoch 20 finished 	RBM Reconstruction error 13.282947
&gt;&gt; Epoch 21 finished 	RBM Reconstruction error 12.865698
&gt;&gt; Epoch 22 finished 	RBM Reconstruction error 12.565104
&gt;&gt; Epoch 23 finished 	RBM Reconstruction error 12.277325
&gt;&gt; Epoch 24 finished 	RBM Reconstruction error 12.016171
&gt;&gt; Epoch 25 finished 	RBM Reconstruction error 11.813180
&gt;&gt; Epoch 26 finished 	RBM Reconstruction error 11.481167
&gt;&gt; Epoch 27 finished 	RBM Reconstruction error 11.220469
&gt;&gt; Epoch 28 finished 	RBM Reconstruction error 11.012957
&gt;&gt; Epoch 29 finished 	RBM Reconstruction error 10.753363
&gt;&gt; Epoch 30 finished 	RBM Reconstruction error 10.548894
&gt;&gt; Epoch 31 finished 	RBM Reconstruction error 10.340069
&gt;&gt; Epoch 32 finished 	RBM Reconstruction error 10.136040
&gt;&gt; Epoch 33 finished 	RBM Reconstruction error 10.000112
&gt;&gt; Epoch 34 finished 	RBM Reconstruction error 9.816780
&gt;&gt; Epoch 35 finished 	RBM Reconstruction error 9.689515
&gt;&gt; Epoch 36 finished 	RBM Reconstruction error 9.468905
&gt;&gt; Epoch 37 finished 	RBM Reconstruction error 9.430795
&gt;&gt; Epoch 38 finished 	RBM Reconstruction error 9.228283
&gt;&gt; Epoch 39 finished 	RBM Reconstruction error 9.073233
&gt;&gt; Epoch 40 finished 	RBM Reconstruction error 8.909946
[END] Pre-training step
[START] Fine tuning step:
&gt;&gt; Epoch 0 finished 	ANN training loss 1.066938
&gt;&gt; Epoch 1 finished 	ANN training loss 0.734275
&gt;&gt; Epoch 2 finished 	ANN training loss 0.594924
&gt;&gt; Epoch 3 finished 	ANN training loss 0.523809
&gt;&gt; Epoch 4 finished 	ANN training loss 0.476906
&gt;&gt; Epoch 5 finished 	ANN training loss 0.449035
&gt;&gt; Epoch 6 finished 	ANN training loss 0.419840
&gt;&gt; Epoch 7 finished 	ANN training loss 0.397110
&gt;&gt; Epoch 8 finished 	ANN training loss 0.380618
&gt;&gt; Epoch 9 finished 	ANN training loss 0.361953
&gt;&gt; Epoch 10 finished 	ANN training loss 0.352017
&gt;&gt; Epoch 11 finished 	ANN training loss 0.337664
&gt;&gt; Epoch 12 finished 	ANN training loss 0.323106
&gt;&gt; Epoch 13 finished 	ANN training loss 0.322950
&gt;&gt; Epoch 14 finished 	ANN training loss 0.301515
&gt;&gt; Epoch 15 finished 	ANN training loss 0.291547
&gt;&gt; Epoch 16 finished 	ANN training loss 0.287913
&gt;&gt; Epoch 17 finished 	ANN training loss 0.275095
&gt;&gt; Epoch 18 finished 	ANN training loss 0.270438
&gt;&gt; Epoch 19 finished 	ANN training loss 0.262995
&gt;&gt; Epoch 20 finished 	ANN training loss 0.257501
&gt;&gt; Epoch 21 finished 	ANN training loss 0.254009
&gt;&gt; Epoch 22 finished 	ANN training loss 0.243380
&gt;&gt; Epoch 23 finished 	ANN training loss 0.239698
&gt;&gt; Epoch 24 finished 	ANN training loss 0.231883
&gt;&gt; Epoch 25 finished 	ANN training loss 0.230240
&gt;&gt; Epoch 26 finished 	ANN training loss 0.226360
&gt;&gt; Epoch 27 finished 	ANN training loss 0.231455
&gt;&gt; Epoch 28 finished 	ANN training loss 0.217701
&gt;&gt; Epoch 29 finished 	ANN training loss 0.215385
&gt;&gt; Epoch 30 finished 	ANN training loss 0.207716
&gt;&gt; Epoch 31 finished 	ANN training loss 0.211143
&gt;&gt; Epoch 32 finished 	ANN training loss 0.200320
&gt;&gt; Epoch 33 finished 	ANN training loss 0.210898
&gt;&gt; Epoch 34 finished 	ANN training loss 0.194683
&gt;&gt; Epoch 35 finished 	ANN training loss 0.192158
&gt;&gt; Epoch 36 finished 	ANN training loss 0.191934
&gt;&gt; Epoch 37 finished 	ANN training loss 0.188256
&gt;&gt; Epoch 38 finished 	ANN training loss 0.183503
&gt;&gt; Epoch 39 finished 	ANN training loss 0.183189
&gt;&gt; Epoch 40 finished 	ANN training loss 0.179966
&gt;&gt; Epoch 41 finished 	ANN training loss 0.181487
&gt;&gt; Epoch 42 finished 	ANN training loss 0.172944
&gt;&gt; Epoch 43 finished 	ANN training loss 0.171273
&gt;&gt; Epoch 44 finished 	ANN training loss 0.165213
&gt;&gt; Epoch 45 finished 	ANN training loss 0.166067
&gt;&gt; Epoch 46 finished 	ANN training loss 0.163789
&gt;&gt; Epoch 47 finished 	ANN training loss 0.167600
&gt;&gt; Epoch 48 finished 	ANN training loss 0.155891
&gt;&gt; Epoch 49 finished 	ANN training loss 0.154835
&gt;&gt; Epoch 50 finished 	ANN training loss 0.151390
&gt;&gt; Epoch 51 finished 	ANN training loss 0.151304
&gt;&gt; Epoch 52 finished 	ANN training loss 0.148279
&gt;&gt; Epoch 53 finished 	ANN training loss 0.147794
&gt;&gt; Epoch 54 finished 	ANN training loss 0.147427
&gt;&gt; Epoch 55 finished 	ANN training loss 0.140367
&gt;&gt; Epoch 56 finished 	ANN training loss 0.138864
&gt;&gt; Epoch 57 finished 	ANN training loss 0.141895
&gt;&gt; Epoch 58 finished 	ANN training loss 0.136450
&gt;&gt; Epoch 59 finished 	ANN training loss 0.135003
&gt;&gt; Epoch 60 finished 	ANN training loss 0.131884
&gt;&gt; Epoch 61 finished 	ANN training loss 0.134179
&gt;&gt; Epoch 62 finished 	ANN training loss 0.131031
&gt;&gt; Epoch 63 finished 	ANN training loss 0.127614
&gt;&gt; Epoch 64 finished 	ANN training loss 0.123993
&gt;&gt; Epoch 65 finished 	ANN training loss 0.121772
&gt;&gt; Epoch 66 finished 	ANN training loss 0.120674
&gt;&gt; Epoch 67 finished 	ANN training loss 0.121848
&gt;&gt; Epoch 68 finished 	ANN training loss 0.119808
&gt;&gt; Epoch 69 finished 	ANN training loss 0.117407
&gt;&gt; Epoch 70 finished 	ANN training loss 0.117347
&gt;&gt; Epoch 71 finished 	ANN training loss 0.118752
&gt;&gt; Epoch 72 finished 	ANN training loss 0.117753
&gt;&gt; Epoch 73 finished 	ANN training loss 0.110410
&gt;&gt; Epoch 74 finished 	ANN training loss 0.109087
&gt;&gt; Epoch 75 finished 	ANN training loss 0.110105
&gt;&gt; Epoch 76 finished 	ANN training loss 0.109425
&gt;&gt; Epoch 77 finished 	ANN training loss 0.107489
&gt;&gt; Epoch 78 finished 	ANN training loss 0.105924
&gt;&gt; Epoch 79 finished 	ANN training loss 0.102476
&gt;&gt; Epoch 80 finished 	ANN training loss 0.104578
&gt;&gt; Epoch 81 finished 	ANN training loss 0.101349
&gt;&gt; Epoch 82 finished 	ANN training loss 0.098437
&gt;&gt; Epoch 83 finished 	ANN training loss 0.098407
&gt;&gt; Epoch 84 finished 	ANN training loss 0.097846
&gt;&gt; Epoch 85 finished 	ANN training loss 0.093516
&gt;&gt; Epoch 86 finished 	ANN training loss 0.095406
&gt;&gt; Epoch 87 finished 	ANN training loss 0.092529
&gt;&gt; Epoch 88 finished 	ANN training loss 0.093897
&gt;&gt; Epoch 89 finished 	ANN training loss 0.091094
&gt;&gt; Epoch 90 finished 	ANN training loss 0.090580
&gt;&gt; Epoch 91 finished 	ANN training loss 0.090806
&gt;&gt; Epoch 92 finished 	ANN training loss 0.088154
&gt;&gt; Epoch 93 finished 	ANN training loss 0.085090
&gt;&gt; Epoch 94 finished 	ANN training loss 0.085001
&gt;&gt; Epoch 95 finished 	ANN training loss 0.084441
&gt;&gt; Epoch 96 finished 	ANN training loss 0.085808
&gt;&gt; Epoch 97 finished 	ANN training loss 0.082942
&gt;&gt; Epoch 98 finished 	ANN training loss 0.080774
&gt;&gt; Epoch 99 finished 	ANN training loss 0.079185
&gt;&gt; Epoch 100 finished 	ANN training loss 0.078713
&gt;&gt; Epoch 101 finished 	ANN training loss 0.076861
&gt;&gt; Epoch 102 finished 	ANN training loss 0.079224
&gt;&gt; Epoch 103 finished 	ANN training loss 0.078161
&gt;&gt; Epoch 104 finished 	ANN training loss 0.075585
&gt;&gt; Epoch 105 finished 	ANN training loss 0.072891
&gt;&gt; Epoch 106 finished 	ANN training loss 0.073512
&gt;&gt; Epoch 107 finished 	ANN training loss 0.072596
&gt;&gt; Epoch 108 finished 	ANN training loss 0.071073
&gt;&gt; Epoch 109 finished 	ANN training loss 0.071214
&gt;&gt; Epoch 110 finished 	ANN training loss 0.068718
&gt;&gt; Epoch 111 finished 	ANN training loss 0.069827
&gt;&gt; Epoch 112 finished 	ANN training loss 0.069690
&gt;&gt; Epoch 113 finished 	ANN training loss 0.066436
&gt;&gt; Epoch 114 finished 	ANN training loss 0.065389
&gt;&gt; Epoch 115 finished 	ANN training loss 0.064521
&gt;&gt; Epoch 116 finished 	ANN training loss 0.065385
&gt;&gt; Epoch 117 finished 	ANN training loss 0.063349
&gt;&gt; Epoch 118 finished 	ANN training loss 0.064051
&gt;&gt; Epoch 119 finished 	ANN training loss 0.062765
&gt;&gt; Epoch 120 finished 	ANN training loss 0.062937
&gt;&gt; Epoch 121 finished 	ANN training loss 0.063619
&gt;&gt; Epoch 122 finished 	ANN training loss 0.060987
&gt;&gt; Epoch 123 finished 	ANN training loss 0.060811
&gt;&gt; Epoch 124 finished 	ANN training loss 0.059828
&gt;&gt; Epoch 125 finished 	ANN training loss 0.060110
&gt;&gt; Epoch 126 finished 	ANN training loss 0.058511
&gt;&gt; Epoch 127 finished 	ANN training loss 0.058385
&gt;&gt; Epoch 128 finished 	ANN training loss 0.058820
&gt;&gt; Epoch 129 finished 	ANN training loss 0.056253
&gt;&gt; Epoch 130 finished 	ANN training loss 0.055482
&gt;&gt; Epoch 131 finished 	ANN training loss 0.055394
&gt;&gt; Epoch 132 finished 	ANN training loss 0.054382
&gt;&gt; Epoch 133 finished 	ANN training loss 0.054062
&gt;&gt; Epoch 134 finished 	ANN training loss 0.054221
&gt;&gt; Epoch 135 finished 	ANN training loss 0.054995
&gt;&gt; Epoch 136 finished 	ANN training loss 0.052053
&gt;&gt; Epoch 137 finished 	ANN training loss 0.052666
&gt;&gt; Epoch 138 finished 	ANN training loss 0.051470
&gt;&gt; Epoch 139 finished 	ANN training loss 0.051639
&gt;&gt; Epoch 140 finished 	ANN training loss 0.050752
&gt;&gt; Epoch 141 finished 	ANN training loss 0.049019
&gt;&gt; Epoch 142 finished 	ANN training loss 0.049914
&gt;&gt; Epoch 143 finished 	ANN training loss 0.049576
&gt;&gt; Epoch 144 finished 	ANN training loss 0.050435
&gt;&gt; Epoch 145 finished 	ANN training loss 0.047030
&gt;&gt; Epoch 146 finished 	ANN training loss 0.050140
&gt;&gt; Epoch 147 finished 	ANN training loss 0.047038
&gt;&gt; Epoch 148 finished 	ANN training loss 0.046597
&gt;&gt; Epoch 149 finished 	ANN training loss 0.044643
&gt;&gt; Epoch 150 finished 	ANN training loss 0.043996
&gt;&gt; Epoch 151 finished 	ANN training loss 0.044778
&gt;&gt; Epoch 152 finished 	ANN training loss 0.043207
&gt;&gt; Epoch 153 finished 	ANN training loss 0.043595
&gt;&gt; Epoch 154 finished 	ANN training loss 0.042821
&gt;&gt; Epoch 155 finished 	ANN training loss 0.041673
&gt;&gt; Epoch 156 finished 	ANN training loss 0.043117
&gt;&gt; Epoch 157 finished 	ANN training loss 0.041032
&gt;&gt; Epoch 158 finished 	ANN training loss 0.040862
&gt;&gt; Epoch 159 finished 	ANN training loss 0.039584
&gt;&gt; Epoch 160 finished 	ANN training loss 0.039544
&gt;&gt; Epoch 161 finished 	ANN training loss 0.039196
&gt;&gt; Epoch 162 finished 	ANN training loss 0.038407
&gt;&gt; Epoch 163 finished 	ANN training loss 0.038817
&gt;&gt; Epoch 164 finished 	ANN training loss 0.037738
&gt;&gt; Epoch 165 finished 	ANN training loss 0.036932
&gt;&gt; Epoch 166 finished 	ANN training loss 0.036816
&gt;&gt; Epoch 167 finished 	ANN training loss 0.037366
&gt;&gt; Epoch 168 finished 	ANN training loss 0.037214
&gt;&gt; Epoch 169 finished 	ANN training loss 0.036398
&gt;&gt; Epoch 170 finished 	ANN training loss 0.036167
&gt;&gt; Epoch 171 finished 	ANN training loss 0.034256
&gt;&gt; Epoch 172 finished 	ANN training loss 0.035277
&gt;&gt; Epoch 173 finished 	ANN training loss 0.034715
&gt;&gt; Epoch 174 finished 	ANN training loss 0.034012
&gt;&gt; Epoch 175 finished 	ANN training loss 0.032673
&gt;&gt; Epoch 176 finished 	ANN training loss 0.032604
&gt;&gt; Epoch 177 finished 	ANN training loss 0.032946
&gt;&gt; Epoch 178 finished 	ANN training loss 0.032165
&gt;&gt; Epoch 179 finished 	ANN training loss 0.032634
&gt;&gt; Epoch 180 finished 	ANN training loss 0.032136
&gt;&gt; Epoch 181 finished 	ANN training loss 0.032435
&gt;&gt; Epoch 182 finished 	ANN training loss 0.033255
&gt;&gt; Epoch 183 finished 	ANN training loss 0.031021
&gt;&gt; Epoch 184 finished 	ANN training loss 0.031597
&gt;&gt; Epoch 185 finished 	ANN training loss 0.029515
&gt;&gt; Epoch 186 finished 	ANN training loss 0.030117
&gt;&gt; Epoch 187 finished 	ANN training loss 0.029094
&gt;&gt; Epoch 188 finished 	ANN training loss 0.029761
&gt;&gt; Epoch 189 finished 	ANN training loss 0.029402
&gt;&gt; Epoch 190 finished 	ANN training loss 0.028540
&gt;&gt; Epoch 191 finished 	ANN training loss 0.028611
&gt;&gt; Epoch 192 finished 	ANN training loss 0.028487
&gt;&gt; Epoch 193 finished 	ANN training loss 0.027111
&gt;&gt; Epoch 194 finished 	ANN training loss 0.027326
&gt;&gt; Epoch 195 finished 	ANN training loss 0.027370
&gt;&gt; Epoch 196 finished 	ANN training loss 0.027969
&gt;&gt; Epoch 197 finished 	ANN training loss 0.027641
&gt;&gt; Epoch 198 finished 	ANN training loss 0.027018
&gt;&gt; Epoch 199 finished 	ANN training loss 0.025903
[END] Fine tuning step
Done.
Accuracy: 0.885000
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[26]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ACCURACY: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">acc5</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>ACCURACY: 0.885
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[27]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Sigmoidal activation, dropout=0.5</span>
<span class="n">acc6</span> <span class="o">=</span> <span class="n">deep_belief_net</span><span class="p">(</span><span class="n">hidden_layers_structure</span><span class="o">=</span><span class="p">[</span><span class="mi">300</span><span class="p">],</span> 
                       <span class="n">learning_rate_rbm</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span>
                       <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                       <span class="n">n_epochs_rbm</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span>
                       <span class="n">n_iter_backprop</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
                       <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
                       <span class="n">activation_function</span><span class="o">=</span><span class="s1">&#39;sigmoid&#39;</span><span class="p">,</span>
                       <span class="n">dropout_p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[START] Pre-training step:
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 44.573238
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 34.630795
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 30.121592
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 26.837561
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 24.935493
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 23.162682
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 21.974854
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 20.764902
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 19.719866
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 18.765295
&gt;&gt; Epoch 11 finished 	RBM Reconstruction error 17.900713
&gt;&gt; Epoch 12 finished 	RBM Reconstruction error 17.103657
&gt;&gt; Epoch 13 finished 	RBM Reconstruction error 16.434870
&gt;&gt; Epoch 14 finished 	RBM Reconstruction error 15.841487
&gt;&gt; Epoch 15 finished 	RBM Reconstruction error 15.312390
&gt;&gt; Epoch 16 finished 	RBM Reconstruction error 14.815372
&gt;&gt; Epoch 17 finished 	RBM Reconstruction error 14.355344
&gt;&gt; Epoch 18 finished 	RBM Reconstruction error 13.964416
&gt;&gt; Epoch 19 finished 	RBM Reconstruction error 13.502627
&gt;&gt; Epoch 20 finished 	RBM Reconstruction error 13.164868
&gt;&gt; Epoch 21 finished 	RBM Reconstruction error 12.808501
&gt;&gt; Epoch 22 finished 	RBM Reconstruction error 12.508628
&gt;&gt; Epoch 23 finished 	RBM Reconstruction error 12.175906
&gt;&gt; Epoch 24 finished 	RBM Reconstruction error 11.939978
&gt;&gt; Epoch 25 finished 	RBM Reconstruction error 11.676118
&gt;&gt; Epoch 26 finished 	RBM Reconstruction error 11.405881
&gt;&gt; Epoch 27 finished 	RBM Reconstruction error 11.143344
&gt;&gt; Epoch 28 finished 	RBM Reconstruction error 10.943728
&gt;&gt; Epoch 29 finished 	RBM Reconstruction error 10.718337
&gt;&gt; Epoch 30 finished 	RBM Reconstruction error 10.472020
&gt;&gt; Epoch 31 finished 	RBM Reconstruction error 10.317543
&gt;&gt; Epoch 32 finished 	RBM Reconstruction error 10.093082
&gt;&gt; Epoch 33 finished 	RBM Reconstruction error 9.895876
&gt;&gt; Epoch 34 finished 	RBM Reconstruction error 9.735769
&gt;&gt; Epoch 35 finished 	RBM Reconstruction error 9.620356
&gt;&gt; Epoch 36 finished 	RBM Reconstruction error 9.389066
&gt;&gt; Epoch 37 finished 	RBM Reconstruction error 9.231999
&gt;&gt; Epoch 38 finished 	RBM Reconstruction error 9.104783
&gt;&gt; Epoch 39 finished 	RBM Reconstruction error 8.953965
&gt;&gt; Epoch 40 finished 	RBM Reconstruction error 8.879975
[END] Pre-training step
[START] Fine tuning step:
&gt;&gt; Epoch 0 finished 	ANN training loss 1.029506
&gt;&gt; Epoch 1 finished 	ANN training loss 0.783091
&gt;&gt; Epoch 2 finished 	ANN training loss 0.615450
&gt;&gt; Epoch 3 finished 	ANN training loss 0.539779
&gt;&gt; Epoch 4 finished 	ANN training loss 0.491151
&gt;&gt; Epoch 5 finished 	ANN training loss 0.470389
&gt;&gt; Epoch 6 finished 	ANN training loss 0.425696
&gt;&gt; Epoch 7 finished 	ANN training loss 0.432870
&gt;&gt; Epoch 8 finished 	ANN training loss 0.388121
&gt;&gt; Epoch 9 finished 	ANN training loss 0.374311
&gt;&gt; Epoch 10 finished 	ANN training loss 0.370608
&gt;&gt; Epoch 11 finished 	ANN training loss 0.344941
&gt;&gt; Epoch 12 finished 	ANN training loss 0.332405
&gt;&gt; Epoch 13 finished 	ANN training loss 0.323167
&gt;&gt; Epoch 14 finished 	ANN training loss 0.316941
&gt;&gt; Epoch 15 finished 	ANN training loss 0.308172
&gt;&gt; Epoch 16 finished 	ANN training loss 0.298531
&gt;&gt; Epoch 17 finished 	ANN training loss 0.304628
&gt;&gt; Epoch 18 finished 	ANN training loss 0.291222
&gt;&gt; Epoch 19 finished 	ANN training loss 0.286756
&gt;&gt; Epoch 20 finished 	ANN training loss 0.275031
&gt;&gt; Epoch 21 finished 	ANN training loss 0.270081
&gt;&gt; Epoch 22 finished 	ANN training loss 0.268243
&gt;&gt; Epoch 23 finished 	ANN training loss 0.255230
&gt;&gt; Epoch 24 finished 	ANN training loss 0.257759
&gt;&gt; Epoch 25 finished 	ANN training loss 0.254832
&gt;&gt; Epoch 26 finished 	ANN training loss 0.248638
&gt;&gt; Epoch 27 finished 	ANN training loss 0.243590
&gt;&gt; Epoch 28 finished 	ANN training loss 0.244656
&gt;&gt; Epoch 29 finished 	ANN training loss 0.243002
&gt;&gt; Epoch 30 finished 	ANN training loss 0.234312
&gt;&gt; Epoch 31 finished 	ANN training loss 0.228486
&gt;&gt; Epoch 32 finished 	ANN training loss 0.226636
&gt;&gt; Epoch 33 finished 	ANN training loss 0.229403
&gt;&gt; Epoch 34 finished 	ANN training loss 0.223033
&gt;&gt; Epoch 35 finished 	ANN training loss 0.215411
&gt;&gt; Epoch 36 finished 	ANN training loss 0.212733
&gt;&gt; Epoch 37 finished 	ANN training loss 0.215245
&gt;&gt; Epoch 38 finished 	ANN training loss 0.215365
&gt;&gt; Epoch 39 finished 	ANN training loss 0.204600
&gt;&gt; Epoch 40 finished 	ANN training loss 0.202094
&gt;&gt; Epoch 41 finished 	ANN training loss 0.204231
&gt;&gt; Epoch 42 finished 	ANN training loss 0.199522
&gt;&gt; Epoch 43 finished 	ANN training loss 0.194332
&gt;&gt; Epoch 44 finished 	ANN training loss 0.196914
&gt;&gt; Epoch 45 finished 	ANN training loss 0.191636
&gt;&gt; Epoch 46 finished 	ANN training loss 0.184668
&gt;&gt; Epoch 47 finished 	ANN training loss 0.188735
&gt;&gt; Epoch 48 finished 	ANN training loss 0.187260
&gt;&gt; Epoch 49 finished 	ANN training loss 0.179908
&gt;&gt; Epoch 50 finished 	ANN training loss 0.178960
&gt;&gt; Epoch 51 finished 	ANN training loss 0.180915
&gt;&gt; Epoch 52 finished 	ANN training loss 0.175768
&gt;&gt; Epoch 53 finished 	ANN training loss 0.177390
&gt;&gt; Epoch 54 finished 	ANN training loss 0.173746
&gt;&gt; Epoch 55 finished 	ANN training loss 0.174097
&gt;&gt; Epoch 56 finished 	ANN training loss 0.169286
&gt;&gt; Epoch 57 finished 	ANN training loss 0.168682
&gt;&gt; Epoch 58 finished 	ANN training loss 0.165092
&gt;&gt; Epoch 59 finished 	ANN training loss 0.161985
&gt;&gt; Epoch 60 finished 	ANN training loss 0.158393
&gt;&gt; Epoch 61 finished 	ANN training loss 0.157357
&gt;&gt; Epoch 62 finished 	ANN training loss 0.158232
&gt;&gt; Epoch 63 finished 	ANN training loss 0.159759
&gt;&gt; Epoch 64 finished 	ANN training loss 0.155991
&gt;&gt; Epoch 65 finished 	ANN training loss 0.154985
&gt;&gt; Epoch 66 finished 	ANN training loss 0.159384
&gt;&gt; Epoch 67 finished 	ANN training loss 0.150831
&gt;&gt; Epoch 68 finished 	ANN training loss 0.150883
&gt;&gt; Epoch 69 finished 	ANN training loss 0.158309
&gt;&gt; Epoch 70 finished 	ANN training loss 0.144489
&gt;&gt; Epoch 71 finished 	ANN training loss 0.147804
&gt;&gt; Epoch 72 finished 	ANN training loss 0.143351
&gt;&gt; Epoch 73 finished 	ANN training loss 0.141749
&gt;&gt; Epoch 74 finished 	ANN training loss 0.145374
&gt;&gt; Epoch 75 finished 	ANN training loss 0.142773
&gt;&gt; Epoch 76 finished 	ANN training loss 0.136577
&gt;&gt; Epoch 77 finished 	ANN training loss 0.133758
&gt;&gt; Epoch 78 finished 	ANN training loss 0.132530
&gt;&gt; Epoch 79 finished 	ANN training loss 0.135015
&gt;&gt; Epoch 80 finished 	ANN training loss 0.137384
&gt;&gt; Epoch 81 finished 	ANN training loss 0.129984
&gt;&gt; Epoch 82 finished 	ANN training loss 0.130589
&gt;&gt; Epoch 83 finished 	ANN training loss 0.134831
&gt;&gt; Epoch 84 finished 	ANN training loss 0.123814
&gt;&gt; Epoch 85 finished 	ANN training loss 0.127403
&gt;&gt; Epoch 86 finished 	ANN training loss 0.122149
&gt;&gt; Epoch 87 finished 	ANN training loss 0.120554
&gt;&gt; Epoch 88 finished 	ANN training loss 0.120919
&gt;&gt; Epoch 89 finished 	ANN training loss 0.128998
&gt;&gt; Epoch 90 finished 	ANN training loss 0.119175
&gt;&gt; Epoch 91 finished 	ANN training loss 0.117204
&gt;&gt; Epoch 92 finished 	ANN training loss 0.117279
&gt;&gt; Epoch 93 finished 	ANN training loss 0.117866
&gt;&gt; Epoch 94 finished 	ANN training loss 0.115391
&gt;&gt; Epoch 95 finished 	ANN training loss 0.115274
&gt;&gt; Epoch 96 finished 	ANN training loss 0.113404
&gt;&gt; Epoch 97 finished 	ANN training loss 0.111995
&gt;&gt; Epoch 98 finished 	ANN training loss 0.112632
&gt;&gt; Epoch 99 finished 	ANN training loss 0.111424
&gt;&gt; Epoch 100 finished 	ANN training loss 0.111886
&gt;&gt; Epoch 101 finished 	ANN training loss 0.109582
&gt;&gt; Epoch 102 finished 	ANN training loss 0.108680
&gt;&gt; Epoch 103 finished 	ANN training loss 0.108157
&gt;&gt; Epoch 104 finished 	ANN training loss 0.105164
&gt;&gt; Epoch 105 finished 	ANN training loss 0.109304
&gt;&gt; Epoch 106 finished 	ANN training loss 0.102602
&gt;&gt; Epoch 107 finished 	ANN training loss 0.104751
&gt;&gt; Epoch 108 finished 	ANN training loss 0.100942
&gt;&gt; Epoch 109 finished 	ANN training loss 0.099841
&gt;&gt; Epoch 110 finished 	ANN training loss 0.099209
&gt;&gt; Epoch 111 finished 	ANN training loss 0.095703
&gt;&gt; Epoch 112 finished 	ANN training loss 0.094847
&gt;&gt; Epoch 113 finished 	ANN training loss 0.101631
&gt;&gt; Epoch 114 finished 	ANN training loss 0.096926
&gt;&gt; Epoch 115 finished 	ANN training loss 0.095116
&gt;&gt; Epoch 116 finished 	ANN training loss 0.097598
&gt;&gt; Epoch 117 finished 	ANN training loss 0.098958
&gt;&gt; Epoch 118 finished 	ANN training loss 0.093077
&gt;&gt; Epoch 119 finished 	ANN training loss 0.089837
&gt;&gt; Epoch 120 finished 	ANN training loss 0.091666
&gt;&gt; Epoch 121 finished 	ANN training loss 0.090795
&gt;&gt; Epoch 122 finished 	ANN training loss 0.090826
&gt;&gt; Epoch 123 finished 	ANN training loss 0.086482
&gt;&gt; Epoch 124 finished 	ANN training loss 0.085695
&gt;&gt; Epoch 125 finished 	ANN training loss 0.085151
&gt;&gt; Epoch 126 finished 	ANN training loss 0.088090
&gt;&gt; Epoch 127 finished 	ANN training loss 0.089811
&gt;&gt; Epoch 128 finished 	ANN training loss 0.084178
&gt;&gt; Epoch 129 finished 	ANN training loss 0.083127
&gt;&gt; Epoch 130 finished 	ANN training loss 0.082611
&gt;&gt; Epoch 131 finished 	ANN training loss 0.084550
&gt;&gt; Epoch 132 finished 	ANN training loss 0.083672
&gt;&gt; Epoch 133 finished 	ANN training loss 0.083040
&gt;&gt; Epoch 134 finished 	ANN training loss 0.080540
&gt;&gt; Epoch 135 finished 	ANN training loss 0.078377
&gt;&gt; Epoch 136 finished 	ANN training loss 0.078733
&gt;&gt; Epoch 137 finished 	ANN training loss 0.078687
&gt;&gt; Epoch 138 finished 	ANN training loss 0.077426
&gt;&gt; Epoch 139 finished 	ANN training loss 0.077130
&gt;&gt; Epoch 140 finished 	ANN training loss 0.079029
&gt;&gt; Epoch 141 finished 	ANN training loss 0.073585
&gt;&gt; Epoch 142 finished 	ANN training loss 0.074813
&gt;&gt; Epoch 143 finished 	ANN training loss 0.073263
&gt;&gt; Epoch 144 finished 	ANN training loss 0.072243
&gt;&gt; Epoch 145 finished 	ANN training loss 0.072398
&gt;&gt; Epoch 146 finished 	ANN training loss 0.070220
&gt;&gt; Epoch 147 finished 	ANN training loss 0.069503
&gt;&gt; Epoch 148 finished 	ANN training loss 0.069849
&gt;&gt; Epoch 149 finished 	ANN training loss 0.069325
&gt;&gt; Epoch 150 finished 	ANN training loss 0.067089
&gt;&gt; Epoch 151 finished 	ANN training loss 0.065187
&gt;&gt; Epoch 152 finished 	ANN training loss 0.065232
&gt;&gt; Epoch 153 finished 	ANN training loss 0.065788
&gt;&gt; Epoch 154 finished 	ANN training loss 0.065809
&gt;&gt; Epoch 155 finished 	ANN training loss 0.064530
&gt;&gt; Epoch 156 finished 	ANN training loss 0.062614
&gt;&gt; Epoch 157 finished 	ANN training loss 0.064008
&gt;&gt; Epoch 158 finished 	ANN training loss 0.063099
&gt;&gt; Epoch 159 finished 	ANN training loss 0.064316
&gt;&gt; Epoch 160 finished 	ANN training loss 0.065581
&gt;&gt; Epoch 161 finished 	ANN training loss 0.060609
&gt;&gt; Epoch 162 finished 	ANN training loss 0.059868
&gt;&gt; Epoch 163 finished 	ANN training loss 0.062034
&gt;&gt; Epoch 164 finished 	ANN training loss 0.059640
&gt;&gt; Epoch 165 finished 	ANN training loss 0.058968
&gt;&gt; Epoch 166 finished 	ANN training loss 0.057682
&gt;&gt; Epoch 167 finished 	ANN training loss 0.058363
&gt;&gt; Epoch 168 finished 	ANN training loss 0.057457
&gt;&gt; Epoch 169 finished 	ANN training loss 0.057689
&gt;&gt; Epoch 170 finished 	ANN training loss 0.058487
&gt;&gt; Epoch 171 finished 	ANN training loss 0.056778
&gt;&gt; Epoch 172 finished 	ANN training loss 0.054990
&gt;&gt; Epoch 173 finished 	ANN training loss 0.053820
&gt;&gt; Epoch 174 finished 	ANN training loss 0.055533
&gt;&gt; Epoch 175 finished 	ANN training loss 0.053863
&gt;&gt; Epoch 176 finished 	ANN training loss 0.052631
&gt;&gt; Epoch 177 finished 	ANN training loss 0.051800
&gt;&gt; Epoch 178 finished 	ANN training loss 0.051748
&gt;&gt; Epoch 179 finished 	ANN training loss 0.053723
&gt;&gt; Epoch 180 finished 	ANN training loss 0.050954
&gt;&gt; Epoch 181 finished 	ANN training loss 0.050201
&gt;&gt; Epoch 182 finished 	ANN training loss 0.055064
&gt;&gt; Epoch 183 finished 	ANN training loss 0.050334
&gt;&gt; Epoch 184 finished 	ANN training loss 0.049993
&gt;&gt; Epoch 185 finished 	ANN training loss 0.050272
&gt;&gt; Epoch 186 finished 	ANN training loss 0.049472
&gt;&gt; Epoch 187 finished 	ANN training loss 0.048316
&gt;&gt; Epoch 188 finished 	ANN training loss 0.050607
&gt;&gt; Epoch 189 finished 	ANN training loss 0.049316
&gt;&gt; Epoch 190 finished 	ANN training loss 0.050565
&gt;&gt; Epoch 191 finished 	ANN training loss 0.050284
&gt;&gt; Epoch 192 finished 	ANN training loss 0.049247
&gt;&gt; Epoch 193 finished 	ANN training loss 0.048176
&gt;&gt; Epoch 194 finished 	ANN training loss 0.048126
&gt;&gt; Epoch 195 finished 	ANN training loss 0.047812
&gt;&gt; Epoch 196 finished 	ANN training loss 0.046406
&gt;&gt; Epoch 197 finished 	ANN training loss 0.047042
&gt;&gt; Epoch 198 finished 	ANN training loss 0.048685
&gt;&gt; Epoch 199 finished 	ANN training loss 0.048325
[END] Fine tuning step
Done.
Accuracy: 0.875000
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[28]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ACCURACY: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">acc6</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>ACCURACY: 0.875
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[29]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Sigmoidal activation, dropout=0.6</span>
<span class="n">acc7</span> <span class="o">=</span> <span class="n">deep_belief_net</span><span class="p">(</span><span class="n">hidden_layers_structure</span><span class="o">=</span><span class="p">[</span><span class="mi">300</span><span class="p">],</span> 
                       <span class="n">learning_rate_rbm</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span>
                       <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                       <span class="n">n_epochs_rbm</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span>
                       <span class="n">n_iter_backprop</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
                       <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
                       <span class="n">activation_function</span><span class="o">=</span><span class="s1">&#39;sigmoid&#39;</span><span class="p">,</span>
                       <span class="n">dropout_p</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[START] Pre-training step:
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 43.978573
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 34.663868
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 29.835709
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 26.734238
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 24.634024
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 23.120874
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 21.792395
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 20.564890
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 19.499239
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 18.631668
&gt;&gt; Epoch 11 finished 	RBM Reconstruction error 17.790821
&gt;&gt; Epoch 12 finished 	RBM Reconstruction error 17.104307
&gt;&gt; Epoch 13 finished 	RBM Reconstruction error 16.414818
&gt;&gt; Epoch 14 finished 	RBM Reconstruction error 15.772669
&gt;&gt; Epoch 15 finished 	RBM Reconstruction error 15.287756
&gt;&gt; Epoch 16 finished 	RBM Reconstruction error 14.800046
&gt;&gt; Epoch 17 finished 	RBM Reconstruction error 14.310347
&gt;&gt; Epoch 18 finished 	RBM Reconstruction error 13.852696
&gt;&gt; Epoch 19 finished 	RBM Reconstruction error 13.496503
&gt;&gt; Epoch 20 finished 	RBM Reconstruction error 13.173393
&gt;&gt; Epoch 21 finished 	RBM Reconstruction error 12.813791
&gt;&gt; Epoch 22 finished 	RBM Reconstruction error 12.492845
&gt;&gt; Epoch 23 finished 	RBM Reconstruction error 12.198965
&gt;&gt; Epoch 24 finished 	RBM Reconstruction error 11.911744
&gt;&gt; Epoch 25 finished 	RBM Reconstruction error 11.605242
&gt;&gt; Epoch 26 finished 	RBM Reconstruction error 11.385957
&gt;&gt; Epoch 27 finished 	RBM Reconstruction error 11.180469
&gt;&gt; Epoch 28 finished 	RBM Reconstruction error 10.864896
&gt;&gt; Epoch 29 finished 	RBM Reconstruction error 10.740730
&gt;&gt; Epoch 30 finished 	RBM Reconstruction error 10.482827
&gt;&gt; Epoch 31 finished 	RBM Reconstruction error 10.279586
&gt;&gt; Epoch 32 finished 	RBM Reconstruction error 10.076273
&gt;&gt; Epoch 33 finished 	RBM Reconstruction error 9.907208
&gt;&gt; Epoch 34 finished 	RBM Reconstruction error 9.734780
&gt;&gt; Epoch 35 finished 	RBM Reconstruction error 9.549190
&gt;&gt; Epoch 36 finished 	RBM Reconstruction error 9.444825
&gt;&gt; Epoch 37 finished 	RBM Reconstruction error 9.257287
&gt;&gt; Epoch 38 finished 	RBM Reconstruction error 9.102218
&gt;&gt; Epoch 39 finished 	RBM Reconstruction error 8.930707
&gt;&gt; Epoch 40 finished 	RBM Reconstruction error 8.805639
[END] Pre-training step
[START] Fine tuning step:
&gt;&gt; Epoch 0 finished 	ANN training loss 1.115489
&gt;&gt; Epoch 1 finished 	ANN training loss 0.791461
&gt;&gt; Epoch 2 finished 	ANN training loss 0.643817
&gt;&gt; Epoch 3 finished 	ANN training loss 0.561672
&gt;&gt; Epoch 4 finished 	ANN training loss 0.529912
&gt;&gt; Epoch 5 finished 	ANN training loss 0.486901
&gt;&gt; Epoch 6 finished 	ANN training loss 0.461026
&gt;&gt; Epoch 7 finished 	ANN training loss 0.429949
&gt;&gt; Epoch 8 finished 	ANN training loss 0.418390
&gt;&gt; Epoch 9 finished 	ANN training loss 0.404743
&gt;&gt; Epoch 10 finished 	ANN training loss 0.387626
&gt;&gt; Epoch 11 finished 	ANN training loss 0.369497
&gt;&gt; Epoch 12 finished 	ANN training loss 0.368574
&gt;&gt; Epoch 13 finished 	ANN training loss 0.358239
&gt;&gt; Epoch 14 finished 	ANN training loss 0.346524
&gt;&gt; Epoch 15 finished 	ANN training loss 0.339551
&gt;&gt; Epoch 16 finished 	ANN training loss 0.337265
&gt;&gt; Epoch 17 finished 	ANN training loss 0.328813
&gt;&gt; Epoch 18 finished 	ANN training loss 0.319953
&gt;&gt; Epoch 19 finished 	ANN training loss 0.320939
&gt;&gt; Epoch 20 finished 	ANN training loss 0.310229
&gt;&gt; Epoch 21 finished 	ANN training loss 0.310573
&gt;&gt; Epoch 22 finished 	ANN training loss 0.309830
&gt;&gt; Epoch 23 finished 	ANN training loss 0.298436
&gt;&gt; Epoch 24 finished 	ANN training loss 0.295336
&gt;&gt; Epoch 25 finished 	ANN training loss 0.288047
&gt;&gt; Epoch 26 finished 	ANN training loss 0.287187
&gt;&gt; Epoch 27 finished 	ANN training loss 0.281451
&gt;&gt; Epoch 28 finished 	ANN training loss 0.276167
&gt;&gt; Epoch 29 finished 	ANN training loss 0.273731
&gt;&gt; Epoch 30 finished 	ANN training loss 0.267410
&gt;&gt; Epoch 31 finished 	ANN training loss 0.271180
&gt;&gt; Epoch 32 finished 	ANN training loss 0.265336
&gt;&gt; Epoch 33 finished 	ANN training loss 0.284411
&gt;&gt; Epoch 34 finished 	ANN training loss 0.258249
&gt;&gt; Epoch 35 finished 	ANN training loss 0.260898
&gt;&gt; Epoch 36 finished 	ANN training loss 0.252059
&gt;&gt; Epoch 37 finished 	ANN training loss 0.253712
&gt;&gt; Epoch 38 finished 	ANN training loss 0.245933
&gt;&gt; Epoch 39 finished 	ANN training loss 0.250076
&gt;&gt; Epoch 40 finished 	ANN training loss 0.253628
&gt;&gt; Epoch 41 finished 	ANN training loss 0.248063
&gt;&gt; Epoch 42 finished 	ANN training loss 0.241777
&gt;&gt; Epoch 43 finished 	ANN training loss 0.242915
&gt;&gt; Epoch 44 finished 	ANN training loss 0.235039
&gt;&gt; Epoch 45 finished 	ANN training loss 0.234279
&gt;&gt; Epoch 46 finished 	ANN training loss 0.231037
&gt;&gt; Epoch 47 finished 	ANN training loss 0.227532
&gt;&gt; Epoch 48 finished 	ANN training loss 0.233283
&gt;&gt; Epoch 49 finished 	ANN training loss 0.236004
&gt;&gt; Epoch 50 finished 	ANN training loss 0.219128
&gt;&gt; Epoch 51 finished 	ANN training loss 0.220457
&gt;&gt; Epoch 52 finished 	ANN training loss 0.219733
&gt;&gt; Epoch 53 finished 	ANN training loss 0.217143
&gt;&gt; Epoch 54 finished 	ANN training loss 0.213681
&gt;&gt; Epoch 55 finished 	ANN training loss 0.217661
&gt;&gt; Epoch 56 finished 	ANN training loss 0.213855
&gt;&gt; Epoch 57 finished 	ANN training loss 0.216073
&gt;&gt; Epoch 58 finished 	ANN training loss 0.204925
&gt;&gt; Epoch 59 finished 	ANN training loss 0.214120
&gt;&gt; Epoch 60 finished 	ANN training loss 0.204864
&gt;&gt; Epoch 61 finished 	ANN training loss 0.205037
&gt;&gt; Epoch 62 finished 	ANN training loss 0.200658
&gt;&gt; Epoch 63 finished 	ANN training loss 0.201439
&gt;&gt; Epoch 64 finished 	ANN training loss 0.201797
&gt;&gt; Epoch 65 finished 	ANN training loss 0.197796
&gt;&gt; Epoch 66 finished 	ANN training loss 0.201273
&gt;&gt; Epoch 67 finished 	ANN training loss 0.199129
&gt;&gt; Epoch 68 finished 	ANN training loss 0.191820
&gt;&gt; Epoch 69 finished 	ANN training loss 0.185820
&gt;&gt; Epoch 70 finished 	ANN training loss 0.191642
&gt;&gt; Epoch 71 finished 	ANN training loss 0.193530
&gt;&gt; Epoch 72 finished 	ANN training loss 0.201490
&gt;&gt; Epoch 73 finished 	ANN training loss 0.183174
&gt;&gt; Epoch 74 finished 	ANN training loss 0.191453
&gt;&gt; Epoch 75 finished 	ANN training loss 0.184355
&gt;&gt; Epoch 76 finished 	ANN training loss 0.177283
&gt;&gt; Epoch 77 finished 	ANN training loss 0.181185
&gt;&gt; Epoch 78 finished 	ANN training loss 0.175510
&gt;&gt; Epoch 79 finished 	ANN training loss 0.177267
&gt;&gt; Epoch 80 finished 	ANN training loss 0.168348
&gt;&gt; Epoch 81 finished 	ANN training loss 0.170192
&gt;&gt; Epoch 82 finished 	ANN training loss 0.171001
&gt;&gt; Epoch 83 finished 	ANN training loss 0.173320
&gt;&gt; Epoch 84 finished 	ANN training loss 0.173404
&gt;&gt; Epoch 85 finished 	ANN training loss 0.171942
&gt;&gt; Epoch 86 finished 	ANN training loss 0.171879
&gt;&gt; Epoch 87 finished 	ANN training loss 0.170637
&gt;&gt; Epoch 88 finished 	ANN training loss 0.166060
&gt;&gt; Epoch 89 finished 	ANN training loss 0.168460
&gt;&gt; Epoch 90 finished 	ANN training loss 0.161801
&gt;&gt; Epoch 91 finished 	ANN training loss 0.165759
&gt;&gt; Epoch 92 finished 	ANN training loss 0.162604
&gt;&gt; Epoch 93 finished 	ANN training loss 0.160371
&gt;&gt; Epoch 94 finished 	ANN training loss 0.156196
&gt;&gt; Epoch 95 finished 	ANN training loss 0.161891
&gt;&gt; Epoch 96 finished 	ANN training loss 0.157538
&gt;&gt; Epoch 97 finished 	ANN training loss 0.155994
&gt;&gt; Epoch 98 finished 	ANN training loss 0.159310
&gt;&gt; Epoch 99 finished 	ANN training loss 0.150435
&gt;&gt; Epoch 100 finished 	ANN training loss 0.152916
&gt;&gt; Epoch 101 finished 	ANN training loss 0.153213
&gt;&gt; Epoch 102 finished 	ANN training loss 0.154478
&gt;&gt; Epoch 103 finished 	ANN training loss 0.153958
&gt;&gt; Epoch 104 finished 	ANN training loss 0.144763
&gt;&gt; Epoch 105 finished 	ANN training loss 0.148127
&gt;&gt; Epoch 106 finished 	ANN training loss 0.146419
&gt;&gt; Epoch 107 finished 	ANN training loss 0.143481
&gt;&gt; Epoch 108 finished 	ANN training loss 0.141018
&gt;&gt; Epoch 109 finished 	ANN training loss 0.146728
&gt;&gt; Epoch 110 finished 	ANN training loss 0.140763
&gt;&gt; Epoch 111 finished 	ANN training loss 0.137469
&gt;&gt; Epoch 112 finished 	ANN training loss 0.139575
&gt;&gt; Epoch 113 finished 	ANN training loss 0.139502
&gt;&gt; Epoch 114 finished 	ANN training loss 0.137920
&gt;&gt; Epoch 115 finished 	ANN training loss 0.136513
&gt;&gt; Epoch 116 finished 	ANN training loss 0.145315
&gt;&gt; Epoch 117 finished 	ANN training loss 0.137122
&gt;&gt; Epoch 118 finished 	ANN training loss 0.138107
&gt;&gt; Epoch 119 finished 	ANN training loss 0.137007
&gt;&gt; Epoch 120 finished 	ANN training loss 0.134427
&gt;&gt; Epoch 121 finished 	ANN training loss 0.135621
&gt;&gt; Epoch 122 finished 	ANN training loss 0.131319
&gt;&gt; Epoch 123 finished 	ANN training loss 0.133787
&gt;&gt; Epoch 124 finished 	ANN training loss 0.131439
&gt;&gt; Epoch 125 finished 	ANN training loss 0.128671
&gt;&gt; Epoch 126 finished 	ANN training loss 0.134186
&gt;&gt; Epoch 127 finished 	ANN training loss 0.129938
&gt;&gt; Epoch 128 finished 	ANN training loss 0.127983
&gt;&gt; Epoch 129 finished 	ANN training loss 0.125656
&gt;&gt; Epoch 130 finished 	ANN training loss 0.126113
&gt;&gt; Epoch 131 finished 	ANN training loss 0.124972
&gt;&gt; Epoch 132 finished 	ANN training loss 0.123501
&gt;&gt; Epoch 133 finished 	ANN training loss 0.128233
&gt;&gt; Epoch 134 finished 	ANN training loss 0.122174
&gt;&gt; Epoch 135 finished 	ANN training loss 0.121157
&gt;&gt; Epoch 136 finished 	ANN training loss 0.123222
&gt;&gt; Epoch 137 finished 	ANN training loss 0.121419
&gt;&gt; Epoch 138 finished 	ANN training loss 0.123946
&gt;&gt; Epoch 139 finished 	ANN training loss 0.125110
&gt;&gt; Epoch 140 finished 	ANN training loss 0.118655
&gt;&gt; Epoch 141 finished 	ANN training loss 0.120244
&gt;&gt; Epoch 142 finished 	ANN training loss 0.117132
&gt;&gt; Epoch 143 finished 	ANN training loss 0.119045
&gt;&gt; Epoch 144 finished 	ANN training loss 0.122723
&gt;&gt; Epoch 145 finished 	ANN training loss 0.115598
&gt;&gt; Epoch 146 finished 	ANN training loss 0.118322
&gt;&gt; Epoch 147 finished 	ANN training loss 0.112165
&gt;&gt; Epoch 148 finished 	ANN training loss 0.111550
&gt;&gt; Epoch 149 finished 	ANN training loss 0.114756
&gt;&gt; Epoch 150 finished 	ANN training loss 0.111263
&gt;&gt; Epoch 151 finished 	ANN training loss 0.111981
&gt;&gt; Epoch 152 finished 	ANN training loss 0.112909
&gt;&gt; Epoch 153 finished 	ANN training loss 0.112969
&gt;&gt; Epoch 154 finished 	ANN training loss 0.109700
&gt;&gt; Epoch 155 finished 	ANN training loss 0.107204
&gt;&gt; Epoch 156 finished 	ANN training loss 0.107628
&gt;&gt; Epoch 157 finished 	ANN training loss 0.107137
&gt;&gt; Epoch 158 finished 	ANN training loss 0.108847
&gt;&gt; Epoch 159 finished 	ANN training loss 0.107382
&gt;&gt; Epoch 160 finished 	ANN training loss 0.108037
&gt;&gt; Epoch 161 finished 	ANN training loss 0.106086
&gt;&gt; Epoch 162 finished 	ANN training loss 0.106475
&gt;&gt; Epoch 163 finished 	ANN training loss 0.102585
&gt;&gt; Epoch 164 finished 	ANN training loss 0.104679
&gt;&gt; Epoch 165 finished 	ANN training loss 0.104645
&gt;&gt; Epoch 166 finished 	ANN training loss 0.106817
&gt;&gt; Epoch 167 finished 	ANN training loss 0.106777
&gt;&gt; Epoch 168 finished 	ANN training loss 0.103360
&gt;&gt; Epoch 169 finished 	ANN training loss 0.104491
&gt;&gt; Epoch 170 finished 	ANN training loss 0.104175
&gt;&gt; Epoch 171 finished 	ANN training loss 0.102827
&gt;&gt; Epoch 172 finished 	ANN training loss 0.099703
&gt;&gt; Epoch 173 finished 	ANN training loss 0.098482
&gt;&gt; Epoch 174 finished 	ANN training loss 0.098762
&gt;&gt; Epoch 175 finished 	ANN training loss 0.097532
&gt;&gt; Epoch 176 finished 	ANN training loss 0.098422
&gt;&gt; Epoch 177 finished 	ANN training loss 0.095419
&gt;&gt; Epoch 178 finished 	ANN training loss 0.094636
&gt;&gt; Epoch 179 finished 	ANN training loss 0.095054
&gt;&gt; Epoch 180 finished 	ANN training loss 0.095446
&gt;&gt; Epoch 181 finished 	ANN training loss 0.094544
&gt;&gt; Epoch 182 finished 	ANN training loss 0.093373
&gt;&gt; Epoch 183 finished 	ANN training loss 0.093241
&gt;&gt; Epoch 184 finished 	ANN training loss 0.092431
&gt;&gt; Epoch 185 finished 	ANN training loss 0.090068
&gt;&gt; Epoch 186 finished 	ANN training loss 0.092778
&gt;&gt; Epoch 187 finished 	ANN training loss 0.090008
&gt;&gt; Epoch 188 finished 	ANN training loss 0.091968
&gt;&gt; Epoch 189 finished 	ANN training loss 0.090693
&gt;&gt; Epoch 190 finished 	ANN training loss 0.092100
&gt;&gt; Epoch 191 finished 	ANN training loss 0.087732
&gt;&gt; Epoch 192 finished 	ANN training loss 0.093978
&gt;&gt; Epoch 193 finished 	ANN training loss 0.089193
&gt;&gt; Epoch 194 finished 	ANN training loss 0.088208
&gt;&gt; Epoch 195 finished 	ANN training loss 0.086890
&gt;&gt; Epoch 196 finished 	ANN training loss 0.086318
&gt;&gt; Epoch 197 finished 	ANN training loss 0.083534
&gt;&gt; Epoch 198 finished 	ANN training loss 0.087466
&gt;&gt; Epoch 199 finished 	ANN training loss 0.086072
[END] Fine tuning step
Done.
Accuracy: 0.880000
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[30]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ACCURACY: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">acc7</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>ACCURACY: 0.88
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[31]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Sigmoidal activation, dropout=0.7</span>
<span class="n">acc8</span> <span class="o">=</span> <span class="n">deep_belief_net</span><span class="p">(</span><span class="n">hidden_layers_structure</span><span class="o">=</span><span class="p">[</span><span class="mi">300</span><span class="p">],</span> 
                       <span class="n">learning_rate_rbm</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span>
                       <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                       <span class="n">n_epochs_rbm</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span>
                       <span class="n">n_iter_backprop</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
                       <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
                       <span class="n">activation_function</span><span class="o">=</span><span class="s1">&#39;sigmoid&#39;</span><span class="p">,</span>
                       <span class="n">dropout_p</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[START] Pre-training step:
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 45.129498
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 35.027809
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 30.016743
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 27.039062
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 24.958534
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 23.295965
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 22.010513
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 20.783161
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 19.772827
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 18.897926
&gt;&gt; Epoch 11 finished 	RBM Reconstruction error 18.084539
&gt;&gt; Epoch 12 finished 	RBM Reconstruction error 17.287664
&gt;&gt; Epoch 13 finished 	RBM Reconstruction error 16.526663
&gt;&gt; Epoch 14 finished 	RBM Reconstruction error 15.959409
&gt;&gt; Epoch 15 finished 	RBM Reconstruction error 15.371964
&gt;&gt; Epoch 16 finished 	RBM Reconstruction error 14.881211
&gt;&gt; Epoch 17 finished 	RBM Reconstruction error 14.398421
&gt;&gt; Epoch 18 finished 	RBM Reconstruction error 13.998322
&gt;&gt; Epoch 19 finished 	RBM Reconstruction error 13.635403
&gt;&gt; Epoch 20 finished 	RBM Reconstruction error 13.256783
&gt;&gt; Epoch 21 finished 	RBM Reconstruction error 12.907571
&gt;&gt; Epoch 22 finished 	RBM Reconstruction error 12.537066
&gt;&gt; Epoch 23 finished 	RBM Reconstruction error 12.248904
&gt;&gt; Epoch 24 finished 	RBM Reconstruction error 11.962505
&gt;&gt; Epoch 25 finished 	RBM Reconstruction error 11.719161
&gt;&gt; Epoch 26 finished 	RBM Reconstruction error 11.460996
&gt;&gt; Epoch 27 finished 	RBM Reconstruction error 11.195226
&gt;&gt; Epoch 28 finished 	RBM Reconstruction error 10.997808
&gt;&gt; Epoch 29 finished 	RBM Reconstruction error 10.800192
&gt;&gt; Epoch 30 finished 	RBM Reconstruction error 10.568363
&gt;&gt; Epoch 31 finished 	RBM Reconstruction error 10.428193
&gt;&gt; Epoch 32 finished 	RBM Reconstruction error 10.189788
&gt;&gt; Epoch 33 finished 	RBM Reconstruction error 10.028370
&gt;&gt; Epoch 34 finished 	RBM Reconstruction error 9.871050
&gt;&gt; Epoch 35 finished 	RBM Reconstruction error 9.679565
&gt;&gt; Epoch 36 finished 	RBM Reconstruction error 9.528945
&gt;&gt; Epoch 37 finished 	RBM Reconstruction error 9.363136
&gt;&gt; Epoch 38 finished 	RBM Reconstruction error 9.186206
&gt;&gt; Epoch 39 finished 	RBM Reconstruction error 9.027005
&gt;&gt; Epoch 40 finished 	RBM Reconstruction error 8.889925
[END] Pre-training step
[START] Fine tuning step:
&gt;&gt; Epoch 0 finished 	ANN training loss 1.168265
&gt;&gt; Epoch 1 finished 	ANN training loss 0.817597
&gt;&gt; Epoch 2 finished 	ANN training loss 0.705631
&gt;&gt; Epoch 3 finished 	ANN training loss 0.611002
&gt;&gt; Epoch 4 finished 	ANN training loss 0.566429
&gt;&gt; Epoch 5 finished 	ANN training loss 0.555223
&gt;&gt; Epoch 6 finished 	ANN training loss 0.538432
&gt;&gt; Epoch 7 finished 	ANN training loss 0.491775
&gt;&gt; Epoch 8 finished 	ANN training loss 0.493121
&gt;&gt; Epoch 9 finished 	ANN training loss 0.455786
&gt;&gt; Epoch 10 finished 	ANN training loss 0.451377
&gt;&gt; Epoch 11 finished 	ANN training loss 0.443678
&gt;&gt; Epoch 12 finished 	ANN training loss 0.447752
&gt;&gt; Epoch 13 finished 	ANN training loss 0.409869
&gt;&gt; Epoch 14 finished 	ANN training loss 0.416259
&gt;&gt; Epoch 15 finished 	ANN training loss 0.403333
&gt;&gt; Epoch 16 finished 	ANN training loss 0.406020
&gt;&gt; Epoch 17 finished 	ANN training loss 0.402217
&gt;&gt; Epoch 18 finished 	ANN training loss 0.386973
&gt;&gt; Epoch 19 finished 	ANN training loss 0.392831
&gt;&gt; Epoch 20 finished 	ANN training loss 0.372542
&gt;&gt; Epoch 21 finished 	ANN training loss 0.367313
&gt;&gt; Epoch 22 finished 	ANN training loss 0.372490
&gt;&gt; Epoch 23 finished 	ANN training loss 0.365962
&gt;&gt; Epoch 24 finished 	ANN training loss 0.374955
&gt;&gt; Epoch 25 finished 	ANN training loss 0.366566
&gt;&gt; Epoch 26 finished 	ANN training loss 0.348136
&gt;&gt; Epoch 27 finished 	ANN training loss 0.357677
&gt;&gt; Epoch 28 finished 	ANN training loss 0.340200
&gt;&gt; Epoch 29 finished 	ANN training loss 0.355988
&gt;&gt; Epoch 30 finished 	ANN training loss 0.340412
&gt;&gt; Epoch 31 finished 	ANN training loss 0.341756
&gt;&gt; Epoch 32 finished 	ANN training loss 0.333226
&gt;&gt; Epoch 33 finished 	ANN training loss 0.340298
&gt;&gt; Epoch 34 finished 	ANN training loss 0.321034
&gt;&gt; Epoch 35 finished 	ANN training loss 0.333012
&gt;&gt; Epoch 36 finished 	ANN training loss 0.343355
&gt;&gt; Epoch 37 finished 	ANN training loss 0.329214
&gt;&gt; Epoch 38 finished 	ANN training loss 0.319972
&gt;&gt; Epoch 39 finished 	ANN training loss 0.316643
&gt;&gt; Epoch 40 finished 	ANN training loss 0.302144
&gt;&gt; Epoch 41 finished 	ANN training loss 0.309066
&gt;&gt; Epoch 42 finished 	ANN training loss 0.305790
&gt;&gt; Epoch 43 finished 	ANN training loss 0.309620
&gt;&gt; Epoch 44 finished 	ANN training loss 0.297511
&gt;&gt; Epoch 45 finished 	ANN training loss 0.298379
&gt;&gt; Epoch 46 finished 	ANN training loss 0.300215
&gt;&gt; Epoch 47 finished 	ANN training loss 0.302717
&gt;&gt; Epoch 48 finished 	ANN training loss 0.299086
&gt;&gt; Epoch 49 finished 	ANN training loss 0.293210
&gt;&gt; Epoch 50 finished 	ANN training loss 0.299337
&gt;&gt; Epoch 51 finished 	ANN training loss 0.291601
&gt;&gt; Epoch 52 finished 	ANN training loss 0.286936
&gt;&gt; Epoch 53 finished 	ANN training loss 0.284368
&gt;&gt; Epoch 54 finished 	ANN training loss 0.291358
&gt;&gt; Epoch 55 finished 	ANN training loss 0.280798
&gt;&gt; Epoch 56 finished 	ANN training loss 0.284360
&gt;&gt; Epoch 57 finished 	ANN training loss 0.287753
&gt;&gt; Epoch 58 finished 	ANN training loss 0.283120
&gt;&gt; Epoch 59 finished 	ANN training loss 0.277539
&gt;&gt; Epoch 60 finished 	ANN training loss 0.288471
&gt;&gt; Epoch 61 finished 	ANN training loss 0.278024
&gt;&gt; Epoch 62 finished 	ANN training loss 0.277743
&gt;&gt; Epoch 63 finished 	ANN training loss 0.277031
&gt;&gt; Epoch 64 finished 	ANN training loss 0.283207
&gt;&gt; Epoch 65 finished 	ANN training loss 0.272383
&gt;&gt; Epoch 66 finished 	ANN training loss 0.278617
&gt;&gt; Epoch 67 finished 	ANN training loss 0.268871
&gt;&gt; Epoch 68 finished 	ANN training loss 0.263037
&gt;&gt; Epoch 69 finished 	ANN training loss 0.271641
&gt;&gt; Epoch 70 finished 	ANN training loss 0.267790
&gt;&gt; Epoch 71 finished 	ANN training loss 0.266842
&gt;&gt; Epoch 72 finished 	ANN training loss 0.259844
&gt;&gt; Epoch 73 finished 	ANN training loss 0.261224
&gt;&gt; Epoch 74 finished 	ANN training loss 0.266322
&gt;&gt; Epoch 75 finished 	ANN training loss 0.255454
&gt;&gt; Epoch 76 finished 	ANN training loss 0.259321
&gt;&gt; Epoch 77 finished 	ANN training loss 0.251795
&gt;&gt; Epoch 78 finished 	ANN training loss 0.257515
&gt;&gt; Epoch 79 finished 	ANN training loss 0.251228
&gt;&gt; Epoch 80 finished 	ANN training loss 0.255615
&gt;&gt; Epoch 81 finished 	ANN training loss 0.252230
&gt;&gt; Epoch 82 finished 	ANN training loss 0.252102
&gt;&gt; Epoch 83 finished 	ANN training loss 0.244738
&gt;&gt; Epoch 84 finished 	ANN training loss 0.242152
&gt;&gt; Epoch 85 finished 	ANN training loss 0.246067
&gt;&gt; Epoch 86 finished 	ANN training loss 0.243291
&gt;&gt; Epoch 87 finished 	ANN training loss 0.248281
&gt;&gt; Epoch 88 finished 	ANN training loss 0.240058
&gt;&gt; Epoch 89 finished 	ANN training loss 0.239271
&gt;&gt; Epoch 90 finished 	ANN training loss 0.238568
&gt;&gt; Epoch 91 finished 	ANN training loss 0.237210
&gt;&gt; Epoch 92 finished 	ANN training loss 0.252273
&gt;&gt; Epoch 93 finished 	ANN training loss 0.246739
&gt;&gt; Epoch 94 finished 	ANN training loss 0.238642
&gt;&gt; Epoch 95 finished 	ANN training loss 0.239068
&gt;&gt; Epoch 96 finished 	ANN training loss 0.231104
&gt;&gt; Epoch 97 finished 	ANN training loss 0.234601
&gt;&gt; Epoch 98 finished 	ANN training loss 0.242512
&gt;&gt; Epoch 99 finished 	ANN training loss 0.228658
&gt;&gt; Epoch 100 finished 	ANN training loss 0.234066
&gt;&gt; Epoch 101 finished 	ANN training loss 0.236769
&gt;&gt; Epoch 102 finished 	ANN training loss 0.225873
&gt;&gt; Epoch 103 finished 	ANN training loss 0.228414
&gt;&gt; Epoch 104 finished 	ANN training loss 0.225910
&gt;&gt; Epoch 105 finished 	ANN training loss 0.225697
&gt;&gt; Epoch 106 finished 	ANN training loss 0.223208
&gt;&gt; Epoch 107 finished 	ANN training loss 0.221879
&gt;&gt; Epoch 108 finished 	ANN training loss 0.221763
&gt;&gt; Epoch 109 finished 	ANN training loss 0.222117
&gt;&gt; Epoch 110 finished 	ANN training loss 0.220784
&gt;&gt; Epoch 111 finished 	ANN training loss 0.227739
&gt;&gt; Epoch 112 finished 	ANN training loss 0.215243
&gt;&gt; Epoch 113 finished 	ANN training loss 0.226366
&gt;&gt; Epoch 114 finished 	ANN training loss 0.214206
&gt;&gt; Epoch 115 finished 	ANN training loss 0.220646
&gt;&gt; Epoch 116 finished 	ANN training loss 0.216523
&gt;&gt; Epoch 117 finished 	ANN training loss 0.212300
&gt;&gt; Epoch 118 finished 	ANN training loss 0.209034
&gt;&gt; Epoch 119 finished 	ANN training loss 0.213692
&gt;&gt; Epoch 120 finished 	ANN training loss 0.213162
&gt;&gt; Epoch 121 finished 	ANN training loss 0.211234
&gt;&gt; Epoch 122 finished 	ANN training loss 0.215013
&gt;&gt; Epoch 123 finished 	ANN training loss 0.218264
&gt;&gt; Epoch 124 finished 	ANN training loss 0.209774
&gt;&gt; Epoch 125 finished 	ANN training loss 0.208416
&gt;&gt; Epoch 126 finished 	ANN training loss 0.204336
&gt;&gt; Epoch 127 finished 	ANN training loss 0.208802
&gt;&gt; Epoch 128 finished 	ANN training loss 0.202960
&gt;&gt; Epoch 129 finished 	ANN training loss 0.208405
&gt;&gt; Epoch 130 finished 	ANN training loss 0.203195
&gt;&gt; Epoch 131 finished 	ANN training loss 0.199603
&gt;&gt; Epoch 132 finished 	ANN training loss 0.204918
&gt;&gt; Epoch 133 finished 	ANN training loss 0.203853
&gt;&gt; Epoch 134 finished 	ANN training loss 0.197251
&gt;&gt; Epoch 135 finished 	ANN training loss 0.197964
&gt;&gt; Epoch 136 finished 	ANN training loss 0.201631
&gt;&gt; Epoch 137 finished 	ANN training loss 0.199562
&gt;&gt; Epoch 138 finished 	ANN training loss 0.195648
&gt;&gt; Epoch 139 finished 	ANN training loss 0.194599
&gt;&gt; Epoch 140 finished 	ANN training loss 0.194024
&gt;&gt; Epoch 141 finished 	ANN training loss 0.189902
&gt;&gt; Epoch 142 finished 	ANN training loss 0.190317
&gt;&gt; Epoch 143 finished 	ANN training loss 0.198279
&gt;&gt; Epoch 144 finished 	ANN training loss 0.191549
&gt;&gt; Epoch 145 finished 	ANN training loss 0.190931
&gt;&gt; Epoch 146 finished 	ANN training loss 0.190822
&gt;&gt; Epoch 147 finished 	ANN training loss 0.190905
&gt;&gt; Epoch 148 finished 	ANN training loss 0.190692
&gt;&gt; Epoch 149 finished 	ANN training loss 0.190779
&gt;&gt; Epoch 150 finished 	ANN training loss 0.190282
&gt;&gt; Epoch 151 finished 	ANN training loss 0.186290
&gt;&gt; Epoch 152 finished 	ANN training loss 0.183520
&gt;&gt; Epoch 153 finished 	ANN training loss 0.185414
&gt;&gt; Epoch 154 finished 	ANN training loss 0.182725
&gt;&gt; Epoch 155 finished 	ANN training loss 0.180402
&gt;&gt; Epoch 156 finished 	ANN training loss 0.182977
&gt;&gt; Epoch 157 finished 	ANN training loss 0.180846
&gt;&gt; Epoch 158 finished 	ANN training loss 0.179465
&gt;&gt; Epoch 159 finished 	ANN training loss 0.187546
&gt;&gt; Epoch 160 finished 	ANN training loss 0.183646
&gt;&gt; Epoch 161 finished 	ANN training loss 0.180903
&gt;&gt; Epoch 162 finished 	ANN training loss 0.187745
&gt;&gt; Epoch 163 finished 	ANN training loss 0.186265
&gt;&gt; Epoch 164 finished 	ANN training loss 0.184046
&gt;&gt; Epoch 165 finished 	ANN training loss 0.178230
&gt;&gt; Epoch 166 finished 	ANN training loss 0.184532
&gt;&gt; Epoch 167 finished 	ANN training loss 0.178035
&gt;&gt; Epoch 168 finished 	ANN training loss 0.177425
&gt;&gt; Epoch 169 finished 	ANN training loss 0.174099
&gt;&gt; Epoch 170 finished 	ANN training loss 0.177033
&gt;&gt; Epoch 171 finished 	ANN training loss 0.179848
&gt;&gt; Epoch 172 finished 	ANN training loss 0.176363
&gt;&gt; Epoch 173 finished 	ANN training loss 0.174290
&gt;&gt; Epoch 174 finished 	ANN training loss 0.172699
&gt;&gt; Epoch 175 finished 	ANN training loss 0.177080
&gt;&gt; Epoch 176 finished 	ANN training loss 0.176676
&gt;&gt; Epoch 177 finished 	ANN training loss 0.172992
&gt;&gt; Epoch 178 finished 	ANN training loss 0.174406
&gt;&gt; Epoch 179 finished 	ANN training loss 0.172644
&gt;&gt; Epoch 180 finished 	ANN training loss 0.171962
&gt;&gt; Epoch 181 finished 	ANN training loss 0.170383
&gt;&gt; Epoch 182 finished 	ANN training loss 0.169441
&gt;&gt; Epoch 183 finished 	ANN training loss 0.171784
&gt;&gt; Epoch 184 finished 	ANN training loss 0.172497
&gt;&gt; Epoch 185 finished 	ANN training loss 0.172056
&gt;&gt; Epoch 186 finished 	ANN training loss 0.166534
&gt;&gt; Epoch 187 finished 	ANN training loss 0.165108
&gt;&gt; Epoch 188 finished 	ANN training loss 0.173310
&gt;&gt; Epoch 189 finished 	ANN training loss 0.169985
&gt;&gt; Epoch 190 finished 	ANN training loss 0.166613
&gt;&gt; Epoch 191 finished 	ANN training loss 0.165692
&gt;&gt; Epoch 192 finished 	ANN training loss 0.165352
&gt;&gt; Epoch 193 finished 	ANN training loss 0.165666
&gt;&gt; Epoch 194 finished 	ANN training loss 0.165380
&gt;&gt; Epoch 195 finished 	ANN training loss 0.162680
&gt;&gt; Epoch 196 finished 	ANN training loss 0.162424
&gt;&gt; Epoch 197 finished 	ANN training loss 0.163724
&gt;&gt; Epoch 198 finished 	ANN training loss 0.159991
&gt;&gt; Epoch 199 finished 	ANN training loss 0.159692
[END] Fine tuning step
Done.
Accuracy: 0.850000
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[32]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ACCURACY: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">acc8</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>ACCURACY: 0.85
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[33]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Sigmoidal activation, dropout=0.8</span>
<span class="n">acc9</span> <span class="o">=</span> <span class="n">deep_belief_net</span><span class="p">(</span><span class="n">hidden_layers_structure</span><span class="o">=</span><span class="p">[</span><span class="mi">300</span><span class="p">],</span> 
                       <span class="n">learning_rate_rbm</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span>
                       <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                       <span class="n">n_epochs_rbm</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span>
                       <span class="n">n_iter_backprop</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
                       <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
                       <span class="n">activation_function</span><span class="o">=</span><span class="s1">&#39;sigmoid&#39;</span><span class="p">,</span>
                       <span class="n">dropout_p</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[START] Pre-training step:
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 44.051876
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 35.091831
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 30.071026
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 26.948423
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 24.779713
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 23.062523
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 21.873995
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 20.669756
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 19.651505
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 18.680679
&gt;&gt; Epoch 11 finished 	RBM Reconstruction error 17.984848
&gt;&gt; Epoch 12 finished 	RBM Reconstruction error 17.155359
&gt;&gt; Epoch 13 finished 	RBM Reconstruction error 16.523981
&gt;&gt; Epoch 14 finished 	RBM Reconstruction error 15.846703
&gt;&gt; Epoch 15 finished 	RBM Reconstruction error 15.326969
&gt;&gt; Epoch 16 finished 	RBM Reconstruction error 14.814431
&gt;&gt; Epoch 17 finished 	RBM Reconstruction error 14.364479
&gt;&gt; Epoch 18 finished 	RBM Reconstruction error 13.926455
&gt;&gt; Epoch 19 finished 	RBM Reconstruction error 13.601260
&gt;&gt; Epoch 20 finished 	RBM Reconstruction error 13.149766
&gt;&gt; Epoch 21 finished 	RBM Reconstruction error 12.794353
&gt;&gt; Epoch 22 finished 	RBM Reconstruction error 12.522580
&gt;&gt; Epoch 23 finished 	RBM Reconstruction error 12.163539
&gt;&gt; Epoch 24 finished 	RBM Reconstruction error 11.921522
&gt;&gt; Epoch 25 finished 	RBM Reconstruction error 11.649753
&gt;&gt; Epoch 26 finished 	RBM Reconstruction error 11.368609
&gt;&gt; Epoch 27 finished 	RBM Reconstruction error 11.125876
&gt;&gt; Epoch 28 finished 	RBM Reconstruction error 10.889465
&gt;&gt; Epoch 29 finished 	RBM Reconstruction error 10.717951
&gt;&gt; Epoch 30 finished 	RBM Reconstruction error 10.532270
&gt;&gt; Epoch 31 finished 	RBM Reconstruction error 10.301174
&gt;&gt; Epoch 32 finished 	RBM Reconstruction error 10.070171
&gt;&gt; Epoch 33 finished 	RBM Reconstruction error 9.890275
&gt;&gt; Epoch 34 finished 	RBM Reconstruction error 9.736781
&gt;&gt; Epoch 35 finished 	RBM Reconstruction error 9.579923
&gt;&gt; Epoch 36 finished 	RBM Reconstruction error 9.400433
&gt;&gt; Epoch 37 finished 	RBM Reconstruction error 9.264004
&gt;&gt; Epoch 38 finished 	RBM Reconstruction error 9.137755
&gt;&gt; Epoch 39 finished 	RBM Reconstruction error 8.970227
&gt;&gt; Epoch 40 finished 	RBM Reconstruction error 8.831291
[END] Pre-training step
[START] Fine tuning step:
&gt;&gt; Epoch 0 finished 	ANN training loss 1.197311
&gt;&gt; Epoch 1 finished 	ANN training loss 0.879009
&gt;&gt; Epoch 2 finished 	ANN training loss 0.786728
&gt;&gt; Epoch 3 finished 	ANN training loss 0.746337
&gt;&gt; Epoch 4 finished 	ANN training loss 0.663383
&gt;&gt; Epoch 5 finished 	ANN training loss 0.650929
&gt;&gt; Epoch 6 finished 	ANN training loss 0.608713
&gt;&gt; Epoch 7 finished 	ANN training loss 0.616611
&gt;&gt; Epoch 8 finished 	ANN training loss 0.570545
&gt;&gt; Epoch 9 finished 	ANN training loss 0.548444
&gt;&gt; Epoch 10 finished 	ANN training loss 0.566834
&gt;&gt; Epoch 11 finished 	ANN training loss 0.543822
&gt;&gt; Epoch 12 finished 	ANN training loss 0.535848
&gt;&gt; Epoch 13 finished 	ANN training loss 0.523275
&gt;&gt; Epoch 14 finished 	ANN training loss 0.546479
&gt;&gt; Epoch 15 finished 	ANN training loss 0.548565
&gt;&gt; Epoch 16 finished 	ANN training loss 0.505017
&gt;&gt; Epoch 17 finished 	ANN training loss 0.504105
&gt;&gt; Epoch 18 finished 	ANN training loss 0.510965
&gt;&gt; Epoch 19 finished 	ANN training loss 0.484821
&gt;&gt; Epoch 20 finished 	ANN training loss 0.505967
&gt;&gt; Epoch 21 finished 	ANN training loss 0.497168
&gt;&gt; Epoch 22 finished 	ANN training loss 0.501077
&gt;&gt; Epoch 23 finished 	ANN training loss 0.491682
&gt;&gt; Epoch 24 finished 	ANN training loss 0.489566
&gt;&gt; Epoch 25 finished 	ANN training loss 0.495663
&gt;&gt; Epoch 26 finished 	ANN training loss 0.468524
&gt;&gt; Epoch 27 finished 	ANN training loss 0.489247
&gt;&gt; Epoch 28 finished 	ANN training loss 0.473931
&gt;&gt; Epoch 29 finished 	ANN training loss 0.486811
&gt;&gt; Epoch 30 finished 	ANN training loss 0.501228
&gt;&gt; Epoch 31 finished 	ANN training loss 0.481001
&gt;&gt; Epoch 32 finished 	ANN training loss 0.486516
&gt;&gt; Epoch 33 finished 	ANN training loss 0.479438
&gt;&gt; Epoch 34 finished 	ANN training loss 0.480938
&gt;&gt; Epoch 35 finished 	ANN training loss 0.459002
&gt;&gt; Epoch 36 finished 	ANN training loss 0.464196
&gt;&gt; Epoch 37 finished 	ANN training loss 0.453129
&gt;&gt; Epoch 38 finished 	ANN training loss 0.468088
&gt;&gt; Epoch 39 finished 	ANN training loss 0.448380
&gt;&gt; Epoch 40 finished 	ANN training loss 0.454965
&gt;&gt; Epoch 41 finished 	ANN training loss 0.434930
&gt;&gt; Epoch 42 finished 	ANN training loss 0.442347
&gt;&gt; Epoch 43 finished 	ANN training loss 0.457862
&gt;&gt; Epoch 44 finished 	ANN training loss 0.449341
&gt;&gt; Epoch 45 finished 	ANN training loss 0.435763
&gt;&gt; Epoch 46 finished 	ANN training loss 0.434142
&gt;&gt; Epoch 47 finished 	ANN training loss 0.434923
&gt;&gt; Epoch 48 finished 	ANN training loss 0.436819
&gt;&gt; Epoch 49 finished 	ANN training loss 0.434249
&gt;&gt; Epoch 50 finished 	ANN training loss 0.430929
&gt;&gt; Epoch 51 finished 	ANN training loss 0.420766
&gt;&gt; Epoch 52 finished 	ANN training loss 0.455648
&gt;&gt; Epoch 53 finished 	ANN training loss 0.430054
&gt;&gt; Epoch 54 finished 	ANN training loss 0.426654
&gt;&gt; Epoch 55 finished 	ANN training loss 0.426639
&gt;&gt; Epoch 56 finished 	ANN training loss 0.459979
&gt;&gt; Epoch 57 finished 	ANN training loss 0.436670
&gt;&gt; Epoch 58 finished 	ANN training loss 0.456835
&gt;&gt; Epoch 59 finished 	ANN training loss 0.419081
&gt;&gt; Epoch 60 finished 	ANN training loss 0.423072
&gt;&gt; Epoch 61 finished 	ANN training loss 0.425299
&gt;&gt; Epoch 62 finished 	ANN training loss 0.416078
&gt;&gt; Epoch 63 finished 	ANN training loss 0.426492
&gt;&gt; Epoch 64 finished 	ANN training loss 0.419595
&gt;&gt; Epoch 65 finished 	ANN training loss 0.410276
&gt;&gt; Epoch 66 finished 	ANN training loss 0.422648
&gt;&gt; Epoch 67 finished 	ANN training loss 0.433058
&gt;&gt; Epoch 68 finished 	ANN training loss 0.421340
&gt;&gt; Epoch 69 finished 	ANN training loss 0.422265
&gt;&gt; Epoch 70 finished 	ANN training loss 0.420131
&gt;&gt; Epoch 71 finished 	ANN training loss 0.420609
&gt;&gt; Epoch 72 finished 	ANN training loss 0.417104
&gt;&gt; Epoch 73 finished 	ANN training loss 0.418925
&gt;&gt; Epoch 74 finished 	ANN training loss 0.407185
&gt;&gt; Epoch 75 finished 	ANN training loss 0.406672
&gt;&gt; Epoch 76 finished 	ANN training loss 0.405924
&gt;&gt; Epoch 77 finished 	ANN training loss 0.412633
&gt;&gt; Epoch 78 finished 	ANN training loss 0.404423
&gt;&gt; Epoch 79 finished 	ANN training loss 0.402863
&gt;&gt; Epoch 80 finished 	ANN training loss 0.398761
&gt;&gt; Epoch 81 finished 	ANN training loss 0.393052
&gt;&gt; Epoch 82 finished 	ANN training loss 0.398918
&gt;&gt; Epoch 83 finished 	ANN training loss 0.398103
&gt;&gt; Epoch 84 finished 	ANN training loss 0.400057
&gt;&gt; Epoch 85 finished 	ANN training loss 0.396988
&gt;&gt; Epoch 86 finished 	ANN training loss 0.389918
&gt;&gt; Epoch 87 finished 	ANN training loss 0.393875
&gt;&gt; Epoch 88 finished 	ANN training loss 0.384677
&gt;&gt; Epoch 89 finished 	ANN training loss 0.396135
&gt;&gt; Epoch 90 finished 	ANN training loss 0.388414
&gt;&gt; Epoch 91 finished 	ANN training loss 0.386503
&gt;&gt; Epoch 92 finished 	ANN training loss 0.387332
&gt;&gt; Epoch 93 finished 	ANN training loss 0.388569
&gt;&gt; Epoch 94 finished 	ANN training loss 0.402984
&gt;&gt; Epoch 95 finished 	ANN training loss 0.393824
&gt;&gt; Epoch 96 finished 	ANN training loss 0.397785
&gt;&gt; Epoch 97 finished 	ANN training loss 0.390353
&gt;&gt; Epoch 98 finished 	ANN training loss 0.400741
&gt;&gt; Epoch 99 finished 	ANN training loss 0.399407
&gt;&gt; Epoch 100 finished 	ANN training loss 0.397126
&gt;&gt; Epoch 101 finished 	ANN training loss 0.388735
&gt;&gt; Epoch 102 finished 	ANN training loss 0.387563
&gt;&gt; Epoch 103 finished 	ANN training loss 0.385610
&gt;&gt; Epoch 104 finished 	ANN training loss 0.400191
&gt;&gt; Epoch 105 finished 	ANN training loss 0.396197
&gt;&gt; Epoch 106 finished 	ANN training loss 0.402334
&gt;&gt; Epoch 107 finished 	ANN training loss 0.414529
&gt;&gt; Epoch 108 finished 	ANN training loss 0.401707
&gt;&gt; Epoch 109 finished 	ANN training loss 0.385221
&gt;&gt; Epoch 110 finished 	ANN training loss 0.378387
&gt;&gt; Epoch 111 finished 	ANN training loss 0.373294
&gt;&gt; Epoch 112 finished 	ANN training loss 0.393576
&gt;&gt; Epoch 113 finished 	ANN training loss 0.374799
&gt;&gt; Epoch 114 finished 	ANN training loss 0.385533
&gt;&gt; Epoch 115 finished 	ANN training loss 0.382824
&gt;&gt; Epoch 116 finished 	ANN training loss 0.373538
&gt;&gt; Epoch 117 finished 	ANN training loss 0.384861
&gt;&gt; Epoch 118 finished 	ANN training loss 0.378914
&gt;&gt; Epoch 119 finished 	ANN training loss 0.376635
&gt;&gt; Epoch 120 finished 	ANN training loss 0.380071
&gt;&gt; Epoch 121 finished 	ANN training loss 0.393818
&gt;&gt; Epoch 122 finished 	ANN training loss 0.374101
&gt;&gt; Epoch 123 finished 	ANN training loss 0.372263
&gt;&gt; Epoch 124 finished 	ANN training loss 0.375740
&gt;&gt; Epoch 125 finished 	ANN training loss 0.362361
&gt;&gt; Epoch 126 finished 	ANN training loss 0.373870
&gt;&gt; Epoch 127 finished 	ANN training loss 0.376361
&gt;&gt; Epoch 128 finished 	ANN training loss 0.370081
&gt;&gt; Epoch 129 finished 	ANN training loss 0.373316
&gt;&gt; Epoch 130 finished 	ANN training loss 0.371415
&gt;&gt; Epoch 131 finished 	ANN training loss 0.371890
&gt;&gt; Epoch 132 finished 	ANN training loss 0.373026
&gt;&gt; Epoch 133 finished 	ANN training loss 0.372642
&gt;&gt; Epoch 134 finished 	ANN training loss 0.364450
&gt;&gt; Epoch 135 finished 	ANN training loss 0.367608
&gt;&gt; Epoch 136 finished 	ANN training loss 0.374121
&gt;&gt; Epoch 137 finished 	ANN training loss 0.364372
&gt;&gt; Epoch 138 finished 	ANN training loss 0.366773
&gt;&gt; Epoch 139 finished 	ANN training loss 0.366469
&gt;&gt; Epoch 140 finished 	ANN training loss 0.365046
&gt;&gt; Epoch 141 finished 	ANN training loss 0.363372
&gt;&gt; Epoch 142 finished 	ANN training loss 0.364958
&gt;&gt; Epoch 143 finished 	ANN training loss 0.361661
&gt;&gt; Epoch 144 finished 	ANN training loss 0.365296
&gt;&gt; Epoch 145 finished 	ANN training loss 0.369483
&gt;&gt; Epoch 146 finished 	ANN training loss 0.373509
&gt;&gt; Epoch 147 finished 	ANN training loss 0.362654
&gt;&gt; Epoch 148 finished 	ANN training loss 0.363740
&gt;&gt; Epoch 149 finished 	ANN training loss 0.372904
&gt;&gt; Epoch 150 finished 	ANN training loss 0.362907
&gt;&gt; Epoch 151 finished 	ANN training loss 0.372450
&gt;&gt; Epoch 152 finished 	ANN training loss 0.359762
&gt;&gt; Epoch 153 finished 	ANN training loss 0.359082
&gt;&gt; Epoch 154 finished 	ANN training loss 0.360989
&gt;&gt; Epoch 155 finished 	ANN training loss 0.361960
&gt;&gt; Epoch 156 finished 	ANN training loss 0.374389
&gt;&gt; Epoch 157 finished 	ANN training loss 0.360220
&gt;&gt; Epoch 158 finished 	ANN training loss 0.358409
&gt;&gt; Epoch 159 finished 	ANN training loss 0.361251
&gt;&gt; Epoch 160 finished 	ANN training loss 0.356795
&gt;&gt; Epoch 161 finished 	ANN training loss 0.357533
&gt;&gt; Epoch 162 finished 	ANN training loss 0.351548
&gt;&gt; Epoch 163 finished 	ANN training loss 0.348298
&gt;&gt; Epoch 164 finished 	ANN training loss 0.351413
&gt;&gt; Epoch 165 finished 	ANN training loss 0.350194
&gt;&gt; Epoch 166 finished 	ANN training loss 0.342127
&gt;&gt; Epoch 167 finished 	ANN training loss 0.345565
&gt;&gt; Epoch 168 finished 	ANN training loss 0.342463
&gt;&gt; Epoch 169 finished 	ANN training loss 0.351872
&gt;&gt; Epoch 170 finished 	ANN training loss 0.354732
&gt;&gt; Epoch 171 finished 	ANN training loss 0.353189
&gt;&gt; Epoch 172 finished 	ANN training loss 0.347826
&gt;&gt; Epoch 173 finished 	ANN training loss 0.348754
&gt;&gt; Epoch 174 finished 	ANN training loss 0.338771
&gt;&gt; Epoch 175 finished 	ANN training loss 0.343471
&gt;&gt; Epoch 176 finished 	ANN training loss 0.362243
&gt;&gt; Epoch 177 finished 	ANN training loss 0.357183
&gt;&gt; Epoch 178 finished 	ANN training loss 0.344816
&gt;&gt; Epoch 179 finished 	ANN training loss 0.342733
&gt;&gt; Epoch 180 finished 	ANN training loss 0.345733
&gt;&gt; Epoch 181 finished 	ANN training loss 0.347818
&gt;&gt; Epoch 182 finished 	ANN training loss 0.339812
&gt;&gt; Epoch 183 finished 	ANN training loss 0.336914
&gt;&gt; Epoch 184 finished 	ANN training loss 0.339579
&gt;&gt; Epoch 185 finished 	ANN training loss 0.342231
&gt;&gt; Epoch 186 finished 	ANN training loss 0.343136
&gt;&gt; Epoch 187 finished 	ANN training loss 0.346076
&gt;&gt; Epoch 188 finished 	ANN training loss 0.345793
&gt;&gt; Epoch 189 finished 	ANN training loss 0.349371
&gt;&gt; Epoch 190 finished 	ANN training loss 0.348608
&gt;&gt; Epoch 191 finished 	ANN training loss 0.343179
&gt;&gt; Epoch 192 finished 	ANN training loss 0.351237
&gt;&gt; Epoch 193 finished 	ANN training loss 0.345751
&gt;&gt; Epoch 194 finished 	ANN training loss 0.348666
&gt;&gt; Epoch 195 finished 	ANN training loss 0.337579
&gt;&gt; Epoch 196 finished 	ANN training loss 0.340430
&gt;&gt; Epoch 197 finished 	ANN training loss 0.344151
&gt;&gt; Epoch 198 finished 	ANN training loss 0.335697
&gt;&gt; Epoch 199 finished 	ANN training loss 0.338536
[END] Fine tuning step
Done.
Accuracy: 0.845000
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[34]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ACCURACY: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">acc9</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>ACCURACY: 0.845
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[35]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Sigmoidal activation, dropout=0.9</span>
<span class="n">acc10</span> <span class="o">=</span> <span class="n">deep_belief_net</span><span class="p">(</span><span class="n">hidden_layers_structure</span><span class="o">=</span><span class="p">[</span><span class="mi">300</span><span class="p">],</span> 
                       <span class="n">learning_rate_rbm</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span>
                       <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                       <span class="n">n_epochs_rbm</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span>
                       <span class="n">n_iter_backprop</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
                       <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
                       <span class="n">activation_function</span><span class="o">=</span><span class="s1">&#39;sigmoid&#39;</span><span class="p">,</span>
                       <span class="n">dropout_p</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[START] Pre-training step:
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 44.711769
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 34.974682
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 30.115257
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 26.901627
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 24.820967
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 23.321211
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 21.972878
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 20.911711
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 19.807693
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 18.874910
&gt;&gt; Epoch 11 finished 	RBM Reconstruction error 17.981621
&gt;&gt; Epoch 12 finished 	RBM Reconstruction error 17.216724
&gt;&gt; Epoch 13 finished 	RBM Reconstruction error 16.589233
&gt;&gt; Epoch 14 finished 	RBM Reconstruction error 15.975595
&gt;&gt; Epoch 15 finished 	RBM Reconstruction error 15.418973
&gt;&gt; Epoch 16 finished 	RBM Reconstruction error 14.935682
&gt;&gt; Epoch 17 finished 	RBM Reconstruction error 14.402187
&gt;&gt; Epoch 18 finished 	RBM Reconstruction error 14.054141
&gt;&gt; Epoch 19 finished 	RBM Reconstruction error 13.662756
&gt;&gt; Epoch 20 finished 	RBM Reconstruction error 13.378401
&gt;&gt; Epoch 21 finished 	RBM Reconstruction error 12.954887
&gt;&gt; Epoch 22 finished 	RBM Reconstruction error 12.631792
&gt;&gt; Epoch 23 finished 	RBM Reconstruction error 12.329613
&gt;&gt; Epoch 24 finished 	RBM Reconstruction error 12.060605
&gt;&gt; Epoch 25 finished 	RBM Reconstruction error 11.800629
&gt;&gt; Epoch 26 finished 	RBM Reconstruction error 11.531872
&gt;&gt; Epoch 27 finished 	RBM Reconstruction error 11.342194
&gt;&gt; Epoch 28 finished 	RBM Reconstruction error 11.117928
&gt;&gt; Epoch 29 finished 	RBM Reconstruction error 10.846569
&gt;&gt; Epoch 30 finished 	RBM Reconstruction error 10.632398
&gt;&gt; Epoch 31 finished 	RBM Reconstruction error 10.447753
&gt;&gt; Epoch 32 finished 	RBM Reconstruction error 10.231464
&gt;&gt; Epoch 33 finished 	RBM Reconstruction error 10.074735
&gt;&gt; Epoch 34 finished 	RBM Reconstruction error 9.834185
&gt;&gt; Epoch 35 finished 	RBM Reconstruction error 9.672203
&gt;&gt; Epoch 36 finished 	RBM Reconstruction error 9.502356
&gt;&gt; Epoch 37 finished 	RBM Reconstruction error 9.373305
&gt;&gt; Epoch 38 finished 	RBM Reconstruction error 9.212821
&gt;&gt; Epoch 39 finished 	RBM Reconstruction error 9.033991
&gt;&gt; Epoch 40 finished 	RBM Reconstruction error 8.924314
[END] Pre-training step
[START] Fine tuning step:
&gt;&gt; Epoch 0 finished 	ANN training loss 1.448642
&gt;&gt; Epoch 1 finished 	ANN training loss 1.237007
&gt;&gt; Epoch 2 finished 	ANN training loss 1.055958
&gt;&gt; Epoch 3 finished 	ANN training loss 1.114109
&gt;&gt; Epoch 4 finished 	ANN training loss 0.981095
&gt;&gt; Epoch 5 finished 	ANN training loss 0.995394
&gt;&gt; Epoch 6 finished 	ANN training loss 0.974523
&gt;&gt; Epoch 7 finished 	ANN training loss 0.943195
&gt;&gt; Epoch 8 finished 	ANN training loss 0.959481
&gt;&gt; Epoch 9 finished 	ANN training loss 0.893483
&gt;&gt; Epoch 10 finished 	ANN training loss 0.889213
&gt;&gt; Epoch 11 finished 	ANN training loss 0.963330
&gt;&gt; Epoch 12 finished 	ANN training loss 0.889548
&gt;&gt; Epoch 13 finished 	ANN training loss 0.898284
&gt;&gt; Epoch 14 finished 	ANN training loss 0.933416
&gt;&gt; Epoch 15 finished 	ANN training loss 0.924219
&gt;&gt; Epoch 16 finished 	ANN training loss 0.866303
&gt;&gt; Epoch 17 finished 	ANN training loss 0.862005
&gt;&gt; Epoch 18 finished 	ANN training loss 0.898758
&gt;&gt; Epoch 19 finished 	ANN training loss 0.881912
&gt;&gt; Epoch 20 finished 	ANN training loss 0.893247
&gt;&gt; Epoch 21 finished 	ANN training loss 0.887337
&gt;&gt; Epoch 22 finished 	ANN training loss 0.875583
&gt;&gt; Epoch 23 finished 	ANN training loss 0.892087
&gt;&gt; Epoch 24 finished 	ANN training loss 0.913884
&gt;&gt; Epoch 25 finished 	ANN training loss 0.928366
&gt;&gt; Epoch 26 finished 	ANN training loss 0.871171
&gt;&gt; Epoch 27 finished 	ANN training loss 0.884568
&gt;&gt; Epoch 28 finished 	ANN training loss 0.871815
&gt;&gt; Epoch 29 finished 	ANN training loss 0.893337
&gt;&gt; Epoch 30 finished 	ANN training loss 0.890160
&gt;&gt; Epoch 31 finished 	ANN training loss 0.866827
&gt;&gt; Epoch 32 finished 	ANN training loss 0.902140
&gt;&gt; Epoch 33 finished 	ANN training loss 0.921352
&gt;&gt; Epoch 34 finished 	ANN training loss 0.904768
&gt;&gt; Epoch 35 finished 	ANN training loss 0.892043
&gt;&gt; Epoch 36 finished 	ANN training loss 0.905209
&gt;&gt; Epoch 37 finished 	ANN training loss 0.916492
&gt;&gt; Epoch 38 finished 	ANN training loss 0.895462
&gt;&gt; Epoch 39 finished 	ANN training loss 0.901257
&gt;&gt; Epoch 40 finished 	ANN training loss 0.891997
&gt;&gt; Epoch 41 finished 	ANN training loss 0.914339
&gt;&gt; Epoch 42 finished 	ANN training loss 0.892619
&gt;&gt; Epoch 43 finished 	ANN training loss 0.904813
&gt;&gt; Epoch 44 finished 	ANN training loss 0.909045
&gt;&gt; Epoch 45 finished 	ANN training loss 0.915594
&gt;&gt; Epoch 46 finished 	ANN training loss 0.955341
&gt;&gt; Epoch 47 finished 	ANN training loss 0.923195
&gt;&gt; Epoch 48 finished 	ANN training loss 0.918469
&gt;&gt; Epoch 49 finished 	ANN training loss 0.940288
&gt;&gt; Epoch 50 finished 	ANN training loss 0.941440
&gt;&gt; Epoch 51 finished 	ANN training loss 0.943729
&gt;&gt; Epoch 52 finished 	ANN training loss 0.902763
&gt;&gt; Epoch 53 finished 	ANN training loss 0.910621
&gt;&gt; Epoch 54 finished 	ANN training loss 0.885924
&gt;&gt; Epoch 55 finished 	ANN training loss 0.906362
&gt;&gt; Epoch 56 finished 	ANN training loss 0.932334
&gt;&gt; Epoch 57 finished 	ANN training loss 0.930039
&gt;&gt; Epoch 58 finished 	ANN training loss 0.922613
&gt;&gt; Epoch 59 finished 	ANN training loss 0.921587
&gt;&gt; Epoch 60 finished 	ANN training loss 0.951778
&gt;&gt; Epoch 61 finished 	ANN training loss 0.963138
&gt;&gt; Epoch 62 finished 	ANN training loss 0.959891
&gt;&gt; Epoch 63 finished 	ANN training loss 0.970203
&gt;&gt; Epoch 64 finished 	ANN training loss 0.966613
&gt;&gt; Epoch 65 finished 	ANN training loss 0.950974
&gt;&gt; Epoch 66 finished 	ANN training loss 0.942440
&gt;&gt; Epoch 67 finished 	ANN training loss 0.977817
&gt;&gt; Epoch 68 finished 	ANN training loss 0.966397
&gt;&gt; Epoch 69 finished 	ANN training loss 0.951992
&gt;&gt; Epoch 70 finished 	ANN training loss 0.968305
&gt;&gt; Epoch 71 finished 	ANN training loss 0.980575
&gt;&gt; Epoch 72 finished 	ANN training loss 0.961645
&gt;&gt; Epoch 73 finished 	ANN training loss 0.983051
&gt;&gt; Epoch 74 finished 	ANN training loss 0.985086
&gt;&gt; Epoch 75 finished 	ANN training loss 0.999105
&gt;&gt; Epoch 76 finished 	ANN training loss 0.980587
&gt;&gt; Epoch 77 finished 	ANN training loss 0.970480
&gt;&gt; Epoch 78 finished 	ANN training loss 1.001633
&gt;&gt; Epoch 79 finished 	ANN training loss 1.004237
&gt;&gt; Epoch 80 finished 	ANN training loss 1.015956
&gt;&gt; Epoch 81 finished 	ANN training loss 1.008122
&gt;&gt; Epoch 82 finished 	ANN training loss 1.026527
&gt;&gt; Epoch 83 finished 	ANN training loss 0.992966
&gt;&gt; Epoch 84 finished 	ANN training loss 0.986935
&gt;&gt; Epoch 85 finished 	ANN training loss 1.003623
&gt;&gt; Epoch 86 finished 	ANN training loss 1.022724
&gt;&gt; Epoch 87 finished 	ANN training loss 1.020194
&gt;&gt; Epoch 88 finished 	ANN training loss 1.012425
&gt;&gt; Epoch 89 finished 	ANN training loss 1.033532
&gt;&gt; Epoch 90 finished 	ANN training loss 1.036311
&gt;&gt; Epoch 91 finished 	ANN training loss 1.040827
&gt;&gt; Epoch 92 finished 	ANN training loss 1.041897
&gt;&gt; Epoch 93 finished 	ANN training loss 1.014408
&gt;&gt; Epoch 94 finished 	ANN training loss 1.020862
&gt;&gt; Epoch 95 finished 	ANN training loss 1.015541
&gt;&gt; Epoch 96 finished 	ANN training loss 1.039117
&gt;&gt; Epoch 97 finished 	ANN training loss 1.056093
&gt;&gt; Epoch 98 finished 	ANN training loss 1.039339
&gt;&gt; Epoch 99 finished 	ANN training loss 1.040997
&gt;&gt; Epoch 100 finished 	ANN training loss 1.050683
&gt;&gt; Epoch 101 finished 	ANN training loss 1.041008
&gt;&gt; Epoch 102 finished 	ANN training loss 1.034413
&gt;&gt; Epoch 103 finished 	ANN training loss 1.044057
&gt;&gt; Epoch 104 finished 	ANN training loss 1.066663
&gt;&gt; Epoch 105 finished 	ANN training loss 1.070550
&gt;&gt; Epoch 106 finished 	ANN training loss 1.067222
&gt;&gt; Epoch 107 finished 	ANN training loss 1.044643
&gt;&gt; Epoch 108 finished 	ANN training loss 1.039049
&gt;&gt; Epoch 109 finished 	ANN training loss 1.045426
&gt;&gt; Epoch 110 finished 	ANN training loss 1.056045
&gt;&gt; Epoch 111 finished 	ANN training loss 1.055706
&gt;&gt; Epoch 112 finished 	ANN training loss 1.056678
&gt;&gt; Epoch 113 finished 	ANN training loss 1.051877
&gt;&gt; Epoch 114 finished 	ANN training loss 1.070614
&gt;&gt; Epoch 115 finished 	ANN training loss 1.053739
&gt;&gt; Epoch 116 finished 	ANN training loss 1.052616
&gt;&gt; Epoch 117 finished 	ANN training loss 1.046241
&gt;&gt; Epoch 118 finished 	ANN training loss 1.073852
&gt;&gt; Epoch 119 finished 	ANN training loss 1.051737
&gt;&gt; Epoch 120 finished 	ANN training loss 1.066807
&gt;&gt; Epoch 121 finished 	ANN training loss 1.066254
&gt;&gt; Epoch 122 finished 	ANN training loss 1.076323
&gt;&gt; Epoch 123 finished 	ANN training loss 1.060406
&gt;&gt; Epoch 124 finished 	ANN training loss 1.047807
&gt;&gt; Epoch 125 finished 	ANN training loss 1.065490
&gt;&gt; Epoch 126 finished 	ANN training loss 1.073627
&gt;&gt; Epoch 127 finished 	ANN training loss 1.075184
&gt;&gt; Epoch 128 finished 	ANN training loss 1.068134
&gt;&gt; Epoch 129 finished 	ANN training loss 1.074774
&gt;&gt; Epoch 130 finished 	ANN training loss 1.084933
&gt;&gt; Epoch 131 finished 	ANN training loss 1.084066
&gt;&gt; Epoch 132 finished 	ANN training loss 1.066242
&gt;&gt; Epoch 133 finished 	ANN training loss 1.081202
&gt;&gt; Epoch 134 finished 	ANN training loss 1.089067
&gt;&gt; Epoch 135 finished 	ANN training loss 1.087328
&gt;&gt; Epoch 136 finished 	ANN training loss 1.104776
&gt;&gt; Epoch 137 finished 	ANN training loss 1.086491
&gt;&gt; Epoch 138 finished 	ANN training loss 1.086645
&gt;&gt; Epoch 139 finished 	ANN training loss 1.102701
&gt;&gt; Epoch 140 finished 	ANN training loss 1.107967
&gt;&gt; Epoch 141 finished 	ANN training loss 1.103331
&gt;&gt; Epoch 142 finished 	ANN training loss 1.101829
&gt;&gt; Epoch 143 finished 	ANN training loss 1.092513
&gt;&gt; Epoch 144 finished 	ANN training loss 1.098590
&gt;&gt; Epoch 145 finished 	ANN training loss 1.103724
&gt;&gt; Epoch 146 finished 	ANN training loss 1.109991
&gt;&gt; Epoch 147 finished 	ANN training loss 1.119724
&gt;&gt; Epoch 148 finished 	ANN training loss 1.103122
&gt;&gt; Epoch 149 finished 	ANN training loss 1.109717
&gt;&gt; Epoch 150 finished 	ANN training loss 1.106461
&gt;&gt; Epoch 151 finished 	ANN training loss 1.109076
&gt;&gt; Epoch 152 finished 	ANN training loss 1.108715
&gt;&gt; Epoch 153 finished 	ANN training loss 1.105100
&gt;&gt; Epoch 154 finished 	ANN training loss 1.093201
&gt;&gt; Epoch 155 finished 	ANN training loss 1.114325
&gt;&gt; Epoch 156 finished 	ANN training loss 1.114015
&gt;&gt; Epoch 157 finished 	ANN training loss 1.106276
&gt;&gt; Epoch 158 finished 	ANN training loss 1.123178
&gt;&gt; Epoch 159 finished 	ANN training loss 1.128970
&gt;&gt; Epoch 160 finished 	ANN training loss 1.129217
&gt;&gt; Epoch 161 finished 	ANN training loss 1.114643
&gt;&gt; Epoch 162 finished 	ANN training loss 1.118690
&gt;&gt; Epoch 163 finished 	ANN training loss 1.110184
&gt;&gt; Epoch 164 finished 	ANN training loss 1.139600
&gt;&gt; Epoch 165 finished 	ANN training loss 1.136358
&gt;&gt; Epoch 166 finished 	ANN training loss 1.123079
&gt;&gt; Epoch 167 finished 	ANN training loss 1.092713
&gt;&gt; Epoch 168 finished 	ANN training loss 1.105672
&gt;&gt; Epoch 169 finished 	ANN training loss 1.087426
&gt;&gt; Epoch 170 finished 	ANN training loss 1.096069
&gt;&gt; Epoch 171 finished 	ANN training loss 1.108930
&gt;&gt; Epoch 172 finished 	ANN training loss 1.101587
&gt;&gt; Epoch 173 finished 	ANN training loss 1.090164
&gt;&gt; Epoch 174 finished 	ANN training loss 1.095603
&gt;&gt; Epoch 175 finished 	ANN training loss 1.112462
&gt;&gt; Epoch 176 finished 	ANN training loss 1.113933
&gt;&gt; Epoch 177 finished 	ANN training loss 1.114493
&gt;&gt; Epoch 178 finished 	ANN training loss 1.114586
&gt;&gt; Epoch 179 finished 	ANN training loss 1.111509
&gt;&gt; Epoch 180 finished 	ANN training loss 1.124164
&gt;&gt; Epoch 181 finished 	ANN training loss 1.106056
&gt;&gt; Epoch 182 finished 	ANN training loss 1.116321
&gt;&gt; Epoch 183 finished 	ANN training loss 1.100466
&gt;&gt; Epoch 184 finished 	ANN training loss 1.103373
&gt;&gt; Epoch 185 finished 	ANN training loss 1.124081
&gt;&gt; Epoch 186 finished 	ANN training loss 1.106868
&gt;&gt; Epoch 187 finished 	ANN training loss 1.107670
&gt;&gt; Epoch 188 finished 	ANN training loss 1.120240
&gt;&gt; Epoch 189 finished 	ANN training loss 1.129315
&gt;&gt; Epoch 190 finished 	ANN training loss 1.127424
&gt;&gt; Epoch 191 finished 	ANN training loss 1.127233
&gt;&gt; Epoch 192 finished 	ANN training loss 1.121810
&gt;&gt; Epoch 193 finished 	ANN training loss 1.115196
&gt;&gt; Epoch 194 finished 	ANN training loss 1.121475
&gt;&gt; Epoch 195 finished 	ANN training loss 1.122354
&gt;&gt; Epoch 196 finished 	ANN training loss 1.115823
&gt;&gt; Epoch 197 finished 	ANN training loss 1.119198
&gt;&gt; Epoch 198 finished 	ANN training loss 1.111917
&gt;&gt; Epoch 199 finished 	ANN training loss 1.113940
[END] Fine tuning step
Done.
Accuracy: 0.700000
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[36]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ACCURACY: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">acc10</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>ACCURACY: 0.7
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[37]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Sigmoidal activation, dropout=1</span>
<span class="n">acc11</span> <span class="o">=</span> <span class="n">deep_belief_net</span><span class="p">(</span><span class="n">hidden_layers_structure</span><span class="o">=</span><span class="p">[</span><span class="mi">300</span><span class="p">],</span> 
                       <span class="n">learning_rate_rbm</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span>
                       <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                       <span class="n">n_epochs_rbm</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span>
                       <span class="n">n_iter_backprop</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
                       <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
                       <span class="n">activation_function</span><span class="o">=</span><span class="s1">&#39;sigmoid&#39;</span><span class="p">,</span>
                       <span class="n">dropout_p</span><span class="o">=</span><span class="mf">1.</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[START] Pre-training step:
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 44.149258
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 34.662308
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 29.567949
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 26.718691
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 24.708843
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 23.210997
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 21.998230
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 20.819212
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 19.719660
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 18.906761
&gt;&gt; Epoch 11 finished 	RBM Reconstruction error 17.922014
&gt;&gt; Epoch 12 finished 	RBM Reconstruction error 17.175596
&gt;&gt; Epoch 13 finished 	RBM Reconstruction error 16.521608
&gt;&gt; Epoch 14 finished 	RBM Reconstruction error 15.899694
&gt;&gt; Epoch 15 finished 	RBM Reconstruction error 15.352788
&gt;&gt; Epoch 16 finished 	RBM Reconstruction error 14.864987
&gt;&gt; Epoch 17 finished 	RBM Reconstruction error 14.368469
&gt;&gt; Epoch 18 finished 	RBM Reconstruction error 13.940740
&gt;&gt; Epoch 19 finished 	RBM Reconstruction error 13.611406
&gt;&gt; Epoch 20 finished 	RBM Reconstruction error 13.196278
&gt;&gt; Epoch 21 finished 	RBM Reconstruction error 12.901297
&gt;&gt; Epoch 22 finished 	RBM Reconstruction error 12.537432
&gt;&gt; Epoch 23 finished 	RBM Reconstruction error 12.228384
&gt;&gt; Epoch 24 finished 	RBM Reconstruction error 11.930333
&gt;&gt; Epoch 25 finished 	RBM Reconstruction error 11.678274
&gt;&gt; Epoch 26 finished 	RBM Reconstruction error 11.430597
&gt;&gt; Epoch 27 finished 	RBM Reconstruction error 11.203878
&gt;&gt; Epoch 28 finished 	RBM Reconstruction error 10.977073
&gt;&gt; Epoch 29 finished 	RBM Reconstruction error 10.768891
&gt;&gt; Epoch 30 finished 	RBM Reconstruction error 10.515144
&gt;&gt; Epoch 31 finished 	RBM Reconstruction error 10.356886
&gt;&gt; Epoch 32 finished 	RBM Reconstruction error 10.145551
&gt;&gt; Epoch 33 finished 	RBM Reconstruction error 9.912625
&gt;&gt; Epoch 34 finished 	RBM Reconstruction error 9.777631
&gt;&gt; Epoch 35 finished 	RBM Reconstruction error 9.614018
&gt;&gt; Epoch 36 finished 	RBM Reconstruction error 9.486042
&gt;&gt; Epoch 37 finished 	RBM Reconstruction error 9.297201
&gt;&gt; Epoch 38 finished 	RBM Reconstruction error 9.130317
&gt;&gt; Epoch 39 finished 	RBM Reconstruction error 8.998882
&gt;&gt; Epoch 40 finished 	RBM Reconstruction error 8.870162
[END] Pre-training step
[START] Fine tuning step:
&gt;&gt; Epoch 0 finished 	ANN training loss nan
&gt;&gt; Epoch 1 finished 	ANN training loss nan
&gt;&gt; Epoch 2 finished 	ANN training loss nan
&gt;&gt; Epoch 3 finished 	ANN training loss nan
&gt;&gt; Epoch 4 finished 	ANN training loss nan
&gt;&gt; Epoch 5 finished 	ANN training loss nan
&gt;&gt; Epoch 6 finished 	ANN training loss nan
&gt;&gt; Epoch 7 finished 	ANN training loss nan
&gt;&gt; Epoch 8 finished 	ANN training loss nan
&gt;&gt; Epoch 9 finished 	ANN training loss nan
&gt;&gt; Epoch 10 finished 	ANN training loss nan
&gt;&gt; Epoch 11 finished 	ANN training loss nan
&gt;&gt; Epoch 12 finished 	ANN training loss nan
&gt;&gt; Epoch 13 finished 	ANN training loss nan
&gt;&gt; Epoch 14 finished 	ANN training loss nan
&gt;&gt; Epoch 15 finished 	ANN training loss nan
&gt;&gt; Epoch 16 finished 	ANN training loss nan
&gt;&gt; Epoch 17 finished 	ANN training loss nan
&gt;&gt; Epoch 18 finished 	ANN training loss nan
&gt;&gt; Epoch 19 finished 	ANN training loss nan
&gt;&gt; Epoch 20 finished 	ANN training loss nan
&gt;&gt; Epoch 21 finished 	ANN training loss nan
&gt;&gt; Epoch 22 finished 	ANN training loss nan
&gt;&gt; Epoch 23 finished 	ANN training loss nan
&gt;&gt; Epoch 24 finished 	ANN training loss nan
&gt;&gt; Epoch 25 finished 	ANN training loss nan
&gt;&gt; Epoch 26 finished 	ANN training loss nan
&gt;&gt; Epoch 27 finished 	ANN training loss nan
&gt;&gt; Epoch 28 finished 	ANN training loss nan
&gt;&gt; Epoch 29 finished 	ANN training loss nan
&gt;&gt; Epoch 30 finished 	ANN training loss nan
&gt;&gt; Epoch 31 finished 	ANN training loss nan
&gt;&gt; Epoch 32 finished 	ANN training loss nan
&gt;&gt; Epoch 33 finished 	ANN training loss nan
&gt;&gt; Epoch 34 finished 	ANN training loss nan
&gt;&gt; Epoch 35 finished 	ANN training loss nan
&gt;&gt; Epoch 36 finished 	ANN training loss nan
&gt;&gt; Epoch 37 finished 	ANN training loss nan
&gt;&gt; Epoch 38 finished 	ANN training loss nan
&gt;&gt; Epoch 39 finished 	ANN training loss nan
&gt;&gt; Epoch 40 finished 	ANN training loss nan
&gt;&gt; Epoch 41 finished 	ANN training loss nan
&gt;&gt; Epoch 42 finished 	ANN training loss nan
&gt;&gt; Epoch 43 finished 	ANN training loss nan
&gt;&gt; Epoch 44 finished 	ANN training loss nan
&gt;&gt; Epoch 45 finished 	ANN training loss nan
&gt;&gt; Epoch 46 finished 	ANN training loss nan
&gt;&gt; Epoch 47 finished 	ANN training loss nan
&gt;&gt; Epoch 48 finished 	ANN training loss nan
&gt;&gt; Epoch 49 finished 	ANN training loss nan
&gt;&gt; Epoch 50 finished 	ANN training loss nan
&gt;&gt; Epoch 51 finished 	ANN training loss nan
&gt;&gt; Epoch 52 finished 	ANN training loss nan
&gt;&gt; Epoch 53 finished 	ANN training loss nan
&gt;&gt; Epoch 54 finished 	ANN training loss nan
&gt;&gt; Epoch 55 finished 	ANN training loss nan
&gt;&gt; Epoch 56 finished 	ANN training loss nan
&gt;&gt; Epoch 57 finished 	ANN training loss nan
&gt;&gt; Epoch 58 finished 	ANN training loss nan
&gt;&gt; Epoch 59 finished 	ANN training loss nan
&gt;&gt; Epoch 60 finished 	ANN training loss nan
&gt;&gt; Epoch 61 finished 	ANN training loss nan
&gt;&gt; Epoch 62 finished 	ANN training loss nan
&gt;&gt; Epoch 63 finished 	ANN training loss nan
&gt;&gt; Epoch 64 finished 	ANN training loss nan
&gt;&gt; Epoch 65 finished 	ANN training loss nan
&gt;&gt; Epoch 66 finished 	ANN training loss nan
&gt;&gt; Epoch 67 finished 	ANN training loss nan
&gt;&gt; Epoch 68 finished 	ANN training loss nan
&gt;&gt; Epoch 69 finished 	ANN training loss nan
&gt;&gt; Epoch 70 finished 	ANN training loss nan
&gt;&gt; Epoch 71 finished 	ANN training loss nan
&gt;&gt; Epoch 72 finished 	ANN training loss nan
&gt;&gt; Epoch 73 finished 	ANN training loss nan
&gt;&gt; Epoch 74 finished 	ANN training loss nan
&gt;&gt; Epoch 75 finished 	ANN training loss nan
&gt;&gt; Epoch 76 finished 	ANN training loss nan
&gt;&gt; Epoch 77 finished 	ANN training loss nan
&gt;&gt; Epoch 78 finished 	ANN training loss nan
&gt;&gt; Epoch 79 finished 	ANN training loss nan
&gt;&gt; Epoch 80 finished 	ANN training loss nan
&gt;&gt; Epoch 81 finished 	ANN training loss nan
&gt;&gt; Epoch 82 finished 	ANN training loss nan
&gt;&gt; Epoch 83 finished 	ANN training loss nan
&gt;&gt; Epoch 84 finished 	ANN training loss nan
&gt;&gt; Epoch 85 finished 	ANN training loss nan
&gt;&gt; Epoch 86 finished 	ANN training loss nan
&gt;&gt; Epoch 87 finished 	ANN training loss nan
&gt;&gt; Epoch 88 finished 	ANN training loss nan
&gt;&gt; Epoch 89 finished 	ANN training loss nan
&gt;&gt; Epoch 90 finished 	ANN training loss nan
&gt;&gt; Epoch 91 finished 	ANN training loss nan
&gt;&gt; Epoch 92 finished 	ANN training loss nan
&gt;&gt; Epoch 93 finished 	ANN training loss nan
&gt;&gt; Epoch 94 finished 	ANN training loss nan
&gt;&gt; Epoch 95 finished 	ANN training loss nan
&gt;&gt; Epoch 96 finished 	ANN training loss nan
&gt;&gt; Epoch 97 finished 	ANN training loss nan
&gt;&gt; Epoch 98 finished 	ANN training loss nan
&gt;&gt; Epoch 99 finished 	ANN training loss nan
&gt;&gt; Epoch 100 finished 	ANN training loss nan
&gt;&gt; Epoch 101 finished 	ANN training loss nan
&gt;&gt; Epoch 102 finished 	ANN training loss nan
&gt;&gt; Epoch 103 finished 	ANN training loss nan
&gt;&gt; Epoch 104 finished 	ANN training loss nan
&gt;&gt; Epoch 105 finished 	ANN training loss nan
&gt;&gt; Epoch 106 finished 	ANN training loss nan
&gt;&gt; Epoch 107 finished 	ANN training loss nan
&gt;&gt; Epoch 108 finished 	ANN training loss nan
&gt;&gt; Epoch 109 finished 	ANN training loss nan
&gt;&gt; Epoch 110 finished 	ANN training loss nan
&gt;&gt; Epoch 111 finished 	ANN training loss nan
&gt;&gt; Epoch 112 finished 	ANN training loss nan
&gt;&gt; Epoch 113 finished 	ANN training loss nan
&gt;&gt; Epoch 114 finished 	ANN training loss nan
&gt;&gt; Epoch 115 finished 	ANN training loss nan
&gt;&gt; Epoch 116 finished 	ANN training loss nan
&gt;&gt; Epoch 117 finished 	ANN training loss nan
&gt;&gt; Epoch 118 finished 	ANN training loss nan
&gt;&gt; Epoch 119 finished 	ANN training loss nan
&gt;&gt; Epoch 120 finished 	ANN training loss nan
&gt;&gt; Epoch 121 finished 	ANN training loss nan
&gt;&gt; Epoch 122 finished 	ANN training loss nan
&gt;&gt; Epoch 123 finished 	ANN training loss nan
&gt;&gt; Epoch 124 finished 	ANN training loss nan
&gt;&gt; Epoch 125 finished 	ANN training loss nan
&gt;&gt; Epoch 126 finished 	ANN training loss nan
&gt;&gt; Epoch 127 finished 	ANN training loss nan
&gt;&gt; Epoch 128 finished 	ANN training loss nan
&gt;&gt; Epoch 129 finished 	ANN training loss nan
&gt;&gt; Epoch 130 finished 	ANN training loss nan
&gt;&gt; Epoch 131 finished 	ANN training loss nan
&gt;&gt; Epoch 132 finished 	ANN training loss nan
&gt;&gt; Epoch 133 finished 	ANN training loss nan
&gt;&gt; Epoch 134 finished 	ANN training loss nan
&gt;&gt; Epoch 135 finished 	ANN training loss nan
&gt;&gt; Epoch 136 finished 	ANN training loss nan
&gt;&gt; Epoch 137 finished 	ANN training loss nan
&gt;&gt; Epoch 138 finished 	ANN training loss nan
&gt;&gt; Epoch 139 finished 	ANN training loss nan
&gt;&gt; Epoch 140 finished 	ANN training loss nan
&gt;&gt; Epoch 141 finished 	ANN training loss nan
&gt;&gt; Epoch 142 finished 	ANN training loss nan
&gt;&gt; Epoch 143 finished 	ANN training loss nan
&gt;&gt; Epoch 144 finished 	ANN training loss nan
&gt;&gt; Epoch 145 finished 	ANN training loss nan
&gt;&gt; Epoch 146 finished 	ANN training loss nan
&gt;&gt; Epoch 147 finished 	ANN training loss nan
&gt;&gt; Epoch 148 finished 	ANN training loss nan
&gt;&gt; Epoch 149 finished 	ANN training loss nan
&gt;&gt; Epoch 150 finished 	ANN training loss nan
&gt;&gt; Epoch 151 finished 	ANN training loss nan
&gt;&gt; Epoch 152 finished 	ANN training loss nan
&gt;&gt; Epoch 153 finished 	ANN training loss nan
&gt;&gt; Epoch 154 finished 	ANN training loss nan
&gt;&gt; Epoch 155 finished 	ANN training loss nan
&gt;&gt; Epoch 156 finished 	ANN training loss nan
&gt;&gt; Epoch 157 finished 	ANN training loss nan
&gt;&gt; Epoch 158 finished 	ANN training loss nan
&gt;&gt; Epoch 159 finished 	ANN training loss nan
&gt;&gt; Epoch 160 finished 	ANN training loss nan
&gt;&gt; Epoch 161 finished 	ANN training loss nan
&gt;&gt; Epoch 162 finished 	ANN training loss nan
&gt;&gt; Epoch 163 finished 	ANN training loss nan
&gt;&gt; Epoch 164 finished 	ANN training loss nan
&gt;&gt; Epoch 165 finished 	ANN training loss nan
&gt;&gt; Epoch 166 finished 	ANN training loss nan
&gt;&gt; Epoch 167 finished 	ANN training loss nan
&gt;&gt; Epoch 168 finished 	ANN training loss nan
&gt;&gt; Epoch 169 finished 	ANN training loss nan
&gt;&gt; Epoch 170 finished 	ANN training loss nan
&gt;&gt; Epoch 171 finished 	ANN training loss nan
&gt;&gt; Epoch 172 finished 	ANN training loss nan
&gt;&gt; Epoch 173 finished 	ANN training loss nan
&gt;&gt; Epoch 174 finished 	ANN training loss nan
&gt;&gt; Epoch 175 finished 	ANN training loss nan
&gt;&gt; Epoch 176 finished 	ANN training loss nan
&gt;&gt; Epoch 177 finished 	ANN training loss nan
&gt;&gt; Epoch 178 finished 	ANN training loss nan
&gt;&gt; Epoch 179 finished 	ANN training loss nan
&gt;&gt; Epoch 180 finished 	ANN training loss nan
&gt;&gt; Epoch 181 finished 	ANN training loss nan
&gt;&gt; Epoch 182 finished 	ANN training loss nan
&gt;&gt; Epoch 183 finished 	ANN training loss nan
&gt;&gt; Epoch 184 finished 	ANN training loss nan
&gt;&gt; Epoch 185 finished 	ANN training loss nan
&gt;&gt; Epoch 186 finished 	ANN training loss nan
&gt;&gt; Epoch 187 finished 	ANN training loss nan
&gt;&gt; Epoch 188 finished 	ANN training loss nan
&gt;&gt; Epoch 189 finished 	ANN training loss nan
&gt;&gt; Epoch 190 finished 	ANN training loss nan
&gt;&gt; Epoch 191 finished 	ANN training loss nan
&gt;&gt; Epoch 192 finished 	ANN training loss nan
&gt;&gt; Epoch 193 finished 	ANN training loss nan
&gt;&gt; Epoch 194 finished 	ANN training loss nan
&gt;&gt; Epoch 195 finished 	ANN training loss nan
&gt;&gt; Epoch 196 finished 	ANN training loss nan
&gt;&gt; Epoch 197 finished 	ANN training loss nan
&gt;&gt; Epoch 198 finished 	ANN training loss nan
&gt;&gt; Epoch 199 finished 	ANN training loss nan
[END] Fine tuning step
Done.
Accuracy: 0.100000
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[38]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ACCURACY: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">acc11</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>ACCURACY: 0.1
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[64]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Create bar graph</span>
<span class="n">plt</span><span class="o">.</span><span class="n">clf</span><span class="p">()</span>
<span class="n">objects</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;0&#39;</span><span class="p">,</span> <span class="s1">&#39;0.1&#39;</span><span class="p">,</span> <span class="s1">&#39;0.2&#39;</span><span class="p">,</span> <span class="s1">&#39;0.3&#39;</span><span class="p">,</span> <span class="s1">&#39;0.4&#39;</span><span class="p">,</span> <span class="s1">&#39;0.5&#39;</span><span class="p">,</span> <span class="s1">&#39;0.6&#39;</span><span class="p">,</span> <span class="s1">&#39;0.7&#39;</span><span class="p">,</span> <span class="s1">&#39;0.8&#39;</span><span class="p">,</span> <span class="s1">&#39;0.9&#39;</span><span class="p">,</span> <span class="s1">&#39;1&#39;</span><span class="p">)</span>
<span class="n">y_pos</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">objects</span><span class="p">))</span>
<span class="n">performance</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.885</span><span class="p">,</span> <span class="mf">0.92</span><span class="p">,</span> <span class="mf">0.895</span><span class="p">,</span> <span class="mf">0.885</span><span class="p">,</span> <span class="mf">0.885</span><span class="p">,</span> <span class="mf">0.875</span><span class="p">,</span> <span class="mf">0.88</span><span class="p">,</span> <span class="mf">0.85</span><span class="p">,</span> <span class="mf">0.845</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">y_pos</span><span class="p">,</span> <span class="n">performance</span><span class="p">,</span> <span class="n">align</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">y_pos</span><span class="p">,</span> <span class="n">objects</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;dropout_p&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Accuracy&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Benchmark 2, sigmoid Settings&#39;</span><span class="p">)</span>
 
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">clf</span><span class="p">()</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>




<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAYUAAAEeCAYAAABlggnIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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"
>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Benchmark-2-(ReLu)">Benchmark 2 (ReLu)<a class="anchor-link" href="#Benchmark-2-(ReLu)">&#182;</a></h3><p>Same benchmark as benchmark 2 but with the ReLu function</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[6]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># ReLu, dropout = 0</span>
<span class="n">acc0</span> <span class="o">=</span> <span class="n">deep_belief_net</span><span class="p">(</span><span class="n">hidden_layers_structure</span><span class="o">=</span><span class="p">[</span><span class="mi">300</span><span class="p">],</span> 
                       <span class="n">learning_rate_rbm</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span>
                       <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                       <span class="n">n_epochs_rbm</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span>
                       <span class="n">n_iter_backprop</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
                       <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
                       <span class="n">activation_function</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span>
                       <span class="n">dropout_p</span><span class="o">=</span><span class="mf">0.</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[START] Pre-training step:
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 76.494263
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 49.433613
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 34.763378
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 53.639404
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 47.919071
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 52.792355
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 34.348198
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 37.879623
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 40.244701
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 24.620647
&gt;&gt; Epoch 11 finished 	RBM Reconstruction error 46.829220
&gt;&gt; Epoch 12 finished 	RBM Reconstruction error 39.373817
&gt;&gt; Epoch 13 finished 	RBM Reconstruction error 34.669823
&gt;&gt; Epoch 14 finished 	RBM Reconstruction error 33.453056
&gt;&gt; Epoch 15 finished 	RBM Reconstruction error 37.854984
&gt;&gt; Epoch 16 finished 	RBM Reconstruction error 41.508236
&gt;&gt; Epoch 17 finished 	RBM Reconstruction error 34.404789
&gt;&gt; Epoch 18 finished 	RBM Reconstruction error 37.564243
&gt;&gt; Epoch 19 finished 	RBM Reconstruction error 29.922712
&gt;&gt; Epoch 20 finished 	RBM Reconstruction error 45.012032
&gt;&gt; Epoch 21 finished 	RBM Reconstruction error 45.408573
&gt;&gt; Epoch 22 finished 	RBM Reconstruction error 28.551580
&gt;&gt; Epoch 23 finished 	RBM Reconstruction error 37.426979
&gt;&gt; Epoch 24 finished 	RBM Reconstruction error 40.499168
&gt;&gt; Epoch 25 finished 	RBM Reconstruction error 35.235374
&gt;&gt; Epoch 26 finished 	RBM Reconstruction error 35.480835
&gt;&gt; Epoch 27 finished 	RBM Reconstruction error 41.282398
&gt;&gt; Epoch 28 finished 	RBM Reconstruction error 42.309975
&gt;&gt; Epoch 29 finished 	RBM Reconstruction error 40.058102
&gt;&gt; Epoch 30 finished 	RBM Reconstruction error 31.168457
&gt;&gt; Epoch 31 finished 	RBM Reconstruction error 43.066540
&gt;&gt; Epoch 32 finished 	RBM Reconstruction error 37.291489
&gt;&gt; Epoch 33 finished 	RBM Reconstruction error 31.662176
&gt;&gt; Epoch 34 finished 	RBM Reconstruction error 35.579239
&gt;&gt; Epoch 35 finished 	RBM Reconstruction error 46.759823
&gt;&gt; Epoch 36 finished 	RBM Reconstruction error 45.324329
&gt;&gt; Epoch 37 finished 	RBM Reconstruction error 42.645927
&gt;&gt; Epoch 38 finished 	RBM Reconstruction error 45.038387
&gt;&gt; Epoch 39 finished 	RBM Reconstruction error 44.802437
&gt;&gt; Epoch 40 finished 	RBM Reconstruction error 40.336037
[END] Pre-training step
[START] Fine tuning step:
&gt;&gt; Epoch 0 finished 	ANN training loss 0.755771
&gt;&gt; Epoch 1 finished 	ANN training loss 0.483619
&gt;&gt; Epoch 2 finished 	ANN training loss 0.382748
&gt;&gt; Epoch 3 finished 	ANN training loss 0.327483
&gt;&gt; Epoch 4 finished 	ANN training loss 0.279391
&gt;&gt; Epoch 5 finished 	ANN training loss 0.246873
&gt;&gt; Epoch 6 finished 	ANN training loss 0.211682
&gt;&gt; Epoch 7 finished 	ANN training loss 0.200859
&gt;&gt; Epoch 8 finished 	ANN training loss 0.170853
&gt;&gt; Epoch 9 finished 	ANN training loss 0.153780
&gt;&gt; Epoch 10 finished 	ANN training loss 0.140875
&gt;&gt; Epoch 11 finished 	ANN training loss 0.123597
&gt;&gt; Epoch 12 finished 	ANN training loss 0.109557
&gt;&gt; Epoch 13 finished 	ANN training loss 0.102595
&gt;&gt; Epoch 14 finished 	ANN training loss 0.089996
&gt;&gt; Epoch 15 finished 	ANN training loss 0.084945
&gt;&gt; Epoch 16 finished 	ANN training loss 0.075162
&gt;&gt; Epoch 17 finished 	ANN training loss 0.069710
&gt;&gt; Epoch 18 finished 	ANN training loss 0.062900
&gt;&gt; Epoch 19 finished 	ANN training loss 0.059716
&gt;&gt; Epoch 20 finished 	ANN training loss 0.054521
&gt;&gt; Epoch 21 finished 	ANN training loss 0.050511
&gt;&gt; Epoch 22 finished 	ANN training loss 0.047649
&gt;&gt; Epoch 23 finished 	ANN training loss 0.044071
&gt;&gt; Epoch 24 finished 	ANN training loss 0.040684
&gt;&gt; Epoch 25 finished 	ANN training loss 0.038501
&gt;&gt; Epoch 26 finished 	ANN training loss 0.036993
&gt;&gt; Epoch 27 finished 	ANN training loss 0.034550
&gt;&gt; Epoch 28 finished 	ANN training loss 0.032743
&gt;&gt; Epoch 29 finished 	ANN training loss 0.031268
&gt;&gt; Epoch 30 finished 	ANN training loss 0.029284
&gt;&gt; Epoch 31 finished 	ANN training loss 0.027729
&gt;&gt; Epoch 32 finished 	ANN training loss 0.026954
&gt;&gt; Epoch 33 finished 	ANN training loss 0.025263
&gt;&gt; Epoch 34 finished 	ANN training loss 0.024667
&gt;&gt; Epoch 35 finished 	ANN training loss 0.023226
&gt;&gt; Epoch 36 finished 	ANN training loss 0.022346
&gt;&gt; Epoch 37 finished 	ANN training loss 0.021432
&gt;&gt; Epoch 38 finished 	ANN training loss 0.020543
&gt;&gt; Epoch 39 finished 	ANN training loss 0.019933
&gt;&gt; Epoch 40 finished 	ANN training loss 0.019033
&gt;&gt; Epoch 41 finished 	ANN training loss 0.018427
&gt;&gt; Epoch 42 finished 	ANN training loss 0.017753
&gt;&gt; Epoch 43 finished 	ANN training loss 0.017092
&gt;&gt; Epoch 44 finished 	ANN training loss 0.016526
&gt;&gt; Epoch 45 finished 	ANN training loss 0.015984
&gt;&gt; Epoch 46 finished 	ANN training loss 0.015479
&gt;&gt; Epoch 47 finished 	ANN training loss 0.015020
&gt;&gt; Epoch 48 finished 	ANN training loss 0.014671
&gt;&gt; Epoch 49 finished 	ANN training loss 0.014150
&gt;&gt; Epoch 50 finished 	ANN training loss 0.013766
&gt;&gt; Epoch 51 finished 	ANN training loss 0.013455
&gt;&gt; Epoch 52 finished 	ANN training loss 0.013073
&gt;&gt; Epoch 53 finished 	ANN training loss 0.012703
&gt;&gt; Epoch 54 finished 	ANN training loss 0.012307
&gt;&gt; Epoch 55 finished 	ANN training loss 0.012017
&gt;&gt; Epoch 56 finished 	ANN training loss 0.011727
&gt;&gt; Epoch 57 finished 	ANN training loss 0.011401
&gt;&gt; Epoch 58 finished 	ANN training loss 0.011191
&gt;&gt; Epoch 59 finished 	ANN training loss 0.010887
&gt;&gt; Epoch 60 finished 	ANN training loss 0.010613
&gt;&gt; Epoch 61 finished 	ANN training loss 0.010374
&gt;&gt; Epoch 62 finished 	ANN training loss 0.010134
&gt;&gt; Epoch 63 finished 	ANN training loss 0.009925
&gt;&gt; Epoch 64 finished 	ANN training loss 0.009697
&gt;&gt; Epoch 65 finished 	ANN training loss 0.009490
&gt;&gt; Epoch 66 finished 	ANN training loss 0.009292
&gt;&gt; Epoch 67 finished 	ANN training loss 0.009128
&gt;&gt; Epoch 68 finished 	ANN training loss 0.008924
&gt;&gt; Epoch 69 finished 	ANN training loss 0.008746
&gt;&gt; Epoch 70 finished 	ANN training loss 0.008568
&gt;&gt; Epoch 71 finished 	ANN training loss 0.008414
&gt;&gt; Epoch 72 finished 	ANN training loss 0.008248
&gt;&gt; Epoch 73 finished 	ANN training loss 0.008089
&gt;&gt; Epoch 74 finished 	ANN training loss 0.007953
&gt;&gt; Epoch 75 finished 	ANN training loss 0.007801
&gt;&gt; Epoch 76 finished 	ANN training loss 0.007668
&gt;&gt; Epoch 77 finished 	ANN training loss 0.007523
&gt;&gt; Epoch 78 finished 	ANN training loss 0.007399
&gt;&gt; Epoch 79 finished 	ANN training loss 0.007277
&gt;&gt; Epoch 80 finished 	ANN training loss 0.007143
&gt;&gt; Epoch 81 finished 	ANN training loss 0.007029
&gt;&gt; Epoch 82 finished 	ANN training loss 0.006910
&gt;&gt; Epoch 83 finished 	ANN training loss 0.006798
&gt;&gt; Epoch 84 finished 	ANN training loss 0.006692
&gt;&gt; Epoch 85 finished 	ANN training loss 0.006583
&gt;&gt; Epoch 86 finished 	ANN training loss 0.006489
&gt;&gt; Epoch 87 finished 	ANN training loss 0.006383
&gt;&gt; Epoch 88 finished 	ANN training loss 0.006290
&gt;&gt; Epoch 89 finished 	ANN training loss 0.006192
&gt;&gt; Epoch 90 finished 	ANN training loss 0.006115
&gt;&gt; Epoch 91 finished 	ANN training loss 0.006008
&gt;&gt; Epoch 92 finished 	ANN training loss 0.005921
&gt;&gt; Epoch 93 finished 	ANN training loss 0.005838
&gt;&gt; Epoch 94 finished 	ANN training loss 0.005755
&gt;&gt; Epoch 95 finished 	ANN training loss 0.005676
&gt;&gt; Epoch 96 finished 	ANN training loss 0.005597
&gt;&gt; Epoch 97 finished 	ANN training loss 0.005521
&gt;&gt; Epoch 98 finished 	ANN training loss 0.005446
&gt;&gt; Epoch 99 finished 	ANN training loss 0.005373
&gt;&gt; Epoch 100 finished 	ANN training loss 0.005301
&gt;&gt; Epoch 101 finished 	ANN training loss 0.005232
&gt;&gt; Epoch 102 finished 	ANN training loss 0.005167
&gt;&gt; Epoch 103 finished 	ANN training loss 0.005100
&gt;&gt; Epoch 104 finished 	ANN training loss 0.005035
&gt;&gt; Epoch 105 finished 	ANN training loss 0.004973
&gt;&gt; Epoch 106 finished 	ANN training loss 0.004911
&gt;&gt; Epoch 107 finished 	ANN training loss 0.004854
&gt;&gt; Epoch 108 finished 	ANN training loss 0.004794
&gt;&gt; Epoch 109 finished 	ANN training loss 0.004735
&gt;&gt; Epoch 110 finished 	ANN training loss 0.004678
&gt;&gt; Epoch 111 finished 	ANN training loss 0.004623
&gt;&gt; Epoch 112 finished 	ANN training loss 0.004570
&gt;&gt; Epoch 113 finished 	ANN training loss 0.004517
&gt;&gt; Epoch 114 finished 	ANN training loss 0.004465
&gt;&gt; Epoch 115 finished 	ANN training loss 0.004413
&gt;&gt; Epoch 116 finished 	ANN training loss 0.004364
&gt;&gt; Epoch 117 finished 	ANN training loss 0.004319
&gt;&gt; Epoch 118 finished 	ANN training loss 0.004269
&gt;&gt; Epoch 119 finished 	ANN training loss 0.004224
&gt;&gt; Epoch 120 finished 	ANN training loss 0.004176
&gt;&gt; Epoch 121 finished 	ANN training loss 0.004132
&gt;&gt; Epoch 122 finished 	ANN training loss 0.004087
&gt;&gt; Epoch 123 finished 	ANN training loss 0.004045
&gt;&gt; Epoch 124 finished 	ANN training loss 0.004003
&gt;&gt; Epoch 125 finished 	ANN training loss 0.003962
&gt;&gt; Epoch 126 finished 	ANN training loss 0.003920
&gt;&gt; Epoch 127 finished 	ANN training loss 0.003880
&gt;&gt; Epoch 128 finished 	ANN training loss 0.003841
&gt;&gt; Epoch 129 finished 	ANN training loss 0.003803
&gt;&gt; Epoch 130 finished 	ANN training loss 0.003765
&gt;&gt; Epoch 131 finished 	ANN training loss 0.003728
&gt;&gt; Epoch 132 finished 	ANN training loss 0.003691
&gt;&gt; Epoch 133 finished 	ANN training loss 0.003656
&gt;&gt; Epoch 134 finished 	ANN training loss 0.003621
&gt;&gt; Epoch 135 finished 	ANN training loss 0.003587
&gt;&gt; Epoch 136 finished 	ANN training loss 0.003555
&gt;&gt; Epoch 137 finished 	ANN training loss 0.003520
&gt;&gt; Epoch 138 finished 	ANN training loss 0.003487
&gt;&gt; Epoch 139 finished 	ANN training loss 0.003455
&gt;&gt; Epoch 140 finished 	ANN training loss 0.003423
&gt;&gt; Epoch 141 finished 	ANN training loss 0.003391
&gt;&gt; Epoch 142 finished 	ANN training loss 0.003362
&gt;&gt; Epoch 143 finished 	ANN training loss 0.003331
&gt;&gt; Epoch 144 finished 	ANN training loss 0.003301
&gt;&gt; Epoch 145 finished 	ANN training loss 0.003272
&gt;&gt; Epoch 146 finished 	ANN training loss 0.003244
&gt;&gt; Epoch 147 finished 	ANN training loss 0.003216
&gt;&gt; Epoch 148 finished 	ANN training loss 0.003188
&gt;&gt; Epoch 149 finished 	ANN training loss 0.003161
&gt;&gt; Epoch 150 finished 	ANN training loss 0.003134
&gt;&gt; Epoch 151 finished 	ANN training loss 0.003108
&gt;&gt; Epoch 152 finished 	ANN training loss 0.003082
&gt;&gt; Epoch 153 finished 	ANN training loss 0.003056
&gt;&gt; Epoch 154 finished 	ANN training loss 0.003031
&gt;&gt; Epoch 155 finished 	ANN training loss 0.003006
&gt;&gt; Epoch 156 finished 	ANN training loss 0.002981
&gt;&gt; Epoch 157 finished 	ANN training loss 0.002957
&gt;&gt; Epoch 158 finished 	ANN training loss 0.002934
&gt;&gt; Epoch 159 finished 	ANN training loss 0.002910
&gt;&gt; Epoch 160 finished 	ANN training loss 0.002887
&gt;&gt; Epoch 161 finished 	ANN training loss 0.002865
&gt;&gt; Epoch 162 finished 	ANN training loss 0.002842
&gt;&gt; Epoch 163 finished 	ANN training loss 0.002820
&gt;&gt; Epoch 164 finished 	ANN training loss 0.002798
&gt;&gt; Epoch 165 finished 	ANN training loss 0.002777
&gt;&gt; Epoch 166 finished 	ANN training loss 0.002756
&gt;&gt; Epoch 167 finished 	ANN training loss 0.002735
&gt;&gt; Epoch 168 finished 	ANN training loss 0.002715
&gt;&gt; Epoch 169 finished 	ANN training loss 0.002695
&gt;&gt; Epoch 170 finished 	ANN training loss 0.002674
&gt;&gt; Epoch 171 finished 	ANN training loss 0.002655
&gt;&gt; Epoch 172 finished 	ANN training loss 0.002635
&gt;&gt; Epoch 173 finished 	ANN training loss 0.002616
&gt;&gt; Epoch 174 finished 	ANN training loss 0.002597
&gt;&gt; Epoch 175 finished 	ANN training loss 0.002578
&gt;&gt; Epoch 176 finished 	ANN training loss 0.002560
&gt;&gt; Epoch 177 finished 	ANN training loss 0.002542
&gt;&gt; Epoch 178 finished 	ANN training loss 0.002524
&gt;&gt; Epoch 179 finished 	ANN training loss 0.002506
&gt;&gt; Epoch 180 finished 	ANN training loss 0.002489
&gt;&gt; Epoch 181 finished 	ANN training loss 0.002472
&gt;&gt; Epoch 182 finished 	ANN training loss 0.002455
&gt;&gt; Epoch 183 finished 	ANN training loss 0.002438
&gt;&gt; Epoch 184 finished 	ANN training loss 0.002421
&gt;&gt; Epoch 185 finished 	ANN training loss 0.002405
&gt;&gt; Epoch 186 finished 	ANN training loss 0.002389
&gt;&gt; Epoch 187 finished 	ANN training loss 0.002373
&gt;&gt; Epoch 188 finished 	ANN training loss 0.002357
&gt;&gt; Epoch 189 finished 	ANN training loss 0.002341
&gt;&gt; Epoch 190 finished 	ANN training loss 0.002326
&gt;&gt; Epoch 191 finished 	ANN training loss 0.002311
&gt;&gt; Epoch 192 finished 	ANN training loss 0.002296
&gt;&gt; Epoch 193 finished 	ANN training loss 0.002281
&gt;&gt; Epoch 194 finished 	ANN training loss 0.002266
&gt;&gt; Epoch 195 finished 	ANN training loss 0.002252
&gt;&gt; Epoch 196 finished 	ANN training loss 0.002237
&gt;&gt; Epoch 197 finished 	ANN training loss 0.002223
&gt;&gt; Epoch 198 finished 	ANN training loss 0.002209
&gt;&gt; Epoch 199 finished 	ANN training loss 0.002195
[END] Fine tuning step
Done.
Accuracy: 0.880000
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[7]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ACCURACY: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">acc0</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>ACCURACY: 0.88
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[8]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># ReLu, dropout = 0.1</span>
<span class="n">acc1</span> <span class="o">=</span> <span class="n">deep_belief_net</span><span class="p">(</span><span class="n">hidden_layers_structure</span><span class="o">=</span><span class="p">[</span><span class="mi">300</span><span class="p">],</span> 
                       <span class="n">learning_rate_rbm</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span>
                       <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                       <span class="n">n_epochs_rbm</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span>
                       <span class="n">n_iter_backprop</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
                       <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
                       <span class="n">activation_function</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span>
                       <span class="n">dropout_p</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[START] Pre-training step:
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 72.055847
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 60.757393
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 61.084770
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 52.822067
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 45.157761
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 43.368752
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 55.778893
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 45.162216
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 37.964745
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 33.017380
&gt;&gt; Epoch 11 finished 	RBM Reconstruction error 54.482384
&gt;&gt; Epoch 12 finished 	RBM Reconstruction error 52.822216
&gt;&gt; Epoch 13 finished 	RBM Reconstruction error 46.630253
&gt;&gt; Epoch 14 finished 	RBM Reconstruction error 47.379044
&gt;&gt; Epoch 15 finished 	RBM Reconstruction error 43.645729
&gt;&gt; Epoch 16 finished 	RBM Reconstruction error 39.365711
&gt;&gt; Epoch 17 finished 	RBM Reconstruction error 47.167969
&gt;&gt; Epoch 18 finished 	RBM Reconstruction error 48.273788
&gt;&gt; Epoch 19 finished 	RBM Reconstruction error 45.013912
&gt;&gt; Epoch 20 finished 	RBM Reconstruction error 53.182144
&gt;&gt; Epoch 21 finished 	RBM Reconstruction error 50.217506
&gt;&gt; Epoch 22 finished 	RBM Reconstruction error 54.040089
&gt;&gt; Epoch 23 finished 	RBM Reconstruction error 50.279682
&gt;&gt; Epoch 24 finished 	RBM Reconstruction error 44.892334
&gt;&gt; Epoch 25 finished 	RBM Reconstruction error 60.720314
&gt;&gt; Epoch 26 finished 	RBM Reconstruction error 47.933243
&gt;&gt; Epoch 27 finished 	RBM Reconstruction error 45.958370
&gt;&gt; Epoch 28 finished 	RBM Reconstruction error 49.027050
&gt;&gt; Epoch 29 finished 	RBM Reconstruction error 51.609917
&gt;&gt; Epoch 30 finished 	RBM Reconstruction error 57.335293
&gt;&gt; Epoch 31 finished 	RBM Reconstruction error 53.782314
&gt;&gt; Epoch 32 finished 	RBM Reconstruction error 60.037567
&gt;&gt; Epoch 33 finished 	RBM Reconstruction error 41.730480
&gt;&gt; Epoch 34 finished 	RBM Reconstruction error 49.835506
&gt;&gt; Epoch 35 finished 	RBM Reconstruction error 45.948681
&gt;&gt; Epoch 36 finished 	RBM Reconstruction error 45.618214
&gt;&gt; Epoch 37 finished 	RBM Reconstruction error 58.829765
&gt;&gt; Epoch 38 finished 	RBM Reconstruction error 44.395443
&gt;&gt; Epoch 39 finished 	RBM Reconstruction error 41.659542
&gt;&gt; Epoch 40 finished 	RBM Reconstruction error 56.914200
[END] Pre-training step
[START] Fine tuning step:
&gt;&gt; Epoch 0 finished 	ANN training loss 0.719136
&gt;&gt; Epoch 1 finished 	ANN training loss 0.504289
&gt;&gt; Epoch 2 finished 	ANN training loss 0.404829
&gt;&gt; Epoch 3 finished 	ANN training loss 0.336654
&gt;&gt; Epoch 4 finished 	ANN training loss 0.293528
&gt;&gt; Epoch 5 finished 	ANN training loss 0.264411
&gt;&gt; Epoch 6 finished 	ANN training loss 0.225134
&gt;&gt; Epoch 7 finished 	ANN training loss 0.204277
&gt;&gt; Epoch 8 finished 	ANN training loss 0.206538
&gt;&gt; Epoch 9 finished 	ANN training loss 0.167138
&gt;&gt; Epoch 10 finished 	ANN training loss 0.155270
&gt;&gt; Epoch 11 finished 	ANN training loss 0.138381
&gt;&gt; Epoch 12 finished 	ANN training loss 0.125381
&gt;&gt; Epoch 13 finished 	ANN training loss 0.110737
&gt;&gt; Epoch 14 finished 	ANN training loss 0.101545
&gt;&gt; Epoch 15 finished 	ANN training loss 0.095920
&gt;&gt; Epoch 16 finished 	ANN training loss 0.087776
&gt;&gt; Epoch 17 finished 	ANN training loss 0.083908
&gt;&gt; Epoch 18 finished 	ANN training loss 0.074305
&gt;&gt; Epoch 19 finished 	ANN training loss 0.067529
&gt;&gt; Epoch 20 finished 	ANN training loss 0.062484
&gt;&gt; Epoch 21 finished 	ANN training loss 0.058037
&gt;&gt; Epoch 22 finished 	ANN training loss 0.054194
&gt;&gt; Epoch 23 finished 	ANN training loss 0.050317
&gt;&gt; Epoch 24 finished 	ANN training loss 0.047677
&gt;&gt; Epoch 25 finished 	ANN training loss 0.045548
&gt;&gt; Epoch 26 finished 	ANN training loss 0.040314
&gt;&gt; Epoch 27 finished 	ANN training loss 0.039659
&gt;&gt; Epoch 28 finished 	ANN training loss 0.036556
&gt;&gt; Epoch 29 finished 	ANN training loss 0.035293
&gt;&gt; Epoch 30 finished 	ANN training loss 0.032459
&gt;&gt; Epoch 31 finished 	ANN training loss 0.030028
&gt;&gt; Epoch 32 finished 	ANN training loss 0.028420
&gt;&gt; Epoch 33 finished 	ANN training loss 0.026577
&gt;&gt; Epoch 34 finished 	ANN training loss 0.026243
&gt;&gt; Epoch 35 finished 	ANN training loss 0.024595
&gt;&gt; Epoch 36 finished 	ANN training loss 0.022506
&gt;&gt; Epoch 37 finished 	ANN training loss 0.021923
&gt;&gt; Epoch 38 finished 	ANN training loss 0.020706
&gt;&gt; Epoch 39 finished 	ANN training loss 0.019700
&gt;&gt; Epoch 40 finished 	ANN training loss 0.018735
&gt;&gt; Epoch 41 finished 	ANN training loss 0.018380
&gt;&gt; Epoch 42 finished 	ANN training loss 0.018464
&gt;&gt; Epoch 43 finished 	ANN training loss 0.016393
&gt;&gt; Epoch 44 finished 	ANN training loss 0.016940
&gt;&gt; Epoch 45 finished 	ANN training loss 0.015171
&gt;&gt; Epoch 46 finished 	ANN training loss 0.014689
&gt;&gt; Epoch 47 finished 	ANN training loss 0.013990
&gt;&gt; Epoch 48 finished 	ANN training loss 0.013253
&gt;&gt; Epoch 49 finished 	ANN training loss 0.013086
&gt;&gt; Epoch 50 finished 	ANN training loss 0.013376
&gt;&gt; Epoch 51 finished 	ANN training loss 0.012374
&gt;&gt; Epoch 52 finished 	ANN training loss 0.011646
&gt;&gt; Epoch 53 finished 	ANN training loss 0.011284
&gt;&gt; Epoch 54 finished 	ANN training loss 0.010702
&gt;&gt; Epoch 55 finished 	ANN training loss 0.010475
&gt;&gt; Epoch 56 finished 	ANN training loss 0.010147
&gt;&gt; Epoch 57 finished 	ANN training loss 0.009713
&gt;&gt; Epoch 58 finished 	ANN training loss 0.009365
&gt;&gt; Epoch 59 finished 	ANN training loss 0.009080
&gt;&gt; Epoch 60 finished 	ANN training loss 0.008744
&gt;&gt; Epoch 61 finished 	ANN training loss 0.008477
&gt;&gt; Epoch 62 finished 	ANN training loss 0.008562
&gt;&gt; Epoch 63 finished 	ANN training loss 0.008399
&gt;&gt; Epoch 64 finished 	ANN training loss 0.007853
&gt;&gt; Epoch 65 finished 	ANN training loss 0.007480
&gt;&gt; Epoch 66 finished 	ANN training loss 0.007428
&gt;&gt; Epoch 67 finished 	ANN training loss 0.007179
&gt;&gt; Epoch 68 finished 	ANN training loss 0.007125
&gt;&gt; Epoch 69 finished 	ANN training loss 0.006811
&gt;&gt; Epoch 70 finished 	ANN training loss 0.006664
&gt;&gt; Epoch 71 finished 	ANN training loss 0.006558
&gt;&gt; Epoch 72 finished 	ANN training loss 0.006270
&gt;&gt; Epoch 73 finished 	ANN training loss 0.006157
&gt;&gt; Epoch 74 finished 	ANN training loss 0.006110
&gt;&gt; Epoch 75 finished 	ANN training loss 0.006064
&gt;&gt; Epoch 76 finished 	ANN training loss 0.005861
&gt;&gt; Epoch 77 finished 	ANN training loss 0.005647
&gt;&gt; Epoch 78 finished 	ANN training loss 0.005401
&gt;&gt; Epoch 79 finished 	ANN training loss 0.005372
&gt;&gt; Epoch 80 finished 	ANN training loss 0.005188
&gt;&gt; Epoch 81 finished 	ANN training loss 0.005044
&gt;&gt; Epoch 82 finished 	ANN training loss 0.005081
&gt;&gt; Epoch 83 finished 	ANN training loss 0.005009
&gt;&gt; Epoch 84 finished 	ANN training loss 0.004811
&gt;&gt; Epoch 85 finished 	ANN training loss 0.004680
&gt;&gt; Epoch 86 finished 	ANN training loss 0.004622
&gt;&gt; Epoch 87 finished 	ANN training loss 0.004423
&gt;&gt; Epoch 88 finished 	ANN training loss 0.004543
&gt;&gt; Epoch 89 finished 	ANN training loss 0.004276
&gt;&gt; Epoch 90 finished 	ANN training loss 0.004191
&gt;&gt; Epoch 91 finished 	ANN training loss 0.004067
&gt;&gt; Epoch 92 finished 	ANN training loss 0.004063
&gt;&gt; Epoch 93 finished 	ANN training loss 0.003892
&gt;&gt; Epoch 94 finished 	ANN training loss 0.003982
&gt;&gt; Epoch 95 finished 	ANN training loss 0.003900
&gt;&gt; Epoch 96 finished 	ANN training loss 0.003828
&gt;&gt; Epoch 97 finished 	ANN training loss 0.003719
&gt;&gt; Epoch 98 finished 	ANN training loss 0.003543
&gt;&gt; Epoch 99 finished 	ANN training loss 0.003537
&gt;&gt; Epoch 100 finished 	ANN training loss 0.003511
&gt;&gt; Epoch 101 finished 	ANN training loss 0.003412
&gt;&gt; Epoch 102 finished 	ANN training loss 0.003446
&gt;&gt; Epoch 103 finished 	ANN training loss 0.003311
&gt;&gt; Epoch 104 finished 	ANN training loss 0.003231
&gt;&gt; Epoch 105 finished 	ANN training loss 0.003341
&gt;&gt; Epoch 106 finished 	ANN training loss 0.003307
&gt;&gt; Epoch 107 finished 	ANN training loss 0.003057
&gt;&gt; Epoch 108 finished 	ANN training loss 0.002992
&gt;&gt; Epoch 109 finished 	ANN training loss 0.002903
&gt;&gt; Epoch 110 finished 	ANN training loss 0.002841
&gt;&gt; Epoch 111 finished 	ANN training loss 0.002836
&gt;&gt; Epoch 112 finished 	ANN training loss 0.002826
&gt;&gt; Epoch 113 finished 	ANN training loss 0.002757
&gt;&gt; Epoch 114 finished 	ANN training loss 0.002888
&gt;&gt; Epoch 115 finished 	ANN training loss 0.002703
&gt;&gt; Epoch 116 finished 	ANN training loss 0.002736
&gt;&gt; Epoch 117 finished 	ANN training loss 0.002767
&gt;&gt; Epoch 118 finished 	ANN training loss 0.002571
&gt;&gt; Epoch 119 finished 	ANN training loss 0.002694
&gt;&gt; Epoch 120 finished 	ANN training loss 0.002488
&gt;&gt; Epoch 121 finished 	ANN training loss 0.002435
&gt;&gt; Epoch 122 finished 	ANN training loss 0.002469
&gt;&gt; Epoch 123 finished 	ANN training loss 0.002422
&gt;&gt; Epoch 124 finished 	ANN training loss 0.002348
&gt;&gt; Epoch 125 finished 	ANN training loss 0.002443
&gt;&gt; Epoch 126 finished 	ANN training loss 0.002441
&gt;&gt; Epoch 127 finished 	ANN training loss 0.002294
&gt;&gt; Epoch 128 finished 	ANN training loss 0.002246
&gt;&gt; Epoch 129 finished 	ANN training loss 0.002297
&gt;&gt; Epoch 130 finished 	ANN training loss 0.002211
&gt;&gt; Epoch 131 finished 	ANN training loss 0.002310
&gt;&gt; Epoch 132 finished 	ANN training loss 0.002305
&gt;&gt; Epoch 133 finished 	ANN training loss 0.002118
&gt;&gt; Epoch 134 finished 	ANN training loss 0.002141
&gt;&gt; Epoch 135 finished 	ANN training loss 0.002060
&gt;&gt; Epoch 136 finished 	ANN training loss 0.002042
&gt;&gt; Epoch 137 finished 	ANN training loss 0.002031
&gt;&gt; Epoch 138 finished 	ANN training loss 0.002056
&gt;&gt; Epoch 139 finished 	ANN training loss 0.001979
&gt;&gt; Epoch 140 finished 	ANN training loss 0.002029
&gt;&gt; Epoch 141 finished 	ANN training loss 0.001950
&gt;&gt; Epoch 142 finished 	ANN training loss 0.001955
&gt;&gt; Epoch 143 finished 	ANN training loss 0.001966
&gt;&gt; Epoch 144 finished 	ANN training loss 0.001910
&gt;&gt; Epoch 145 finished 	ANN training loss 0.001879
&gt;&gt; Epoch 146 finished 	ANN training loss 0.001856
&gt;&gt; Epoch 147 finished 	ANN training loss 0.001843
&gt;&gt; Epoch 148 finished 	ANN training loss 0.001780
&gt;&gt; Epoch 149 finished 	ANN training loss 0.001761
&gt;&gt; Epoch 150 finished 	ANN training loss 0.001729
&gt;&gt; Epoch 151 finished 	ANN training loss 0.001707
&gt;&gt; Epoch 152 finished 	ANN training loss 0.001705
&gt;&gt; Epoch 153 finished 	ANN training loss 0.001686
&gt;&gt; Epoch 154 finished 	ANN training loss 0.001666
&gt;&gt; Epoch 155 finished 	ANN training loss 0.001641
&gt;&gt; Epoch 156 finished 	ANN training loss 0.001602
&gt;&gt; Epoch 157 finished 	ANN training loss 0.001607
&gt;&gt; Epoch 158 finished 	ANN training loss 0.001651
&gt;&gt; Epoch 159 finished 	ANN training loss 0.001547
&gt;&gt; Epoch 160 finished 	ANN training loss 0.001616
&gt;&gt; Epoch 161 finished 	ANN training loss 0.001520
&gt;&gt; Epoch 162 finished 	ANN training loss 0.001496
&gt;&gt; Epoch 163 finished 	ANN training loss 0.001513
&gt;&gt; Epoch 164 finished 	ANN training loss 0.001478
&gt;&gt; Epoch 165 finished 	ANN training loss 0.001435
&gt;&gt; Epoch 166 finished 	ANN training loss 0.001418
&gt;&gt; Epoch 167 finished 	ANN training loss 0.001439
&gt;&gt; Epoch 168 finished 	ANN training loss 0.001394
&gt;&gt; Epoch 169 finished 	ANN training loss 0.001379
&gt;&gt; Epoch 170 finished 	ANN training loss 0.001406
&gt;&gt; Epoch 171 finished 	ANN training loss 0.001360
&gt;&gt; Epoch 172 finished 	ANN training loss 0.001347
&gt;&gt; Epoch 173 finished 	ANN training loss 0.001317
&gt;&gt; Epoch 174 finished 	ANN training loss 0.001292
&gt;&gt; Epoch 175 finished 	ANN training loss 0.001286
&gt;&gt; Epoch 176 finished 	ANN training loss 0.001330
&gt;&gt; Epoch 177 finished 	ANN training loss 0.001292
&gt;&gt; Epoch 178 finished 	ANN training loss 0.001255
&gt;&gt; Epoch 179 finished 	ANN training loss 0.001262
&gt;&gt; Epoch 180 finished 	ANN training loss 0.001244
&gt;&gt; Epoch 181 finished 	ANN training loss 0.001239
&gt;&gt; Epoch 182 finished 	ANN training loss 0.001227
&gt;&gt; Epoch 183 finished 	ANN training loss 0.001194
&gt;&gt; Epoch 184 finished 	ANN training loss 0.001229
&gt;&gt; Epoch 185 finished 	ANN training loss 0.001202
&gt;&gt; Epoch 186 finished 	ANN training loss 0.001213
&gt;&gt; Epoch 187 finished 	ANN training loss 0.001196
&gt;&gt; Epoch 188 finished 	ANN training loss 0.001206
&gt;&gt; Epoch 189 finished 	ANN training loss 0.001159
&gt;&gt; Epoch 190 finished 	ANN training loss 0.001127
&gt;&gt; Epoch 191 finished 	ANN training loss 0.001106
&gt;&gt; Epoch 192 finished 	ANN training loss 0.001119
&gt;&gt; Epoch 193 finished 	ANN training loss 0.001094
&gt;&gt; Epoch 194 finished 	ANN training loss 0.001082
&gt;&gt; Epoch 195 finished 	ANN training loss 0.001071
&gt;&gt; Epoch 196 finished 	ANN training loss 0.001051
&gt;&gt; Epoch 197 finished 	ANN training loss 0.001081
&gt;&gt; Epoch 198 finished 	ANN training loss 0.001072
&gt;&gt; Epoch 199 finished 	ANN training loss 0.001053
[END] Fine tuning step
Done.
Accuracy: 0.890000
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[9]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ACCURACY: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">acc1</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>ACCURACY: 0.89
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[10]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># ReLu, dropout = 0.3</span>
<span class="n">acc3</span> <span class="o">=</span> <span class="n">deep_belief_net</span><span class="p">(</span><span class="n">hidden_layers_structure</span><span class="o">=</span><span class="p">[</span><span class="mi">300</span><span class="p">],</span> 
                       <span class="n">learning_rate_rbm</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span>
                       <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                       <span class="n">n_epochs_rbm</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span>
                       <span class="n">n_iter_backprop</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
                       <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
                       <span class="n">activation_function</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span>
                       <span class="n">dropout_p</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[START] Pre-training step:
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 56.206200
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 77.201317
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 43.497169
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 35.043930
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 29.859175
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 36.687565
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 43.480713
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 46.617226
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 41.150391
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 35.490501
&gt;&gt; Epoch 11 finished 	RBM Reconstruction error 34.342411
&gt;&gt; Epoch 12 finished 	RBM Reconstruction error 29.118097
&gt;&gt; Epoch 13 finished 	RBM Reconstruction error 33.308773
&gt;&gt; Epoch 14 finished 	RBM Reconstruction error 37.300919
&gt;&gt; Epoch 15 finished 	RBM Reconstruction error 44.223289
&gt;&gt; Epoch 16 finished 	RBM Reconstruction error 43.610748
&gt;&gt; Epoch 17 finished 	RBM Reconstruction error 42.425957
&gt;&gt; Epoch 18 finished 	RBM Reconstruction error 33.686352
&gt;&gt; Epoch 19 finished 	RBM Reconstruction error 32.535210
&gt;&gt; Epoch 20 finished 	RBM Reconstruction error 40.499744
&gt;&gt; Epoch 21 finished 	RBM Reconstruction error 30.235912
&gt;&gt; Epoch 22 finished 	RBM Reconstruction error 39.815880
&gt;&gt; Epoch 23 finished 	RBM Reconstruction error 46.056564
&gt;&gt; Epoch 24 finished 	RBM Reconstruction error 43.469513
&gt;&gt; Epoch 25 finished 	RBM Reconstruction error 37.808216
&gt;&gt; Epoch 26 finished 	RBM Reconstruction error 38.587616
&gt;&gt; Epoch 27 finished 	RBM Reconstruction error 39.078217
&gt;&gt; Epoch 28 finished 	RBM Reconstruction error 31.427929
&gt;&gt; Epoch 29 finished 	RBM Reconstruction error 38.274857
&gt;&gt; Epoch 30 finished 	RBM Reconstruction error 37.169632
&gt;&gt; Epoch 31 finished 	RBM Reconstruction error 36.369694
&gt;&gt; Epoch 32 finished 	RBM Reconstruction error 44.888748
&gt;&gt; Epoch 33 finished 	RBM Reconstruction error 60.992508
&gt;&gt; Epoch 34 finished 	RBM Reconstruction error 43.566807
&gt;&gt; Epoch 35 finished 	RBM Reconstruction error 71.165504
&gt;&gt; Epoch 36 finished 	RBM Reconstruction error 48.004337
&gt;&gt; Epoch 37 finished 	RBM Reconstruction error 56.236366
&gt;&gt; Epoch 38 finished 	RBM Reconstruction error 37.914242
&gt;&gt; Epoch 39 finished 	RBM Reconstruction error 40.765881
&gt;&gt; Epoch 40 finished 	RBM Reconstruction error 58.849442
[END] Pre-training step
[START] Fine tuning step:
&gt;&gt; Epoch 0 finished 	ANN training loss 0.763057
&gt;&gt; Epoch 1 finished 	ANN training loss 0.530294
&gt;&gt; Epoch 2 finished 	ANN training loss 0.407203
&gt;&gt; Epoch 3 finished 	ANN training loss 0.366598
&gt;&gt; Epoch 4 finished 	ANN training loss 0.309556
&gt;&gt; Epoch 5 finished 	ANN training loss 0.273724
&gt;&gt; Epoch 6 finished 	ANN training loss 0.249534
&gt;&gt; Epoch 7 finished 	ANN training loss 0.225815
&gt;&gt; Epoch 8 finished 	ANN training loss 0.228244
&gt;&gt; Epoch 9 finished 	ANN training loss 0.196719
&gt;&gt; Epoch 10 finished 	ANN training loss 0.180711
&gt;&gt; Epoch 11 finished 	ANN training loss 0.159670
&gt;&gt; Epoch 12 finished 	ANN training loss 0.151425
&gt;&gt; Epoch 13 finished 	ANN training loss 0.145652
&gt;&gt; Epoch 14 finished 	ANN training loss 0.130921
&gt;&gt; Epoch 15 finished 	ANN training loss 0.117465
&gt;&gt; Epoch 16 finished 	ANN training loss 0.108762
&gt;&gt; Epoch 17 finished 	ANN training loss 0.103295
&gt;&gt; Epoch 18 finished 	ANN training loss 0.101955
&gt;&gt; Epoch 19 finished 	ANN training loss 0.095027
&gt;&gt; Epoch 20 finished 	ANN training loss 0.085982
&gt;&gt; Epoch 21 finished 	ANN training loss 0.082087
&gt;&gt; Epoch 22 finished 	ANN training loss 0.075599
&gt;&gt; Epoch 23 finished 	ANN training loss 0.069679
&gt;&gt; Epoch 24 finished 	ANN training loss 0.063957
&gt;&gt; Epoch 25 finished 	ANN training loss 0.063379
&gt;&gt; Epoch 26 finished 	ANN training loss 0.057712
&gt;&gt; Epoch 27 finished 	ANN training loss 0.057132
&gt;&gt; Epoch 28 finished 	ANN training loss 0.051099
&gt;&gt; Epoch 29 finished 	ANN training loss 0.050962
&gt;&gt; Epoch 30 finished 	ANN training loss 0.045647
&gt;&gt; Epoch 31 finished 	ANN training loss 0.041795
&gt;&gt; Epoch 32 finished 	ANN training loss 0.040142
&gt;&gt; Epoch 33 finished 	ANN training loss 0.037400
&gt;&gt; Epoch 34 finished 	ANN training loss 0.037238
&gt;&gt; Epoch 35 finished 	ANN training loss 0.033529
&gt;&gt; Epoch 36 finished 	ANN training loss 0.032800
&gt;&gt; Epoch 37 finished 	ANN training loss 0.030926
&gt;&gt; Epoch 38 finished 	ANN training loss 0.029241
&gt;&gt; Epoch 39 finished 	ANN training loss 0.031015
&gt;&gt; Epoch 40 finished 	ANN training loss 0.028163
&gt;&gt; Epoch 41 finished 	ANN training loss 0.027726
&gt;&gt; Epoch 42 finished 	ANN training loss 0.024614
&gt;&gt; Epoch 43 finished 	ANN training loss 0.022441
&gt;&gt; Epoch 44 finished 	ANN training loss 0.020821
&gt;&gt; Epoch 45 finished 	ANN training loss 0.020281
&gt;&gt; Epoch 46 finished 	ANN training loss 0.020302
&gt;&gt; Epoch 47 finished 	ANN training loss 0.019095
&gt;&gt; Epoch 48 finished 	ANN training loss 0.018188
&gt;&gt; Epoch 49 finished 	ANN training loss 0.017712
&gt;&gt; Epoch 50 finished 	ANN training loss 0.018247
&gt;&gt; Epoch 51 finished 	ANN training loss 0.015728
&gt;&gt; Epoch 52 finished 	ANN training loss 0.015964
&gt;&gt; Epoch 53 finished 	ANN training loss 0.015110
&gt;&gt; Epoch 54 finished 	ANN training loss 0.015308
&gt;&gt; Epoch 55 finished 	ANN training loss 0.014228
&gt;&gt; Epoch 56 finished 	ANN training loss 0.013407
&gt;&gt; Epoch 57 finished 	ANN training loss 0.012899
&gt;&gt; Epoch 58 finished 	ANN training loss 0.011997
&gt;&gt; Epoch 59 finished 	ANN training loss 0.012016
&gt;&gt; Epoch 60 finished 	ANN training loss 0.011425
&gt;&gt; Epoch 61 finished 	ANN training loss 0.011710
&gt;&gt; Epoch 62 finished 	ANN training loss 0.011182
&gt;&gt; Epoch 63 finished 	ANN training loss 0.010530
&gt;&gt; Epoch 64 finished 	ANN training loss 0.010343
&gt;&gt; Epoch 65 finished 	ANN training loss 0.010221
&gt;&gt; Epoch 66 finished 	ANN training loss 0.009676
&gt;&gt; Epoch 67 finished 	ANN training loss 0.009703
&gt;&gt; Epoch 68 finished 	ANN training loss 0.009371
&gt;&gt; Epoch 69 finished 	ANN training loss 0.008724
&gt;&gt; Epoch 70 finished 	ANN training loss 0.008507
&gt;&gt; Epoch 71 finished 	ANN training loss 0.008132
&gt;&gt; Epoch 72 finished 	ANN training loss 0.007937
&gt;&gt; Epoch 73 finished 	ANN training loss 0.007824
&gt;&gt; Epoch 74 finished 	ANN training loss 0.007243
&gt;&gt; Epoch 75 finished 	ANN training loss 0.007016
&gt;&gt; Epoch 76 finished 	ANN training loss 0.007076
&gt;&gt; Epoch 77 finished 	ANN training loss 0.007010
&gt;&gt; Epoch 78 finished 	ANN training loss 0.007044
&gt;&gt; Epoch 79 finished 	ANN training loss 0.006529
&gt;&gt; Epoch 80 finished 	ANN training loss 0.006264
&gt;&gt; Epoch 81 finished 	ANN training loss 0.006160
&gt;&gt; Epoch 82 finished 	ANN training loss 0.005895
&gt;&gt; Epoch 83 finished 	ANN training loss 0.005891
&gt;&gt; Epoch 84 finished 	ANN training loss 0.006012
&gt;&gt; Epoch 85 finished 	ANN training loss 0.005802
&gt;&gt; Epoch 86 finished 	ANN training loss 0.005600
&gt;&gt; Epoch 87 finished 	ANN training loss 0.005332
&gt;&gt; Epoch 88 finished 	ANN training loss 0.005320
&gt;&gt; Epoch 89 finished 	ANN training loss 0.005220
&gt;&gt; Epoch 90 finished 	ANN training loss 0.004750
&gt;&gt; Epoch 91 finished 	ANN training loss 0.004751
&gt;&gt; Epoch 92 finished 	ANN training loss 0.004809
&gt;&gt; Epoch 93 finished 	ANN training loss 0.004848
&gt;&gt; Epoch 94 finished 	ANN training loss 0.004493
&gt;&gt; Epoch 95 finished 	ANN training loss 0.004508
&gt;&gt; Epoch 96 finished 	ANN training loss 0.004324
&gt;&gt; Epoch 97 finished 	ANN training loss 0.003982
&gt;&gt; Epoch 98 finished 	ANN training loss 0.004481
&gt;&gt; Epoch 99 finished 	ANN training loss 0.003984
&gt;&gt; Epoch 100 finished 	ANN training loss 0.003924
&gt;&gt; Epoch 101 finished 	ANN training loss 0.003975
&gt;&gt; Epoch 102 finished 	ANN training loss 0.003715
&gt;&gt; Epoch 103 finished 	ANN training loss 0.003609
&gt;&gt; Epoch 104 finished 	ANN training loss 0.004448
&gt;&gt; Epoch 105 finished 	ANN training loss 0.003661
&gt;&gt; Epoch 106 finished 	ANN training loss 0.003542
&gt;&gt; Epoch 107 finished 	ANN training loss 0.003499
&gt;&gt; Epoch 108 finished 	ANN training loss 0.003301
&gt;&gt; Epoch 109 finished 	ANN training loss 0.003398
&gt;&gt; Epoch 110 finished 	ANN training loss 0.003172
&gt;&gt; Epoch 111 finished 	ANN training loss 0.003518
&gt;&gt; Epoch 112 finished 	ANN training loss 0.003141
&gt;&gt; Epoch 113 finished 	ANN training loss 0.003205
&gt;&gt; Epoch 114 finished 	ANN training loss 0.003429
&gt;&gt; Epoch 115 finished 	ANN training loss 0.003150
&gt;&gt; Epoch 116 finished 	ANN training loss 0.002912
&gt;&gt; Epoch 117 finished 	ANN training loss 0.002873
&gt;&gt; Epoch 118 finished 	ANN training loss 0.002985
&gt;&gt; Epoch 119 finished 	ANN training loss 0.002825
&gt;&gt; Epoch 120 finished 	ANN training loss 0.002813
&gt;&gt; Epoch 121 finished 	ANN training loss 0.002841
&gt;&gt; Epoch 122 finished 	ANN training loss 0.002842
&gt;&gt; Epoch 123 finished 	ANN training loss 0.002755
&gt;&gt; Epoch 124 finished 	ANN training loss 0.002744
&gt;&gt; Epoch 125 finished 	ANN training loss 0.002752
&gt;&gt; Epoch 126 finished 	ANN training loss 0.002589
&gt;&gt; Epoch 127 finished 	ANN training loss 0.002507
&gt;&gt; Epoch 128 finished 	ANN training loss 0.002486
&gt;&gt; Epoch 129 finished 	ANN training loss 0.002604
&gt;&gt; Epoch 130 finished 	ANN training loss 0.002511
&gt;&gt; Epoch 131 finished 	ANN training loss 0.002479
&gt;&gt; Epoch 132 finished 	ANN training loss 0.002458
&gt;&gt; Epoch 133 finished 	ANN training loss 0.002214
&gt;&gt; Epoch 134 finished 	ANN training loss 0.002235
&gt;&gt; Epoch 135 finished 	ANN training loss 0.002137
&gt;&gt; Epoch 136 finished 	ANN training loss 0.002161
&gt;&gt; Epoch 137 finished 	ANN training loss 0.002176
&gt;&gt; Epoch 138 finished 	ANN training loss 0.002038
&gt;&gt; Epoch 139 finished 	ANN training loss 0.002051
&gt;&gt; Epoch 140 finished 	ANN training loss 0.002209
&gt;&gt; Epoch 141 finished 	ANN training loss 0.002014
&gt;&gt; Epoch 142 finished 	ANN training loss 0.002032
&gt;&gt; Epoch 143 finished 	ANN training loss 0.001981
&gt;&gt; Epoch 144 finished 	ANN training loss 0.001948
&gt;&gt; Epoch 145 finished 	ANN training loss 0.001881
&gt;&gt; Epoch 146 finished 	ANN training loss 0.001842
&gt;&gt; Epoch 147 finished 	ANN training loss 0.001859
&gt;&gt; Epoch 148 finished 	ANN training loss 0.001873
&gt;&gt; Epoch 149 finished 	ANN training loss 0.001858
&gt;&gt; Epoch 150 finished 	ANN training loss 0.001693
&gt;&gt; Epoch 151 finished 	ANN training loss 0.001726
&gt;&gt; Epoch 152 finished 	ANN training loss 0.001710
&gt;&gt; Epoch 153 finished 	ANN training loss 0.001604
&gt;&gt; Epoch 154 finished 	ANN training loss 0.001633
&gt;&gt; Epoch 155 finished 	ANN training loss 0.001661
&gt;&gt; Epoch 156 finished 	ANN training loss 0.001658
&gt;&gt; Epoch 157 finished 	ANN training loss 0.001620
&gt;&gt; Epoch 158 finished 	ANN training loss 0.001585
&gt;&gt; Epoch 159 finished 	ANN training loss 0.001647
&gt;&gt; Epoch 160 finished 	ANN training loss 0.001519
&gt;&gt; Epoch 161 finished 	ANN training loss 0.001536
&gt;&gt; Epoch 162 finished 	ANN training loss 0.001548
&gt;&gt; Epoch 163 finished 	ANN training loss 0.001427
&gt;&gt; Epoch 164 finished 	ANN training loss 0.001456
&gt;&gt; Epoch 165 finished 	ANN training loss 0.001384
&gt;&gt; Epoch 166 finished 	ANN training loss 0.001317
&gt;&gt; Epoch 167 finished 	ANN training loss 0.001319
&gt;&gt; Epoch 168 finished 	ANN training loss 0.001314
&gt;&gt; Epoch 169 finished 	ANN training loss 0.001441
&gt;&gt; Epoch 170 finished 	ANN training loss 0.001251
&gt;&gt; Epoch 171 finished 	ANN training loss 0.001252
&gt;&gt; Epoch 172 finished 	ANN training loss 0.001346
&gt;&gt; Epoch 173 finished 	ANN training loss 0.001220
&gt;&gt; Epoch 174 finished 	ANN training loss 0.001165
&gt;&gt; Epoch 175 finished 	ANN training loss 0.001212
&gt;&gt; Epoch 176 finished 	ANN training loss 0.001158
&gt;&gt; Epoch 177 finished 	ANN training loss 0.001117
&gt;&gt; Epoch 178 finished 	ANN training loss 0.001086
&gt;&gt; Epoch 179 finished 	ANN training loss 0.001166
&gt;&gt; Epoch 180 finished 	ANN training loss 0.001109
&gt;&gt; Epoch 181 finished 	ANN training loss 0.001073
&gt;&gt; Epoch 182 finished 	ANN training loss 0.001125
&gt;&gt; Epoch 183 finished 	ANN training loss 0.001089
&gt;&gt; Epoch 184 finished 	ANN training loss 0.001099
&gt;&gt; Epoch 185 finished 	ANN training loss 0.001049
&gt;&gt; Epoch 186 finished 	ANN training loss 0.001056
&gt;&gt; Epoch 187 finished 	ANN training loss 0.001073
&gt;&gt; Epoch 188 finished 	ANN training loss 0.001067
&gt;&gt; Epoch 189 finished 	ANN training loss 0.001031
&gt;&gt; Epoch 190 finished 	ANN training loss 0.000993
&gt;&gt; Epoch 191 finished 	ANN training loss 0.000976
&gt;&gt; Epoch 192 finished 	ANN training loss 0.001036
&gt;&gt; Epoch 193 finished 	ANN training loss 0.001026
&gt;&gt; Epoch 194 finished 	ANN training loss 0.000949
&gt;&gt; Epoch 195 finished 	ANN training loss 0.000948
&gt;&gt; Epoch 196 finished 	ANN training loss 0.000929
&gt;&gt; Epoch 197 finished 	ANN training loss 0.000967
&gt;&gt; Epoch 198 finished 	ANN training loss 0.000988
&gt;&gt; Epoch 199 finished 	ANN training loss 0.001036
[END] Fine tuning step
Done.
Accuracy: 0.900000
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[11]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ACCURACY: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">acc3</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>ACCURACY: 0.9
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[12]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># ReLu, dropout = 0.4</span>
<span class="n">acc4</span> <span class="o">=</span> <span class="n">deep_belief_net</span><span class="p">(</span><span class="n">hidden_layers_structure</span><span class="o">=</span><span class="p">[</span><span class="mi">300</span><span class="p">],</span> 
                       <span class="n">learning_rate_rbm</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span>
                       <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                       <span class="n">n_epochs_rbm</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span>
                       <span class="n">n_iter_backprop</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
                       <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
                       <span class="n">activation_function</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span>
                       <span class="n">dropout_p</span><span class="o">=</span><span class="mf">0.4</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[START] Pre-training step:
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 65.088890
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 31.600693
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 38.023712
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 38.858429
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 33.014267
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 27.287451
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 27.105581
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 36.422344
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 59.886730
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 32.112198
&gt;&gt; Epoch 11 finished 	RBM Reconstruction error 26.038902
&gt;&gt; Epoch 12 finished 	RBM Reconstruction error 29.801992
&gt;&gt; Epoch 13 finished 	RBM Reconstruction error 28.663811
&gt;&gt; Epoch 14 finished 	RBM Reconstruction error 26.208193
&gt;&gt; Epoch 15 finished 	RBM Reconstruction error 25.640636
&gt;&gt; Epoch 16 finished 	RBM Reconstruction error 25.160589
&gt;&gt; Epoch 17 finished 	RBM Reconstruction error 33.832520
&gt;&gt; Epoch 18 finished 	RBM Reconstruction error 35.933903
&gt;&gt; Epoch 19 finished 	RBM Reconstruction error 38.471088
&gt;&gt; Epoch 20 finished 	RBM Reconstruction error 40.283703
&gt;&gt; Epoch 21 finished 	RBM Reconstruction error 34.686333
&gt;&gt; Epoch 22 finished 	RBM Reconstruction error 36.088261
&gt;&gt; Epoch 23 finished 	RBM Reconstruction error 33.995361
&gt;&gt; Epoch 24 finished 	RBM Reconstruction error 32.413864
&gt;&gt; Epoch 25 finished 	RBM Reconstruction error 35.722523
&gt;&gt; Epoch 26 finished 	RBM Reconstruction error 27.134438
&gt;&gt; Epoch 27 finished 	RBM Reconstruction error 35.185036
&gt;&gt; Epoch 28 finished 	RBM Reconstruction error 20.068762
&gt;&gt; Epoch 29 finished 	RBM Reconstruction error 29.205156
&gt;&gt; Epoch 30 finished 	RBM Reconstruction error 27.306763
&gt;&gt; Epoch 31 finished 	RBM Reconstruction error 38.133926
&gt;&gt; Epoch 32 finished 	RBM Reconstruction error 29.115192
&gt;&gt; Epoch 33 finished 	RBM Reconstruction error 31.015118
&gt;&gt; Epoch 34 finished 	RBM Reconstruction error 30.365646
&gt;&gt; Epoch 35 finished 	RBM Reconstruction error 27.149424
&gt;&gt; Epoch 36 finished 	RBM Reconstruction error 34.585625
&gt;&gt; Epoch 37 finished 	RBM Reconstruction error 39.828106
&gt;&gt; Epoch 38 finished 	RBM Reconstruction error 36.040863
&gt;&gt; Epoch 39 finished 	RBM Reconstruction error 51.653358
&gt;&gt; Epoch 40 finished 	RBM Reconstruction error 43.784103
[END] Pre-training step
[START] Fine tuning step:
&gt;&gt; Epoch 0 finished 	ANN training loss 0.760663
&gt;&gt; Epoch 1 finished 	ANN training loss 0.540571
&gt;&gt; Epoch 2 finished 	ANN training loss 0.433031
&gt;&gt; Epoch 3 finished 	ANN training loss 0.367661
&gt;&gt; Epoch 4 finished 	ANN training loss 0.334500
&gt;&gt; Epoch 5 finished 	ANN training loss 0.305008
&gt;&gt; Epoch 6 finished 	ANN training loss 0.263462
&gt;&gt; Epoch 7 finished 	ANN training loss 0.247169
&gt;&gt; Epoch 8 finished 	ANN training loss 0.230201
&gt;&gt; Epoch 9 finished 	ANN training loss 0.207002
&gt;&gt; Epoch 10 finished 	ANN training loss 0.187797
&gt;&gt; Epoch 11 finished 	ANN training loss 0.187918
&gt;&gt; Epoch 12 finished 	ANN training loss 0.167484
&gt;&gt; Epoch 13 finished 	ANN training loss 0.161223
&gt;&gt; Epoch 14 finished 	ANN training loss 0.140987
&gt;&gt; Epoch 15 finished 	ANN training loss 0.131901
&gt;&gt; Epoch 16 finished 	ANN training loss 0.132539
&gt;&gt; Epoch 17 finished 	ANN training loss 0.113445
&gt;&gt; Epoch 18 finished 	ANN training loss 0.107376
&gt;&gt; Epoch 19 finished 	ANN training loss 0.099010
&gt;&gt; Epoch 20 finished 	ANN training loss 0.093148
&gt;&gt; Epoch 21 finished 	ANN training loss 0.087651
&gt;&gt; Epoch 22 finished 	ANN training loss 0.088220
&gt;&gt; Epoch 23 finished 	ANN training loss 0.076922
&gt;&gt; Epoch 24 finished 	ANN training loss 0.070750
&gt;&gt; Epoch 25 finished 	ANN training loss 0.067853
&gt;&gt; Epoch 26 finished 	ANN training loss 0.061786
&gt;&gt; Epoch 27 finished 	ANN training loss 0.060651
&gt;&gt; Epoch 28 finished 	ANN training loss 0.059891
&gt;&gt; Epoch 29 finished 	ANN training loss 0.054117
&gt;&gt; Epoch 30 finished 	ANN training loss 0.049212
&gt;&gt; Epoch 31 finished 	ANN training loss 0.047731
&gt;&gt; Epoch 32 finished 	ANN training loss 0.047263
&gt;&gt; Epoch 33 finished 	ANN training loss 0.045344
&gt;&gt; Epoch 34 finished 	ANN training loss 0.039465
&gt;&gt; Epoch 35 finished 	ANN training loss 0.040156
&gt;&gt; Epoch 36 finished 	ANN training loss 0.038256
&gt;&gt; Epoch 37 finished 	ANN training loss 0.033516
&gt;&gt; Epoch 38 finished 	ANN training loss 0.032147
&gt;&gt; Epoch 39 finished 	ANN training loss 0.030175
&gt;&gt; Epoch 40 finished 	ANN training loss 0.028922
&gt;&gt; Epoch 41 finished 	ANN training loss 0.028049
&gt;&gt; Epoch 42 finished 	ANN training loss 0.028505
&gt;&gt; Epoch 43 finished 	ANN training loss 0.025962
&gt;&gt; Epoch 44 finished 	ANN training loss 0.027882
&gt;&gt; Epoch 45 finished 	ANN training loss 0.023889
&gt;&gt; Epoch 46 finished 	ANN training loss 0.024069
&gt;&gt; Epoch 47 finished 	ANN training loss 0.022757
&gt;&gt; Epoch 48 finished 	ANN training loss 0.020988
&gt;&gt; Epoch 49 finished 	ANN training loss 0.019761
&gt;&gt; Epoch 50 finished 	ANN training loss 0.019384
&gt;&gt; Epoch 51 finished 	ANN training loss 0.018481
&gt;&gt; Epoch 52 finished 	ANN training loss 0.018197
&gt;&gt; Epoch 53 finished 	ANN training loss 0.017184
&gt;&gt; Epoch 54 finished 	ANN training loss 0.016559
&gt;&gt; Epoch 55 finished 	ANN training loss 0.015837
&gt;&gt; Epoch 56 finished 	ANN training loss 0.015603
&gt;&gt; Epoch 57 finished 	ANN training loss 0.014552
&gt;&gt; Epoch 58 finished 	ANN training loss 0.014755
&gt;&gt; Epoch 59 finished 	ANN training loss 0.014393
&gt;&gt; Epoch 60 finished 	ANN training loss 0.013081
&gt;&gt; Epoch 61 finished 	ANN training loss 0.013090
&gt;&gt; Epoch 62 finished 	ANN training loss 0.012672
&gt;&gt; Epoch 63 finished 	ANN training loss 0.012080
&gt;&gt; Epoch 64 finished 	ANN training loss 0.011608
&gt;&gt; Epoch 65 finished 	ANN training loss 0.011293
&gt;&gt; Epoch 66 finished 	ANN training loss 0.011283
&gt;&gt; Epoch 67 finished 	ANN training loss 0.010149
&gt;&gt; Epoch 68 finished 	ANN training loss 0.010224
&gt;&gt; Epoch 69 finished 	ANN training loss 0.009526
&gt;&gt; Epoch 70 finished 	ANN training loss 0.009065
&gt;&gt; Epoch 71 finished 	ANN training loss 0.008932
&gt;&gt; Epoch 72 finished 	ANN training loss 0.008509
&gt;&gt; Epoch 73 finished 	ANN training loss 0.008653
&gt;&gt; Epoch 74 finished 	ANN training loss 0.008121
&gt;&gt; Epoch 75 finished 	ANN training loss 0.008296
&gt;&gt; Epoch 76 finished 	ANN training loss 0.009232
&gt;&gt; Epoch 77 finished 	ANN training loss 0.008287
&gt;&gt; Epoch 78 finished 	ANN training loss 0.007904
&gt;&gt; Epoch 79 finished 	ANN training loss 0.007892
&gt;&gt; Epoch 80 finished 	ANN training loss 0.007847
&gt;&gt; Epoch 81 finished 	ANN training loss 0.007044
&gt;&gt; Epoch 82 finished 	ANN training loss 0.007070
&gt;&gt; Epoch 83 finished 	ANN training loss 0.007527
&gt;&gt; Epoch 84 finished 	ANN training loss 0.006631
&gt;&gt; Epoch 85 finished 	ANN training loss 0.006455
&gt;&gt; Epoch 86 finished 	ANN training loss 0.006417
&gt;&gt; Epoch 87 finished 	ANN training loss 0.006191
&gt;&gt; Epoch 88 finished 	ANN training loss 0.006217
&gt;&gt; Epoch 89 finished 	ANN training loss 0.005586
&gt;&gt; Epoch 90 finished 	ANN training loss 0.005784
&gt;&gt; Epoch 91 finished 	ANN training loss 0.006124
&gt;&gt; Epoch 92 finished 	ANN training loss 0.005275
&gt;&gt; Epoch 93 finished 	ANN training loss 0.005089
&gt;&gt; Epoch 94 finished 	ANN training loss 0.005158
&gt;&gt; Epoch 95 finished 	ANN training loss 0.005294
&gt;&gt; Epoch 96 finished 	ANN training loss 0.005072
&gt;&gt; Epoch 97 finished 	ANN training loss 0.004727
&gt;&gt; Epoch 98 finished 	ANN training loss 0.004769
&gt;&gt; Epoch 99 finished 	ANN training loss 0.004815
&gt;&gt; Epoch 100 finished 	ANN training loss 0.004321
&gt;&gt; Epoch 101 finished 	ANN training loss 0.004229
&gt;&gt; Epoch 102 finished 	ANN training loss 0.004163
&gt;&gt; Epoch 103 finished 	ANN training loss 0.003947
&gt;&gt; Epoch 104 finished 	ANN training loss 0.004345
&gt;&gt; Epoch 105 finished 	ANN training loss 0.004266
&gt;&gt; Epoch 106 finished 	ANN training loss 0.004294
&gt;&gt; Epoch 107 finished 	ANN training loss 0.004411
&gt;&gt; Epoch 108 finished 	ANN training loss 0.004287
&gt;&gt; Epoch 109 finished 	ANN training loss 0.004515
&gt;&gt; Epoch 110 finished 	ANN training loss 0.003764
&gt;&gt; Epoch 111 finished 	ANN training loss 0.003756
&gt;&gt; Epoch 112 finished 	ANN training loss 0.003706
&gt;&gt; Epoch 113 finished 	ANN training loss 0.003733
&gt;&gt; Epoch 114 finished 	ANN training loss 0.003600
&gt;&gt; Epoch 115 finished 	ANN training loss 0.003618
&gt;&gt; Epoch 116 finished 	ANN training loss 0.003612
&gt;&gt; Epoch 117 finished 	ANN training loss 0.003341
&gt;&gt; Epoch 118 finished 	ANN training loss 0.003087
&gt;&gt; Epoch 119 finished 	ANN training loss 0.002835
&gt;&gt; Epoch 120 finished 	ANN training loss 0.002890
&gt;&gt; Epoch 121 finished 	ANN training loss 0.002846
&gt;&gt; Epoch 122 finished 	ANN training loss 0.002828
&gt;&gt; Epoch 123 finished 	ANN training loss 0.002786
&gt;&gt; Epoch 124 finished 	ANN training loss 0.002700
&gt;&gt; Epoch 125 finished 	ANN training loss 0.002664
&gt;&gt; Epoch 126 finished 	ANN training loss 0.002936
&gt;&gt; Epoch 127 finished 	ANN training loss 0.002664
&gt;&gt; Epoch 128 finished 	ANN training loss 0.002792
&gt;&gt; Epoch 129 finished 	ANN training loss 0.002532
&gt;&gt; Epoch 130 finished 	ANN training loss 0.002754
&gt;&gt; Epoch 131 finished 	ANN training loss 0.002500
&gt;&gt; Epoch 132 finished 	ANN training loss 0.002630
&gt;&gt; Epoch 133 finished 	ANN training loss 0.002571
&gt;&gt; Epoch 134 finished 	ANN training loss 0.002294
&gt;&gt; Epoch 135 finished 	ANN training loss 0.002241
&gt;&gt; Epoch 136 finished 	ANN training loss 0.002210
&gt;&gt; Epoch 137 finished 	ANN training loss 0.002151
&gt;&gt; Epoch 138 finished 	ANN training loss 0.002121
&gt;&gt; Epoch 139 finished 	ANN training loss 0.002192
&gt;&gt; Epoch 140 finished 	ANN training loss 0.002248
&gt;&gt; Epoch 141 finished 	ANN training loss 0.002020
&gt;&gt; Epoch 142 finished 	ANN training loss 0.002027
&gt;&gt; Epoch 143 finished 	ANN training loss 0.002152
&gt;&gt; Epoch 144 finished 	ANN training loss 0.002011
&gt;&gt; Epoch 145 finished 	ANN training loss 0.002446
&gt;&gt; Epoch 146 finished 	ANN training loss 0.002088
&gt;&gt; Epoch 147 finished 	ANN training loss 0.002051
&gt;&gt; Epoch 148 finished 	ANN training loss 0.002013
&gt;&gt; Epoch 149 finished 	ANN training loss 0.002015
&gt;&gt; Epoch 150 finished 	ANN training loss 0.001877
&gt;&gt; Epoch 151 finished 	ANN training loss 0.002002
&gt;&gt; Epoch 152 finished 	ANN training loss 0.001985
&gt;&gt; Epoch 153 finished 	ANN training loss 0.002217
&gt;&gt; Epoch 154 finished 	ANN training loss 0.001890
&gt;&gt; Epoch 155 finished 	ANN training loss 0.001975
&gt;&gt; Epoch 156 finished 	ANN training loss 0.001878
&gt;&gt; Epoch 157 finished 	ANN training loss 0.001709
&gt;&gt; Epoch 158 finished 	ANN training loss 0.001701
&gt;&gt; Epoch 159 finished 	ANN training loss 0.001623
&gt;&gt; Epoch 160 finished 	ANN training loss 0.001566
&gt;&gt; Epoch 161 finished 	ANN training loss 0.001873
&gt;&gt; Epoch 162 finished 	ANN training loss 0.001759
&gt;&gt; Epoch 163 finished 	ANN training loss 0.001612
&gt;&gt; Epoch 164 finished 	ANN training loss 0.001554
&gt;&gt; Epoch 165 finished 	ANN training loss 0.001533
&gt;&gt; Epoch 166 finished 	ANN training loss 0.001443
&gt;&gt; Epoch 167 finished 	ANN training loss 0.001404
&gt;&gt; Epoch 168 finished 	ANN training loss 0.001473
&gt;&gt; Epoch 169 finished 	ANN training loss 0.001419
&gt;&gt; Epoch 170 finished 	ANN training loss 0.001429
&gt;&gt; Epoch 171 finished 	ANN training loss 0.001300
&gt;&gt; Epoch 172 finished 	ANN training loss 0.001283
&gt;&gt; Epoch 173 finished 	ANN training loss 0.001290
&gt;&gt; Epoch 174 finished 	ANN training loss 0.001271
&gt;&gt; Epoch 175 finished 	ANN training loss 0.001236
&gt;&gt; Epoch 176 finished 	ANN training loss 0.001290
&gt;&gt; Epoch 177 finished 	ANN training loss 0.001356
&gt;&gt; Epoch 178 finished 	ANN training loss 0.001264
&gt;&gt; Epoch 179 finished 	ANN training loss 0.001360
&gt;&gt; Epoch 180 finished 	ANN training loss 0.001280
&gt;&gt; Epoch 181 finished 	ANN training loss 0.001267
&gt;&gt; Epoch 182 finished 	ANN training loss 0.001251
&gt;&gt; Epoch 183 finished 	ANN training loss 0.001207
&gt;&gt; Epoch 184 finished 	ANN training loss 0.001175
&gt;&gt; Epoch 185 finished 	ANN training loss 0.001172
&gt;&gt; Epoch 186 finished 	ANN training loss 0.001127
&gt;&gt; Epoch 187 finished 	ANN training loss 0.001076
&gt;&gt; Epoch 188 finished 	ANN training loss 0.001185
&gt;&gt; Epoch 189 finished 	ANN training loss 0.001050
&gt;&gt; Epoch 190 finished 	ANN training loss 0.001120
&gt;&gt; Epoch 191 finished 	ANN training loss 0.001193
&gt;&gt; Epoch 192 finished 	ANN training loss 0.001123
&gt;&gt; Epoch 193 finished 	ANN training loss 0.001136
&gt;&gt; Epoch 194 finished 	ANN training loss 0.001106
&gt;&gt; Epoch 195 finished 	ANN training loss 0.001060
&gt;&gt; Epoch 196 finished 	ANN training loss 0.001148
&gt;&gt; Epoch 197 finished 	ANN training loss 0.000971
&gt;&gt; Epoch 198 finished 	ANN training loss 0.000978
&gt;&gt; Epoch 199 finished 	ANN training loss 0.000997
[END] Fine tuning step
Done.
Accuracy: 0.890000
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[13]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ACCURACY: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">acc4</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>ACCURACY: 0.89
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[14]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># ReLu, dropout = 0.5</span>
<span class="n">acc5</span> <span class="o">=</span> <span class="n">deep_belief_net</span><span class="p">(</span><span class="n">hidden_layers_structure</span><span class="o">=</span><span class="p">[</span><span class="mi">300</span><span class="p">],</span> 
                       <span class="n">learning_rate_rbm</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span>
                       <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                       <span class="n">n_epochs_rbm</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span>
                       <span class="n">n_iter_backprop</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
                       <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
                       <span class="n">activation_function</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span>
                       <span class="n">dropout_p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[START] Pre-training step:
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 61.709766
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 54.472504
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 75.873703
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 41.272430
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 43.754398
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 42.647362
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 40.159855
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 60.702248
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 37.104389
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 36.450516
&gt;&gt; Epoch 11 finished 	RBM Reconstruction error 50.309250
&gt;&gt; Epoch 12 finished 	RBM Reconstruction error 32.404320
&gt;&gt; Epoch 13 finished 	RBM Reconstruction error 40.761646
&gt;&gt; Epoch 14 finished 	RBM Reconstruction error 39.678955
&gt;&gt; Epoch 15 finished 	RBM Reconstruction error 43.482391
&gt;&gt; Epoch 16 finished 	RBM Reconstruction error 48.358349
&gt;&gt; Epoch 17 finished 	RBM Reconstruction error 41.619835
&gt;&gt; Epoch 18 finished 	RBM Reconstruction error 28.050406
&gt;&gt; Epoch 19 finished 	RBM Reconstruction error 44.146065
&gt;&gt; Epoch 20 finished 	RBM Reconstruction error 64.320587
&gt;&gt; Epoch 21 finished 	RBM Reconstruction error 34.633278
&gt;&gt; Epoch 22 finished 	RBM Reconstruction error 50.839394
&gt;&gt; Epoch 23 finished 	RBM Reconstruction error 50.193310
&gt;&gt; Epoch 24 finished 	RBM Reconstruction error 50.998871
&gt;&gt; Epoch 25 finished 	RBM Reconstruction error 46.640747
&gt;&gt; Epoch 26 finished 	RBM Reconstruction error 43.082211
&gt;&gt; Epoch 27 finished 	RBM Reconstruction error 48.230694
&gt;&gt; Epoch 28 finished 	RBM Reconstruction error 66.244759
&gt;&gt; Epoch 29 finished 	RBM Reconstruction error 50.090733
&gt;&gt; Epoch 30 finished 	RBM Reconstruction error 43.973370
&gt;&gt; Epoch 31 finished 	RBM Reconstruction error 42.613564
&gt;&gt; Epoch 32 finished 	RBM Reconstruction error 47.859264
&gt;&gt; Epoch 33 finished 	RBM Reconstruction error 52.287685
&gt;&gt; Epoch 34 finished 	RBM Reconstruction error 51.919575
&gt;&gt; Epoch 35 finished 	RBM Reconstruction error 54.550453
&gt;&gt; Epoch 36 finished 	RBM Reconstruction error 50.095306
&gt;&gt; Epoch 37 finished 	RBM Reconstruction error 53.549805
&gt;&gt; Epoch 38 finished 	RBM Reconstruction error 46.231861
&gt;&gt; Epoch 39 finished 	RBM Reconstruction error 56.310070
&gt;&gt; Epoch 40 finished 	RBM Reconstruction error 48.051514
[END] Pre-training step
[START] Fine tuning step:
&gt;&gt; Epoch 0 finished 	ANN training loss 0.836896
&gt;&gt; Epoch 1 finished 	ANN training loss 0.605394
&gt;&gt; Epoch 2 finished 	ANN training loss 0.482195
&gt;&gt; Epoch 3 finished 	ANN training loss 0.447327
&gt;&gt; Epoch 4 finished 	ANN training loss 0.407878
&gt;&gt; Epoch 5 finished 	ANN training loss 0.364694
&gt;&gt; Epoch 6 finished 	ANN training loss 0.331751
&gt;&gt; Epoch 7 finished 	ANN training loss 0.307161
&gt;&gt; Epoch 8 finished 	ANN training loss 0.286104
&gt;&gt; Epoch 9 finished 	ANN training loss 0.264597
&gt;&gt; Epoch 10 finished 	ANN training loss 0.254600
&gt;&gt; Epoch 11 finished 	ANN training loss 0.236196
&gt;&gt; Epoch 12 finished 	ANN training loss 0.222835
&gt;&gt; Epoch 13 finished 	ANN training loss 0.217063
&gt;&gt; Epoch 14 finished 	ANN training loss 0.194284
&gt;&gt; Epoch 15 finished 	ANN training loss 0.184488
&gt;&gt; Epoch 16 finished 	ANN training loss 0.180249
&gt;&gt; Epoch 17 finished 	ANN training loss 0.166425
&gt;&gt; Epoch 18 finished 	ANN training loss 0.163207
&gt;&gt; Epoch 19 finished 	ANN training loss 0.154860
&gt;&gt; Epoch 20 finished 	ANN training loss 0.144113
&gt;&gt; Epoch 21 finished 	ANN training loss 0.137016
&gt;&gt; Epoch 22 finished 	ANN training loss 0.130890
&gt;&gt; Epoch 23 finished 	ANN training loss 0.125865
&gt;&gt; Epoch 24 finished 	ANN training loss 0.125159
&gt;&gt; Epoch 25 finished 	ANN training loss 0.116664
&gt;&gt; Epoch 26 finished 	ANN training loss 0.114315
&gt;&gt; Epoch 27 finished 	ANN training loss 0.108297
&gt;&gt; Epoch 28 finished 	ANN training loss 0.106967
&gt;&gt; Epoch 29 finished 	ANN training loss 0.096678
&gt;&gt; Epoch 30 finished 	ANN training loss 0.089675
&gt;&gt; Epoch 31 finished 	ANN training loss 0.088421
&gt;&gt; Epoch 32 finished 	ANN training loss 0.082038
&gt;&gt; Epoch 33 finished 	ANN training loss 0.075160
&gt;&gt; Epoch 34 finished 	ANN training loss 0.075184
&gt;&gt; Epoch 35 finished 	ANN training loss 0.072693
&gt;&gt; Epoch 36 finished 	ANN training loss 0.073774
&gt;&gt; Epoch 37 finished 	ANN training loss 0.064967
&gt;&gt; Epoch 38 finished 	ANN training loss 0.060920
&gt;&gt; Epoch 39 finished 	ANN training loss 0.067424
&gt;&gt; Epoch 40 finished 	ANN training loss 0.059324
&gt;&gt; Epoch 41 finished 	ANN training loss 0.056265
&gt;&gt; Epoch 42 finished 	ANN training loss 0.051049
&gt;&gt; Epoch 43 finished 	ANN training loss 0.051929
&gt;&gt; Epoch 44 finished 	ANN training loss 0.053568
&gt;&gt; Epoch 45 finished 	ANN training loss 0.049829
&gt;&gt; Epoch 46 finished 	ANN training loss 0.048672
&gt;&gt; Epoch 47 finished 	ANN training loss 0.044799
&gt;&gt; Epoch 48 finished 	ANN training loss 0.043688
&gt;&gt; Epoch 49 finished 	ANN training loss 0.043264
&gt;&gt; Epoch 50 finished 	ANN training loss 0.039973
&gt;&gt; Epoch 51 finished 	ANN training loss 0.041697
&gt;&gt; Epoch 52 finished 	ANN training loss 0.039316
&gt;&gt; Epoch 53 finished 	ANN training loss 0.037126
&gt;&gt; Epoch 54 finished 	ANN training loss 0.037368
&gt;&gt; Epoch 55 finished 	ANN training loss 0.037381
&gt;&gt; Epoch 56 finished 	ANN training loss 0.036043
&gt;&gt; Epoch 57 finished 	ANN training loss 0.034141
&gt;&gt; Epoch 58 finished 	ANN training loss 0.033120
&gt;&gt; Epoch 59 finished 	ANN training loss 0.034545
&gt;&gt; Epoch 60 finished 	ANN training loss 0.031147
&gt;&gt; Epoch 61 finished 	ANN training loss 0.030991
&gt;&gt; Epoch 62 finished 	ANN training loss 0.030868
&gt;&gt; Epoch 63 finished 	ANN training loss 0.030967
&gt;&gt; Epoch 64 finished 	ANN training loss 0.029502
&gt;&gt; Epoch 65 finished 	ANN training loss 0.028446
&gt;&gt; Epoch 66 finished 	ANN training loss 0.027263
&gt;&gt; Epoch 67 finished 	ANN training loss 0.027058
&gt;&gt; Epoch 68 finished 	ANN training loss 0.026121
&gt;&gt; Epoch 69 finished 	ANN training loss 0.026443
&gt;&gt; Epoch 70 finished 	ANN training loss 0.022847
&gt;&gt; Epoch 71 finished 	ANN training loss 0.023266
&gt;&gt; Epoch 72 finished 	ANN training loss 0.021971
&gt;&gt; Epoch 73 finished 	ANN training loss 0.021610
&gt;&gt; Epoch 74 finished 	ANN training loss 0.021076
&gt;&gt; Epoch 75 finished 	ANN training loss 0.020608
&gt;&gt; Epoch 76 finished 	ANN training loss 0.019689
&gt;&gt; Epoch 77 finished 	ANN training loss 0.020859
&gt;&gt; Epoch 78 finished 	ANN training loss 0.019367
&gt;&gt; Epoch 79 finished 	ANN training loss 0.017330
&gt;&gt; Epoch 80 finished 	ANN training loss 0.015969
&gt;&gt; Epoch 81 finished 	ANN training loss 0.016560
&gt;&gt; Epoch 82 finished 	ANN training loss 0.016135
&gt;&gt; Epoch 83 finished 	ANN training loss 0.015515
&gt;&gt; Epoch 84 finished 	ANN training loss 0.018853
&gt;&gt; Epoch 85 finished 	ANN training loss 0.015582
&gt;&gt; Epoch 86 finished 	ANN training loss 0.015680
&gt;&gt; Epoch 87 finished 	ANN training loss 0.015350
&gt;&gt; Epoch 88 finished 	ANN training loss 0.014875
&gt;&gt; Epoch 89 finished 	ANN training loss 0.013924
&gt;&gt; Epoch 90 finished 	ANN training loss 0.014588
&gt;&gt; Epoch 91 finished 	ANN training loss 0.014222
&gt;&gt; Epoch 92 finished 	ANN training loss 0.012982
&gt;&gt; Epoch 93 finished 	ANN training loss 0.012732
&gt;&gt; Epoch 94 finished 	ANN training loss 0.012364
&gt;&gt; Epoch 95 finished 	ANN training loss 0.013103
&gt;&gt; Epoch 96 finished 	ANN training loss 0.012489
&gt;&gt; Epoch 97 finished 	ANN training loss 0.012275
&gt;&gt; Epoch 98 finished 	ANN training loss 0.011450
&gt;&gt; Epoch 99 finished 	ANN training loss 0.011205
&gt;&gt; Epoch 100 finished 	ANN training loss 0.010934
&gt;&gt; Epoch 101 finished 	ANN training loss 0.011645
&gt;&gt; Epoch 102 finished 	ANN training loss 0.010772
&gt;&gt; Epoch 103 finished 	ANN training loss 0.011830
&gt;&gt; Epoch 104 finished 	ANN training loss 0.011680
&gt;&gt; Epoch 105 finished 	ANN training loss 0.010858
&gt;&gt; Epoch 106 finished 	ANN training loss 0.009570
&gt;&gt; Epoch 107 finished 	ANN training loss 0.009713
&gt;&gt; Epoch 108 finished 	ANN training loss 0.009374
&gt;&gt; Epoch 109 finished 	ANN training loss 0.009717
&gt;&gt; Epoch 110 finished 	ANN training loss 0.009514
&gt;&gt; Epoch 111 finished 	ANN training loss 0.009393
&gt;&gt; Epoch 112 finished 	ANN training loss 0.009223
&gt;&gt; Epoch 113 finished 	ANN training loss 0.008878
&gt;&gt; Epoch 114 finished 	ANN training loss 0.009303
&gt;&gt; Epoch 115 finished 	ANN training loss 0.009527
&gt;&gt; Epoch 116 finished 	ANN training loss 0.009009
&gt;&gt; Epoch 117 finished 	ANN training loss 0.008854
&gt;&gt; Epoch 118 finished 	ANN training loss 0.009319
&gt;&gt; Epoch 119 finished 	ANN training loss 0.008620
&gt;&gt; Epoch 120 finished 	ANN training loss 0.008130
&gt;&gt; Epoch 121 finished 	ANN training loss 0.008190
&gt;&gt; Epoch 122 finished 	ANN training loss 0.007763
&gt;&gt; Epoch 123 finished 	ANN training loss 0.007867
&gt;&gt; Epoch 124 finished 	ANN training loss 0.009135
&gt;&gt; Epoch 125 finished 	ANN training loss 0.007060
&gt;&gt; Epoch 126 finished 	ANN training loss 0.007608
&gt;&gt; Epoch 127 finished 	ANN training loss 0.007474
&gt;&gt; Epoch 128 finished 	ANN training loss 0.007104
&gt;&gt; Epoch 129 finished 	ANN training loss 0.007016
&gt;&gt; Epoch 130 finished 	ANN training loss 0.007401
&gt;&gt; Epoch 131 finished 	ANN training loss 0.006690
&gt;&gt; Epoch 132 finished 	ANN training loss 0.007211
&gt;&gt; Epoch 133 finished 	ANN training loss 0.006723
&gt;&gt; Epoch 134 finished 	ANN training loss 0.006278
&gt;&gt; Epoch 135 finished 	ANN training loss 0.006473
&gt;&gt; Epoch 136 finished 	ANN training loss 0.006607
&gt;&gt; Epoch 137 finished 	ANN training loss 0.006358
&gt;&gt; Epoch 138 finished 	ANN training loss 0.006463
&gt;&gt; Epoch 139 finished 	ANN training loss 0.006845
&gt;&gt; Epoch 140 finished 	ANN training loss 0.007025
&gt;&gt; Epoch 141 finished 	ANN training loss 0.006666
&gt;&gt; Epoch 142 finished 	ANN training loss 0.006112
&gt;&gt; Epoch 143 finished 	ANN training loss 0.005562
&gt;&gt; Epoch 144 finished 	ANN training loss 0.006110
&gt;&gt; Epoch 145 finished 	ANN training loss 0.007540
&gt;&gt; Epoch 146 finished 	ANN training loss 0.005666
&gt;&gt; Epoch 147 finished 	ANN training loss 0.006056
&gt;&gt; Epoch 148 finished 	ANN training loss 0.005590
&gt;&gt; Epoch 149 finished 	ANN training loss 0.005782
&gt;&gt; Epoch 150 finished 	ANN training loss 0.005757
&gt;&gt; Epoch 151 finished 	ANN training loss 0.005601
&gt;&gt; Epoch 152 finished 	ANN training loss 0.005293
&gt;&gt; Epoch 153 finished 	ANN training loss 0.005452
&gt;&gt; Epoch 154 finished 	ANN training loss 0.004550
&gt;&gt; Epoch 155 finished 	ANN training loss 0.004464
&gt;&gt; Epoch 156 finished 	ANN training loss 0.004388
&gt;&gt; Epoch 157 finished 	ANN training loss 0.005063
&gt;&gt; Epoch 158 finished 	ANN training loss 0.004621
&gt;&gt; Epoch 159 finished 	ANN training loss 0.004457
&gt;&gt; Epoch 160 finished 	ANN training loss 0.004434
&gt;&gt; Epoch 161 finished 	ANN training loss 0.004422
&gt;&gt; Epoch 162 finished 	ANN training loss 0.004695
&gt;&gt; Epoch 163 finished 	ANN training loss 0.004668
&gt;&gt; Epoch 164 finished 	ANN training loss 0.004531
&gt;&gt; Epoch 165 finished 	ANN training loss 0.004790
&gt;&gt; Epoch 166 finished 	ANN training loss 0.004258
&gt;&gt; Epoch 167 finished 	ANN training loss 0.004111
&gt;&gt; Epoch 168 finished 	ANN training loss 0.004585
&gt;&gt; Epoch 169 finished 	ANN training loss 0.003985
&gt;&gt; Epoch 170 finished 	ANN training loss 0.004056
&gt;&gt; Epoch 171 finished 	ANN training loss 0.004231
&gt;&gt; Epoch 172 finished 	ANN training loss 0.004339
&gt;&gt; Epoch 173 finished 	ANN training loss 0.005517
&gt;&gt; Epoch 174 finished 	ANN training loss 0.004709
&gt;&gt; Epoch 175 finished 	ANN training loss 0.004159
&gt;&gt; Epoch 176 finished 	ANN training loss 0.004013
&gt;&gt; Epoch 177 finished 	ANN training loss 0.003818
&gt;&gt; Epoch 178 finished 	ANN training loss 0.003683
&gt;&gt; Epoch 179 finished 	ANN training loss 0.003377
&gt;&gt; Epoch 180 finished 	ANN training loss 0.003048
&gt;&gt; Epoch 181 finished 	ANN training loss 0.003115
&gt;&gt; Epoch 182 finished 	ANN training loss 0.003113
&gt;&gt; Epoch 183 finished 	ANN training loss 0.003275
&gt;&gt; Epoch 184 finished 	ANN training loss 0.003483
&gt;&gt; Epoch 185 finished 	ANN training loss 0.003450
&gt;&gt; Epoch 186 finished 	ANN training loss 0.003731
&gt;&gt; Epoch 187 finished 	ANN training loss 0.003489
&gt;&gt; Epoch 188 finished 	ANN training loss 0.003478
&gt;&gt; Epoch 189 finished 	ANN training loss 0.003374
&gt;&gt; Epoch 190 finished 	ANN training loss 0.003038
&gt;&gt; Epoch 191 finished 	ANN training loss 0.003253
&gt;&gt; Epoch 192 finished 	ANN training loss 0.002931
&gt;&gt; Epoch 193 finished 	ANN training loss 0.002856
&gt;&gt; Epoch 194 finished 	ANN training loss 0.002921
&gt;&gt; Epoch 195 finished 	ANN training loss 0.002922
&gt;&gt; Epoch 196 finished 	ANN training loss 0.003118
&gt;&gt; Epoch 197 finished 	ANN training loss 0.003332
&gt;&gt; Epoch 198 finished 	ANN training loss 0.003246
&gt;&gt; Epoch 199 finished 	ANN training loss 0.002912
[END] Fine tuning step
Done.
Accuracy: 0.890000
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[15]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ACCURACY: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">acc5</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>ACCURACY: 0.89
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[18]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># ReLu, dropout = 0.6</span>
<span class="n">acc6</span> <span class="o">=</span> <span class="n">deep_belief_net</span><span class="p">(</span><span class="n">hidden_layers_structure</span><span class="o">=</span><span class="p">[</span><span class="mi">300</span><span class="p">],</span> 
                       <span class="n">learning_rate_rbm</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span>
                       <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                       <span class="n">n_epochs_rbm</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span>
                       <span class="n">n_iter_backprop</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
                       <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
                       <span class="n">activation_function</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span>
                       <span class="n">dropout_p</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[START] Pre-training step:
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 61.378994
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 38.144863
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 48.835243
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 43.290958
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 44.144161
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 42.889774
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 33.065033
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 24.430702
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 32.138363
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 45.784576
&gt;&gt; Epoch 11 finished 	RBM Reconstruction error 32.016129
&gt;&gt; Epoch 12 finished 	RBM Reconstruction error 25.012192
&gt;&gt; Epoch 13 finished 	RBM Reconstruction error 32.610817
&gt;&gt; Epoch 14 finished 	RBM Reconstruction error 54.927158
&gt;&gt; Epoch 15 finished 	RBM Reconstruction error 47.476357
&gt;&gt; Epoch 16 finished 	RBM Reconstruction error 28.648687
&gt;&gt; Epoch 17 finished 	RBM Reconstruction error 27.258995
&gt;&gt; Epoch 18 finished 	RBM Reconstruction error 28.335403
&gt;&gt; Epoch 19 finished 	RBM Reconstruction error 43.332287
&gt;&gt; Epoch 20 finished 	RBM Reconstruction error 34.647114
&gt;&gt; Epoch 21 finished 	RBM Reconstruction error 33.540207
&gt;&gt; Epoch 22 finished 	RBM Reconstruction error 43.665752
&gt;&gt; Epoch 23 finished 	RBM Reconstruction error 35.126572
&gt;&gt; Epoch 24 finished 	RBM Reconstruction error 33.687454
&gt;&gt; Epoch 25 finished 	RBM Reconstruction error 26.624083
&gt;&gt; Epoch 26 finished 	RBM Reconstruction error 35.344112
&gt;&gt; Epoch 27 finished 	RBM Reconstruction error 35.424717
&gt;&gt; Epoch 28 finished 	RBM Reconstruction error 23.503057
&gt;&gt; Epoch 29 finished 	RBM Reconstruction error 28.569208
&gt;&gt; Epoch 30 finished 	RBM Reconstruction error 29.112249
&gt;&gt; Epoch 31 finished 	RBM Reconstruction error 38.162247
&gt;&gt; Epoch 32 finished 	RBM Reconstruction error 39.954365
&gt;&gt; Epoch 33 finished 	RBM Reconstruction error 34.830547
&gt;&gt; Epoch 34 finished 	RBM Reconstruction error 18.477585
&gt;&gt; Epoch 35 finished 	RBM Reconstruction error 26.755713
&gt;&gt; Epoch 36 finished 	RBM Reconstruction error 37.530590
&gt;&gt; Epoch 37 finished 	RBM Reconstruction error 39.838272
&gt;&gt; Epoch 38 finished 	RBM Reconstruction error 48.593719
&gt;&gt; Epoch 39 finished 	RBM Reconstruction error 33.874222
&gt;&gt; Epoch 40 finished 	RBM Reconstruction error 39.812855
[END] Pre-training step
[START] Fine tuning step:
&gt;&gt; Epoch 0 finished 	ANN training loss 0.859841
&gt;&gt; Epoch 1 finished 	ANN training loss 0.614823
&gt;&gt; Epoch 2 finished 	ANN training loss 0.536251
&gt;&gt; Epoch 3 finished 	ANN training loss 0.453935
&gt;&gt; Epoch 4 finished 	ANN training loss 0.411489
&gt;&gt; Epoch 5 finished 	ANN training loss 0.381506
&gt;&gt; Epoch 6 finished 	ANN training loss 0.354274
&gt;&gt; Epoch 7 finished 	ANN training loss 0.341939
&gt;&gt; Epoch 8 finished 	ANN training loss 0.326547
&gt;&gt; Epoch 9 finished 	ANN training loss 0.309168
&gt;&gt; Epoch 10 finished 	ANN training loss 0.271252
&gt;&gt; Epoch 11 finished 	ANN training loss 0.266615
&gt;&gt; Epoch 12 finished 	ANN training loss 0.263812
&gt;&gt; Epoch 13 finished 	ANN training loss 0.231092
&gt;&gt; Epoch 14 finished 	ANN training loss 0.236166
&gt;&gt; Epoch 15 finished 	ANN training loss 0.221801
&gt;&gt; Epoch 16 finished 	ANN training loss 0.206456
&gt;&gt; Epoch 17 finished 	ANN training loss 0.193446
&gt;&gt; Epoch 18 finished 	ANN training loss 0.186529
&gt;&gt; Epoch 19 finished 	ANN training loss 0.178988
&gt;&gt; Epoch 20 finished 	ANN training loss 0.170951
&gt;&gt; Epoch 21 finished 	ANN training loss 0.167949
&gt;&gt; Epoch 22 finished 	ANN training loss 0.162326
&gt;&gt; Epoch 23 finished 	ANN training loss 0.150751
&gt;&gt; Epoch 24 finished 	ANN training loss 0.149804
&gt;&gt; Epoch 25 finished 	ANN training loss 0.142745
&gt;&gt; Epoch 26 finished 	ANN training loss 0.138540
&gt;&gt; Epoch 27 finished 	ANN training loss 0.133972
&gt;&gt; Epoch 28 finished 	ANN training loss 0.130696
&gt;&gt; Epoch 29 finished 	ANN training loss 0.123060
&gt;&gt; Epoch 30 finished 	ANN training loss 0.120239
&gt;&gt; Epoch 31 finished 	ANN training loss 0.117836
&gt;&gt; Epoch 32 finished 	ANN training loss 0.114693
&gt;&gt; Epoch 33 finished 	ANN training loss 0.110741
&gt;&gt; Epoch 34 finished 	ANN training loss 0.100533
&gt;&gt; Epoch 35 finished 	ANN training loss 0.093474
&gt;&gt; Epoch 36 finished 	ANN training loss 0.095851
&gt;&gt; Epoch 37 finished 	ANN training loss 0.091107
&gt;&gt; Epoch 38 finished 	ANN training loss 0.089250
&gt;&gt; Epoch 39 finished 	ANN training loss 0.086267
&gt;&gt; Epoch 40 finished 	ANN training loss 0.084422
&gt;&gt; Epoch 41 finished 	ANN training loss 0.081175
&gt;&gt; Epoch 42 finished 	ANN training loss 0.080481
&gt;&gt; Epoch 43 finished 	ANN training loss 0.079708
&gt;&gt; Epoch 44 finished 	ANN training loss 0.078181
&gt;&gt; Epoch 45 finished 	ANN training loss 0.070690
&gt;&gt; Epoch 46 finished 	ANN training loss 0.068087
&gt;&gt; Epoch 47 finished 	ANN training loss 0.068190
&gt;&gt; Epoch 48 finished 	ANN training loss 0.067907
&gt;&gt; Epoch 49 finished 	ANN training loss 0.062348
&gt;&gt; Epoch 50 finished 	ANN training loss 0.060985
&gt;&gt; Epoch 51 finished 	ANN training loss 0.059478
&gt;&gt; Epoch 52 finished 	ANN training loss 0.061557
&gt;&gt; Epoch 53 finished 	ANN training loss 0.062623
&gt;&gt; Epoch 54 finished 	ANN training loss 0.056796
&gt;&gt; Epoch 55 finished 	ANN training loss 0.064966
&gt;&gt; Epoch 56 finished 	ANN training loss 0.058383
&gt;&gt; Epoch 57 finished 	ANN training loss 0.058235
&gt;&gt; Epoch 58 finished 	ANN training loss 0.052088
&gt;&gt; Epoch 59 finished 	ANN training loss 0.052234
&gt;&gt; Epoch 60 finished 	ANN training loss 0.050665
&gt;&gt; Epoch 61 finished 	ANN training loss 0.048629
&gt;&gt; Epoch 62 finished 	ANN training loss 0.045685
&gt;&gt; Epoch 63 finished 	ANN training loss 0.046149
&gt;&gt; Epoch 64 finished 	ANN training loss 0.045293
&gt;&gt; Epoch 65 finished 	ANN training loss 0.046871
&gt;&gt; Epoch 66 finished 	ANN training loss 0.047676
&gt;&gt; Epoch 67 finished 	ANN training loss 0.048357
&gt;&gt; Epoch 68 finished 	ANN training loss 0.040732
&gt;&gt; Epoch 69 finished 	ANN training loss 0.041063
&gt;&gt; Epoch 70 finished 	ANN training loss 0.040346
&gt;&gt; Epoch 71 finished 	ANN training loss 0.039275
&gt;&gt; Epoch 72 finished 	ANN training loss 0.037873
&gt;&gt; Epoch 73 finished 	ANN training loss 0.037714
&gt;&gt; Epoch 74 finished 	ANN training loss 0.036731
&gt;&gt; Epoch 75 finished 	ANN training loss 0.036229
&gt;&gt; Epoch 76 finished 	ANN training loss 0.035779
&gt;&gt; Epoch 77 finished 	ANN training loss 0.037705
&gt;&gt; Epoch 78 finished 	ANN training loss 0.034248
&gt;&gt; Epoch 79 finished 	ANN training loss 0.031439
&gt;&gt; Epoch 80 finished 	ANN training loss 0.033669
&gt;&gt; Epoch 81 finished 	ANN training loss 0.030233
&gt;&gt; Epoch 82 finished 	ANN training loss 0.029235
&gt;&gt; Epoch 83 finished 	ANN training loss 0.031905
&gt;&gt; Epoch 84 finished 	ANN training loss 0.032913
&gt;&gt; Epoch 85 finished 	ANN training loss 0.031066
&gt;&gt; Epoch 86 finished 	ANN training loss 0.030548
&gt;&gt; Epoch 87 finished 	ANN training loss 0.028794
&gt;&gt; Epoch 88 finished 	ANN training loss 0.026432
&gt;&gt; Epoch 89 finished 	ANN training loss 0.028076
&gt;&gt; Epoch 90 finished 	ANN training loss 0.025435
&gt;&gt; Epoch 91 finished 	ANN training loss 0.025525
&gt;&gt; Epoch 92 finished 	ANN training loss 0.024881
&gt;&gt; Epoch 93 finished 	ANN training loss 0.025738
&gt;&gt; Epoch 94 finished 	ANN training loss 0.023477
&gt;&gt; Epoch 95 finished 	ANN training loss 0.029293
&gt;&gt; Epoch 96 finished 	ANN training loss 0.025131
&gt;&gt; Epoch 97 finished 	ANN training loss 0.023704
&gt;&gt; Epoch 98 finished 	ANN training loss 0.021918
&gt;&gt; Epoch 99 finished 	ANN training loss 0.020476
&gt;&gt; Epoch 100 finished 	ANN training loss 0.020894
&gt;&gt; Epoch 101 finished 	ANN training loss 0.022340
&gt;&gt; Epoch 102 finished 	ANN training loss 0.020515
&gt;&gt; Epoch 103 finished 	ANN training loss 0.019186
&gt;&gt; Epoch 104 finished 	ANN training loss 0.019892
&gt;&gt; Epoch 105 finished 	ANN training loss 0.019944
&gt;&gt; Epoch 106 finished 	ANN training loss 0.018603
&gt;&gt; Epoch 107 finished 	ANN training loss 0.018664
&gt;&gt; Epoch 108 finished 	ANN training loss 0.018768
&gt;&gt; Epoch 109 finished 	ANN training loss 0.020847
&gt;&gt; Epoch 110 finished 	ANN training loss 0.018714
&gt;&gt; Epoch 111 finished 	ANN training loss 0.017809
&gt;&gt; Epoch 112 finished 	ANN training loss 0.017677
&gt;&gt; Epoch 113 finished 	ANN training loss 0.016760
&gt;&gt; Epoch 114 finished 	ANN training loss 0.019247
&gt;&gt; Epoch 115 finished 	ANN training loss 0.017183
&gt;&gt; Epoch 116 finished 	ANN training loss 0.016561
&gt;&gt; Epoch 117 finished 	ANN training loss 0.016859
&gt;&gt; Epoch 118 finished 	ANN training loss 0.016207
&gt;&gt; Epoch 119 finished 	ANN training loss 0.016616
&gt;&gt; Epoch 120 finished 	ANN training loss 0.016659
&gt;&gt; Epoch 121 finished 	ANN training loss 0.015730
&gt;&gt; Epoch 122 finished 	ANN training loss 0.016864
&gt;&gt; Epoch 123 finished 	ANN training loss 0.016000
&gt;&gt; Epoch 124 finished 	ANN training loss 0.016408
&gt;&gt; Epoch 125 finished 	ANN training loss 0.015784
&gt;&gt; Epoch 126 finished 	ANN training loss 0.015176
&gt;&gt; Epoch 127 finished 	ANN training loss 0.015397
&gt;&gt; Epoch 128 finished 	ANN training loss 0.015785
&gt;&gt; Epoch 129 finished 	ANN training loss 0.015116
&gt;&gt; Epoch 130 finished 	ANN training loss 0.014889
&gt;&gt; Epoch 131 finished 	ANN training loss 0.015282
&gt;&gt; Epoch 132 finished 	ANN training loss 0.014992
&gt;&gt; Epoch 133 finished 	ANN training loss 0.015322
&gt;&gt; Epoch 134 finished 	ANN training loss 0.013337
&gt;&gt; Epoch 135 finished 	ANN training loss 0.012993
&gt;&gt; Epoch 136 finished 	ANN training loss 0.012806
&gt;&gt; Epoch 137 finished 	ANN training loss 0.013074
&gt;&gt; Epoch 138 finished 	ANN training loss 0.012969
&gt;&gt; Epoch 139 finished 	ANN training loss 0.013080
&gt;&gt; Epoch 140 finished 	ANN training loss 0.013136
&gt;&gt; Epoch 141 finished 	ANN training loss 0.011857
&gt;&gt; Epoch 142 finished 	ANN training loss 0.012001
&gt;&gt; Epoch 143 finished 	ANN training loss 0.012229
&gt;&gt; Epoch 144 finished 	ANN training loss 0.012342
&gt;&gt; Epoch 145 finished 	ANN training loss 0.011292
&gt;&gt; Epoch 146 finished 	ANN training loss 0.011222
&gt;&gt; Epoch 147 finished 	ANN training loss 0.011230
&gt;&gt; Epoch 148 finished 	ANN training loss 0.010981
&gt;&gt; Epoch 149 finished 	ANN training loss 0.010827
&gt;&gt; Epoch 150 finished 	ANN training loss 0.012160
&gt;&gt; Epoch 151 finished 	ANN training loss 0.011082
&gt;&gt; Epoch 152 finished 	ANN training loss 0.010370
&gt;&gt; Epoch 153 finished 	ANN training loss 0.011008
&gt;&gt; Epoch 154 finished 	ANN training loss 0.011031
&gt;&gt; Epoch 155 finished 	ANN training loss 0.010472
&gt;&gt; Epoch 156 finished 	ANN training loss 0.011178
&gt;&gt; Epoch 157 finished 	ANN training loss 0.010868
&gt;&gt; Epoch 158 finished 	ANN training loss 0.011205
&gt;&gt; Epoch 159 finished 	ANN training loss 0.013078
&gt;&gt; Epoch 160 finished 	ANN training loss 0.012192
&gt;&gt; Epoch 161 finished 	ANN training loss 0.012053
&gt;&gt; Epoch 162 finished 	ANN training loss 0.013559
&gt;&gt; Epoch 163 finished 	ANN training loss 0.013747
&gt;&gt; Epoch 164 finished 	ANN training loss 0.012188
&gt;&gt; Epoch 165 finished 	ANN training loss 0.010757
&gt;&gt; Epoch 166 finished 	ANN training loss 0.010679
&gt;&gt; Epoch 167 finished 	ANN training loss 0.011204
&gt;&gt; Epoch 168 finished 	ANN training loss 0.010283
&gt;&gt; Epoch 169 finished 	ANN training loss 0.010288
&gt;&gt; Epoch 170 finished 	ANN training loss 0.008808
&gt;&gt; Epoch 171 finished 	ANN training loss 0.010152
&gt;&gt; Epoch 172 finished 	ANN training loss 0.008897
&gt;&gt; Epoch 173 finished 	ANN training loss 0.009171
&gt;&gt; Epoch 174 finished 	ANN training loss 0.010024
&gt;&gt; Epoch 175 finished 	ANN training loss 0.008512
&gt;&gt; Epoch 176 finished 	ANN training loss 0.008596
&gt;&gt; Epoch 177 finished 	ANN training loss 0.008314
&gt;&gt; Epoch 178 finished 	ANN training loss 0.008407
&gt;&gt; Epoch 179 finished 	ANN training loss 0.008145
&gt;&gt; Epoch 180 finished 	ANN training loss 0.008254
&gt;&gt; Epoch 181 finished 	ANN training loss 0.008751
&gt;&gt; Epoch 182 finished 	ANN training loss 0.009264
&gt;&gt; Epoch 183 finished 	ANN training loss 0.008374
&gt;&gt; Epoch 184 finished 	ANN training loss 0.007908
&gt;&gt; Epoch 185 finished 	ANN training loss 0.008612
&gt;&gt; Epoch 186 finished 	ANN training loss 0.008368
&gt;&gt; Epoch 187 finished 	ANN training loss 0.008049
&gt;&gt; Epoch 188 finished 	ANN training loss 0.008544
&gt;&gt; Epoch 189 finished 	ANN training loss 0.008372
&gt;&gt; Epoch 190 finished 	ANN training loss 0.008228
&gt;&gt; Epoch 191 finished 	ANN training loss 0.007931
&gt;&gt; Epoch 192 finished 	ANN training loss 0.007178
&gt;&gt; Epoch 193 finished 	ANN training loss 0.007475
&gt;&gt; Epoch 194 finished 	ANN training loss 0.007936
&gt;&gt; Epoch 195 finished 	ANN training loss 0.007389
&gt;&gt; Epoch 196 finished 	ANN training loss 0.007859
&gt;&gt; Epoch 197 finished 	ANN training loss 0.007093
&gt;&gt; Epoch 198 finished 	ANN training loss 0.007480
&gt;&gt; Epoch 199 finished 	ANN training loss 0.007590
[END] Fine tuning step
Done.
Accuracy: 0.870000
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[19]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ACCURACY: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">acc6</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>ACCURACY: 0.87
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[20]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># ReLu, dropout = 0.7</span>
<span class="n">acc7</span> <span class="o">=</span> <span class="n">deep_belief_net</span><span class="p">(</span><span class="n">hidden_layers_structure</span><span class="o">=</span><span class="p">[</span><span class="mi">300</span><span class="p">],</span> 
                       <span class="n">learning_rate_rbm</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span>
                       <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                       <span class="n">n_epochs_rbm</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span>
                       <span class="n">n_iter_backprop</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
                       <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
                       <span class="n">activation_function</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span>
                       <span class="n">dropout_p</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[START] Pre-training step:
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 56.047256
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 75.743523
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 30.459770
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 47.690372
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 45.758965
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 34.811928
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 34.652184
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 34.783401
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 29.111069
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 31.374155
&gt;&gt; Epoch 11 finished 	RBM Reconstruction error 57.646584
&gt;&gt; Epoch 12 finished 	RBM Reconstruction error 22.685879
&gt;&gt; Epoch 13 finished 	RBM Reconstruction error 38.631672
&gt;&gt; Epoch 14 finished 	RBM Reconstruction error 44.106903
&gt;&gt; Epoch 15 finished 	RBM Reconstruction error 46.644821
&gt;&gt; Epoch 16 finished 	RBM Reconstruction error 33.263466
&gt;&gt; Epoch 17 finished 	RBM Reconstruction error 32.393909
&gt;&gt; Epoch 18 finished 	RBM Reconstruction error 36.074516
&gt;&gt; Epoch 19 finished 	RBM Reconstruction error 37.753258
&gt;&gt; Epoch 20 finished 	RBM Reconstruction error 27.625925
&gt;&gt; Epoch 21 finished 	RBM Reconstruction error 24.151003
&gt;&gt; Epoch 22 finished 	RBM Reconstruction error 46.662666
&gt;&gt; Epoch 23 finished 	RBM Reconstruction error 37.606102
&gt;&gt; Epoch 24 finished 	RBM Reconstruction error 37.305729
&gt;&gt; Epoch 25 finished 	RBM Reconstruction error 30.290499
&gt;&gt; Epoch 26 finished 	RBM Reconstruction error 29.300077
&gt;&gt; Epoch 27 finished 	RBM Reconstruction error 25.525688
&gt;&gt; Epoch 28 finished 	RBM Reconstruction error 38.064743
&gt;&gt; Epoch 29 finished 	RBM Reconstruction error 29.105440
&gt;&gt; Epoch 30 finished 	RBM Reconstruction error 44.112823
&gt;&gt; Epoch 31 finished 	RBM Reconstruction error 30.791983
&gt;&gt; Epoch 32 finished 	RBM Reconstruction error 43.724434
&gt;&gt; Epoch 33 finished 	RBM Reconstruction error 26.208227
&gt;&gt; Epoch 34 finished 	RBM Reconstruction error 34.571922
&gt;&gt; Epoch 35 finished 	RBM Reconstruction error 36.998856
&gt;&gt; Epoch 36 finished 	RBM Reconstruction error 31.790464
&gt;&gt; Epoch 37 finished 	RBM Reconstruction error 23.368891
&gt;&gt; Epoch 38 finished 	RBM Reconstruction error 27.893360
&gt;&gt; Epoch 39 finished 	RBM Reconstruction error 44.985336
&gt;&gt; Epoch 40 finished 	RBM Reconstruction error 54.031116
[END] Pre-training step
[START] Fine tuning step:
&gt;&gt; Epoch 0 finished 	ANN training loss 0.973052
&gt;&gt; Epoch 1 finished 	ANN training loss 0.703370
&gt;&gt; Epoch 2 finished 	ANN training loss 0.585994
&gt;&gt; Epoch 3 finished 	ANN training loss 0.559072
&gt;&gt; Epoch 4 finished 	ANN training loss 0.513076
&gt;&gt; Epoch 5 finished 	ANN training loss 0.465241
&gt;&gt; Epoch 6 finished 	ANN training loss 0.460248
&gt;&gt; Epoch 7 finished 	ANN training loss 0.427421
&gt;&gt; Epoch 8 finished 	ANN training loss 0.436733
&gt;&gt; Epoch 9 finished 	ANN training loss 0.408710
&gt;&gt; Epoch 10 finished 	ANN training loss 0.400848
&gt;&gt; Epoch 11 finished 	ANN training loss 0.384625
&gt;&gt; Epoch 12 finished 	ANN training loss 0.369857
&gt;&gt; Epoch 13 finished 	ANN training loss 0.344506
&gt;&gt; Epoch 14 finished 	ANN training loss 0.333850
&gt;&gt; Epoch 15 finished 	ANN training loss 0.316969
&gt;&gt; Epoch 16 finished 	ANN training loss 0.317043
&gt;&gt; Epoch 17 finished 	ANN training loss 0.304677
&gt;&gt; Epoch 18 finished 	ANN training loss 0.287227
&gt;&gt; Epoch 19 finished 	ANN training loss 0.296242
&gt;&gt; Epoch 20 finished 	ANN training loss 0.281737
&gt;&gt; Epoch 21 finished 	ANN training loss 0.299196
&gt;&gt; Epoch 22 finished 	ANN training loss 0.280769
&gt;&gt; Epoch 23 finished 	ANN training loss 0.270248
&gt;&gt; Epoch 24 finished 	ANN training loss 0.258146
&gt;&gt; Epoch 25 finished 	ANN training loss 0.268486
&gt;&gt; Epoch 26 finished 	ANN training loss 0.255776
&gt;&gt; Epoch 27 finished 	ANN training loss 0.244936
&gt;&gt; Epoch 28 finished 	ANN training loss 0.252485
&gt;&gt; Epoch 29 finished 	ANN training loss 0.220985
&gt;&gt; Epoch 30 finished 	ANN training loss 0.229774
&gt;&gt; Epoch 31 finished 	ANN training loss 0.211965
&gt;&gt; Epoch 32 finished 	ANN training loss 0.218526
&gt;&gt; Epoch 33 finished 	ANN training loss 0.218491
&gt;&gt; Epoch 34 finished 	ANN training loss 0.205666
&gt;&gt; Epoch 35 finished 	ANN training loss 0.194410
&gt;&gt; Epoch 36 finished 	ANN training loss 0.194929
&gt;&gt; Epoch 37 finished 	ANN training loss 0.187216
&gt;&gt; Epoch 38 finished 	ANN training loss 0.181864
&gt;&gt; Epoch 39 finished 	ANN training loss 0.186409
&gt;&gt; Epoch 40 finished 	ANN training loss 0.183393
&gt;&gt; Epoch 41 finished 	ANN training loss 0.193113
&gt;&gt; Epoch 42 finished 	ANN training loss 0.183741
&gt;&gt; Epoch 43 finished 	ANN training loss 0.174380
&gt;&gt; Epoch 44 finished 	ANN training loss 0.174699
&gt;&gt; Epoch 45 finished 	ANN training loss 0.177344
&gt;&gt; Epoch 46 finished 	ANN training loss 0.177890
&gt;&gt; Epoch 47 finished 	ANN training loss 0.180121
&gt;&gt; Epoch 48 finished 	ANN training loss 0.178715
&gt;&gt; Epoch 49 finished 	ANN training loss 0.171130
&gt;&gt; Epoch 50 finished 	ANN training loss 0.171832
&gt;&gt; Epoch 51 finished 	ANN training loss 0.168926
&gt;&gt; Epoch 52 finished 	ANN training loss 0.171720
&gt;&gt; Epoch 53 finished 	ANN training loss 0.168353
&gt;&gt; Epoch 54 finished 	ANN training loss 0.156128
&gt;&gt; Epoch 55 finished 	ANN training loss 0.155456
&gt;&gt; Epoch 56 finished 	ANN training loss 0.150065
&gt;&gt; Epoch 57 finished 	ANN training loss 0.150066
&gt;&gt; Epoch 58 finished 	ANN training loss 0.147124
&gt;&gt; Epoch 59 finished 	ANN training loss 0.150208
&gt;&gt; Epoch 60 finished 	ANN training loss 0.144634
&gt;&gt; Epoch 61 finished 	ANN training loss 0.147727
&gt;&gt; Epoch 62 finished 	ANN training loss 0.146366
&gt;&gt; Epoch 63 finished 	ANN training loss 0.148834
&gt;&gt; Epoch 64 finished 	ANN training loss 0.156989
&gt;&gt; Epoch 65 finished 	ANN training loss 0.150146
&gt;&gt; Epoch 66 finished 	ANN training loss 0.139876
&gt;&gt; Epoch 67 finished 	ANN training loss 0.139270
&gt;&gt; Epoch 68 finished 	ANN training loss 0.145022
&gt;&gt; Epoch 69 finished 	ANN training loss 0.144172
&gt;&gt; Epoch 70 finished 	ANN training loss 0.133995
&gt;&gt; Epoch 71 finished 	ANN training loss 0.132417
&gt;&gt; Epoch 72 finished 	ANN training loss 0.128212
&gt;&gt; Epoch 73 finished 	ANN training loss 0.127191
&gt;&gt; Epoch 74 finished 	ANN training loss 0.133897
&gt;&gt; Epoch 75 finished 	ANN training loss 0.130620
&gt;&gt; Epoch 76 finished 	ANN training loss 0.120538
&gt;&gt; Epoch 77 finished 	ANN training loss 0.121108
&gt;&gt; Epoch 78 finished 	ANN training loss 0.119742
&gt;&gt; Epoch 79 finished 	ANN training loss 0.121870
&gt;&gt; Epoch 80 finished 	ANN training loss 0.126240
&gt;&gt; Epoch 81 finished 	ANN training loss 0.119546
&gt;&gt; Epoch 82 finished 	ANN training loss 0.123664
&gt;&gt; Epoch 83 finished 	ANN training loss 0.115060
&gt;&gt; Epoch 84 finished 	ANN training loss 0.119965
&gt;&gt; Epoch 85 finished 	ANN training loss 0.120976
&gt;&gt; Epoch 86 finished 	ANN training loss 0.110317
&gt;&gt; Epoch 87 finished 	ANN training loss 0.105291
&gt;&gt; Epoch 88 finished 	ANN training loss 0.107320
&gt;&gt; Epoch 89 finished 	ANN training loss 0.112926
&gt;&gt; Epoch 90 finished 	ANN training loss 0.114774
&gt;&gt; Epoch 91 finished 	ANN training loss 0.109561
&gt;&gt; Epoch 92 finished 	ANN training loss 0.101217
&gt;&gt; Epoch 93 finished 	ANN training loss 0.096898
&gt;&gt; Epoch 94 finished 	ANN training loss 0.099306
&gt;&gt; Epoch 95 finished 	ANN training loss 0.103072
&gt;&gt; Epoch 96 finished 	ANN training loss 0.113116
&gt;&gt; Epoch 97 finished 	ANN training loss 0.107537
&gt;&gt; Epoch 98 finished 	ANN training loss 0.101779
&gt;&gt; Epoch 99 finished 	ANN training loss 0.106275
&gt;&gt; Epoch 100 finished 	ANN training loss 0.100516
&gt;&gt; Epoch 101 finished 	ANN training loss 0.103054
&gt;&gt; Epoch 102 finished 	ANN training loss 0.105381
&gt;&gt; Epoch 103 finished 	ANN training loss 0.102134
&gt;&gt; Epoch 104 finished 	ANN training loss 0.104943
&gt;&gt; Epoch 105 finished 	ANN training loss 0.096621
&gt;&gt; Epoch 106 finished 	ANN training loss 0.091742
&gt;&gt; Epoch 107 finished 	ANN training loss 0.092124
&gt;&gt; Epoch 108 finished 	ANN training loss 0.090472
&gt;&gt; Epoch 109 finished 	ANN training loss 0.094556
&gt;&gt; Epoch 110 finished 	ANN training loss 0.093748
&gt;&gt; Epoch 111 finished 	ANN training loss 0.099566
&gt;&gt; Epoch 112 finished 	ANN training loss 0.093945
&gt;&gt; Epoch 113 finished 	ANN training loss 0.092885
&gt;&gt; Epoch 114 finished 	ANN training loss 0.092413
&gt;&gt; Epoch 115 finished 	ANN training loss 0.098421
&gt;&gt; Epoch 116 finished 	ANN training loss 0.100564
&gt;&gt; Epoch 117 finished 	ANN training loss 0.095143
&gt;&gt; Epoch 118 finished 	ANN training loss 0.088131
&gt;&gt; Epoch 119 finished 	ANN training loss 0.090358
&gt;&gt; Epoch 120 finished 	ANN training loss 0.094863
&gt;&gt; Epoch 121 finished 	ANN training loss 0.096233
&gt;&gt; Epoch 122 finished 	ANN training loss 0.086690
&gt;&gt; Epoch 123 finished 	ANN training loss 0.087597
&gt;&gt; Epoch 124 finished 	ANN training loss 0.087954
&gt;&gt; Epoch 125 finished 	ANN training loss 0.086892
&gt;&gt; Epoch 126 finished 	ANN training loss 0.088518
&gt;&gt; Epoch 127 finished 	ANN training loss 0.087626
&gt;&gt; Epoch 128 finished 	ANN training loss 0.088492
&gt;&gt; Epoch 129 finished 	ANN training loss 0.087782
&gt;&gt; Epoch 130 finished 	ANN training loss 0.090687
&gt;&gt; Epoch 131 finished 	ANN training loss 0.087098
&gt;&gt; Epoch 132 finished 	ANN training loss 0.083938
&gt;&gt; Epoch 133 finished 	ANN training loss 0.083891
&gt;&gt; Epoch 134 finished 	ANN training loss 0.085967
&gt;&gt; Epoch 135 finished 	ANN training loss 0.088994
&gt;&gt; Epoch 136 finished 	ANN training loss 0.081334
&gt;&gt; Epoch 137 finished 	ANN training loss 0.080027
&gt;&gt; Epoch 138 finished 	ANN training loss 0.083530
&gt;&gt; Epoch 139 finished 	ANN training loss 0.074376
&gt;&gt; Epoch 140 finished 	ANN training loss 0.069983
&gt;&gt; Epoch 141 finished 	ANN training loss 0.071416
&gt;&gt; Epoch 142 finished 	ANN training loss 0.073563
&gt;&gt; Epoch 143 finished 	ANN training loss 0.072102
&gt;&gt; Epoch 144 finished 	ANN training loss 0.075368
&gt;&gt; Epoch 145 finished 	ANN training loss 0.070104
&gt;&gt; Epoch 146 finished 	ANN training loss 0.074181
&gt;&gt; Epoch 147 finished 	ANN training loss 0.071471
&gt;&gt; Epoch 148 finished 	ANN training loss 0.072015
&gt;&gt; Epoch 149 finished 	ANN training loss 0.073198
&gt;&gt; Epoch 150 finished 	ANN training loss 0.069105
&gt;&gt; Epoch 151 finished 	ANN training loss 0.077705
&gt;&gt; Epoch 152 finished 	ANN training loss 0.073047
&gt;&gt; Epoch 153 finished 	ANN training loss 0.071821
&gt;&gt; Epoch 154 finished 	ANN training loss 0.070960
&gt;&gt; Epoch 155 finished 	ANN training loss 0.071478
&gt;&gt; Epoch 156 finished 	ANN training loss 0.074110
&gt;&gt; Epoch 157 finished 	ANN training loss 0.071285
&gt;&gt; Epoch 158 finished 	ANN training loss 0.074812
&gt;&gt; Epoch 159 finished 	ANN training loss 0.076291
&gt;&gt; Epoch 160 finished 	ANN training loss 0.071774
&gt;&gt; Epoch 161 finished 	ANN training loss 0.069770
&gt;&gt; Epoch 162 finished 	ANN training loss 0.071379
&gt;&gt; Epoch 163 finished 	ANN training loss 0.066810
&gt;&gt; Epoch 164 finished 	ANN training loss 0.072542
&gt;&gt; Epoch 165 finished 	ANN training loss 0.074412
&gt;&gt; Epoch 166 finished 	ANN training loss 0.065361
&gt;&gt; Epoch 167 finished 	ANN training loss 0.072267
&gt;&gt; Epoch 168 finished 	ANN training loss 0.074628
&gt;&gt; Epoch 169 finished 	ANN training loss 0.076956
&gt;&gt; Epoch 170 finished 	ANN training loss 0.077255
&gt;&gt; Epoch 171 finished 	ANN training loss 0.076564
&gt;&gt; Epoch 172 finished 	ANN training loss 0.074154
&gt;&gt; Epoch 173 finished 	ANN training loss 0.075528
&gt;&gt; Epoch 174 finished 	ANN training loss 0.070980
&gt;&gt; Epoch 175 finished 	ANN training loss 0.070886
&gt;&gt; Epoch 176 finished 	ANN training loss 0.076924
&gt;&gt; Epoch 177 finished 	ANN training loss 0.075772
&gt;&gt; Epoch 178 finished 	ANN training loss 0.073212
&gt;&gt; Epoch 179 finished 	ANN training loss 0.072196
&gt;&gt; Epoch 180 finished 	ANN training loss 0.067445
&gt;&gt; Epoch 181 finished 	ANN training loss 0.061758
&gt;&gt; Epoch 182 finished 	ANN training loss 0.063756
&gt;&gt; Epoch 183 finished 	ANN training loss 0.064796
&gt;&gt; Epoch 184 finished 	ANN training loss 0.064416
&gt;&gt; Epoch 185 finished 	ANN training loss 0.064928
&gt;&gt; Epoch 186 finished 	ANN training loss 0.062131
&gt;&gt; Epoch 187 finished 	ANN training loss 0.059296
&gt;&gt; Epoch 188 finished 	ANN training loss 0.061075
&gt;&gt; Epoch 189 finished 	ANN training loss 0.063031
&gt;&gt; Epoch 190 finished 	ANN training loss 0.058655
&gt;&gt; Epoch 191 finished 	ANN training loss 0.056797
&gt;&gt; Epoch 192 finished 	ANN training loss 0.056967
&gt;&gt; Epoch 193 finished 	ANN training loss 0.054322
&gt;&gt; Epoch 194 finished 	ANN training loss 0.056132
&gt;&gt; Epoch 195 finished 	ANN training loss 0.062337
&gt;&gt; Epoch 196 finished 	ANN training loss 0.061040
&gt;&gt; Epoch 197 finished 	ANN training loss 0.055425
&gt;&gt; Epoch 198 finished 	ANN training loss 0.058720
&gt;&gt; Epoch 199 finished 	ANN training loss 0.066024
[END] Fine tuning step
Done.
Accuracy: 0.900000
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[21]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ACCURACY: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">acc7</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>ACCURACY: 0.9
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[22]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># ReLu, dropout = 0.8</span>
<span class="n">acc8</span> <span class="o">=</span> <span class="n">deep_belief_net</span><span class="p">(</span><span class="n">hidden_layers_structure</span><span class="o">=</span><span class="p">[</span><span class="mi">300</span><span class="p">],</span> 
                       <span class="n">learning_rate_rbm</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span>
                       <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                       <span class="n">n_epochs_rbm</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span>
                       <span class="n">n_iter_backprop</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
                       <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
                       <span class="n">activation_function</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span>
                       <span class="n">dropout_p</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[START] Pre-training step:
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 60.473331
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 56.021935
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 71.656853
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 50.487885
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 50.820499
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 42.040997
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 34.458988
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 42.691341
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 52.118073
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 46.811501
&gt;&gt; Epoch 11 finished 	RBM Reconstruction error 32.541492
&gt;&gt; Epoch 12 finished 	RBM Reconstruction error 34.893253
&gt;&gt; Epoch 13 finished 	RBM Reconstruction error 52.058681
&gt;&gt; Epoch 14 finished 	RBM Reconstruction error 38.734688
&gt;&gt; Epoch 15 finished 	RBM Reconstruction error 37.273239
&gt;&gt; Epoch 16 finished 	RBM Reconstruction error 34.313351
&gt;&gt; Epoch 17 finished 	RBM Reconstruction error 48.837479
&gt;&gt; Epoch 18 finished 	RBM Reconstruction error 39.533573
&gt;&gt; Epoch 19 finished 	RBM Reconstruction error 30.994461
&gt;&gt; Epoch 20 finished 	RBM Reconstruction error 36.145123
&gt;&gt; Epoch 21 finished 	RBM Reconstruction error 26.398838
&gt;&gt; Epoch 22 finished 	RBM Reconstruction error 29.877769
&gt;&gt; Epoch 23 finished 	RBM Reconstruction error 32.888607
&gt;&gt; Epoch 24 finished 	RBM Reconstruction error 38.913994
&gt;&gt; Epoch 25 finished 	RBM Reconstruction error 34.648663
&gt;&gt; Epoch 26 finished 	RBM Reconstruction error 46.099884
&gt;&gt; Epoch 27 finished 	RBM Reconstruction error 42.561821
&gt;&gt; Epoch 28 finished 	RBM Reconstruction error 41.133087
&gt;&gt; Epoch 29 finished 	RBM Reconstruction error 35.349255
&gt;&gt; Epoch 30 finished 	RBM Reconstruction error 29.608042
&gt;&gt; Epoch 31 finished 	RBM Reconstruction error 34.103809
&gt;&gt; Epoch 32 finished 	RBM Reconstruction error 45.162472
&gt;&gt; Epoch 33 finished 	RBM Reconstruction error 43.641148
&gt;&gt; Epoch 34 finished 	RBM Reconstruction error 30.042070
&gt;&gt; Epoch 35 finished 	RBM Reconstruction error 38.409088
&gt;&gt; Epoch 36 finished 	RBM Reconstruction error 37.312607
&gt;&gt; Epoch 37 finished 	RBM Reconstruction error 34.084312
&gt;&gt; Epoch 38 finished 	RBM Reconstruction error 37.813393
&gt;&gt; Epoch 39 finished 	RBM Reconstruction error 40.761871
&gt;&gt; Epoch 40 finished 	RBM Reconstruction error 42.557953
[END] Pre-training step
[START] Fine tuning step:
&gt;&gt; Epoch 0 finished 	ANN training loss 1.132074
&gt;&gt; Epoch 1 finished 	ANN training loss 0.935824
&gt;&gt; Epoch 2 finished 	ANN training loss 0.899670
&gt;&gt; Epoch 3 finished 	ANN training loss 0.857884
&gt;&gt; Epoch 4 finished 	ANN training loss 0.895157
&gt;&gt; Epoch 5 finished 	ANN training loss 0.831615
&gt;&gt; Epoch 6 finished 	ANN training loss 0.847662
&gt;&gt; Epoch 7 finished 	ANN training loss 0.814241
&gt;&gt; Epoch 8 finished 	ANN training loss 0.768015
&gt;&gt; Epoch 9 finished 	ANN training loss 0.741344
&gt;&gt; Epoch 10 finished 	ANN training loss 0.741145
&gt;&gt; Epoch 11 finished 	ANN training loss 0.737562
&gt;&gt; Epoch 12 finished 	ANN training loss 0.760413
&gt;&gt; Epoch 13 finished 	ANN training loss 0.710643
&gt;&gt; Epoch 14 finished 	ANN training loss 0.712115
&gt;&gt; Epoch 15 finished 	ANN training loss 0.715964
&gt;&gt; Epoch 16 finished 	ANN training loss 0.677715
&gt;&gt; Epoch 17 finished 	ANN training loss 0.694968
&gt;&gt; Epoch 18 finished 	ANN training loss 0.690122
&gt;&gt; Epoch 19 finished 	ANN training loss 0.689990
&gt;&gt; Epoch 20 finished 	ANN training loss 0.696561
&gt;&gt; Epoch 21 finished 	ANN training loss 0.633091
&gt;&gt; Epoch 22 finished 	ANN training loss 0.647647
&gt;&gt; Epoch 23 finished 	ANN training loss 0.658715
&gt;&gt; Epoch 24 finished 	ANN training loss 0.643404
&gt;&gt; Epoch 25 finished 	ANN training loss 0.628812
&gt;&gt; Epoch 26 finished 	ANN training loss 0.640817
&gt;&gt; Epoch 27 finished 	ANN training loss 0.598085
&gt;&gt; Epoch 28 finished 	ANN training loss 0.618935
&gt;&gt; Epoch 29 finished 	ANN training loss 0.581256
&gt;&gt; Epoch 30 finished 	ANN training loss 0.595062
&gt;&gt; Epoch 31 finished 	ANN training loss 0.577188
&gt;&gt; Epoch 32 finished 	ANN training loss 0.616411
&gt;&gt; Epoch 33 finished 	ANN training loss 0.599234
&gt;&gt; Epoch 34 finished 	ANN training loss 0.615360
&gt;&gt; Epoch 35 finished 	ANN training loss 0.620599
&gt;&gt; Epoch 36 finished 	ANN training loss 0.588794
&gt;&gt; Epoch 37 finished 	ANN training loss 0.624106
&gt;&gt; Epoch 38 finished 	ANN training loss 0.577390
&gt;&gt; Epoch 39 finished 	ANN training loss 0.577865
&gt;&gt; Epoch 40 finished 	ANN training loss 0.581162
&gt;&gt; Epoch 41 finished 	ANN training loss 0.558854
&gt;&gt; Epoch 42 finished 	ANN training loss 0.594308
&gt;&gt; Epoch 43 finished 	ANN training loss 0.574351
&gt;&gt; Epoch 44 finished 	ANN training loss 0.550056
&gt;&gt; Epoch 45 finished 	ANN training loss 0.558080
&gt;&gt; Epoch 46 finished 	ANN training loss 0.576704
&gt;&gt; Epoch 47 finished 	ANN training loss 0.586663
&gt;&gt; Epoch 48 finished 	ANN training loss 0.565888
&gt;&gt; Epoch 49 finished 	ANN training loss 0.551348
&gt;&gt; Epoch 50 finished 	ANN training loss 0.565030
&gt;&gt; Epoch 51 finished 	ANN training loss 0.591374
&gt;&gt; Epoch 52 finished 	ANN training loss 0.581353
&gt;&gt; Epoch 53 finished 	ANN training loss 0.562089
&gt;&gt; Epoch 54 finished 	ANN training loss 0.563112
&gt;&gt; Epoch 55 finished 	ANN training loss 0.562353
&gt;&gt; Epoch 56 finished 	ANN training loss 0.554455
&gt;&gt; Epoch 57 finished 	ANN training loss 0.568357
&gt;&gt; Epoch 58 finished 	ANN training loss 0.552972
&gt;&gt; Epoch 59 finished 	ANN training loss 0.555093
&gt;&gt; Epoch 60 finished 	ANN training loss 0.544792
&gt;&gt; Epoch 61 finished 	ANN training loss 0.542625
&gt;&gt; Epoch 62 finished 	ANN training loss 0.556875
&gt;&gt; Epoch 63 finished 	ANN training loss 0.530907
&gt;&gt; Epoch 64 finished 	ANN training loss 0.555902
&gt;&gt; Epoch 65 finished 	ANN training loss 0.578795
&gt;&gt; Epoch 66 finished 	ANN training loss 0.553687
&gt;&gt; Epoch 67 finished 	ANN training loss 0.533920
&gt;&gt; Epoch 68 finished 	ANN training loss 0.527071
&gt;&gt; Epoch 69 finished 	ANN training loss 0.514335
&gt;&gt; Epoch 70 finished 	ANN training loss 0.507415
&gt;&gt; Epoch 71 finished 	ANN training loss 0.510460
&gt;&gt; Epoch 72 finished 	ANN training loss 0.480948
&gt;&gt; Epoch 73 finished 	ANN training loss 0.501444
&gt;&gt; Epoch 74 finished 	ANN training loss 0.510695
&gt;&gt; Epoch 75 finished 	ANN training loss 0.509253
&gt;&gt; Epoch 76 finished 	ANN training loss 0.496056
&gt;&gt; Epoch 77 finished 	ANN training loss 0.496375
&gt;&gt; Epoch 78 finished 	ANN training loss 0.485085
&gt;&gt; Epoch 79 finished 	ANN training loss 0.495151
&gt;&gt; Epoch 80 finished 	ANN training loss 0.506187
&gt;&gt; Epoch 81 finished 	ANN training loss 0.491050
&gt;&gt; Epoch 82 finished 	ANN training loss 0.517487
&gt;&gt; Epoch 83 finished 	ANN training loss 0.527828
&gt;&gt; Epoch 84 finished 	ANN training loss 0.516435
&gt;&gt; Epoch 85 finished 	ANN training loss 0.501296
&gt;&gt; Epoch 86 finished 	ANN training loss 0.507035
&gt;&gt; Epoch 87 finished 	ANN training loss 0.526031
&gt;&gt; Epoch 88 finished 	ANN training loss 0.506426
&gt;&gt; Epoch 89 finished 	ANN training loss 0.505084
&gt;&gt; Epoch 90 finished 	ANN training loss 0.524261
&gt;&gt; Epoch 91 finished 	ANN training loss 0.517440
&gt;&gt; Epoch 92 finished 	ANN training loss 0.523115
&gt;&gt; Epoch 93 finished 	ANN training loss 0.477661
&gt;&gt; Epoch 94 finished 	ANN training loss 0.471512
&gt;&gt; Epoch 95 finished 	ANN training loss 0.488449
&gt;&gt; Epoch 96 finished 	ANN training loss 0.478747
&gt;&gt; Epoch 97 finished 	ANN training loss 0.460625
&gt;&gt; Epoch 98 finished 	ANN training loss 0.455149
&gt;&gt; Epoch 99 finished 	ANN training loss 0.454151
&gt;&gt; Epoch 100 finished 	ANN training loss 0.484615
&gt;&gt; Epoch 101 finished 	ANN training loss 0.509662
&gt;&gt; Epoch 102 finished 	ANN training loss 0.500093
&gt;&gt; Epoch 103 finished 	ANN training loss 0.497978
&gt;&gt; Epoch 104 finished 	ANN training loss 0.479377
&gt;&gt; Epoch 105 finished 	ANN training loss 0.479578
&gt;&gt; Epoch 106 finished 	ANN training loss 0.484659
&gt;&gt; Epoch 107 finished 	ANN training loss 0.465586
&gt;&gt; Epoch 108 finished 	ANN training loss 0.455251
&gt;&gt; Epoch 109 finished 	ANN training loss 0.470125
&gt;&gt; Epoch 110 finished 	ANN training loss 0.462641
&gt;&gt; Epoch 111 finished 	ANN training loss 0.468803
&gt;&gt; Epoch 112 finished 	ANN training loss 0.480760
&gt;&gt; Epoch 113 finished 	ANN training loss 0.474909
&gt;&gt; Epoch 114 finished 	ANN training loss 0.476098
&gt;&gt; Epoch 115 finished 	ANN training loss 0.458587
&gt;&gt; Epoch 116 finished 	ANN training loss 0.453551
&gt;&gt; Epoch 117 finished 	ANN training loss 0.472606
&gt;&gt; Epoch 118 finished 	ANN training loss 0.463366
&gt;&gt; Epoch 119 finished 	ANN training loss 0.491521
&gt;&gt; Epoch 120 finished 	ANN training loss 0.478163
&gt;&gt; Epoch 121 finished 	ANN training loss 0.479955
&gt;&gt; Epoch 122 finished 	ANN training loss 0.478597
&gt;&gt; Epoch 123 finished 	ANN training loss 0.482490
&gt;&gt; Epoch 124 finished 	ANN training loss 0.475040
&gt;&gt; Epoch 125 finished 	ANN training loss 0.466021
&gt;&gt; Epoch 126 finished 	ANN training loss 0.467682
&gt;&gt; Epoch 127 finished 	ANN training loss 0.478625
&gt;&gt; Epoch 128 finished 	ANN training loss 0.492616
&gt;&gt; Epoch 129 finished 	ANN training loss 0.473480
&gt;&gt; Epoch 130 finished 	ANN training loss 0.461541
&gt;&gt; Epoch 131 finished 	ANN training loss 0.465311
&gt;&gt; Epoch 132 finished 	ANN training loss 0.465637
&gt;&gt; Epoch 133 finished 	ANN training loss 0.470708
&gt;&gt; Epoch 134 finished 	ANN training loss 0.483775
&gt;&gt; Epoch 135 finished 	ANN training loss 0.486034
&gt;&gt; Epoch 136 finished 	ANN training loss 0.463178
&gt;&gt; Epoch 137 finished 	ANN training loss 0.468607
&gt;&gt; Epoch 138 finished 	ANN training loss 0.473074
&gt;&gt; Epoch 139 finished 	ANN training loss 0.463344
&gt;&gt; Epoch 140 finished 	ANN training loss 0.446094
&gt;&gt; Epoch 141 finished 	ANN training loss 0.429035
&gt;&gt; Epoch 142 finished 	ANN training loss 0.420760
&gt;&gt; Epoch 143 finished 	ANN training loss 0.424564
&gt;&gt; Epoch 144 finished 	ANN training loss 0.444490
&gt;&gt; Epoch 145 finished 	ANN training loss 0.435584
&gt;&gt; Epoch 146 finished 	ANN training loss 0.419039
&gt;&gt; Epoch 147 finished 	ANN training loss 0.451174
&gt;&gt; Epoch 148 finished 	ANN training loss 0.446619
&gt;&gt; Epoch 149 finished 	ANN training loss 0.444245
&gt;&gt; Epoch 150 finished 	ANN training loss 0.426693
&gt;&gt; Epoch 151 finished 	ANN training loss 0.439684
&gt;&gt; Epoch 152 finished 	ANN training loss 0.456615
&gt;&gt; Epoch 153 finished 	ANN training loss 0.435980
&gt;&gt; Epoch 154 finished 	ANN training loss 0.446294
&gt;&gt; Epoch 155 finished 	ANN training loss 0.458092
&gt;&gt; Epoch 156 finished 	ANN training loss 0.477440
&gt;&gt; Epoch 157 finished 	ANN training loss 0.461287
&gt;&gt; Epoch 158 finished 	ANN training loss 0.446175
&gt;&gt; Epoch 159 finished 	ANN training loss 0.466359
&gt;&gt; Epoch 160 finished 	ANN training loss 0.448909
&gt;&gt; Epoch 161 finished 	ANN training loss 0.469638
&gt;&gt; Epoch 162 finished 	ANN training loss 0.488412
&gt;&gt; Epoch 163 finished 	ANN training loss 0.514167
&gt;&gt; Epoch 164 finished 	ANN training loss 0.468001
&gt;&gt; Epoch 165 finished 	ANN training loss 0.464470
&gt;&gt; Epoch 166 finished 	ANN training loss 0.445501
&gt;&gt; Epoch 167 finished 	ANN training loss 0.447396
&gt;&gt; Epoch 168 finished 	ANN training loss 0.438374
&gt;&gt; Epoch 169 finished 	ANN training loss 0.440141
&gt;&gt; Epoch 170 finished 	ANN training loss 0.445051
&gt;&gt; Epoch 171 finished 	ANN training loss 0.432966
&gt;&gt; Epoch 172 finished 	ANN training loss 0.420078
&gt;&gt; Epoch 173 finished 	ANN training loss 0.453872
&gt;&gt; Epoch 174 finished 	ANN training loss 0.451927
&gt;&gt; Epoch 175 finished 	ANN training loss 0.435021
&gt;&gt; Epoch 176 finished 	ANN training loss 0.448320
&gt;&gt; Epoch 177 finished 	ANN training loss 0.449271
&gt;&gt; Epoch 178 finished 	ANN training loss 0.448843
&gt;&gt; Epoch 179 finished 	ANN training loss 0.430400
&gt;&gt; Epoch 180 finished 	ANN training loss 0.426055
&gt;&gt; Epoch 181 finished 	ANN training loss 0.413947
&gt;&gt; Epoch 182 finished 	ANN training loss 0.432023
&gt;&gt; Epoch 183 finished 	ANN training loss 0.422865
&gt;&gt; Epoch 184 finished 	ANN training loss 0.416759
&gt;&gt; Epoch 185 finished 	ANN training loss 0.431288
&gt;&gt; Epoch 186 finished 	ANN training loss 0.421466
&gt;&gt; Epoch 187 finished 	ANN training loss 0.439843
&gt;&gt; Epoch 188 finished 	ANN training loss 0.452089
&gt;&gt; Epoch 189 finished 	ANN training loss 0.447998
&gt;&gt; Epoch 190 finished 	ANN training loss 0.428183
&gt;&gt; Epoch 191 finished 	ANN training loss 0.431471
&gt;&gt; Epoch 192 finished 	ANN training loss 0.415347
&gt;&gt; Epoch 193 finished 	ANN training loss 0.437009
&gt;&gt; Epoch 194 finished 	ANN training loss 0.427055
&gt;&gt; Epoch 195 finished 	ANN training loss 0.428547
&gt;&gt; Epoch 196 finished 	ANN training loss 0.420437
&gt;&gt; Epoch 197 finished 	ANN training loss 0.409065
&gt;&gt; Epoch 198 finished 	ANN training loss 0.418048
&gt;&gt; Epoch 199 finished 	ANN training loss 0.421983
[END] Fine tuning step
Done.
Accuracy: 0.800000
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[23]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ACCURACY: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">acc8</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>ACCURACY: 0.8
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[24]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># ReLu, dropout = 0.9</span>
<span class="n">acc9</span> <span class="o">=</span> <span class="n">deep_belief_net</span><span class="p">(</span><span class="n">hidden_layers_structure</span><span class="o">=</span><span class="p">[</span><span class="mi">300</span><span class="p">],</span> 
                       <span class="n">learning_rate_rbm</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span>
                       <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                       <span class="n">n_epochs_rbm</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span>
                       <span class="n">n_iter_backprop</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
                       <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
                       <span class="n">activation_function</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span>
                       <span class="n">dropout_p</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[START] Pre-training step:
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 51.625134
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 76.770470
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 72.536652
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 62.944092
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 53.268398
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 58.766701
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 48.798634
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 60.624760
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 56.839169
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 43.366913
&gt;&gt; Epoch 11 finished 	RBM Reconstruction error 43.427475
&gt;&gt; Epoch 12 finished 	RBM Reconstruction error 27.791582
&gt;&gt; Epoch 13 finished 	RBM Reconstruction error 40.248569
&gt;&gt; Epoch 14 finished 	RBM Reconstruction error 34.714977
&gt;&gt; Epoch 15 finished 	RBM Reconstruction error 57.294037
&gt;&gt; Epoch 16 finished 	RBM Reconstruction error 51.249142
&gt;&gt; Epoch 17 finished 	RBM Reconstruction error 49.431450
&gt;&gt; Epoch 18 finished 	RBM Reconstruction error 57.282455
&gt;&gt; Epoch 19 finished 	RBM Reconstruction error 39.906178
&gt;&gt; Epoch 20 finished 	RBM Reconstruction error 64.982338
&gt;&gt; Epoch 21 finished 	RBM Reconstruction error 46.795116
&gt;&gt; Epoch 22 finished 	RBM Reconstruction error 46.363087
&gt;&gt; Epoch 23 finished 	RBM Reconstruction error 48.862373
&gt;&gt; Epoch 24 finished 	RBM Reconstruction error 44.838661
&gt;&gt; Epoch 25 finished 	RBM Reconstruction error 36.712193
&gt;&gt; Epoch 26 finished 	RBM Reconstruction error 50.306717
&gt;&gt; Epoch 27 finished 	RBM Reconstruction error 41.899246
&gt;&gt; Epoch 28 finished 	RBM Reconstruction error 49.440899
&gt;&gt; Epoch 29 finished 	RBM Reconstruction error 54.250488
&gt;&gt; Epoch 30 finished 	RBM Reconstruction error 50.092716
&gt;&gt; Epoch 31 finished 	RBM Reconstruction error 50.622646
&gt;&gt; Epoch 32 finished 	RBM Reconstruction error 47.596855
&gt;&gt; Epoch 33 finished 	RBM Reconstruction error 43.325298
&gt;&gt; Epoch 34 finished 	RBM Reconstruction error 51.524601
&gt;&gt; Epoch 35 finished 	RBM Reconstruction error 48.152855
&gt;&gt; Epoch 36 finished 	RBM Reconstruction error 43.356514
&gt;&gt; Epoch 37 finished 	RBM Reconstruction error 59.101551
&gt;&gt; Epoch 38 finished 	RBM Reconstruction error 49.825939
&gt;&gt; Epoch 39 finished 	RBM Reconstruction error 47.288212
&gt;&gt; Epoch 40 finished 	RBM Reconstruction error 56.597286
[END] Pre-training step
[START] Fine tuning step:
&gt;&gt; Epoch 0 finished 	ANN training loss 1.732169
&gt;&gt; Epoch 1 finished 	ANN training loss 1.941332
&gt;&gt; Epoch 2 finished 	ANN training loss 2.087359
&gt;&gt; Epoch 3 finished 	ANN training loss 2.121989
&gt;&gt; Epoch 4 finished 	ANN training loss 2.151807
&gt;&gt; Epoch 5 finished 	ANN training loss 2.160697
&gt;&gt; Epoch 6 finished 	ANN training loss 2.152679
&gt;&gt; Epoch 7 finished 	ANN training loss 2.130618
&gt;&gt; Epoch 8 finished 	ANN training loss 2.174693
&gt;&gt; Epoch 9 finished 	ANN training loss 2.188563
&gt;&gt; Epoch 10 finished 	ANN training loss 2.129323
&gt;&gt; Epoch 11 finished 	ANN training loss 2.088039
&gt;&gt; Epoch 12 finished 	ANN training loss 2.109590
&gt;&gt; Epoch 13 finished 	ANN training loss 2.092322
&gt;&gt; Epoch 14 finished 	ANN training loss 2.128825
&gt;&gt; Epoch 15 finished 	ANN training loss 2.093973
&gt;&gt; Epoch 16 finished 	ANN training loss 2.103493
&gt;&gt; Epoch 17 finished 	ANN training loss 2.120016
&gt;&gt; Epoch 18 finished 	ANN training loss 2.146897
&gt;&gt; Epoch 19 finished 	ANN training loss 2.146122
&gt;&gt; Epoch 20 finished 	ANN training loss 2.125919
&gt;&gt; Epoch 21 finished 	ANN training loss 2.137984
&gt;&gt; Epoch 22 finished 	ANN training loss 2.155367
&gt;&gt; Epoch 23 finished 	ANN training loss 2.158597
&gt;&gt; Epoch 24 finished 	ANN training loss 2.141308
&gt;&gt; Epoch 25 finished 	ANN training loss 2.129461
&gt;&gt; Epoch 26 finished 	ANN training loss 2.172035
&gt;&gt; Epoch 27 finished 	ANN training loss 2.153636
&gt;&gt; Epoch 28 finished 	ANN training loss 2.130220
&gt;&gt; Epoch 29 finished 	ANN training loss 2.167326
&gt;&gt; Epoch 30 finished 	ANN training loss 2.147037
&gt;&gt; Epoch 31 finished 	ANN training loss 2.139156
&gt;&gt; Epoch 32 finished 	ANN training loss 2.146676
&gt;&gt; Epoch 33 finished 	ANN training loss 2.145547
&gt;&gt; Epoch 34 finished 	ANN training loss 2.145848
&gt;&gt; Epoch 35 finished 	ANN training loss 2.131047
&gt;&gt; Epoch 36 finished 	ANN training loss 2.145546
&gt;&gt; Epoch 37 finished 	ANN training loss 2.163685
&gt;&gt; Epoch 38 finished 	ANN training loss 2.174070
&gt;&gt; Epoch 39 finished 	ANN training loss 2.167590
&gt;&gt; Epoch 40 finished 	ANN training loss 2.170692
&gt;&gt; Epoch 41 finished 	ANN training loss 2.172338
&gt;&gt; Epoch 42 finished 	ANN training loss 2.153179
&gt;&gt; Epoch 43 finished 	ANN training loss 2.179023
&gt;&gt; Epoch 44 finished 	ANN training loss 2.170428
&gt;&gt; Epoch 45 finished 	ANN training loss 2.174390
&gt;&gt; Epoch 46 finished 	ANN training loss 2.150687
&gt;&gt; Epoch 47 finished 	ANN training loss 2.157766
&gt;&gt; Epoch 48 finished 	ANN training loss 2.184215
&gt;&gt; Epoch 49 finished 	ANN training loss 2.156837
&gt;&gt; Epoch 50 finished 	ANN training loss 2.159907
&gt;&gt; Epoch 51 finished 	ANN training loss 2.176643
&gt;&gt; Epoch 52 finished 	ANN training loss 2.176802
&gt;&gt; Epoch 53 finished 	ANN training loss 2.169082
&gt;&gt; Epoch 54 finished 	ANN training loss 2.149548
&gt;&gt; Epoch 55 finished 	ANN training loss 2.154392
&gt;&gt; Epoch 56 finished 	ANN training loss 2.144527
&gt;&gt; Epoch 57 finished 	ANN training loss 2.161858
&gt;&gt; Epoch 58 finished 	ANN training loss 2.186012
&gt;&gt; Epoch 59 finished 	ANN training loss 2.160159
&gt;&gt; Epoch 60 finished 	ANN training loss 2.174149
&gt;&gt; Epoch 61 finished 	ANN training loss 2.182539
&gt;&gt; Epoch 62 finished 	ANN training loss 2.190384
&gt;&gt; Epoch 63 finished 	ANN training loss 2.168181
&gt;&gt; Epoch 64 finished 	ANN training loss 2.164005
&gt;&gt; Epoch 65 finished 	ANN training loss 2.146474
&gt;&gt; Epoch 66 finished 	ANN training loss 2.160167
&gt;&gt; Epoch 67 finished 	ANN training loss 2.154632
&gt;&gt; Epoch 68 finished 	ANN training loss 2.152047
&gt;&gt; Epoch 69 finished 	ANN training loss 2.183812
&gt;&gt; Epoch 70 finished 	ANN training loss 2.173420
&gt;&gt; Epoch 71 finished 	ANN training loss 2.178834
&gt;&gt; Epoch 72 finished 	ANN training loss 2.175931
&gt;&gt; Epoch 73 finished 	ANN training loss 2.168635
&gt;&gt; Epoch 74 finished 	ANN training loss 2.152881
&gt;&gt; Epoch 75 finished 	ANN training loss 2.153654
&gt;&gt; Epoch 76 finished 	ANN training loss 2.174276
&gt;&gt; Epoch 77 finished 	ANN training loss 2.187140
&gt;&gt; Epoch 78 finished 	ANN training loss 2.150635
&gt;&gt; Epoch 79 finished 	ANN training loss 2.151319
&gt;&gt; Epoch 80 finished 	ANN training loss 2.151626
&gt;&gt; Epoch 81 finished 	ANN training loss 2.125837
&gt;&gt; Epoch 82 finished 	ANN training loss 2.165482
&gt;&gt; Epoch 83 finished 	ANN training loss 2.158957
&gt;&gt; Epoch 84 finished 	ANN training loss 2.166259
&gt;&gt; Epoch 85 finished 	ANN training loss 2.153850
&gt;&gt; Epoch 86 finished 	ANN training loss 2.149735
&gt;&gt; Epoch 87 finished 	ANN training loss 2.171729
&gt;&gt; Epoch 88 finished 	ANN training loss 2.180561
&gt;&gt; Epoch 89 finished 	ANN training loss 2.210935
&gt;&gt; Epoch 90 finished 	ANN training loss 2.178512
&gt;&gt; Epoch 91 finished 	ANN training loss 2.150921
&gt;&gt; Epoch 92 finished 	ANN training loss 2.173715
&gt;&gt; Epoch 93 finished 	ANN training loss 2.194286
&gt;&gt; Epoch 94 finished 	ANN training loss 2.173527
&gt;&gt; Epoch 95 finished 	ANN training loss 2.138774
&gt;&gt; Epoch 96 finished 	ANN training loss 2.161061
&gt;&gt; Epoch 97 finished 	ANN training loss 2.164414
&gt;&gt; Epoch 98 finished 	ANN training loss 2.169857
&gt;&gt; Epoch 99 finished 	ANN training loss 2.157480
&gt;&gt; Epoch 100 finished 	ANN training loss 2.153630
&gt;&gt; Epoch 101 finished 	ANN training loss 2.149297
&gt;&gt; Epoch 102 finished 	ANN training loss 2.146027
&gt;&gt; Epoch 103 finished 	ANN training loss 2.134158
&gt;&gt; Epoch 104 finished 	ANN training loss 2.185303
&gt;&gt; Epoch 105 finished 	ANN training loss 2.192255
&gt;&gt; Epoch 106 finished 	ANN training loss 2.167178
&gt;&gt; Epoch 107 finished 	ANN training loss 2.155923
&gt;&gt; Epoch 108 finished 	ANN training loss 2.179603
&gt;&gt; Epoch 109 finished 	ANN training loss 2.165329
&gt;&gt; Epoch 110 finished 	ANN training loss 2.187927
&gt;&gt; Epoch 111 finished 	ANN training loss 2.167096
&gt;&gt; Epoch 112 finished 	ANN training loss 2.179219
&gt;&gt; Epoch 113 finished 	ANN training loss 2.178646
&gt;&gt; Epoch 114 finished 	ANN training loss 2.175715
&gt;&gt; Epoch 115 finished 	ANN training loss 2.194725
&gt;&gt; Epoch 116 finished 	ANN training loss 2.175158
&gt;&gt; Epoch 117 finished 	ANN training loss 2.184798
&gt;&gt; Epoch 118 finished 	ANN training loss 2.188413
&gt;&gt; Epoch 119 finished 	ANN training loss 2.178125
&gt;&gt; Epoch 120 finished 	ANN training loss 2.176945
&gt;&gt; Epoch 121 finished 	ANN training loss 2.183486
&gt;&gt; Epoch 122 finished 	ANN training loss 2.163584
&gt;&gt; Epoch 123 finished 	ANN training loss 2.176296
&gt;&gt; Epoch 124 finished 	ANN training loss 2.159366
&gt;&gt; Epoch 125 finished 	ANN training loss 2.138997
&gt;&gt; Epoch 126 finished 	ANN training loss 2.149578
&gt;&gt; Epoch 127 finished 	ANN training loss 2.182500
&gt;&gt; Epoch 128 finished 	ANN training loss 2.168003
&gt;&gt; Epoch 129 finished 	ANN training loss 2.183071
&gt;&gt; Epoch 130 finished 	ANN training loss 2.190913
&gt;&gt; Epoch 131 finished 	ANN training loss 2.170638
&gt;&gt; Epoch 132 finished 	ANN training loss 2.212713
&gt;&gt; Epoch 133 finished 	ANN training loss 2.186089
&gt;&gt; Epoch 134 finished 	ANN training loss 2.195692
&gt;&gt; Epoch 135 finished 	ANN training loss 2.166410
&gt;&gt; Epoch 136 finished 	ANN training loss 2.148310
&gt;&gt; Epoch 137 finished 	ANN training loss 2.162990
&gt;&gt; Epoch 138 finished 	ANN training loss 2.150542
&gt;&gt; Epoch 139 finished 	ANN training loss 2.176941
&gt;&gt; Epoch 140 finished 	ANN training loss 2.186961
&gt;&gt; Epoch 141 finished 	ANN training loss 2.192263
&gt;&gt; Epoch 142 finished 	ANN training loss 2.182734
&gt;&gt; Epoch 143 finished 	ANN training loss 2.180749
&gt;&gt; Epoch 144 finished 	ANN training loss 2.152255
&gt;&gt; Epoch 145 finished 	ANN training loss 2.113683
&gt;&gt; Epoch 146 finished 	ANN training loss 2.173158
&gt;&gt; Epoch 147 finished 	ANN training loss 2.154315
&gt;&gt; Epoch 148 finished 	ANN training loss 2.165677
&gt;&gt; Epoch 149 finished 	ANN training loss 2.166407
&gt;&gt; Epoch 150 finished 	ANN training loss 2.152326
&gt;&gt; Epoch 151 finished 	ANN training loss 2.150802
&gt;&gt; Epoch 152 finished 	ANN training loss 2.173071
&gt;&gt; Epoch 153 finished 	ANN training loss 2.181078
&gt;&gt; Epoch 154 finished 	ANN training loss 2.169631
&gt;&gt; Epoch 155 finished 	ANN training loss 2.163605
&gt;&gt; Epoch 156 finished 	ANN training loss 2.163581
&gt;&gt; Epoch 157 finished 	ANN training loss 2.185646
&gt;&gt; Epoch 158 finished 	ANN training loss 2.182969
&gt;&gt; Epoch 159 finished 	ANN training loss 2.151333
&gt;&gt; Epoch 160 finished 	ANN training loss 2.183060
&gt;&gt; Epoch 161 finished 	ANN training loss 2.153569
&gt;&gt; Epoch 162 finished 	ANN training loss 2.129071
&gt;&gt; Epoch 163 finished 	ANN training loss 2.138309
&gt;&gt; Epoch 164 finished 	ANN training loss 2.142051
&gt;&gt; Epoch 165 finished 	ANN training loss 2.161184
&gt;&gt; Epoch 166 finished 	ANN training loss 2.123452
&gt;&gt; Epoch 167 finished 	ANN training loss 2.144324
&gt;&gt; Epoch 168 finished 	ANN training loss 2.156217
&gt;&gt; Epoch 169 finished 	ANN training loss 2.151315
&gt;&gt; Epoch 170 finished 	ANN training loss 2.180839
&gt;&gt; Epoch 171 finished 	ANN training loss 2.178761
&gt;&gt; Epoch 172 finished 	ANN training loss 2.160014
&gt;&gt; Epoch 173 finished 	ANN training loss 2.143563
&gt;&gt; Epoch 174 finished 	ANN training loss 2.170612
&gt;&gt; Epoch 175 finished 	ANN training loss 2.185767
&gt;&gt; Epoch 176 finished 	ANN training loss 2.161528
&gt;&gt; Epoch 177 finished 	ANN training loss 2.180478
&gt;&gt; Epoch 178 finished 	ANN training loss 2.163069
&gt;&gt; Epoch 179 finished 	ANN training loss 2.175171
&gt;&gt; Epoch 180 finished 	ANN training loss 2.147868
&gt;&gt; Epoch 181 finished 	ANN training loss 2.165702
&gt;&gt; Epoch 182 finished 	ANN training loss 2.159899
&gt;&gt; Epoch 183 finished 	ANN training loss 2.166686
&gt;&gt; Epoch 184 finished 	ANN training loss 2.157096
&gt;&gt; Epoch 185 finished 	ANN training loss 2.145808
&gt;&gt; Epoch 186 finished 	ANN training loss 2.167522
&gt;&gt; Epoch 187 finished 	ANN training loss 2.187545
&gt;&gt; Epoch 188 finished 	ANN training loss 2.160564
&gt;&gt; Epoch 189 finished 	ANN training loss 2.168859
&gt;&gt; Epoch 190 finished 	ANN training loss 2.152675
&gt;&gt; Epoch 191 finished 	ANN training loss 2.160844
&gt;&gt; Epoch 192 finished 	ANN training loss 2.137596
&gt;&gt; Epoch 193 finished 	ANN training loss 2.140012
&gt;&gt; Epoch 194 finished 	ANN training loss 2.136550
&gt;&gt; Epoch 195 finished 	ANN training loss 2.130581
&gt;&gt; Epoch 196 finished 	ANN training loss 2.121011
&gt;&gt; Epoch 197 finished 	ANN training loss 2.138720
&gt;&gt; Epoch 198 finished 	ANN training loss 2.155964
&gt;&gt; Epoch 199 finished 	ANN training loss 2.149277
[END] Fine tuning step
Done.
Accuracy: 0.155000
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[25]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ACCURACY: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">acc9</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>ACCURACY: 0.155
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[26]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># ReLu, dropout = 1</span>
<span class="n">acc10</span> <span class="o">=</span> <span class="n">deep_belief_net</span><span class="p">(</span><span class="n">hidden_layers_structure</span><span class="o">=</span><span class="p">[</span><span class="mi">300</span><span class="p">],</span> 
                       <span class="n">learning_rate_rbm</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span>
                       <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                       <span class="n">n_epochs_rbm</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span>
                       <span class="n">n_iter_backprop</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
                       <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
                       <span class="n">activation_function</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span>
                       <span class="n">dropout_p</span><span class="o">=</span><span class="mf">1.</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>[START] Pre-training step:
&gt;&gt; Epoch 1 finished 	RBM Reconstruction error 79.109840
&gt;&gt; Epoch 2 finished 	RBM Reconstruction error 63.176834
&gt;&gt; Epoch 3 finished 	RBM Reconstruction error 53.967606
&gt;&gt; Epoch 4 finished 	RBM Reconstruction error 32.274902
&gt;&gt; Epoch 5 finished 	RBM Reconstruction error 32.726784
&gt;&gt; Epoch 6 finished 	RBM Reconstruction error 37.979504
&gt;&gt; Epoch 7 finished 	RBM Reconstruction error 31.923939
&gt;&gt; Epoch 8 finished 	RBM Reconstruction error 36.889969
&gt;&gt; Epoch 9 finished 	RBM Reconstruction error 26.216021
&gt;&gt; Epoch 10 finished 	RBM Reconstruction error 25.015684
&gt;&gt; Epoch 11 finished 	RBM Reconstruction error 28.614719
&gt;&gt; Epoch 12 finished 	RBM Reconstruction error 24.293184
&gt;&gt; Epoch 13 finished 	RBM Reconstruction error 26.673061
&gt;&gt; Epoch 14 finished 	RBM Reconstruction error 25.868410
&gt;&gt; Epoch 15 finished 	RBM Reconstruction error 37.665257
&gt;&gt; Epoch 16 finished 	RBM Reconstruction error 37.511024
&gt;&gt; Epoch 17 finished 	RBM Reconstruction error 43.258770
&gt;&gt; Epoch 18 finished 	RBM Reconstruction error 30.991518
&gt;&gt; Epoch 19 finished 	RBM Reconstruction error 42.074825
&gt;&gt; Epoch 20 finished 	RBM Reconstruction error 21.609245
&gt;&gt; Epoch 21 finished 	RBM Reconstruction error 28.272455
&gt;&gt; Epoch 22 finished 	RBM Reconstruction error 28.798027
&gt;&gt; Epoch 23 finished 	RBM Reconstruction error 27.145079
&gt;&gt; Epoch 24 finished 	RBM Reconstruction error 27.364443
&gt;&gt; Epoch 25 finished 	RBM Reconstruction error 24.284081
&gt;&gt; Epoch 26 finished 	RBM Reconstruction error 50.532295
&gt;&gt; Epoch 27 finished 	RBM Reconstruction error 22.311724
&gt;&gt; Epoch 28 finished 	RBM Reconstruction error 25.176216
&gt;&gt; Epoch 29 finished 	RBM Reconstruction error 26.835533
&gt;&gt; Epoch 30 finished 	RBM Reconstruction error 23.717609
&gt;&gt; Epoch 31 finished 	RBM Reconstruction error 28.740952
&gt;&gt; Epoch 32 finished 	RBM Reconstruction error 32.097599
&gt;&gt; Epoch 33 finished 	RBM Reconstruction error 38.287491
&gt;&gt; Epoch 34 finished 	RBM Reconstruction error 33.548656
&gt;&gt; Epoch 35 finished 	RBM Reconstruction error 38.731396
&gt;&gt; Epoch 36 finished 	RBM Reconstruction error 25.642601
&gt;&gt; Epoch 37 finished 	RBM Reconstruction error 40.658470
&gt;&gt; Epoch 38 finished 	RBM Reconstruction error 22.271059
&gt;&gt; Epoch 39 finished 	RBM Reconstruction error 39.835522
&gt;&gt; Epoch 40 finished 	RBM Reconstruction error 36.841530
[END] Pre-training step
[START] Fine tuning step:
&gt;&gt; Epoch 0 finished 	ANN training loss nan
&gt;&gt; Epoch 1 finished 	ANN training loss nan
&gt;&gt; Epoch 2 finished 	ANN training loss nan
&gt;&gt; Epoch 3 finished 	ANN training loss nan
&gt;&gt; Epoch 4 finished 	ANN training loss nan
&gt;&gt; Epoch 5 finished 	ANN training loss nan
&gt;&gt; Epoch 6 finished 	ANN training loss nan
&gt;&gt; Epoch 7 finished 	ANN training loss nan
&gt;&gt; Epoch 8 finished 	ANN training loss nan
&gt;&gt; Epoch 9 finished 	ANN training loss nan
&gt;&gt; Epoch 10 finished 	ANN training loss nan
&gt;&gt; Epoch 11 finished 	ANN training loss nan
&gt;&gt; Epoch 12 finished 	ANN training loss nan
&gt;&gt; Epoch 13 finished 	ANN training loss nan
&gt;&gt; Epoch 14 finished 	ANN training loss nan
&gt;&gt; Epoch 15 finished 	ANN training loss nan
&gt;&gt; Epoch 16 finished 	ANN training loss nan
&gt;&gt; Epoch 17 finished 	ANN training loss nan
&gt;&gt; Epoch 18 finished 	ANN training loss nan
&gt;&gt; Epoch 19 finished 	ANN training loss nan
&gt;&gt; Epoch 20 finished 	ANN training loss nan
&gt;&gt; Epoch 21 finished 	ANN training loss nan
&gt;&gt; Epoch 22 finished 	ANN training loss nan
&gt;&gt; Epoch 23 finished 	ANN training loss nan
&gt;&gt; Epoch 24 finished 	ANN training loss nan
&gt;&gt; Epoch 25 finished 	ANN training loss nan
&gt;&gt; Epoch 26 finished 	ANN training loss nan
&gt;&gt; Epoch 27 finished 	ANN training loss nan
&gt;&gt; Epoch 28 finished 	ANN training loss nan
&gt;&gt; Epoch 29 finished 	ANN training loss nan
&gt;&gt; Epoch 30 finished 	ANN training loss nan
&gt;&gt; Epoch 31 finished 	ANN training loss nan
&gt;&gt; Epoch 32 finished 	ANN training loss nan
&gt;&gt; Epoch 33 finished 	ANN training loss nan
&gt;&gt; Epoch 34 finished 	ANN training loss nan
&gt;&gt; Epoch 35 finished 	ANN training loss nan
&gt;&gt; Epoch 36 finished 	ANN training loss nan
&gt;&gt; Epoch 37 finished 	ANN training loss nan
&gt;&gt; Epoch 38 finished 	ANN training loss nan
&gt;&gt; Epoch 39 finished 	ANN training loss nan
&gt;&gt; Epoch 40 finished 	ANN training loss nan
&gt;&gt; Epoch 41 finished 	ANN training loss nan
&gt;&gt; Epoch 42 finished 	ANN training loss nan
&gt;&gt; Epoch 43 finished 	ANN training loss nan
&gt;&gt; Epoch 44 finished 	ANN training loss nan
&gt;&gt; Epoch 45 finished 	ANN training loss nan
&gt;&gt; Epoch 46 finished 	ANN training loss nan
&gt;&gt; Epoch 47 finished 	ANN training loss nan
&gt;&gt; Epoch 48 finished 	ANN training loss nan
&gt;&gt; Epoch 49 finished 	ANN training loss nan
&gt;&gt; Epoch 50 finished 	ANN training loss nan
&gt;&gt; Epoch 51 finished 	ANN training loss nan
&gt;&gt; Epoch 52 finished 	ANN training loss nan
&gt;&gt; Epoch 53 finished 	ANN training loss nan
&gt;&gt; Epoch 54 finished 	ANN training loss nan
&gt;&gt; Epoch 55 finished 	ANN training loss nan
&gt;&gt; Epoch 56 finished 	ANN training loss nan
&gt;&gt; Epoch 57 finished 	ANN training loss nan
&gt;&gt; Epoch 58 finished 	ANN training loss nan
&gt;&gt; Epoch 59 finished 	ANN training loss nan
&gt;&gt; Epoch 60 finished 	ANN training loss nan
&gt;&gt; Epoch 61 finished 	ANN training loss nan
&gt;&gt; Epoch 62 finished 	ANN training loss nan
&gt;&gt; Epoch 63 finished 	ANN training loss nan
&gt;&gt; Epoch 64 finished 	ANN training loss nan
&gt;&gt; Epoch 65 finished 	ANN training loss nan
&gt;&gt; Epoch 66 finished 	ANN training loss nan
&gt;&gt; Epoch 67 finished 	ANN training loss nan
&gt;&gt; Epoch 68 finished 	ANN training loss nan
&gt;&gt; Epoch 69 finished 	ANN training loss nan
&gt;&gt; Epoch 70 finished 	ANN training loss nan
&gt;&gt; Epoch 71 finished 	ANN training loss nan
&gt;&gt; Epoch 72 finished 	ANN training loss nan
&gt;&gt; Epoch 73 finished 	ANN training loss nan
&gt;&gt; Epoch 74 finished 	ANN training loss nan
&gt;&gt; Epoch 75 finished 	ANN training loss nan
&gt;&gt; Epoch 76 finished 	ANN training loss nan
&gt;&gt; Epoch 77 finished 	ANN training loss nan
&gt;&gt; Epoch 78 finished 	ANN training loss nan
&gt;&gt; Epoch 79 finished 	ANN training loss nan
&gt;&gt; Epoch 80 finished 	ANN training loss nan
&gt;&gt; Epoch 81 finished 	ANN training loss nan
&gt;&gt; Epoch 82 finished 	ANN training loss nan
&gt;&gt; Epoch 83 finished 	ANN training loss nan
&gt;&gt; Epoch 84 finished 	ANN training loss nan
&gt;&gt; Epoch 85 finished 	ANN training loss nan
&gt;&gt; Epoch 86 finished 	ANN training loss nan
&gt;&gt; Epoch 87 finished 	ANN training loss nan
&gt;&gt; Epoch 88 finished 	ANN training loss nan
&gt;&gt; Epoch 89 finished 	ANN training loss nan
&gt;&gt; Epoch 90 finished 	ANN training loss nan
&gt;&gt; Epoch 91 finished 	ANN training loss nan
&gt;&gt; Epoch 92 finished 	ANN training loss nan
&gt;&gt; Epoch 93 finished 	ANN training loss nan
&gt;&gt; Epoch 94 finished 	ANN training loss nan
&gt;&gt; Epoch 95 finished 	ANN training loss nan
&gt;&gt; Epoch 96 finished 	ANN training loss nan
&gt;&gt; Epoch 97 finished 	ANN training loss nan
&gt;&gt; Epoch 98 finished 	ANN training loss nan
&gt;&gt; Epoch 99 finished 	ANN training loss nan
&gt;&gt; Epoch 100 finished 	ANN training loss nan
&gt;&gt; Epoch 101 finished 	ANN training loss nan
&gt;&gt; Epoch 102 finished 	ANN training loss nan
&gt;&gt; Epoch 103 finished 	ANN training loss nan
&gt;&gt; Epoch 104 finished 	ANN training loss nan
&gt;&gt; Epoch 105 finished 	ANN training loss nan
&gt;&gt; Epoch 106 finished 	ANN training loss nan
&gt;&gt; Epoch 107 finished 	ANN training loss nan
&gt;&gt; Epoch 108 finished 	ANN training loss nan
&gt;&gt; Epoch 109 finished 	ANN training loss nan
&gt;&gt; Epoch 110 finished 	ANN training loss nan
&gt;&gt; Epoch 111 finished 	ANN training loss nan
&gt;&gt; Epoch 112 finished 	ANN training loss nan
&gt;&gt; Epoch 113 finished 	ANN training loss nan
&gt;&gt; Epoch 114 finished 	ANN training loss nan
&gt;&gt; Epoch 115 finished 	ANN training loss nan
&gt;&gt; Epoch 116 finished 	ANN training loss nan
&gt;&gt; Epoch 117 finished 	ANN training loss nan
&gt;&gt; Epoch 118 finished 	ANN training loss nan
&gt;&gt; Epoch 119 finished 	ANN training loss nan
&gt;&gt; Epoch 120 finished 	ANN training loss nan
&gt;&gt; Epoch 121 finished 	ANN training loss nan
&gt;&gt; Epoch 122 finished 	ANN training loss nan
&gt;&gt; Epoch 123 finished 	ANN training loss nan
&gt;&gt; Epoch 124 finished 	ANN training loss nan
&gt;&gt; Epoch 125 finished 	ANN training loss nan
&gt;&gt; Epoch 126 finished 	ANN training loss nan
&gt;&gt; Epoch 127 finished 	ANN training loss nan
&gt;&gt; Epoch 128 finished 	ANN training loss nan
&gt;&gt; Epoch 129 finished 	ANN training loss nan
&gt;&gt; Epoch 130 finished 	ANN training loss nan
&gt;&gt; Epoch 131 finished 	ANN training loss nan
&gt;&gt; Epoch 132 finished 	ANN training loss nan
&gt;&gt; Epoch 133 finished 	ANN training loss nan
&gt;&gt; Epoch 134 finished 	ANN training loss nan
&gt;&gt; Epoch 135 finished 	ANN training loss nan
&gt;&gt; Epoch 136 finished 	ANN training loss nan
&gt;&gt; Epoch 137 finished 	ANN training loss nan
&gt;&gt; Epoch 138 finished 	ANN training loss nan
&gt;&gt; Epoch 139 finished 	ANN training loss nan
&gt;&gt; Epoch 140 finished 	ANN training loss nan
&gt;&gt; Epoch 141 finished 	ANN training loss nan
&gt;&gt; Epoch 142 finished 	ANN training loss nan
&gt;&gt; Epoch 143 finished 	ANN training loss nan
&gt;&gt; Epoch 144 finished 	ANN training loss nan
&gt;&gt; Epoch 145 finished 	ANN training loss nan
&gt;&gt; Epoch 146 finished 	ANN training loss nan
&gt;&gt; Epoch 147 finished 	ANN training loss nan
&gt;&gt; Epoch 148 finished 	ANN training loss nan
&gt;&gt; Epoch 149 finished 	ANN training loss nan
&gt;&gt; Epoch 150 finished 	ANN training loss nan
&gt;&gt; Epoch 151 finished 	ANN training loss nan
&gt;&gt; Epoch 152 finished 	ANN training loss nan
&gt;&gt; Epoch 153 finished 	ANN training loss nan
&gt;&gt; Epoch 154 finished 	ANN training loss nan
&gt;&gt; Epoch 155 finished 	ANN training loss nan
&gt;&gt; Epoch 156 finished 	ANN training loss nan
&gt;&gt; Epoch 157 finished 	ANN training loss nan
&gt;&gt; Epoch 158 finished 	ANN training loss nan
&gt;&gt; Epoch 159 finished 	ANN training loss nan
&gt;&gt; Epoch 160 finished 	ANN training loss nan
&gt;&gt; Epoch 161 finished 	ANN training loss nan
&gt;&gt; Epoch 162 finished 	ANN training loss nan
&gt;&gt; Epoch 163 finished 	ANN training loss nan
&gt;&gt; Epoch 164 finished 	ANN training loss nan
&gt;&gt; Epoch 165 finished 	ANN training loss nan
&gt;&gt; Epoch 166 finished 	ANN training loss nan
&gt;&gt; Epoch 167 finished 	ANN training loss nan
&gt;&gt; Epoch 168 finished 	ANN training loss nan
&gt;&gt; Epoch 169 finished 	ANN training loss nan
&gt;&gt; Epoch 170 finished 	ANN training loss nan
&gt;&gt; Epoch 171 finished 	ANN training loss nan
&gt;&gt; Epoch 172 finished 	ANN training loss nan
&gt;&gt; Epoch 173 finished 	ANN training loss nan
&gt;&gt; Epoch 174 finished 	ANN training loss nan
&gt;&gt; Epoch 175 finished 	ANN training loss nan
&gt;&gt; Epoch 176 finished 	ANN training loss nan
&gt;&gt; Epoch 177 finished 	ANN training loss nan
&gt;&gt; Epoch 178 finished 	ANN training loss nan
&gt;&gt; Epoch 179 finished 	ANN training loss nan
&gt;&gt; Epoch 180 finished 	ANN training loss nan
&gt;&gt; Epoch 181 finished 	ANN training loss nan
&gt;&gt; Epoch 182 finished 	ANN training loss nan
&gt;&gt; Epoch 183 finished 	ANN training loss nan
&gt;&gt; Epoch 184 finished 	ANN training loss nan
&gt;&gt; Epoch 185 finished 	ANN training loss nan
&gt;&gt; Epoch 186 finished 	ANN training loss nan
&gt;&gt; Epoch 187 finished 	ANN training loss nan
&gt;&gt; Epoch 188 finished 	ANN training loss nan
&gt;&gt; Epoch 189 finished 	ANN training loss nan
&gt;&gt; Epoch 190 finished 	ANN training loss nan
&gt;&gt; Epoch 191 finished 	ANN training loss nan
&gt;&gt; Epoch 192 finished 	ANN training loss nan
&gt;&gt; Epoch 193 finished 	ANN training loss nan
&gt;&gt; Epoch 194 finished 	ANN training loss nan
&gt;&gt; Epoch 195 finished 	ANN training loss nan
&gt;&gt; Epoch 196 finished 	ANN training loss nan
&gt;&gt; Epoch 197 finished 	ANN training loss nan
&gt;&gt; Epoch 198 finished 	ANN training loss nan
&gt;&gt; Epoch 199 finished 	ANN training loss nan
[END] Fine tuning step
Done.
Accuracy: 0.100000
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[27]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ACCURACY: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">acc10</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>ACCURACY: 0.1
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[65]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Create bar graph</span>
<span class="n">plt</span><span class="o">.</span><span class="n">clf</span><span class="p">()</span>
<span class="n">objects</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;0&#39;</span><span class="p">,</span> <span class="s1">&#39;0.1&#39;</span><span class="p">,</span> <span class="s1">&#39;0.2&#39;</span><span class="p">,</span> <span class="s1">&#39;0.3&#39;</span><span class="p">,</span> <span class="s1">&#39;0.4&#39;</span><span class="p">,</span> <span class="s1">&#39;0.5&#39;</span><span class="p">,</span> <span class="s1">&#39;0.6&#39;</span><span class="p">,</span> <span class="s1">&#39;0.7&#39;</span><span class="p">,</span> <span class="s1">&#39;0.8&#39;</span><span class="p">,</span> <span class="s1">&#39;0.9&#39;</span><span class="p">,</span> <span class="s1">&#39;1&#39;</span><span class="p">)</span>
<span class="n">y_pos</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">objects</span><span class="p">))</span>
<span class="n">performance</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.88</span><span class="p">,</span> <span class="mf">0.89</span><span class="p">,</span> <span class="mf">0.92</span><span class="p">,</span> <span class="mf">0.90</span><span class="p">,</span> <span class="mf">0.89</span><span class="p">,</span> <span class="mf">0.89</span><span class="p">,</span> <span class="mf">0.87</span><span class="p">,</span> <span class="mf">0.90</span><span class="p">,</span> <span class="mf">0.80</span><span class="p">,</span> <span class="mf">0.155</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">y_pos</span><span class="p">,</span> <span class="n">performance</span><span class="p">,</span> <span class="n">align</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">y_pos</span><span class="p">,</span> <span class="n">objects</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;dropout_p&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Accuracy&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Benchmark 2, relu Settings&#39;</span><span class="p">)</span>
 
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">clf</span><span class="p">()</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>




<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAYUAAAEeCAYAAABlggnIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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"
>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="SVM-Tests:">SVM Tests:<a class="anchor-link" href="#SVM-Tests:">&#182;</a></h2>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[30]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Import SVM modules</span>
<span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="k">import</span> <span class="n">LinearSVC</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="k">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="k">import</span> <span class="n">AdaBoostClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="k">import</span> <span class="n">RandomForestClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="k">import</span> <span class="n">SVC</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[39]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Linear SVC Performance</span>
<span class="k">def</span> <span class="nf">linSVC</span><span class="p">(</span><span class="n">C</span><span class="p">):</span>
    <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">Y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">clf</span> <span class="o">=</span> <span class="n">LinearSVC</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="n">C</span><span class="p">)</span>
    <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">)</span>
    <span class="n">Y_pred</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="n">correct</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">Y_test</span><span class="p">)):</span>
        <span class="k">if</span> <span class="n">Y_pred</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">==</span> <span class="n">Y_test</span><span class="p">[</span><span class="n">i</span><span class="p">]:</span>
            <span class="n">correct</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="nb">float</span><span class="p">(</span><span class="n">correct</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">Y_test</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[40]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">linSVC_acc1</span> <span class="o">=</span> <span class="n">linSVC</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ACCURACY: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">linSVC_acc1</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>ACCURACY: 0.86
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[41]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">linSVC_acc2</span> <span class="o">=</span> <span class="n">linSVC</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mf">1.</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ACCURACY: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">linSVC_acc2</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>ACCURACY: 0.845
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[42]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">linSVC_acc3</span> <span class="o">=</span> <span class="n">linSVC</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ACCURACY: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">linSVC_acc3</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>ACCURACY: 0.84
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[43]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">linSVC_acc4</span> <span class="o">=</span> <span class="n">linSVC</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ACCURACY: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">linSVC_acc4</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>ACCURACY: 0.84
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[10]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Decision Tree Performance</span>
<span class="k">def</span> <span class="nf">decision_tree</span><span class="p">():</span>
    <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">Y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">clf</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">()</span>
    <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">)</span>
    <span class="n">Y_pred</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="n">correct</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">Y_test</span><span class="p">)):</span>
        <span class="k">if</span> <span class="n">Y_pred</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">==</span> <span class="n">Y_test</span><span class="p">[</span><span class="n">i</span><span class="p">]:</span>
            <span class="n">correct</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="nb">float</span><span class="p">(</span><span class="n">correct</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">Y_test</span><span class="p">))</span>

<span class="n">decision_tree_acc</span> <span class="o">=</span> <span class="n">decision_tree</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ACCURACY: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">decision_tree_acc</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>ACCURACY: 0.665
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[11]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Boosting Performance</span>
<span class="k">def</span> <span class="nf">boosting</span><span class="p">():</span>
    <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">Y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">clf</span> <span class="o">=</span> <span class="n">AdaBoostClassifier</span><span class="p">()</span>
    <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">)</span>
    <span class="n">Y_pred</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="n">correct</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">Y_test</span><span class="p">)):</span>
        <span class="k">if</span> <span class="n">Y_pred</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">==</span> <span class="n">Y_test</span><span class="p">[</span><span class="n">i</span><span class="p">]:</span>
            <span class="n">correct</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="nb">float</span><span class="p">(</span><span class="n">correct</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">Y_test</span><span class="p">))</span>

<span class="n">boosting_acc</span> <span class="o">=</span> <span class="n">boosting</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ACCURACY: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">boosting_acc</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>ACCURACY: 0.395
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[12]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Random Forest Performance</span>
<span class="k">def</span> <span class="nf">random_forest</span><span class="p">():</span>
    <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">Y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">clf</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">()</span>
    <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">)</span>
    <span class="n">Y_pred</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="n">correct</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">Y_test</span><span class="p">)):</span>
        <span class="k">if</span> <span class="n">Y_pred</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">==</span> <span class="n">Y_test</span><span class="p">[</span><span class="n">i</span><span class="p">]:</span>
            <span class="n">correct</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="nb">float</span><span class="p">(</span><span class="n">correct</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">Y_test</span><span class="p">))</span>

<span class="n">random_forest_acc</span> <span class="o">=</span> <span class="n">random_forest</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ACCURACY: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">random_forest_acc</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>ACCURACY: 0.795
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[31]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># SVC, 3rd degree rbf</span>
<span class="k">def</span> <span class="nf">svc_classifier</span><span class="p">(</span><span class="n">C</span><span class="p">):</span>
    <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">Y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">clf</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="n">C</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">)</span>
    <span class="n">Y_pred</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="n">correct</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">Y_test</span><span class="p">)):</span>
        <span class="k">if</span> <span class="n">Y_pred</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">==</span> <span class="n">Y_test</span><span class="p">[</span><span class="n">i</span><span class="p">]:</span>
            <span class="n">correct</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="nb">float</span><span class="p">(</span><span class="n">correct</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">Y_test</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[32]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">svc1_acc</span> <span class="o">=</span> <span class="n">svc_classifier</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ACCURACY: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">svc1_acc</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>ACCURACY: 0.115
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[33]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">svc2_acc</span> <span class="o">=</span> <span class="n">svc_classifier</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ACCURACY: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">svc2_acc</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>ACCURACY: 0.805
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[34]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">svc3_acc</span> <span class="o">=</span> <span class="n">svc_classifier</span><span class="p">(</span><span class="mf">10.</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ACCURACY: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">svc3_acc</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>ACCURACY: 0.885
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[38]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">svc4_acc</span> <span class="o">=</span> <span class="n">svc_classifier</span><span class="p">(</span><span class="mf">100.</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ACCURACY: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">svc4_acc</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>ACCURACY: 0.895
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[67]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Create bar graph</span>
<span class="n">plt</span><span class="o">.</span><span class="n">clf</span><span class="p">()</span>
<span class="n">objects</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;ReLu (0.2)&#39;</span><span class="p">,</span> <span class="s1">&#39;Sigmoid (0.1)&#39;</span><span class="p">,</span> 
           <span class="s1">&#39;LinearSVC (0.1)&#39;</span><span class="p">,</span> <span class="s1">&#39;LinearSVC (1)&#39;</span><span class="p">,</span> <span class="s1">&#39;LinearSVC (10)&#39;</span><span class="p">,</span> <span class="s1">&#39;LinearSVC (100)&#39;</span><span class="p">,</span> 
           <span class="s1">&#39;DecisionTree&#39;</span><span class="p">,</span> <span class="s1">&#39;AdaBoost&#39;</span><span class="p">,</span> <span class="s1">&#39;RandomForest&#39;</span><span class="p">,</span> 
           <span class="s1">&#39;SVC (0.1)&#39;</span><span class="p">,</span> <span class="s1">&#39;SVC (1)&#39;</span><span class="p">,</span> <span class="s1">&#39;SVC (10)&#39;</span><span class="p">,</span> <span class="s1">&#39;SVC (100)&#39;</span><span class="p">)</span>

<span class="n">y_pos</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">objects</span><span class="p">))</span>
<span class="n">performance</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.92</span><span class="p">,</span> <span class="mf">0.92</span><span class="p">,</span> 
               <span class="mf">0.86</span><span class="p">,</span> <span class="mf">0.845</span><span class="p">,</span> <span class="mf">0.84</span><span class="p">,</span> <span class="mf">0.84</span><span class="p">,</span>
               <span class="mf">0.665</span><span class="p">,</span> <span class="mf">0.395</span><span class="p">,</span> <span class="mf">0.795</span><span class="p">,</span>
               <span class="mf">0.115</span><span class="p">,</span> <span class="mf">0.805</span><span class="p">,</span> <span class="mf">0.885</span><span class="p">,</span> <span class="mf">0.895</span><span class="p">]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">y_pos</span><span class="p">,</span> <span class="n">performance</span><span class="p">,</span> <span class="n">align</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">y_pos</span><span class="p">,</span> <span class="n">objects</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">90</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Model&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Accuracy, Performance&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Model Comparisons&#39;</span><span class="p">)</span>
 
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">clf</span><span class="p">()</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>




<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAYUAAAFdCAYAAADsTnEKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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"
>
</div>

</div>

</div>
</div>

</div>
    </div>
  </div>
</body>

 


</html>
